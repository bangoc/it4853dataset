{
  "authors": [
    "Quách Ngọc Toàn",
    "Võ Văn Tài",
    "Lê Thị Nga"
  ],
  "modified": "19/11/2017 - 15:34:00",
  "collections": [
    {
      "query": "Cách phân nhóm các giải thuật Machine Learning",
      "description": "Phân biệt các nhóm giải thuật Machine Learning và các tiêu chí để chia nhóm các giải thuật đó.",
      "sites": [
        {
          "title": "Machine Learning - Classification - phần 1",
          "relevance": "1",
          "url": "http://vnoi.info/wiki/translate/ml/Machine-Learning-Classification-phan-1",
          "content": "Machine Learning - Classification - phần 1 Search Home All Nguồn :  Achoum's blog Có thể bạn đã nghe đến cụm từ  Machine Learning  (hay  Data Mining ,  Big Data ,  Data Science ,...) nhưng bạn chưa bao giờ hiểu ý nghĩa thực sự của chúng là gì. Có thể tất cả những gì bạn biết là \"À! Đó là những thuật ngữ trong Toán học và Tin học\", và ...thế là hết. Có thể bạn phải sử dụng những phần mềm dựa trên Machine Learning trong công việc nhưng với bạn, chúng hoạt động như là ma thuật vậy. Nếu bạn vẫn đang còn tò mò, thì Machine Learning được dùng trong hầu hết mọi lĩnh vực. Ví dụ, Machine Learning được sử dụng bởi các ngân hàng để phát hiện gian lận và dự đoán rủi ro, được dùng bởi nhà quản lí email để phát hiện thư rác. Các bác sĩ dùng Machine Learning để hỗ trợ chẩn đoán, các nhà sinh học ứng dụng Machine Learning trong việc phân tích DNA. Chúng ta cũng đang tiếp xúc với nó hàng ngày khi Google/Bing/Yahoo đều sử dụng Machine Learning để xử lý mỗi khi bạn gõ lệnh tìm kiếm. Mục tiêu của tác giả với chuỗi bài viết này là giúp các bạn có một hiểu biết sơ lược về Machine Learning và một cái nhìn trực quan về cách mà kỹ thuật này hoạt động. Thay vì nêu ra một đống lý thuyết, bài viết này sẽ chủ yếu dựa trên các ví dụ. Để đọc hiểu bài viết này, bạn không cần phải là một nhà khoa học máy tính; thậm chí, bạn không cần phải biết bất cứ điều gì liên quan đến máy tính và toán học. Chuỗi bài viết trong chủ đề này gồm có 3 bài. Bài viết đầu tiên sẽ giới thiệu về một trong những nhiệm vụ chính của Machine Learning:  Phân loại  ( Classification ). Bài viết thứ hai trình bày và minh hoạ hai kỹ thuật quan trọng của Machine Learning được áp dụng trong bài toán phân loại: một trong số đó tuy rất đơn giản nhưng được sử dụng rộng rãi, cái còn lại là một kỹ thuật rất mới. Cuối cùng, bài viết thứ ba sẽ nói về cách các nhà khoa học máy tính đánh giá chất lượng của một kỹ thuật trong Machine Learning. Định nghĩa Thật sự không dễ để có thể đưa ra một định nghĩa chính xác của Machine Learning. Nói một cách nôm na, Machine Learning là các chương trình giúp đưa ra dự đoán hoặc giúp người dùng hiểu về các hiện tượng phức tạp. Machine Learning là một chủ đề rộng lớn với rất nhiều chủ đề con. Thay vì cố gắng đưa ra những định nghĩa chung chung, bài viết sẽ chỉ tập trung vào một chủ đề duy nhất: Phân loại  (Classification) . Bài toán phân loại chỉ là một trong rất nhiều chủ đề của Machine Learning, nhưng là một trong những chủ đề quan trọng nhất. Khi người ta nói đến Machine Learning, thông thường họ nói đến bài toán phân loại (nếu họ không chỉ rõ chủ đề khác). Phân loại (Classification) Ý tưởng của bài toán phân loại bắt đầu như sau: Chúng ta có một danh sách các  quan sát  ( observation ). Một  quan sát  là một khái niệm chung chung. Với Machine Learning, một quan sát thường là sự mô tả một đối tượng trong một trạng thái nào đó. Để hiểu rõ hơn, hãy xét một ví dụ rất phổ biến trong Machine Learning: hoa diên vĩ (Iris flowers). Giả sử một quan sát là sự mô tả một bông hoa. Với mỗi bông hoa, chúng ta có bốn thông số: độ dày và chiều dài của đài hoa (sepal) và cánh hoa (petal). Trên thực tế, ví dụ về hoa diên vĩ nổi tiếng với cái tên  the Iris dataset  và đây là bộ dữ liệu được sử dụng rất rộng rãi trong giới nghiên cứu Machine Learning để tìm hiểu về các thuật toán phân loại. Bốn thông số của mỗi bông hoa được gọi là các  thuộc tính  ( attributes ). Trong Machine Learning, một quan sát đơn thuần chỉ là một danh sách các thuộc tính. Khi so sánh các quan sát, thực ra chúng ta so sánh các thuộc tính. Ví dụ:  \"Bông hoa này có cánh hoa dài hơn bông hoa kia\" . Các thuộc tính có thể đại diện cho những thứ rất khác nhau. Ví dụ: độ dài của máy bay, thời gian của cơn bão, nhiệt độ của căn phòng, màu sắc của bức tường,... Nói chung, các thuộc tính thường được chia thành hai loại: thuộc tính dạng số (numbers) và thuộc tính dạng thể loại (categories). Sự khác biệt của hai loại thuộc tính này không quá nghiêm ngặt, nhưng thông thường, khi thuộc tính có thể so sánh được ( >  hoặc  < ), ta coi nó là thuộc tính dạng số (ví dụ như độ dài của cánh hoa diên vĩ), ngược lại chúng ta coi nó là thuộc tính dạng thể loại (ví dụ như màu sắc của một bông hoa hay chủng loại của một chú chó).  Ngoài ra, với mỗi quan sát (hay với mỗi bông hoa diên vĩ), chúng ta có một  lớp  ( class ). Một lớp là một thông tin bổ sung về một quan sát. Trong ví dụ về hoa diên vĩ, ta giả sử rằng lớp của một bông hoa diên vĩ chính là loài của nó. Trong ví dụ này, chúng ta chỉ quan tâm ba loài hoa diên vĩ:  Setosa ,  Versicolour  và  Viginica . Tóm lại, chúng ta có một tập hợp các quan sát (các bông hoa diên vĩ). Với mỗi quan sát, chúng ta có một số các thuộc tính và một lớp. Bảng dưới đây minh hoạ một tập hợp các quan sát trong ví dụ về hoa diên vĩ. Một bảng kiểu này thường được gọi là một  dataset  hay bộ dữ liệu. Giả sử rằng chúng ta có 150 quan sát như vậy (tương ứng với 150 bông hoa diên vĩ). Tuy nhiên, với quan sát cuối cùng (hay bông hoa diên vĩ cuối cùng), chúng ta không biết lớp của nó là gì (hay không biết bông hoa thuộc loài gì). Chúng ta có thể tự hỏi: Liệu loài hoa có liên quan đến kích thước của cánh hoa và đài hoa không? Có chăng một loài hoa có cánh hoa dài hơn so với các loài diên vĩ khác? Nói cách khác, liệu chúng ta có thể sử dụng 149 bông hoa diên vĩ mà chúng ta đã biết lớp của nó, và tìm ra lớp của bông hoa cuối cùng không? Đó chính xác là bài toán phân loại. Diễn đạt theo cách khác, bài toán phân loại có thể mô tả như sau: Giả sử chúng ta có một tập hợp các quan sát được đánh nhãn (hay các quan sát mà chúng ta đã biết lớp của nó) và một quan sát chưa được đánh nhãn. Làm thế nào để tìm ra lớp của quan sát đó? Trong thực tế, dĩ nhiên các kỹ thuật phân loại không chỉ được dùng để phân loại hoa. Các ngân hàng sử dụng nó để phân loại khách hàng (ví dụ: liệu khách hàng có hoàn lại nợ tín dụng hay không?). Các bệnh viện ứng dụng nó để phân loại bệnh nhân, các công ty bảo hiểm dùng trong việc phân loại các thân chủ của mình... Phân loại còn có thể được sử dụng ở một mức độ sâu sắc hơn. Ví dụ, các loại camera hiện đại dùng  classification  để nhận diện khuôn mặt, dịch vụ bưu điện sử dụng nó trong việc nhận dạng địa chỉ viết tay. Các thương gia dự đoán thị trường bằng các thuật toán phân loại, trong khi những nhà khí tượng sử dụng nó để dự báo thời tiết. Điểm khác biệt chính giữa những ví dụ trên chính là các thuộc tính. Với khách hàng của các ngân hàng, thuộc tính là thu nhập hàng tháng, điểm tín dụng, số lần sử dụng thẻ gần đây,... Với bệnh nhân thì thuộc tính là giới tính, nhịp tim, huyết áp,... Với các bức ảnh, thuộc tính chính là giá trị màu của từng điểm ảnh. Trong bài viết tiếp theo, tác giả sẽ trình bày cách giải bài toán phân loại. Chính xác hơn, tác giả sẽ trình bày hai thuật toán phân loại được sử dụng rộng rãi mà bất cứ nhà nghiên cứu về Machine Learning nào cũng biết. Hẹn gặp lại các bạn trong  bài viết tiếp theo . Last edited by  Thanh Trung Nguyen , 2016-08-12 07:31:49"
        },
        {
          "title": "Các thuật toán Machine Learning cần biết",
          "relevance": "1",
          "url": "http://tapit.vn/cac-thuat-toan-machine-learning-can-biet/",
          "content": "TAPIT Build the future with us! Menu CỘNG ĐỒNG Các cuộc thi khoa học Hội nghị – hội thảo Hoạt động ngoại khóa ĐÀO TẠO Khóa VĐK MSP430 Khóa Internet of Thing Khóa Xử lý ảnh Tài liệu Học viên Kiểm tra DỰ ÁN HỆ THỐNG NHÚNG VI ĐIỀU KHIỂN Vi điều khiển MSP430 Vi điều khiển lõi ARM Vi điều khiển PIC Arduino MÁY TÍNH NHÚNG INTERNET OF THING PHẦN CỨNG Thiết kế PCB Thiết kế 3D DSP-AI LIÊN HỆ Các thuật toán Machine Learning cần biết Dựa theo phương thức học thì các thuật toán Machine Learning có thể chia ra làm 3 loại: Supervised Learning (Học có giám sát), Unsupervised Learning (Học không giám sát) và Reinforcement learning (Học củng cố). Supervised Learning là thuật toán dự đoán đầu ra của một dữ liệu mới dựa trên các cặp dữ liệu đầu vào cho trước. Unsupervised Learning chỉ có dữ liệu đầu vào và được áp dụng cho các trường hợp không dự đoán được câu trả lời chính xác cho mỗi dữ liệu đầu vào. Trong bài giới thiệu này sẽ không đề cập đến Reinforcement learning mà chỉ chú trọng vào Supervised Learning và Unsupervised Learning. Dưới đây là 10 thuật toán về Supervised Learning và Unsupervised Learning: 1.Supervised Learning \nCây quyết định là một công cụ hỗ trợ quyết định sử dụng biểu đồ dạng cây hoặc mô hình của các quyết định và kết quả có thể xảy ra của chúng, bao gồm kết quả sự kiện ngẫu nhiên, chi phí tài nguyên và tiện ích.  Cây quyết định  (Decision Trees)       Cây quyết định là số câu hỏi Có/Không tối thiểu mà người ta phải hỏi, để đánh giá xác suất đưa ra quyết định đúng. Cây quyết định là một phương pháp mà nó cho phép bạn tiếp cận vấn đề một cách có cấu trúc và có hệ thống để đạt được một kết luận hợp lý. Sự phân lớp Naïve Bayes       Nhóm phân loại Naïve Bayes là một nhóm các sự phân loại xác suất đơn giản dựa trên việc áp dụng định lý Bayes với các giả định độc lập giữa các tính năng.       Hình ảnh đặc trưng trên là phương trình – với P (A | B) là xác suất sau, P (B | A) là khả năng, P(A) là xác suất lớp trước, và P (B) là dự báo xác suất trước. Một vài ví dụ thực tế: Đánh dấu email là spam hoặc không phải spam Phân loại bài viết tin tức về công nghệ, chính trị hoặc thể thao Kiểm tra một đoạn văn thể hiện cảm xúc tích cực, hoặc cảm xúc tiêu cực? Được sử dụng cho phần mềm nhận diện khuôn mặt Phép hồi quy bình phương nhỏ nhất       Phép bình phương nhỏ nhất là một phương pháp để thực hiện hồi quy tuyến tính. Hồi quy tuyến tính như là nhiệm vụ lắp một đường thẳng thông qua một tập hợp các điểm. Có nhiều cách có thể để thực hiện việc này, đầu tiên ta có thể vẽ một đường thẳng, và sau đó cho mỗi điểm dữ liệu, đo khoảng cách dọc giữa điểm và đường thẳng, và tiếp tục thêm các điểm này; đường sau khi được điều chỉnh là kết quả của tổng các khoảng cách càng nhỏ càng tốt.       Phương pháp tuyến tính đề cập đến loại mô hình đang sử dụng để phù hợp với dữ liệu, trong khi phép bình phương nhỏ nhất đề cập đến loại số liệu lỗi đang giảm thiểu. Hồi quy logistic       Hồi quy logistic giúp xem xét mối quan hệ giữa biến phân loại phụ thuộc với một hoặc nhiều biến độc lập bằng cách ước lượng các xác suất sử dụng một hàm logistic, được gọi là sự phân bố logistic tích luỹ (the cumulative logistic distribution). Một vài ví dụ thực tế: Điểm tín dụng Đo lường tỷ lệ thành công của các chiến dịch tiếp thị Dự đoán doanh thu của một sản phẩm nhất định Sẽ có một trận động đất vào một ngày cụ thể? Support Vector Machines (SVM)       SVM là thuật toán phân loại nhị phân. Với một bộ điểm 2 loại ở vị trí N chiều, SVM tạo ra một siêu mặt phẳng (N – 1) chiều để tách các điểm đó thành 2 nhóm. Giả sử phân chia tập các quả bóng xanh và đỏ đặt trên một mặt phẳng mà có thể phân chia tuyến tính. Nếu các quả bóng phân bố không quá đan xen vào nhau, SVM sẽ tìm ra một mặt phẳng tách những quả bóng này thành 2 loại và nằm cách xa nhau càng tốt.       Xét về quy mô, một số vấn đề lớn nhất đã được giải quyết bằng cách sử dụng SVM (với việc thực hiện sửa đổi phù hợp) là quảng cáo hiển thị, phát hiện giới tính dựa trên hình ảnh, phân loại hình ảnh có quy mô lớn… Ensemble Methods       Ensemble Methods là kỹ thuật tạo ra nhiều mô hình và sau đó kết hợp chúng lại để đưa ra kết quả. Một vài phương pháp Ensemble Methods được biết đến rộng rãi : voting, stacking, bagging and boosting. 2.Unsupervised Learning \nCho một tập hợp điểm dữ liệu và gộp những điểm đó vào từng nhóm (cluster). Mỗi nhóm chứa những điểm dữ liệu giống nhau, ngược lại những điểm ở khác nhóm thì khác nhau. Clustering Algorithms       Một số thuật toán Clustering: Centroid-based algorithms Connectivity-based algorithms Density-based algorithms Probabilistic Dimensionality Reduction Neural networks / Deep Learning Principal Component Analysis       Là một thuật toán thống kê sử dụng phép biến đổi trực giao để biến đổi một tập hợp dữ liệu từ một không gian nhiều chiều sang một không gian mới ít chiều hơn (2 hoặc 3 chiều) nhằm tối ưu hóa việc thể hiện sự biến thiên của dữ liệu.       Một số ứng dụng của Principal Component Analysis: nén dữ liệu, đơn giản hóa dữ liệu. Singular Value Decomposition       Trong phương pháp SVD, mọi ma trận, không nhất thiết là vuông, đều có thể được phân tích thành tích của ba ma trận đặc biệt. M: input data matrix  U: left singular vectors  V: right singular vectors       Phương pháp PCA thực chất là một ứng dụng của SVD. Thuật toán nhận diện khuôn mặt đầu tiên sử dụng cả 2 phương pháp PCA và SVD để hiện ra những khuôn mặt như một sự kết hợp tuyến tính của các “eigenfaces”. Independen Component Analysis       ICA có thể được ứng dụng vào xử lý tín hiệu sinh học, phân tách tín hiệu âm thanh truyền thông, chuẩn đoán lỗi, trích đặc trưng, phân tích kế toán, và nhiều ứng dụng khác đang được phát triển.       ICA liên quan đến PCA nhưng nó là một kỹ thuật mạnh hơn nhiều do có khả năng tìm ra các yếu tố bên dưới của các nguồn trong khi những phương pháp trước đều thất bại hoàn toàn. Sưu tầm Trần Thụy Ngọc Hằng Các thuật toán Machine Learning cần biết Comments  comments TAPIT 27/06/2017 21/07/2017 DSP - AI ←  [TAPIT Share Contest][MS 03] Hệ thống phát hiện và cảnh báo sạt lở đất Những hệ điều hành dành cho Internet Of Thing trong tương lai  → October 2017 M T W T F S S « Jul       1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31   Meta Register Log in Entries  RSS Comments  RSS WordPress.org Categories Các cuộc thi khoa học  (10)\n CỘNG ĐỒNG  (11)\n DSP – AI  (3)\n HỆ THỐNG NHÚNG  (14)\n INTERNET OF THING  (4)\n MÁY TÍNH NHÚNG  (5)\n PHẦN CỨNG  (2)\n Thiết kế mạch  (2)\n VI ĐIỀU KHIỂN  (10)\n Vi điều khiển MSP430  (4)\n Views Tài liệu  - 43,609 views Machine learning decision tree (cây quyết định)  - 42,702 views Khóa Xử lý ảnh  - 33,072 views [TAPIT Share Contest][MS 02] Mô hình Aquaponics  - 31,292 views Giới thiệu về RASPBERRY PI 3  - 14,886 views Trang chủ  - 10,823 views Copyright © 2017  TAPIT . Powered by  WordPress . Theme: Spacious by  ThemeGrill ."
        },
        {
          "title": "AI, Machine Learning và Deep Learning – Định nghĩa và cách phân biệt chúng",
          "relevance": "0",
          "url": "https://tech.fpt.com.vn/ai-machine-learning-va-deep-learning-dinh-nghia-va-cach-phan-biet-chung/",
          "content": "Tin FPT Chuyên gia FPT viết Artificial Intelligence Internet of Things Cloud Computing Mobility Security Khác Dự án Công nghệ quanh ta Sự kiện Tài liệu Về chúng tôi Giới thiệu Định hướng công nghệ Cán bộ công nghệ Search FPT TechInsight Ban công nghệ tập đoàn FPT tuyển dụng chuyên gia phần… Giám đốc Công nghệ FPT Retail đạt danh hiệu Trạng nguyên… Đoàn FPT kéo quân vào cố đô tham dự KSE 2017 FPT Telecom phối hợp cùng ISC tạo ra IQC – Ứng… Chinh phục thử thách, hẹn hò cùng CyRadar Tất cả Artificial Intelligence Internet of Things Cloud Computing Mobility Security Khác Giới thiệu về Xử lí Ngữ nghĩa trong Ngôn ngữ Tự… Phụ nữ là nhân tố quan trọng giúp Việt Nam cất… Kỹ thuật Attention trong mô hình Sequence-to-Sequence và ứng dụng trong… Nghị định 58 về kinh doanh mật mã dân sự và… Chung tay xây dựng kho dữ liệu lớn nhất Việt Nam… Giải quyết ùn tắc giao thông bằng hệ thống iBus Triển khai giải pháp tối ưu hóa nguồn lực của FPT… Ứng dụng Trí tuệ nhân tạo trong giải pháp Camera thông… Phát triển Search Engine với TensorFlow Bức tranh khái quát về Mạng từ và Mạng từ tiếng… Facebook tung ra hàng loạt tính năng mới cho người dùng Alibaba đầu tư 15 tỷ USD vào R&D với 7 phòng… iPhone 8 – Nên hay không móc hầu bao cho siêu… Airbus sẽ bắt đầu các tuyến taxi bay vào năm 2018 “Bot of the Year” – Xây dựng Chatbot, nhận ngay 50… Tiến sĩ công nghệ chia sẻ thông tin từ Hội nghị… Một số thông tin từ Hội nghị ACL – Chia sẻ… Hội thảo “Tiền điện tử và chuỗi khối blockchain” Công nghệ tương tác giọng nói và vai trò của tương… Tài liệu Xử lý ngôn ngữ tự nhiên thu thập từ… 7 ngày free với khóa học mới về Deep Learning từ… Khám phá những cuốn sách hay về Python Bí kíp giải quyết cấu trúc dữ liệu Tài liệu hướng dẫn sử dụng API Vietnam AI Hackathon Về chúng tôi Giới thiệu Định hướng công nghệ Cán bộ công nghệ Công nghệ quanh ta AI, Machine Learning và Deep Learning – Định nghĩa và cách phân... AI, Machine Learning và Deep Learning – Định nghĩa và cách phân biệt chúng 16/03/2017 561 Với những tiến bộ vượt bậc của AI – từ lĩnh vực xe hơi không người lái, đến việc làm chủ các trò chơi như Poker và Go, cho tới tự động hóa các dịch vụ tương tác với khách hàng – công nghệ tiên tiến này đã sẵn sàng để cách mạng hóa các doanh nghiệp. Trong quá trình này, thuật ngữ AI, machine learning, và deep learning thường được sử dụng một cách ngẫu nhiên và thường gây ra sự nhầm lẫn với nhau, trong khi mỗi loại công nghệ đều có những khác biệt riêng. Dưới đây là một số đặc điểm khác biệt giữa ba công cụ này. 1. AI AI hiểu một cách sơ khai là trí tuệ máy tính tiên tiến, công nghệ này được mô tả như sau: “Mọi khía cạnh của học tập hoặc bất kỳ tính năng nào khác của trí thông minh trên thực tế có thể được mô tả chính xác đến mức có thể làm được một cái máy để mô phỏng nó.” AI có thể mô phỏng bất cứ thứ gì từ một chương trình chơi cờ vua, đến một hệ thống nhận dạng giọng nói như Alexa của Amazon. Công nghệ này có thể được phân loại thành ba nhóm: Kiểu trí tuệ nhân tạo hẹp, trí tuệ nhân tạo tổng hợp (AGI), và trí tuệ nhân tạo siêu thông minh. Trí tuệ nhân tạo hẹp là những AI có kỹ năng trong một nhiệm vụ cụ thể, ví dụ như AlplaGo của Google đã đánh bại nhà vô địch thế giới Lee Sedol ở bộ môn Go. Điều này tạo nên sự khác biệt với trí thông minh tổng hợp nhân tạo (AGI), ở đó AI được mô phỏng ở mức gần giống với con người, và có thể thực hiện một loạt các nhiệm vụ khác nhau. Trí tuệ nhân tạo siêu thông minh sẽ đưa mọi thứ lên một bước xa hơn, đây là “trí tuệ thông minh hơn bộ não người tốt nhất trên thực tế mọi lĩnh vực, bao gồm sáng tạo khoa học, sự khôn ngoan chung và các kỹ năng xã hội”. Nói cách khác, đó là khi máy móc đã vượt ra khỏi tầm kiểm soát của con người. 2. Machine Learning Machine Learning là một lĩnh vực con của AI. Nguyên tắc cốt lõi của Machine Learning là các máy tiếp nhận dữ liệu và tự học. Machine learning là một phương pháp phân tích dữ liệu mà sẽ tự động hóa việc xây dựng mô hình phân tích. Sử dụng các thuật toán lặp để học từ dữ liệu, machine learning cho phép máy tính tìm thấy những thông tin giá trị ẩn sâu mà không được lập trình một cách rõ ràng nơi để tìm.Khía cạnh lặp lại của machine learning là quan trọng bởi vì khi các mô hình này được tiếp xúc với dữ liệu mới thì chúng có thể thích ứng một cách độc lập. Chúng học từ các tính toán trước đó để tạo ra những quyết định cũng như kết quả lặp lại và đáng tin cậy. Nó hiện là công cụ hứa hẹn nhất của AI dành cho doanh nghiệp. Các hệ thống Machine Learning có thể nhanh chóng áp dụng kiến thức và đào tạo từ các bộ dữ liệu lớn để thực hiện các công việc về nhận dạng khuôn mặt, nhận dạng giọng nói, nhận diện đối tượng, dịch và nhiều công việc khác một cách xuất sắc. Không giống mã hóa thủ công một chương trình phần mềm với các hướng dẫn cụ thể để hoàn thành một tác vụ, Machine Learning cho phép một hệ thống tự học để nhận dạng các biểu mẫu và đưa ra dự đoán một cách chính xác. Alpha Go là một ví dụ hoàn hảo về Machine Learning, khi nó tiếp nhận và học hỏi một lượng lớn dữ liệu từ cách bước đi cũng như tính toán của các cao thủ để đánh bại nhà vô địch thế giới Lee Sedol. Hiện tại, các tập đoàn lớn như IBM, Google, Amazon, Microsoft… đều cung cấp các nền tảng Machine Learning để các doanh nghiệp ứng dụng và tích hợp vào các chiến lược kinh doanh. 3. Deep learning Deep learning là một lĩnh vực chuyên sâu của Machine Leaning. Nó sử dụng một số kỹ thuật của Machine Learning để giải quyết các vấn đề thực tế bằng cách khai thác các mạng thần kinh nhân tạo (dựa trên các thiết bị phần cứng và phần mềm được kết nối với nhau theo cách nào đó) và mô phỏng việc đưa ra các quyết định của con người. Deep Learning có chi phí khá đắt đỏ, và đòi hỏi các bộ dữ liệu lớn để tự tập luyện, bởi vì có một số lượng lớn các tham số cần được tìm hiểu theo giải thuật, mà ban đầu có thể tạo ra rất nhiều dữ liệu tích cực giả. Ví dụ, một thuật toán deep learning có thể được hướng dẫn để “học” về việc một con mèo trông như thế nào. Nó sẽ có một bộ dữ liệu khổng lồ của hình ảnh để nó hiểu được các chi tiết rất nhỏ mà phân biệt một con mèo với một con báo hoa, một con báo đen hay một con cáo. Tiếp tục với ví dụ về Alpha Go, Google đã lý giải về việc hệ thống đã sử dụng deep learning theo cách kết hợp tìm kiếm cây Monte-Carlo với mạng thần kinh nhân tạo đã được đào tạo bằng cách học có giám sát các trận của của những chuyên gia và bằng cách tăng cường học tập từ các trận đấu tự chơi. Deep Learning có ứng dụng sâu rộng trong các lĩnh vực của đời sống, ví dụ như tìm kiếm dựa trên văn bản, phát hiện gian lận, phát hiện spam, nhận dạng chữ viết tay, tìm kiếm hình ảnh, nhận dạng giọng nói, phát hiện chế độ xem phố và bản dịch là tất cả các tác vụ có thể được thực hiện thông qua deep learning, thay thế nhiều hệ thống dựa trên các nguyên tắc thủ công. Tuy nhiên, deep learning cũng rất dễ bị thiên lệch, nếu trong bộ dữ liệu không có những tham số cần thiết. Tống Minh Đức – FPT Telecom TAGS AI Deep Learning định nghĩa phân biệt machine learning Facebook Twitter Tin trước Thêm nhiều tiện ích cho người dân tham gia giao thông tại Thành phố Hồ Chí Minh Tin sau Đổi mới sáng tạo: Góc nhìn hợp tác giữa Doanh nghiệp lớn và các công ty khởi nghiệp Please enter your name here You have entered an incorrect email address! Please enter your email address here Tin liên quan Google sử dụng trí tuệ nhân tạo để chẩn đoán ung thư vú 07/03/2017 Liệu Trí thông minh nhân tạo và Internet of things có thể giúp loài người vượt qua cơn khủng hoảng dân số? 21/09/2017 Trí tuệ nhân tạo: hiện tại và tương lai 20/12/2016 Dự đoán sự phát triển Big Data, IoT, và AI trong năm 2017 29/06/2017 Những cuốn sách về Java bạn không nên bỏ qua 16/02/2017 Phạm Quang Việt: “CEO mà có lúc trong túi không đủ... 13/10/2017 Ban công nghệ tập đoàn FPT tuyển dụng chuyên gia phần... 19/10/2017 Giải quyết ùn tắc giao thông bằng hệ thống iBus 18/10/2017 Chuyên trang về công nghệ của tập đoàn FPT, cung cấp các thông tin về những công nghệ mới, công nghệ đặc thù, các xu hướng công nghệ mới nhất cho cộng đồng công nghệ. Ban công nghệ FPT Địa chỉ: Tầng 4, Tòa Nhà FPT,  Duy Tân, Cầu Giấy, Hà Nội  Email:  Techinsight@fpt.com.vn Điện thoại: (84-4) 7300 7300 - 48825 Edit with Live CSS Save Write CSS OR LESS and hit save. CTRL + SPACE for auto-complete."
        },
        {
          "title": "Bài 2: Phân nhóm các thuật toán Machine Learning",
          "relevance": "1",
          "url": "https://machinelearningcoban.com/2016/12/27/categories/",
          "content": "Latest FundaML.com 33. Đánh giá hệ thống phân lớp (1/2) 32. Naive Bayes Classifier Viết và nhận xét các bài báo khoa học 31. Maximum Likelihood và Maximum A Posteriori Con đường học Toán của tôi 30. Ôn tập Xác Suất Q2. Transfer Learning 29. Linear Discriminant Analysis Q1. Quick Notes 1 28. Principal Component Analysis (2/2) 27. Principal Component Analysis (1/2) 26. Singular Value Decomposition 25. Matrix Factorization Collaborative Filtering 24. Neighborhood-Based Collaborative Filtering 23. Content-based Recommendation Systems 22. Multi-class SVM 21. Kernel SVM 20. Soft Margin SVM 19. Support Vector Machine 18. Duality 17. Convex Optimization Problems 16. Convex sets và convex functions 15. Overfitting 14. Multi-layer Perceptron và Backpropagation 13. Softmax Regression 12. Binary Classifiers 11. Feature Engineering 10. Logistic Regression 9. Perceptron Learning Algorithm 8. Gradient Descent (2/2) 7. Gradient Descent (1/2) 6. K-nearest neighbors 5. K-means Clustering - Applications 4. K-means Clustering 3. Linear Regression 2. Phân nhóm các thuật toán Machine Learning 1. Giới thiệu về Machine Learning Machine Learning cơ bản  About Index Tags Categories Archive Math Copyrights ebook Search Có hai cách phổ biến phân nhóm các thuật toán Machine learning. Một là dựa trên phương thức học (learning style), hai là dựa trên chức năng (function) (của mỗi thuật toán). Trong trang này: 1. Phân nhóm dựa trên phương thức học Supervised Learning (Học có giám sát) Classification (Phân loại) Regression (Hồi quy) Unsupervised Learning (Học không giám sát) Clustering (phân nhóm) Association Semi-Supervised Learning (Học bán giám sát) Reinforcement Learning (Học Củng Cố) 2. Phân nhóm dựa trên chức năng Regression Algorithms Classification Algorithms Instance-based Algorithms Regularization Algorithms Bayesian Algorithms Clustering Algorithms Artificial Neural Network Algorithms Dimensionality Reduction Algorithms Ensemble Algorithms 3. Tài liệu tham khảo 1. Phân nhóm dựa trên phương thức học Theo phương thức học, các thuật toán Machine Learning thường được chia làm 4 nhóm: Supervise learning, Unsupervised learning, Semi-supervised lerning và Reinforcement learning.  Có một số cách phân nhóm không có Semi-supervised learning hoặc Reinforcement learning. Supervised Learning (Học có giám sát) Supervised learning là thuật toán dự đoán đầu ra (outcome) của một dữ liệu mới (new input) dựa trên các cặp ( input, outcome ) đã biết từ trước. Cặp dữ liệu này còn được gọi là ( data, label ), tức ( dữ liệu, nhãn ). Supervised learning là nhóm phổ biến nhất trong các thuật toán Machine Learning. Một cách toán học, Supervised learning là khi chúng ra có một tập hợp biến đầu vào \\( \\mathcal{X} = \\{\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_N\\} \\) và một tập hợp nhãn tương ứng \\( \\mathcal{Y} = \\{\\mathbf{y}_1, \\mathbf{y}_2, \\dots, \\mathbf{y}_N\\} \\), trong đó \\( \\mathbf{x}_i, \\mathbf{y}_i \\) là các vector. \nCác cặp dữ liệu biết trước \\( (\\mathbf{x}_i, \\mathbf{y}_i) \\in \\mathcal{X} \\times \\mathcal{Y} \\) \nđược gọi là tập  training data  (dữ liệu huấn luyện). Từ tập traing data này, chúng ta cần tạo ra một hàm số ánh xạ mỗi phần tử từ tập \\(\\mathcal{X}\\) sang một phần tử (xấp xỉ) tương ứng của tập \\(\\mathcal{Y}\\): \\[ \\mathbf{y}_i \\approx f(\\mathbf{x}_i), ~~ \\forall i = 1, 2, \\dots, N\\] \nMục đích là xấp xỉ hàm số \\(f\\) thật tốt để khi có một dữ liệu \\(\\mathbf{x}\\) mới, chúng ta có thể tính được nhãn tương ứng của nó \\( \\mathbf{y} = f(\\mathbf{x}) \\). Ví dụ 1:  trong nhận dạng chữ viết tay, ta có ảnh của hàng nghìn ví dụ của mỗi chữ số được viết bởi nhiều người khác nhau. Chúng ta đưa các bức ảnh này vào trong một thuật toán và chỉ cho nó biết mỗi bức ảnh tương ứng với chữ số nào. Sau khi thuật toán tạo ra (sau khi  học ) một mô hình, tức một hàm số mà đầu vào là một bức ảnh và đầu ra là một chữ số, khi nhận được một bức ảnh mới mà mô hình  chưa nhìn thấy bao giờ , nó sẽ dự đoán bức ảnh đó chứa chữ số nào. MNIST : bộ cơ sở dữ liệu của chữ số viết tay.   (Nguồn:  Simple Neural Network implementation in Ruby) Ví dụ này khá giống với cách học của con người khi còn nhỏ. Ta đưa bảng chữ cái cho một đứa trẻ và chỉ cho chúng đây là chữ A, đây là chữ B. Sau một vài lần được dạy thì trẻ có thể nhận biết được đâu là chữ A, đâu là chữ B trong một cuốn sách mà chúng chưa nhìn thấy bao giờ. Ví dụ 2:  Thuật toán dò các khuôn mặt trong một bức ảnh đã được phát triển từ rất lâu. Thời gian đầu, facebook sử dụng thuật toán này để chỉ ra các khuôn mặt trong một bức ảnh và yêu cầu người dùng  tag friends  - tức gán nhãn cho mỗi khuôn mặt. Số lượng cặp dữ liệu ( khuôn mặt, tên người ) càng lớn, độ chính xác ở những lần tự động  tag  tiếp theo sẽ càng lớn. Ví dụ 3:  Bản thân thuật toán dò tìm các khuôn mặt trong 1 bức ảnh cũng là một thuật toán Supervised learning với training data (dữ liệu học) là hàng ngàn cặp ( ảnh, mặt người ) và ( ảnh, không phải mặt người ) được đưa vào. Chú ý là dữ liệu này chỉ phân biệt  mặt người  và  không phải mặt người  mà không phân biệt khuôn mặt của những người khác nhau. Thuật toán supervised learning còn được tiếp tục chia nhỏ ra thành hai loại chính: Classification (Phân loại) Một bài toán được gọi là  classification  nếu các  label  của  input data  được chia thành một số hữu hạn nhóm. Ví dụ: Gmail xác định xem một email có phải là spam hay không; các hãng tín dụng xác định xem một khách hàng có khả năng thanh toán nợ hay không. Ba ví dụ phía trên được chia vào loại này. Regression (Hồi quy) (tiếng Việt dịch là  Hồi quy , tôi không thích cách dịch này vì bản thân không hiểu nó nghĩa là gì) Nếu  label  không được chia thành các nhóm mà là một giá trị thực cụ thể. Ví dụ: một căn nhà rộng \\(x ~ \\text{m}^2\\), có \\(y\\) phòng ngủ và cách trung tâm thành phố \\(z~ \\text{km}\\) sẽ có giá là bao nhiêu? Gần đây  Microsoft có một ứng dụng dự đoán giới tính và tuổi dựa trên khuôn mặt . Phần dự đoán giới tính có thể coi là thuật toán  Classification , phần dự đoán tuổi có thể coi là thuật toán  Regression .  Chú ý rằng phần dự đoán tuổi cũng có thể coi là  Classification  nếu ta coi tuổi là một số nguyên dương không lớn hơn 150, chúng ta sẽ có 150 class (lớp) khác nhau. Unsupervised Learning (Học không giám sát) Trong thuật toán này, chúng ta không biết được  outcome  hay  nhãn  mà chỉ có dữ liệu đầu vào. Thuật toán unsupervised learning sẽ dựa vào cấu trúc của dữ liệu để thực hiện một công việc nào đó, ví dụ như phân nhóm (clustering) hoặc giảm số chiều của dữ liệu (dimension reduction) để thuận tiện trong việc lưu trữ và tính toán. Một cách toán học, Unsupervised learning là khi chúng ta chỉ có dữ liệu vào \\(\\mathcal{X} \\) mà không biết  nhãn  \\(\\mathcal{Y}\\) tương ứng. Những thuật toán loại này được gọi là Unsupervised learning vì không giống như Supervised learning, chúng ta không biết câu trả lời chính xác cho mỗi dữ liệu đầu vào. Giống như khi ta học, không có thầy cô giáo nào chỉ cho ta biết đó là chữ A hay chữ B. Cụm  không giám sát  được đặt tên theo nghĩa này. Các bài toán Unsupervised learning được tiếp tục chia nhỏ thành hai loại: Clustering (phân nhóm) Một bài toán phân nhóm toàn bộ dữ liệu \\(\\mathcal{X}\\) thành các nhóm nhỏ dựa trên sự liên quan giữa các dữ liệu trong mỗi nhóm. Ví dụ: phân nhóm khách hàng dựa trên hành vi mua hàng. Điều này cũng giống như việc ta đưa cho một đứa trẻ rất nhiều mảnh ghép với các hình thù và màu sắc khác nhau, ví dụ tam giác, vuông, tròn với màu xanh và đỏ, sau đó yêu cầu trẻ phân chúng thành từng nhóm. Mặc dù không cho trẻ biết mảnh nào tương ứng với hình nào hoặc màu nào, nhiều khả năng chúng vẫn có thể phân loại các mảnh ghép theo màu hoặc hình dạng. Association Là bài toán khi chúng ta muốn khám phá ra một quy luật dựa trên nhiều dữ liệu cho trước. Ví dụ: những khách hàng nam mua quần áo thường có xu hướng mua thêm đồng hồ hoặc thắt lưng; những khán giả xem phim Spider Man thường có xu hướng xem thêm phim Bat Man, dựa vào đó tạo ra một hệ thống gợi ý khách hàng (Recommendation System), thúc đẩy nhu cầu mua sắm. Semi-Supervised Learning (Học bán giám sát) Các bài toán khi chúng ta có một lượng lớn dữ liệu \\(\\mathcal{X}\\) nhưng chỉ một phần trong chúng được gán nhãn được gọi là Semi-Supervised Learning. Những bài toán thuộc nhóm này nằm giữa hai nhóm được nêu bên trên. Một ví dụ điển hình của nhóm này là chỉ có một phần ảnh hoặc văn bản được gán nhãn (ví dụ bức ảnh về người, động vật hoặc các văn bản khoa học, chính trị) và phần lớn các bức ảnh/văn bản khác chưa được gán nhãn được thu thập từ internet. Thực tế cho thấy rất nhiều các bài toán Machine Learning thuộc vào nhóm này vì việc thu thập dữ liệu có nhãn tốn rất nhiều thời gian và có chi phí cao. Rất nhiều loại dữ liệu thậm chí cần phải có chuyên gia mới gán nhãn được (ảnh y học chẳng hạn). Ngược lại, dữ liệu chưa có nhãn có thể được thu thập với chi phí thấp từ internet. Reinforcement Learning (Học Củng Cố) Reinforcement learning là các bài toán giúp cho một hệ thống tự động xác định hành vi dựa trên hoàn cảnh để đạt được lợi ích cao nhất (maximizing the performance). Hiện tại, Reinforcement learning chủ yếu được áp dụng vào Lý Thuyết Trò Chơi (Game Theory), các thuật toán cần xác định nước đi tiếp theo để đạt được điểm số cao nhất. AlphaGo chơi cờ vây với Lee Sedol. AlphaGo là một ví dụ của Reinforcement learning.   (Nguồn:  AlphaGo AI Defeats Sedol Again, With 'Near Perfect Game') Ví dụ 1: AlphaGo gần đây nổi tiếng với việc chơi cờ vây thắng cả con người .  Cờ vây được xem là có độ phức tạp cực kỳ cao  với tổng số nước đi là xấp xỉ \\(10^{761} \\), so với cờ vua là \\(10^{120} \\) và tổng số nguyên tử trong toàn vũ trụ là khoảng \\(10^{80}\\)!! Vì vậy, thuật toán phải chọn ra 1 nước đi tối ưu trong số hàng nhiều tỉ tỉ lựa chọn, và tất nhiên, không thể áp dụng thuật toán tương tự như  IBM Deep Blue  (IBM Deep Blue đã thắng con người trong môn cờ vua 20 năm trước). Về cơ bản, AlphaGo bao gồm các thuật toán thuộc cả Supervised learning và Reinforcement learning. Trong phần Supervised learning, dữ liệu từ các ván cờ do con người chơi với nhau được đưa vào để huấn luyện. Tuy nhiên, mục đích cuối cùng của AlphaGo không phải là chơi như con người mà phải thậm chí thắng cả con người. Vì vậy, sau khi  học  xong các ván cờ của con người, AlphaGo tự chơi với chính nó với hàng triệu ván chơi để tìm ra các nước đi mới tối ưu hơn. Thuật toán trong phần tự chơi này được xếp vào loại Reinforcement learning. (Xem thêm tại  Google DeepMind’s AlphaGo: How it works ). Ví dụ 2: Huấn luyện cho máy tính chơi game Mario . Đây là một chương trình thú vị dạy máy tính chơi game Mario. Game này đơn giản hơn cờ vây vì tại một thời điểm, người chơi chỉ phải bấm một số lượng nhỏ các nút (di chuyển, nhảy, bắn đạn) hoặc không cần bấm nút nào. Đồng thời, phản ứng của máy cũng đơn giản hơn và lặp lại ở mỗi lần chơi (tại thời điểm cụ thể sẽ xuất hiện một chướng ngại vật cố định ở một vị trí cố định). Đầu vào của thuật toán là sơ đồ của màn hình tại thời điểm hiện tại, nhiệm vụ của thuật toán là với đầu vào đó, tổ hợp phím nào nên được bấm. Việc huấn luyện này được dựa trên điểm số cho việc di chuyển được bao xa trong thời gian bao lâu trong game, càng xa và càng nhanh thì được điểm thưởng càng cao (điểm thưởng này không phải là điểm của trò chơi mà là điểm do chính người lập trình tạo ra). Thông qua huấn luyện, thuật toán sẽ tìm ra một cách tối ưu để tối đa số điểm trên, qua đó đạt được mục đích cuối cùng là cứu công chúa. Huấn luyện cho máy tính chơi game Mario 2. Phân nhóm dựa trên chức năng Có một cách phân nhóm thứ hai dựa trên chức năng của các thuật toán. Trong phần này, tôi xin chỉ liệt kê các thuật toán. Thông tin cụ thể sẽ được trình bày trong các bài viết khác tại blog này. Trong quá trình viết, tôi có thể sẽ thêm bớt một số thuật toán. Regression Algorithms Linear Regression Logistic Regression Stepwise Regression Classification Algorithms Linear Classifier Support Vector Machine (SVM) Kernel SVM Sparse Representation-based classification (SRC) Instance-based Algorithms k-Nearest Neighbor (kNN) Learning Vector Quantization (LVQ) Regularization Algorithms Ridge Regression Least Absolute Shrinkage and Selection Operator (LASSO) Least-Angle Regression (LARS) Bayesian Algorithms Naive Bayes Gaussian Naive Bayes Clustering Algorithms k-Means clustering k-Medians Expectation Maximization (EM) Artificial Neural Network Algorithms Perceptron Softmax Regression Multi-layer Perceptron Back-Propagation  Dimensionality Reduction Algorithms Principal Component Analysis (PCA) Linear Discriminant Analysis (LDA) Ensemble Algorithms Boosting AdaBoost Random Forest Và còn rất nhiều các thuật toán khác. 3. Tài liệu tham khảo A Tour of Machine Learning Algorithms Điểm qua các thuật toán Machine Learning hiện đại Share Share Interactive Learning Facebook page Machine Learning cơ bản Forum Recommended books \"Pattern recognition and Machine Learning.\", C. Bishop  \"The Elements of Statistical Learning\", T. Hastie et al.   \"Computer Vision:  Models, Learning, and Inference\", Simon J.D. Prince  \"Convex Optimization\", Boyd and Vandenberghe Recommended courses \"Machine Learning\", Andrew Ng  CS224n: Natural Language Processing with Deep Learning CS231n: Convolutional Neural Networks for Visual Recognition CS246: Mining Massive Data Sets CS20SI: Tensorflow for Deep Learning Research  Introduction to Computer Science and Programming Using Python Others Top-down learning path: Machine Learning for Software Engineers Blog này được tạo như thế nào? Chúng tôi đã apply và học tiến sỹ như thế nào? (1/2) Chúng tôi đã apply và học tiến sỹ như thế nào? (2/2) 8 Inspirational Applications of Deep Learning Matrix calculus TensorFlow-Examples Eight Easy Steps To Get Started Learning Artificial Intelligence The 9 Deep Learning Papers You Need To Know About"
        },
        {
          "title": "Top machine Learning algorithms",
          "relevance": "1",
          "url": "https://viblo.asia/p/top-machine-learning-algorithms-DbmemLZwGAg",
          "content": "Posts Questions Discussions Announcements  No announcement yet. All Announcements  Sign In/Sign up\n     0 NguyenDuong  Follow Published Nov 28th, 2:02 pm Top machine Learning algorithms Machine Learning  Nov 28th, 2:02 pm\n          420  2  0  Report\n     Bài viết này sẽ như là 1 tour đơn giản giới thiệu 1 vòng về cuộc hành trình trong khoa học về dữ liệu và machine learning. Xuyên qua bài viêt này, bạn sẽ có thể làm việc với các vấn đề trong các thuật toán của machine learning với code Python. Bài viết có lược bỏ các phần có liên quan đến toán thống kê, thực sự nó quá rối rắm để hiểu lúc đầu. Mình đã chán nản khi nhìn vài công thức thống kê khi tìm hiểu về machine learning :D . Đầu tiên, bạn cần nắm rõ. Có 3 loại thuật toán trong Machine Learning Dưới đây mình sẽ nói về tư tưởng của từng thuật toán. 1. Supervised Learning Cách thức vận hành : Đây là thuật toát bao gồm một biến mục tiêu / kết quả (hoặc biến phụ thuộc). biến này được dự đoán từ 1 tập hợp các nhân tố ( biến độc lập) . Sử dụng những tập hợp của các biến, chúng ta tạo được một hàm có thể cho phép các kết quả output đúng với dự đoán tương ứng với kết quả input. Quá trình \"training\" này sẽ tiếp tục cho đến các chúng ta đạt được một độ chính xác mong muốn liên quan đến việc \"traing dữ liệu\". Ví dụ của các thuật toán Supervised Learning : Regression, Decision Tree, Random Forest, KNN, Logistic Regression v.v.v… 2. Unsupervised Learning Cách thức vận hành : Trong thuật toán này, chúng ta không có bất kì biến mục tiêu / kết quả để dự đoán / đánh giá . Nó được sử dụng để phân nhóm các tập dữ liệu vào các nhóm khác nhau. Các nhóm này được sử dụng cho việc phân khúc các \"khách hàng' vào các nhóm khác nhau nữa cho mục tiêu cụ thể. Ví dụ của Unsupervised Learning : thuật toán Apriori, K-means. 3. Reinforcement Learning Cách thức vận hành : Sử dụng thuật toán này, máy sẽ được luyện tập để ra các quyết định. Máy sẽ được tiếp xúc với một môi trường nơi mà nó được tự luyện tập liên tục sử dụng với phương thức \"thử và sửa sai\" cho đến khi \"sai\" đạt đến ngưỡng chấp nhận được hoặc là sẽ bằng 0 trong điều kiện tuyệt đối. Máy sẽ học từ các \"kinh nghiệm trong quá khứ\" và \"học các kiến thức\" tốt nhất có thể được để tạo nên các quyết định chính xác về mặt logic business. Ví dụ của Reinforcement Learning: Markov Decision Process Dưới đây là danh sách các thuật toán Machine Learning phổ biến. Những thuật toán này có thể được áp dung cho bất kì vấn đề nào về dữ liệu : Linear Regression Logistic Regression Decision Tree SVM Naive Bayes KNN K-Means Random Forest Dimensionality Reduction Algorithms Gradient Boost & Adaboost 1. Linear Regression Line Regression được sử dụng để ước tính giá trị thực (giá nhà, số lượng cuộc gọi, doanh thu bán hàng v.v.v.. dựa trên các biến thay đổi liên tục). Ở đây, chúng ta thiết lập mối quan hệ giữa các biến độc lập và biến phục thuộc bằng cách lắp 1  line  tốt nhất. line  này được biết đến như một dòng đệ qui và được biểu diễn bằng công thức  Y = a*X + b Cách tốt nhất để hiểu linear regression là nhớ laị kinh nghiệm tuổi thơ. Hãy xem, bạn yêu cầu một đứa bé lớp 5 sắp mọi người của nó trong lớp bằng thứ tự tăng dần cân năng mà không hỏi cả lớp về cân năng gì cả. Đứa bé sẽ làm gì ? Khả năng cao nó sẽ nhìn vào chiều cao và kết cấu cơ thể để sắp xếp cả lớp sử dụng sự kết hợp các thông số mà nó có thể nhìn thấy. Đây chính là Linear Regression trong đời sống thực. Đứa bé thực tế đã quan sát được chiều và kết cấu cơ thể sẽ có sự tương quan với trọng lượng, đây chính là điều giống như công thức ở trên. Trong công thức trên : Y  - Biến phục thuộc a  - Slope X  - Biến độc lập b  - Intercept Các hệ số  a  và  b  đều bắt nguồn từ việc giảm thiếu các khoảng chênh lệch bình phương giữa khoảng cách của các điểm dữ liệu và regression line. Tham khảo ví dụ dưới đây. Ở đây chúng ta đã xác định rõ được line tốt nhất có công thức là  y=0.2811x+13.9 . Bây giờ, sử dụng công thức này, chúng ta có thể tìm được cân năng, nếu chúng ta biết được chiều cao của một người nào đó. Linear Regression được phân làm 2 loại chính : Simple Linear Regression và Multiple Linear Regression. Simple Linear Regression được đặc trưng bởi một biến đọc lập. Multiple Linear Regression được đặc trung bởi nhiều hơn 1 biến độc lập. Trong khi tìm kiếm line tốt nhất, bạn có thể sẽ tìm được một hôi qui đa thức hoặc đường cong. #Import Library #Import other necessary libraries like pandas, numpy... from  sklearn  import  linear_model\n\n #Load Train and Test datasets #Identify feature and response variable(s) and values must be numeric and numpy arrays \nx_train=input_variables_values_training_datasets\ny_train=target_variables_values_training_datasets\nx_test=input_variables_values_test_datasets\n\n # Create linear regression object \nlinear = linear_model.LinearRegression()\n\n # Train the model using the training sets and check score \nlinear.fit(x_train, y_train)\nlinear.score(x_train, y_train)\n\n #Equation coefficient and Intercept \nprint( 'Coefficient: \\n' , linear.coef_)\nprint( 'Intercept: \\n' , linear.intercept_)\n #Predict Output \npredicted= linear.predict(x_test)\n 2. Logistic Regression Đừng nhầm lẫn bởi tên của nó ! Đây là một sự phân loại, khoogn phải là một thuật toán hồi qui. Nó được sử dụng để ddanhs giá ước tính các giá trị ời rạc (các giá trị nhị phân) dựa trên tập hợp các biến độc lập. Một cách đơn giản, nó dự đoán xác suất xảy ra của một sử kiện bằng việc khớp dữ liệu với một hàm logit. Do đó, nó còn được gọi là hồi qui logit. Và cũng vì lì do vậy, nó dự đoán xác suất, output của nó luôn nằm giữa 0 và 1. Một lần nữa, chúng ta hãy cố gắng hiểu thuật toán này thông qua một ví dụ đơn giản . Hãy giả sử bạn của bạn cho bạn 1 bài toàn để giải. Chỉ có 2 kịch bản kết quả - bản giải được or ko giải được. Bây giờ thử tưởng tượng rằng ban đang được đưa 1 giải các câu đố/toán/quizz để có thể hiểu được bản thân là bạn phù hợp với types câu đố nào. Kết quả của việc nghiên cứu bản thân này sẽ trông như thế này - nếu bạn được đưa một vấn đề kiểu giải đố toán lớp 10, bạn có 70% xác suất giải được nó. Nếu nó là câu hỏi lịch sử của lớp 5, xác suất chỉ còn 30%. Đây chính là Logistic Regression. Quay trở về với toán, các tỷ lệ log kết quả được mô hình hóa bằng sự kết hợp tuyến tính của các biến dự đoán. odds= p/ ( 1 -p) = probability  of  event occurrence / probability  of not  event occurrence\n ln (odds)  =  ln (p/( 1 -p) )\n logit (p)  =  ln (p/( 1 -p) ) =  b0 + b1X1 + b2X2 + b3X3 ....+ bkXk Ở trên,  p  là xác suất của các sự hiện diện của các đặc tính cần quan tâm. Nó chọn các param có thể tối đa hóa sự chuẩn xác của các giá trị mẫu hơn là việc giảm thiểu bình phương lỗi (giảm thiểu bình phương lỗi là cách tiếp cận của hồi qui thông thường) Bây giờ, bạn có thể hỏi, tại sao phải log ? Để không đi sâu quá nhiều vào toán học trong bài viết này, tôi sẽ nói một cách đơn giản là việc log là một trong số những cách tốt nhất của toán học để mô phỏng lại một step function. #Import Library from  sklearn.linear_model  import  LogisticRegression\n #Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset # Create logistic regression object \nmodel = LogisticRegression()\n # Train the model using the training sets and check score \nmodel.fit(X, y)\nmodel.score(X, y)\n #Equation coefficient and Intercept \nprint( 'Coefficient: \\n' , model.coef_)\nprint( 'Intercept: \\n' , model.intercept_)\n #Predict Output \npredicted= model.predict(x_test)\n 3. Decision Tree Đây là một trong những thuật toán yêu thích của tôi và tôi sử dụng nó khá thường xuyên. Nó là một loại thuật toán supervised learning được sử dụng chủ yếu cho các vấn đề phân loại. Đáng ngạc nhiên, nó hoạt động cho cả hai biến phụ thuộc phân loại và biến phụ thuộc liên tục. Trong thuật toán này, chúng ta chia \"population\" thành hai hoặc các nhóm tập hợp. Điều này được thực hiện dựa trên các biến thuộc tính quan trọng nhất / các biến độc lập để phân loại các groups có thể được. Để biết thêm, bạn có thể đọc: Decision Tree Simplified. Trong hình trên, bạn có thể thấy \"population\" được phân thành bốn groups khác nhau dựa trên nhiều thuộc tính để xác định 'Liệu họ sẽ chơi hay không ? \". Để chia \"population\" thành các nhóm đồng nhất khác nhau, chúng ta sử dụng các kỹ thuật khác nhau như Gini, Information Gain, Chi-square, entropy. Cách tốt nhất để hiểu làm thế nào cây quyết định hoạt động, là chơi trò Jezzball - một trò chơi cổ điển từ Microsoft (hình dưới đây). Về cơ bản, bạn có một căn phòng với những bức tường di chuyển và bạn cần tạo ra bức tường mà diện tích tối đa được xóa đi với các quả bóng. Vì vậy, mỗi khi bạn chia phòng với một bức tường, bạn đang cố gắng để tạo ra 2 \"population\" khác nhau trong cùng một phòng. Decision trees làm việc bằng cách chia một \"population\" vào các nhóm khác nhau trong khả năng có thể. #Import Library #Import other necessary libraries like pandas, numpy... from  sklearn  import  tree\n #Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset # Create tree object \nmodel = tree.DecisionTreeClassifier(criterion= 'gini' )  # for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini # model = tree.DecisionTreeRegressor() for regression # Train the model using the training sets and check score \nmodel.fit(X, y)\nmodel.score(X, y)\n #Predict Output \npredicted= model.predict(x_test)\n 4. SVM (Support Vector Machine) Đây là một phương pháp phân loại. Trong thuật toán này, chúng ta sẽ vẽ mỗi mục dữ liệu như là một điểm trong không gian n-chiều (n là số tính năng) với các giá trị của mội tính năng là các giá trị của một trục tọa độ cụ thể. Ví dụ, nếu chúng ta có 2 tính năng là Chiều cao và Độ dài tóc của một cá nhân, đầu tiên chúng ta sẽ có 2 biến với 2 chiều không gian nơi một điểm có 2 tọa độ. Bây giờ, chúng ta sẽ tìm một vài lines mà phân tách dữ liệu giữa 2 nhóm dữ liệu khác nhau đã được phân loại. Line này sẽ có tinh chaatgs là khoảng cách từ điểm gần nhất đến line trong mỗi groups sẽ là xa nhất. Trong ví dụ ở trên, line phân tách dữ liệu thành 2 nhóm là line đen vì khoảng cách của điểm gần nhất đến line là xa nhất. Line này chính là \"phân loại\". Sau đó, tùy thuộc vào dữ liệu tiếp theo sẽ nằm ở phía nào của line, chúng ta lại thực hiện việc phân tách dữ liệu mới. #Import Library from  sklearn  import  svm\n #Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset # Create SVM classification object \nmodel = svm.svc()  # there is various option associated with it, this is simple for classification. You can refer link, for mo# re detail. # Train the model using the training sets and check score \nmodel.fit(X, y)\nmodel.score(X, y)\n #Predict Output \npredicted= model.predict(x_test)\n 5. Naive Bayes Đây là một ký thuật phân lọa dựa trên định lý Bayes với một giả định độc lập giữa các dự đoán. Nói một cách đơn giản, một phân loại Naive Bayes giả định rằng sự hiện diện của một tính năng cụ thể trong class không liên quan đến sự hiện hiện của tính năng khác. Ví dụ, một trái cây có thể đc coi là quả táo nếu not màu đỏ, tròn và đường kính khoảng 3 inch. Thậm chí, nếu các tính năng đó phụ thuộc vào nhau hoặc sự tồn tồn tại của các tính khác, một phân loại naive Bayes sẽ cân nhắc tất cả mọi tính chất để tính toán một cách độc lập xác suât lại trái cây này quả táo hay ko ? Mô hình Naive bayesian khá dễ để xây dựng và hữu dụng cho tập dữ liệu lớn. Cùng với sự đơn giản của nó, mô hình này được biết đến với việc outperform các phương pháp phân loại tinh vi hơn khác. Định lý Bayes cung cấp một cách để tính toán xác suất  P(c|x)  từ  P(c) ,  P(x)  và  P(x|c) . Hãy nhìn công thức dưới Ví dụ:  Dưới đây tôi có một tập dữ liệu training của thời tiết và biến mục tiêu tương ứng 'Play'.  Bây giờ, chúng ta cần phải phân loại cho dù người chơi sẽ chơi hay không dựa trên điều kiện thời tiết. Hãy làm theo các bước dưới đây để thực hiện nó. Bước 1: Chuyển đổi các dữ liệu thành bảng tần số Bước 2: Tạo bảng \"Khả năng\" bằng cách tìm các xác suất như  \"khả năng trời u ám ám\" = 0,29  và  \"khả năng PLAY\" = 0,64 . Bước 3: Bây giờ, sử dụng phương trình Bayesian Naive để tính toán xác suất hậu nghiệm cho mỗi lớp. Các nhóm với xác suất hậu nghiệm cao nhất là kết quả của dự đoán. Vấn đề: Liệu người chơi sẽ PLAY nếu thời tiết nắng, là tuyên bố này là đúng? Chúng tôi có thể giải quyết nó bằng phương pháp thảo luận ở trên, do đó  P (Yes | Sunny) = P (Sunny | Yes) * P (Yes) / P (Sunny) Ở đây chúng ta có  P (Sunny | Yes) = 3/9 = 0.33, P (Sunny) = 5/14 = 0,36, P (Yes) = 9/14 = 0,64 Bây giờ,  P (Yes | Sunny) = 0,33 * 0,64 / 0,36 = 0,60 , với xác suất cao hơn. Naive Bayes sử dụng một phương pháp tương tự để dự đoán xác suất của các nhóm khác nhau dựa trên các thuộc tính khác nhau. Thuật toán này được sử dụng chủ yếu trong phân loại văn bản text có nhiều class. #Import Library from  sklearn.naive_bayes  import  GaussianNB\n #Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset # Create SVM classification object model = GaussianNB() # there is other distribution for multinomial classes like Bernoulli Naive Bayes, Refer link # Train the model using the training sets and check score \nmodel.fit(X, y)\n #Predict Output \npredicted= model.predict(x_test)\n 6. KNN (K- Nearest Neighbors) Thuật toán này được sử dụng cho cả viecj phân loại và hồi qui. Tuy nhiên, nó được sử dụng nhiều hơn trong vấn đề phân loại.  kNN  là một thuật toán khá đơn giản : tìm kiếm  k  hàng xóm gần nhất (gần nhất trong một khái niệm nào đó ) so với vị trí đang xét đến. Các hàm tính khoảng cách có thể là các hàm Euclidean, Manhattan, Minkowski và Hamming. Đầu tiên, 3 hàm ở đầu đều được sử dụng cho các hàm liên tục và hàm thứ 4 (Hamming) thì cho các biến phân loại. kNN  có thể dễ dàng được liên tưởng đến trong cuộc sống đời thực. Nếu bạn muốn tìm hiểu về một người - người mà bạn ko có tí thông tin nào, bạn sẽ tìm hiểu hiểu đó bằng cách tiếp cận bạn bè của người đó và vòng tròn circle xung quanh ng đó để đạt thông tin mà bạn muốn. #Import Library from  sklearn.neighbors  import  KNeighborsClassifier\n #Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset # Create KNeighbors classifier object model \nKNeighborsClassifier(n_neighbors= 6 )  # default value for n_neighbors is 5 # Train the model using the training sets and check score \nmodel.fit(X, y)\n #Predict Output \npredicted= model.predict(x_test)\n 7. K-Means Đây là một loại thuật toán unsupervised algorithm giải quyết các vấn đề về phân nhóm. Cách thức của nó khá đơn giản : phân tách dữ liệu thành một nhóm các  cluster  ( giả sử  k  cái) . Các điểm dữ liệu bên trong  cluster  là đòng nhất và không đồng nhất với các điểm dữ liệu của các nhóm khác. Bạn có để ý vết mực bị đổ ra trên bàn không  ?  k means  khá tương tự với việc này. Bạn nhìn vào vết mục và hình dạng lan rộng để giã mã có bao nhiêu  cluster  khác nhau / có bao nhiêu nhóm \"population\" Cách mà  K-Means  tạo nên các  cluster * k-means  chọn  k  điểm cho mỗi  cluster và được gọi là trọng tâm. Mỗi điểm dữ liệu tạo thành một  cluster  với trọng tâm gần nhất Tìm kiếm trọng tâm của mỗi  cluster  dựa trên các thành viên bên trong mỗi  cluster . Ở đây chúng ta sẽ có các trọng tâm mới. Khi có các trọng tậm mới, lặp lại bước 2 và bước 3. Tìm kiếm khoanrgcacsh gần nhất của mỗi điểm dữ liệu tạo nên bởi các trọng tâm mới và được liên kết với  k-cluster  mới. Lặp lại quá trình này cho đến khi các trọng tâm hội tụ thành một điểm. Cách thức tìm kiếm giá trị  k  : Trong  k-means , chúng ta có các  cluster  và mối  cluster  có trọng tâm của nó. Tổng bình phương khác biệt giữa trọng tâm và các điểm dữ liệu trong  cluster  tạo nên tổng giá trị bình phương cho cluster  đó. Chúng ta biết rằng khi số lượng  cluster  được tăng lên, giá trị sẽ giảm đi nhưng nếu bạn quan sát kết qua, bạn sẽ thấy rằng tổng bình phương khoảng cách sẽ giảm mạnh đến một giá trị  k , sau đó nó sẽ giảm chậm đi. Tại đây, bạn có thể tìm kiếm số lượng tối ưu của  cluster  - giá trị  k . #Import Library from  sklearn.cluster  import  KMeans\n #Assumed you have, X (attributes) for training data set and x_test(attributes) of test_dataset # Create KNeighbors classifier object model \nk_means = KMeans(n_clusters= 3 , random_state= 0 )\n # Train the model using the training sets and check score \nmodel.fit(X)\n #Predict Output \npredicted= model.predict(x_test)\n 8. Random Forest Random Forest  là một thuật ngữ nổi tiến cho một tập hợp các cây quyết định ( decision trees ). Trong  Random Forest , chúng ta tập hợp các  desicion trees  (được gọi là  Forest ). Để phân biệt một đối tượng mới dựa trên các thuộc tính, mỗi  tree  được cho một phân loại và chúng ta sẽ nói rằng mỗi  tree  sẽ  vote  cho loại đó. Forest sẽ chọn các phân lọai có nhiều  votes  nhất (thông qua toàn thể  tree  trong  forest ). Mỗi  tree  được trồng và phát triển. Vì tính phức tạp khá lằng nhằng nên mình xin phép không đề cập trong bài viết này. #Import Library from  sklearn.ensemble  import  RandomForestClassifier\n #Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset # Create Random Forest object \nmodel= RandomForestClassifier()\n # Train the model using the training sets and check score \nmodel.fit(X, y)\n #Predict Output \npredicted= model.predict(x_test)\n 9. Dimensionality Reduction Algorithms Trong 4-5 năm qua, có có khá nhiều sự gia tăng mạnh mẽ trọng việc thu giữ các stage có thể được. Cơ quan / Chính phủ các doanh nghiệp / tổ chức nghiên cứu cũng đang thực hiện thụ thập dữ liệu khá chi tiết. Ví dụ : \nCác công ty thương mại điện tử đang nắm bắt thêm thông tin chi tiết về khách hàng như nhân khẩu học của họ, lịch sử crawl web, những gì họ thích hay không thích, lịch sử mua hàng, thông tin phản hồi và nhiều thứ khác để cung cấp cho họ sự quan tâm cá nhân hơn. Dữ liệu bao gồm khá nhiều tính năng, điều này có vẻ tốt cho việc xây dựng một mô hình ổn điịnh vững chắc nhưng thực sự rất thách thức. Làm cách nào bạn có thể xác định các biến có độ quan trọng cao trong khoảng 1000 or 2000 biến ? Trong các trường hợp này, thuật toán Dimensionality Reduction Algorithms sẽ hỗ trợ chúng ta kết hợp với các thuật toán khác như Decision Tree, Random Forest, PCA, Factor Analysis, Identify based on correlation matrix, missing value ratio v.v.v.... #Import Library from  sklearn  import  decomposition\n #Assumed you have training and test data set as train and test # Create PCA obeject pca= decomposition.PCA(n_components=k) #default value of k =min(n_sample, n_features) # For Factor analysis #fa= decomposition.FactorAnalysis() # Reduced the dimension of training dataset using PCA \ntrain_reduced = pca.fit_transform(train)\n #Reduced the dimension of test dataset \ntest_reduced = pca.transform(test)\n 10. Gradient Boosting & AdaBoost GBM & AdaBoost đang thúc đẩy các thuật toán được sử dụng khi chúng ta đối phó với rất nhiều dữ liệu để đưa ra dự đoán với công suất dự đoán cao. Boosting là một thuật toán \"học quần thể\" kết hợp với các dự báo của một vài  estimator  để cải thiện mạnh mẽ hơn một  estimator  duy nhất. Nó kết hợp nhiều yếu tố dự báo yếu hoặc trung bình để tạo nên một yếu tố dự báo mạnh mẽ. Các thuật toán Boosting luôn thể hiện tốt trong các cuộc thi khoa học dữ liệu như Kaggle, AV Hackathon, CrowdAnalytix. #Import Library from  sklearn.ensemble  import  GradientBoostingClassifier\n #Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset # Create Gradient Boosting Classifier object \nmodel= GradientBoostingClassifier(n_estimators= 100 , learning_rate= 1.0 , max_depth= 1 , random_state= 0 )\n # Train the model using the training sets and check score \nmodel.fit(X, y)\n #Predict Output \npredicted= model.predict(x_test)\n End Notes Bây giờ , hẳn bạn đã có một chút tư duy về các thuật toán cơ bản được sử dụng rộng rãi trong machine learning. Mục đích duy nhất của bài viết này là cung cấp tư tưởng của các thuật toán - cách tư duy và code Python để các bạn hiểu hơn về nó. Nguồn dịch https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/ Tham khảo http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/ http://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html NguyenDuong @duongichi Follow  509\n          25\n          39\n         \n                            Clip this post\n                         Have problems with  Machine Learning ?  Ask on Viblo » Comments  No comments yet.\n         0 • • • Facebook Viblo Hot authors Resources Posts Questions Videos Tags Authors Help Terms  RSS Feed Browser extension Atom plugin Feedback FAQ \n                    © 2017  Viblo . All rights reserved.\n                 \n        Viblo - Free service for technical knowledge sharing\n     Feedback \n                        Your feedback will be shared with our product delivery teams, and taken into consideration\n                        for future development.\n                     Name Email Send \n                Sign In\n             \n                Sign Up\n             Join us to find useful information required to improve your skills \n            Forgot your password?\n         Sign In Free  service for technical knowledge  sharing Join us to find useful information required to improve your skills \n                    I agree to  Viblo Terms of Service Sign Up or"
        },
        {
          "title": "Bức tranh tổng quan về thuật toán phân cụm",
          "relevance": "0",
          "url": "https://techmaster.vn/posts/33893/thuat-toan-phan-cum",
          "content": " 090.220.9011  \n                                cuong@techmaster.vn Đăng nhập Đăng ký \n                Trang chủ\n             \n                Khóa học\n             \n                Thực tập\n             \n                Lịch khai giảng\n             \n                Blog\n             \n                Việc làm\n             \n                Phỏng vấn\n             \n                Chúng tôi  arrow_drop_down \n                Khách hàng - Đối tác\n             \n                Giảng viên\n             \n                Thiết bị đào tạo\n             \n                Quy định chung\n             \n                Tuyển dụng giảng viên\n             \n                Chơi thể thao\n             \n                Team building - Royal City\n             \n                Video hoạt động\n             \n                Thực tập\n             menu \n                Trang chủ\n             \n                Khóa học\n             \n                Thực tập\n             \n                Lịch khai giảng\n             \n                Blog\n             \n                Việc làm\n             \n                Phỏng vấn\n             \n                Chúng tôi  arrow_drop_down \n                Khách hàng - Đối tác\n             \n                Giảng viên\n             \n                Thiết bị đào tạo\n             \n                Quy định chung\n             \n                Tuyển dụng giảng viên\n             \n                Chơi thể thao\n             \n                Team building - Royal City\n             \n                Video hoạt động\n             \n                Thực tập\n             Trang chủ Blog Bức tranh tổng quan về thuật toán phân cụm close Danh mục bài viết \n                        Agile (13)\n                     \n                        Altassian (4)\n                     \n                        Android (75)\n                     \n                        Arduino (11)\n                     \n                        ASP.net (13)\n                     \n                        C++ (14)\n                     \n                        Cloud (6)\n                     \n                        công việc (142)\n                     \n                        CSS (19)\n                     \n                        Cuộc sống (129)\n                     \n                        Database (19)\n                     \n                        Design (43)\n                     \n                        Điện tử (35)\n                     \n                        Docker (1)\n                     \n                        Education (60)\n                     \n                        Eletronics (4)\n                     \n                        Facebook (2)\n                     \n                        Game (23)\n                     \n                        Git (8)\n                     \n                        Go (2)\n                     \n                        HTML (31)\n                     \n                        iOS (228)\n                     \n                        Java (115)\n                     \n                        JavaScript (59)\n                     \n                        John Vũ (7)\n                     \n                        Kiểm thử (9)\n                     \n                        lập trình (293)\n                     \n                        lập trình Android (32)\n                     \n                        lập trình ios (157)\n                     \n                        lập trình php (31)\n                     \n                        lập trình ứng dụng (93)\n                     \n                        lập trình ứng dụng iphone (42)\n                     \n                        Lập trình web (170)\n                     \n                        Linux (38)\n                     \n                        Mac (9)\n                     \n                        Machine Learning (11)\n                     \n                        Magento (4)\n                     \n                        Microservice (7)\n                     \n                        Microsoft (16)\n                     \n                        News (59)\n                     \n                        Nghề nghiệp (80)\n                     \n                        Node.js (71)\n                     \n                        Odoo (3)\n                     \n                        OpenCV (1)\n                     \n                        PHP (66)\n                     \n                        Project Management (21)\n                     \n                        Python (13)\n                     \n                        Ruby on Rails (54)\n                     \n                        Scala (15)\n                     \n                        SharePoint (6)\n                     \n                        STEM (10)\n                     \n                        Swift (86)\n                     \n                        TechMaster (130)\n                     \n                        Tips and tricks (18)\n                     \n                        Tutor (72)\n                     \n                        Twitter Boostrap (2)\n                     \n                        Uncategorized (26)\n                     \n                        ứng dụng (2)\n                     \n                        Unity3D (13)\n                     \n                        việc làm (126)\n                     \n                        VMware (1)\n                     \n                        WindowsPhone (43)\n                     \n                        WordPress (4)\n                     Bức tranh tổng quan về thuật toán phân cụm 28/05/2016 Bởi  Phan Đức Việt trong\n                                         Machine Learning Nếu bạn đang có ý định trở thành một Data Scientist (nhà khoa học dữ liệu) thì hiện tại đang là 1 thời điểm không hề tồi chút nào. Những con người kể cả khó tính nhất cũng sẽ đổ dồn sự chú ý khi bạn đề cập tới Big Data trong cuộc hội thoại, đám đông sẽ cảm thấy hào hứng khi được nghe bạn chém gió về Trí tuệ nhân tạo cũng như Học máy. Thậm chí những con số do Google cung cấp tại  đây  còn cho thấy: tất cả vẫn chưa có dấu hiệu dừng lại, chúng vẫn tiếp tục phát triển với tốc độ rất nhanh. Càng ngày càng có rất nhiều các giải thuật 'thông minh' đã được phát minh ra để giúp đỡ các nhà khoa học dữ liệu. Tất cả chúng nhìn chung đều có vẻ rất phức tạp, nhưng nếu chúng ta hiểu được và biết cách phối hợp một cách nhuần nhuyễn thì mọi việc sẽ trở nên dễ dàng hơn rất nhiều. Các khóa học về khai phá dữ liệu (Data Mining) hoặc học máy (Machine Learning) vẫn thường mở đầu bằng những ví dụ về phân cụm, lí do đơn giản bởi vì chúng rất thực tế và không quá khó hiểu. Bài toán phân cụm là 1 nhánh ứng dụng chính của lĩnh vực Unsupervised Learning (Học không giám sát), trong đó dữ liệu được mô tả trong bài toán không được dán nhãn (tức là không có đầu ra). Trong trường hợp này, thuật toán sẽ tìm cách phân cụm - chia dữ liệu thành từng nhóm có đặc điểm tương tự nhau, nhưng đồng thời đặc tính giữa các nhóm đó lại phải càng khác biệt càng tốt. Dữ liệu của chúng ta có thể là bất cứ thứ gì, chẳng hạn như dữ liệu về khách hàng: Thuật toán phân cụm sẽ rất hữu ích trong việc đánh giá và chia thành các nhóm người dùng khác nhau, rồi từ đó ta có thể đưa ra những chiến lược marketing phù hợp trên từng nhóm người dùng đó. Tham khảo khóa học  nhập môn Machine Learning  tại TechMaster K-Means Clustering Sau khi dạo qua những màn giới thiệu chung, đa số các khóa học Data Mining sẽ bắt đầu luôn với K-Means: 1 thuật toán tuy đơn giản nhưng lại khá hiệu quả và được sử dụng rộng khắp. Trước khi bắt tay vào làm, chúng ta cần phải xác định sẵn 2 thứ: đó là  hàm khoảng cách  được sử dụng (ví dụ như khoảng cách Euclid) và  số lượng nhóm  mong muốn (ta sẽ kí hiệu trong bài viết này là  k ) Mô phỏng quá trình phân cụm K-Means Thuật toán bắt đầu với việc chọn ra tâm của từng cụm. Chúng ta có thể đơn giản chọn k điểm ngẫu nhiên trong bộ, hoặc sử dụng một số hướng tiếp cận nào khác, nhưng nhìn chung ngẫu nhiên vẫn là cách tốt nhất. Rồi kế tiếp, luân phiên lặp lại 2 giai đoạn sau: Giai đoạn gán : gán từng phần tử trong bộ dữ liệu của chúng ta vào các cụm. Cách thức tiến hành đó là: với mỗi điểm, hãy tính khoảng cách từ điểm đó tới vị trí các tâm, sau cùng: tâm nào gần nhất thì gán vào cụm ứng với cái tâm đó Giai đoạn cập nhật : duyệt từng cụm, cập nhật lại tọa độ của tâm: Như đã biết, sau giai đoạn 1, chúng ta đã thu được k cụm ứng với dãy các điểm được gán cho từng cụm. Tọa độ tâm mới của cụm sẽ bằng trung bình cộng tọa độ các điểm trong cụm Sau càng nhiều vòng lặp, các tâm càng di chuyển chậm dần, và tổng khoảng cách từ mỗi điểm trong cụm tới tâm cụm lại càng nhỏ đi. Quá trình sẽ kết thúc cho tới khi hàm tổng khoảng cách hội tụ (tức là không có sự thay đổi nào xảy ra ở giai đoạn gán nữa). Lúc này tọa độ tâm vẫn sẽ bằng trung bình cộng các điểm hiện tại trong cụm, hay nói cách khác tâm sẽ không còn di chuyển tiếp nữa.  Chú ý thuật toán K-Means chỉ đảm bảo được quá trình này sẽ đưa hàm tổng khoảng cách hội tụ tới điểm cực tiểu địa phương, chứ không chắc chắn đó là giá trị nhỏ nhất của toàn bộ hàm số . Tuy nhiên, điều này là có thể chấp nhận được vì  KHÔNG  phải mô hình nào càng sát với bộ dữ liệu huấn luyện thì cũng sẽ càng tốt. Ta có thể nhận thấy rằng việc lựa chọn tâm lúc khởi điểm cũng có ảnh hưởng tới kết quả cuối cùng thu được, do đó đã nảy sinh rất nhiều ý kiến trái chiều về vấn đề này. Một ý tưởng đơn giản là cho chạy K-Means nhiều lần với mỗi bộ tâm ngẫu nhiên khác nhau, rồi sau đó chọn ra mô hình tốt nhất thông qua việc xét giá trị nhỏ nhất của các hàm tổng khoảng cách ứng với chúng. Một hướng tiếp cận khác trong việc chọn tâm ban đầu đó là chọn những điểm \"xa nhất\". Việc này có thể cho kết quả tốt hơn, tuy nhiên ta sẽ mắc phải vấn đề với những phần tử \"nhiễu\", đó là những phần tử nằm riêng lẻ một mình tách biệt với phần còn lại trong bộ dữ liệu. Do đó chúng sẽ tự lập ra 1 cụm riêng của chính mình. Có một cách giải quyết đã được phát minh để cân bằng đồng thời được cả 2 điều trên, nó có tên gọi là  K-Means++ : trong đó, tâm khởi đầu vẫn được chọn ngẫu nhiên, nhưng là  chọn lần lượt (thay vì đồng loạt)  và kèm theo  xác suất ngẫu nhiên tỉ lệ thuận với khoảng cách tới điểm tâm vừa chọn trước đó . Tức là, các điểm càng nằm phía xa sẽ có khả năng được chọn làm tâm kế tiếp càng lớn. Do đó, nếu có 1 nhóm các điểm, khả năng chỉ 1 điểm từ nhóm đó được chọn làm tâm cũng sẽ cao hơn. K-Means++ cũng đang được chọn sử dụng cho bước khởi tạo của thuật toán K-Mean trong thư viện  Scikit-learn  của Python. Nếu bạn đang lập trình Python, bạn có thể dùng ngay thư viện này. Đối với Java, thư viện  Weka  sẽ là 1 sự lựa chọn đáng để cân nhắc. Java (Weka) // Load some data\r\nInstances data = DataSource.read(\"data.arff\");\r\n\r\n// Create the model\r\nSimpleKMeans kMeans = new SimpleKMeans();\r\n\r\n// We want three clusters\r\nkMeans.setNumClusters(3);\r\n\r\n// Run K-Means\r\nkMeans.buildClusterer(data);\r\n\r\n// Print the centroids\r\nInstances centroids = kMeans.getClusterCentroids();\r\nfor (Instance centroid: centroids) {\r\n  System.out.println(centroid);\r\n}\r\n\r\n// Print cluster membership for each instance\r\nfor (Instance point: data) {\r\n  System.out.println(point + \" is in cluster \" + kMeans.clusterInstance(point));\r\n} Python (Scikit-learn) >>> from sklearn import cluster, datasets\r\n>>> iris = datasets.load_iris()\r\n>>> X_iris = iris.data\r\n>>> y_iris = iris.target\r\n\r\n>>> k_means = cluster.KMeans(n_clusters=3)\r\n>>> k_means.fit(X_iris)\r\nKMeans(copy_x=True, init='k-means++', ...\r\n>>> print(k_means.labels_[::10])\r\n[1 1 1 1 1 0 0 0 0 0 2 2 2 2 2]\r\n>>> print(y_iris[::10])\r\n[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2] Ở trong ví dụ Python phía trên, ta sử dụng bộ dữ liệu  Iris  chứa kích thước đài hoa và cánh hoa cho 3 giống hoa Iris khác nhau, chia những dữ liệu này thành 3 cụm, rồi sau đó so sánh với giá trị thực tế của chúng, để kiểm tra độ chính xác của thuật toán. Trong trường hợp này, chúng ta thấy rằng dữ liệu được tách thành 3 cụm (ứng với 3 giống hoa) khác nhau và K-Means đã nhận ra chính xác những phần tử nào cùng nằm chung 1 cụm ( Chú ý rằng Unsupervised Learning là bài toán không có nhãn nên chỉ số k bằng (0, 1, 2) ở trên chỉ là ngẫu nhiên, có tác dụng phân biệt chứ không phải là nhãn đầu ra ). Tuy nhiên, làm cách nào mà ta chọn ra được số cụm (k) thích hợp? Câu hỏi tương tự như vậy thường rất phổ biến trong Học máy. Nếu chúng ta yêu cầu nhiều cụm hơn, dữ liệu sẽ được chia nhỏ ra, và giá trị error (tổng khoảng cách) cũng sẽ nhỏ hơn. Vậy, như thế có phải sẽ là tốt hơn nếu như ta chọn k lớn nhất có thể? Chúng ta có thể chọn k = m (số điểm), như thế mỗi điểm sẽ trở thành tâm của chính nó và mỗi cụm sẽ chỉ có 1 điểm? Điều đó không sai, error sẽ bằng 0, nhưng chúng ta sẽ không thể tìm được mô tả đơn giản cho dữ liệu, và mô hình thu được cũng không thể phủ được những điểm mới thêm vào. Vấn đề này có tên gọi là  overfitting , và tất nhiên chúng ta sẽ không mong gặp phải nó. Một cách để giải quyết vấn đề này là bổ sung thêm hàm phạt (penalty) cho số lượng cụm. Từ đó, mục tiêu của ta lúc này không chỉ còn giảm thiểu error, mà phải cân bằng cả  error + penalty . Giá trị error sẽ tiến dần tới 0 khi chúng ta tăng số lượng cụm, nhưng đồng thời penalty cũng tăng theo. Quá trình tăng số lượng cụm sẽ dừng lại khi mà lượng error giảm đi thấp hơn so với giá trị penalty, và kết quả thu được là kết quả tối ưu. Có một giải pháp sử dụng  Bayesian Information Criterion  (BIC) để tính k có tên gọi là X-Means [Pelleg and Moore, 2000]. Một thứ khác chúng ta cần quan tâm đó là hàm khoảng cách. Hiển nhiên, với những điểm nằm trong không gian, khoảng cách Euclid rõ ràng là hiệu quả nhất, nhưng đôi khi ta cần thêm vài \"mánh khóe\" cho những loại dữ liệu đặc trưng khác nhau, ví dụ như các giá trị rời rạc,... Việc này yêu cầu khá nhiều kiến thức chuyên ngành liên quan tới dữ liệu đó. Hoặc, chúng ta có thể nhờ tới sự trợ giúp của Học máy để huấn luyện ra hàm khoảng cách thích hợp nhất. Nếu bạn có 1 tập các dữ liệu huấn luyện (đã biết trước chúng được phân cụm thế nào qua nhãn của chúng), kĩ thuật  Supervised Learning  (học có giám sát) có thể được ứng dụng để tìm ra hàm khoảng cách thích hợp, rồi áp dụng nó vào trong dữ liệu cần phân cụm. Ngoài ra, có 1 thuật toán phân cụm khác có tên là  Expectation-Maximization  (EM) cũng gần tương tự với 2 giai đoạn được dùng trong K-Means. Nói chính xác thì K-Means có thể coi là 1 phiên bản đơn giản hơn của EM. Tuy nhiên, đừng nhầm lẫn chúng với nhau mặc dù có rất nhiều điểm chung giữa 2 thuật toán này. EM Clustering Như vậy, với K-Means: mỗi điểm sẽ được gán cho 1 nhóm và mỗi nhóm được đại diện bởi 1 tâm. Điều này không quá phức tạp, vì chúng ta chưa gặp phải vấn đề cụm chồng chéo, hoặc những cụm có hình dạng khác hình tròn. Với  EM , ta bây giờ có thể tiến một bước xa hơn nữa và đặc tả mỗi cụm bằng tâm của nó (kì vọng), covariance (hiệp phương sai - qua đó ta có thể biểu diễn được cụm hình elip) và weight (kích thước của cụm). Xác suất mà 1 điểm thuộc về 1 cụm bây giờ được tính bằng xác suất phân phối Gauss đa biến.  Chúng ta sẽ bắt đầu EM bằng cách tính, với mỗi điểm, xác suất mà nó thuộc về từng cụm là bao nhiêu (tất nhiên, các cụm ban đầu cũng được khởi tạo ngẫu nhiên). Đây là bước E-step. Nếu 1 cụm là \"ứng viên\" tốt đối với 1 điểm, nó sẽ có xác suất gần với 1. Tuy nhiên, có xảy ra trường hợp 2 hay nhiều cụm cùng là ứng viên tốt, do đó điểm của chúng ta lúc này sẽ có phân phối xác suất giữa các cụm. Tính chất này của thuật toán được gọi là \"soft clustering\". Bước M-step bây giờ tính toán lại các tham số của mỗi cụm, bằng cách sử dụng kết quả xác suất của các điểm được tính ở bước E-step. Để tính toán tâm mới, covariance mới và weight mới của 1 cụm, mỗi dữ liệu điểm sẽ được đánh trọng số tỉ lệ thuận với xác suất biến cố \"điểm đó thuộc cụm\" (lấy từ E-step). Luân phiên 2 bước này sẽ làm tăng giá trị log-likelihood của hàm xác suất cho tới khi giá trị này hội tụ tới cực đại. Nói thêm, tương tự với K-Means, thuật toán EM chỉ cho ta giá trị cực đại địa phương, vì vậy ta có thể sẽ cần phải thực hiện thuật toán nhiều lần để tìm được mô hình tốt hơn nữa. Nếu ta muốn đưa ra quyết định 1 điểm bất kỳ thuộc cụm nào, đơn giản chỉ cần chọn cụm cho ta giá trị xác suất cao nhất ứng với điểm đó. Và ta cũng có thể hoàn toàn tái tạo lại được 1 mẫu tương tự như dữ liệu ban đầu từ mô hình dựa vào dãy các xác suất thu được. Bài viết gốc:  https://www.toptal.com/machine-learning/clustering-algorithms AlphaGo Vs TensorFlow - Trí thông minh nhân tạo trong tay bạn 01/07/2016 \n                                                Techmaster team\n                                             Blog Home TensorFlow vs Google AI - giấc mơ sâu của Google 04/07/2016 \n                                            Techmaster team\n                                         \n                                    Bởi  Phan Đức Việt Tác giả đang bận code dạo (PHP, Java, C#, Nodejs, React) kiếm tiền mua đất cưới vợ nên chưa viết đoạn mô tả. Techmaster.  Quy định  Về chúng tôi Việc làm Về chúng tôi Giảng viên Cơ sở vật chất Contact Ms Khuê: 090.863.6458 khue@techmaster.vn Mr Cường: 090.220.9011 cuong@techmaster.vn Địa chỉ\n                          Số 78, ngõ 106, Hoàng Quốc Việt, Cầu Giấy, Hà Nội  Giờ mở cửa:  9:00  đến  18:00 Đăng nhập Đăng ký Ghi nhớ tài khoản Đăng nhập Quên mật khẩu? Facebook Google Hiển thị mật khẩu \n                    Đăng ký\n                 \n                    Bằng cách nhấp vào Đăng ký, bạn đã đồng ý với các\n                      Quy định   của chúng tôi.\n                  Hỗ trợ trực tuyến"
        },
        {
          "title": "Khái quát về máy học",
          "relevance": "1",
          "url": "http://viet.jnlp.org/kien-thuc-co-ban-ve-xu-ly-ngon-ngu-tu-nhien/machine-learning-trong-nlp/khai-quat-ve-may-hoc",
          "content": "Tìm kiếm trang web này Trang chủ LY NAM PHONG Lưu Tuấn Anh Nguyễn Văn Hải Các công cụ xử lý Trích lọc tiếng Việt từ HTML DongDu Download dữ liệu Giới thiệu về các nghiên cứu mới [Máy học]Learning Combination Features with L1 Regularization [phân loại]Text Categorization with All Substring Features  [in Japanese] Automatic Tree and String Based Wrapper Generation for Semi-structured Documents Extracting Structured Data from Web Pages Online Feature Selection using Grafting Kiến thức cơ bản về xử lý ngôn ngữ tự nhiên Khởi đầu NLP với Python Liblinear-thư viện học máy Lựa chọn đặc trưng (Feature selection) Machine Learning trong NLP Mô hình ngôn ngữ NLP là gì ? Phân nhóm dữ liệu (Clustering) Thuật toán tách từ (Tokenizer) Xử lý tiếng Việt bằng Python (1) Ứng dụng Pointwise để tách từ Nghiên cứu của tác giả Bài toán thêm dấu cho tiếng Việt Việt hoá Mecab Nhập môn Linux SHELL là gì SHELL mạnh nhất : zsh Tài nguyên ngôn ngữ tiếng Việt Khái yếu về corpus Khái yếu về từ điển Kế hoạch xây dựng tự động corpus từ nguồn Web Đặc trưng của tiếng Việt Tạp đàm seminar là gì Sơ đồ trang web Hoạt động gần đây của trang web Tác giả trang anh tháng hai 5, 2012 Kiến thức cơ bản về xử lý ngôn ngữ tự nhiên ‎ > ‎ Machine Learning trong NLP ‎ > ‎\n   Khái quát về máy học Máy học  Bách khoa\ntoàn thư mở Wikipedia Học máy , có tài liệu gọi là  Máy học ,\n(tiếng Anh:  machine learning ) là một lĩnh vực của trí tuệ nhân\ntạo liên quan đến việc phát triển các kĩ thuật cho phép các máy tính có thể\n\"học\". Cụ thể hơn, học máy là một phương pháp để tạo ra các chương\ntrình máy tính bằng việc phân tích các tập dữ liệu. Học máy có liên quan lớn đến thống\nkê, vì cả hai lĩnh vực đều nghiên cứu việc phân tích dữ liệu, nhưng khác với thống\nkê, học máy tập trung vào sự phức tạp của các giải thuật trong việc thực thi\ntính toán. Nhiều bài toán suy luận được xếp vào loại bài toán NP-khó, vì\nthế một phần của học máy là nghiên cứu sự phát triển các giải thuật suy luận xấp\nxỉ mà có thể xử lí được. Học máy có tính ứng dụng\nrất cao bao gồm máy truy tìm dữ liệu, chẩn đoán y khoa, phát hiện thẻ\ntín dụng giả, phân tích thị trường chứng khoán, phân loại các chuỗi\nDNA, nhận dạng tiếng nói và chữ viết, dịch tự động, chơi\ntrò chơi và cử động rô-bốt ( robot locomotion ). Tương tác với con người Một số hệ thống học máy nỗ lực loại bỏ nhu cầu trực giác của con người\ntrong việc phân tích dữ liệu, trong khi các hệ thống khác hướng đến việc tăng sự\ncộng tác giữa người và máy.  Không\nthể loại bỏ hoàn toàn tác động của con người vì các nhà thiết kế hệ thống phải\nchỉ định cách biểu diễn của dữ liệu và những cơ chế nào sẽ được dùng để tìm kiếm\ncác đặc tính của dữ liệu. Học máy có thể được xem là một nỗ lực để tự động hóa\nmột số phần của   phương pháp\nkhoa học . Một số nhà nghiên cứu học máy tạo ra các phương\npháp bên trong các khuôn khổ của   thống kê\nBayes .   Các loại giải thuật Các thuật\ntoán học máy được phân loại theo kết quả mong muốn của thuật toán. Các loại\nthuật toán thường dùng bao gồm: §    Học\ncó giám sát -- trong đó, thuật toán tạo ra một hàm ánh xạ dữ liệu vào tới\nkết quả mong muốn. Một phát biểu chuẩn về một việc học có giám sát là bài toán  phân loại : chương trình cần học (cách xấp\nxỉ biểu hiện của) một hàm ánh xạ một vector [X1,X2,..,Xn]  tới\nmột vài lớp bằng cách xem xét một số mẫu dữ_liệu - kết_quả của hàm đó. §    Học\nkhông giám sát -- mô hình hóa một tập dữ liệu, không có sẵn các ví dụ đã\nđược gắn nhãn. §    Học\nnửa giám sát -- kết hợp các ví dụ có gắn nhãn và không gắn nhãn để sinh một\nhàm hoặc một bộ phân loại thích hợp. §    Học\ntăng cường -- trong đó, thuật toán học một chính sách hành động tùy theo\ncác quan sát về thế giới. Mỗi hành động đều có tác động tới môi trường, và môi\ntrường cung cấp thông tin phản hồi để hướng dẫn cho thuật toán của quá trình học. §    Chuyển đổi  -- tương tự\nhọc có giám sát nhưng không xây dựng hàm một cách rõ ràng. Thay vì thế, cố gắng\nđoán kết quả mới dựa vào các dữ liệu huấn luyện, kết quả huấn luyện, và dữ liệu\nthử nghiệm có sẵn trong quá trình huấn luyện. §    Học cách học  -- trong\nđó thuật toán học thiên kiến quy nạp của chính mình, dựa theo các\nkinh nghiệm đã gặp. Phân tích hiệu\nquả các thuật toán học máy là một nhánh của ngành  thống kê ,\nđược biết với tên  lý thuyết học điện toán . Các chủ đề về học máy Danh sách các\nchủ đề của môn học này: §    Mô\nhình hóa các hàm mật độ xác suất điều kiện: hồi quy và phân\nloại §    Mạng\nnơ-ron §    Cây\nquyết định §    Lập\ntrình biểu thức gen §    Lập\ntrình di truyền §    Hồi\nquy quá trình Gauss §    Phân\ntích biệt thức tuyến tính §    k\nláng giềng gần nhất §    Độ\ndài thông điệp tối thiểu §    Cảm\ntri nguyên §    Hàm\ncơ sở xuyên tâm §    Máy\nhỗ trợ vector §    Mô\nhình hóa các hàm mật độ xác suất qua các mô hình phát sinh: §    Thuật\ntoán cực đại kì vọng §    Các mô\nhình đồ họa gồm mạng Bayes và mạng Markov §    Ánh\nxạ topo phát sinh §    Các\nkỹ thuật suy luận xấp xỉ đúng: §    Chuỗi\nMarkov  phương pháp Monte Carlo §    Phương\npháp biến thiên §    Tối\nưu hóa: hầu hết các phương pháp trên đều sử dụng tối ưu hóa hoặc là các thể hiện\ncủa các thuật toán tối ưu hóa.   Comments Sign in | Recent Site Activity | Report Abuse | Print Page | Powered By Google Sites"
        },
        {
          "title": "Những nguồn nghiên cứu tốt nhất về Machine Learning cho người mới bắt đầu",
          "relevance": "0",
          "url": "https://blog.topdev.vn/nhung-nguon-nghien-cuu-tot-nhat-ve-machine-learning-cho-nguoi-moi-bat-dau/",
          "content": "Hot Jobs AMA Event HR Lập Trình Phỏng vấn Công nghệ Design Search TopDev Jobs TechTalk Devvui MobileDay VietNam Web Summit TopDev Onsite Salary Sign in Welcome! Log into your account your username your password Forgot your password? Get help Password recovery Recover your password your email A password will be e-mailed to you. TopDev Blog Hot Jobs AMA Event HR Lập Trình Phỏng vấn Công nghệ Design Home Lập Trình Những nguồn nghiên cứu tốt nhất về Machine Learning cho người mới... Lập Trình Những nguồn nghiên cứu tốt nhất về Machine Learning cho người mới bắt đầu 441 Share on Facebook Tweet on Twitter Bài viết này sẽ liệt kê một số nguồn tài nguyên tốt nhất dành cho người mới bắt đầu học về  Machine Learning. Các thư viện lập trình Sau đây là những thư viện lập trình  Machine Learning  mã nguồn mở tốt nhất hiện nay. Tôi không nghĩ rằng tất cả đều thích hợp để sử dụng trong các hệ thống thực tế, nhưng chúng là cơ sở lý tưởng để bạn học tập, tìm hiểu và tạo nguyên mẫu (prototyping). Hãy bắt đầu với 1 thư viện trong 1 ngôn ngữ mà bạn biết rõ, sau đó chuyển sang các thư viện khác mạnh hơn. Nếu là một lập trình viên giỏi, thì bạn có thể dễ dàng chuyển từ ngôn ngữ này sang ngôn ngữ khác. Tất cả đều có cùng một logic, chỉ khác nhau về cú pháp và APIs. R Project for Statistical Computing : Đây là môi trường và ngôn ngữ kịch bản tương tự như lisp. Tất cả các số liệu thống kê sẽ được cung cấp qua ngôn ngữ R, bao gồm cả vẽ đồ thị. Ở mục  Machine Learning  trên CRAN (các package về Machine Learning của bên thứ ba) có phần code được viết bởi các chuyên gia trong lĩnh vực này. Nếu muốn tạo nguyên mẫu (prototype) thì bạn nhất định phải học R WEKA : Đây là một Data Mining workbench cung cấp APIs, một số dòng lệnh và giao diện đồ họa cho toàn bộ vòng đời khai thác dữ liệu (data mining). Bạn có thể chuẩn bị dữ liệu, tìm hiểu trực quan, phân loại, hồi quy và các mô hình phân nhóm và nhiều thuật toán được cung cấp qua các plugin của bên thứ ba. Không liên quan đến WEKA,  Mahout  là một framework Java rất tốt cho Machine Learning trên cơ sở hạ tầng Hadoop. Nếu mới bắt đầu với lĩnh vực big data và  machine learning , thì hãy gắn bó với WEKA và học nó từng phần một. Scikit Learn : Machine Learning trong Python được xây dựng dựa trên NumPy và Scipy. Thư viện này rất phù hợp với các lập trình viên Python hoặc Ruby. Nó rất dễ sử dụng, mạnh mẽ và đi kèm với tài liệu hướng dẫn tuyệt vời.  Orange  sẽ là một lựa chọn tốt nếu bạn muốn thử nghiệm điều gì đó mới mẻ. Octave : Nếu đã quen thuộc với Matlab hoặc là 1 lập trình viên NumPy đang tìm kiếm 1 cái gì đó khác biệt, thì hãy thử Octave. Octave là môi trường hỗ trợ tính toán số, tương tự như Matlab, có thể viết các chương trình giải quyết dễ dàng vấn đề tuyến tính và phi tuyến tính. Các bạn đã có nền tảng về kỹ thuật thì nên bắt đầu với Octave. BigML : Bạn không muốn viết code. Bạn có thể sử dụng các công cụ như WEKA mà hoàn toàn không cần lập trình chút nào cả hoặc sử dụng các dịch vụ như BigML cung cấp các interface machine learning trên web, nơi mà giúp bạn khám phá tất cả các mô hình xây dựng trong trình duyệt. Tham gia ngay sự kiện Machine Learning – Công nghệ của Tương lai! Chọn lấy 1 nền tảng và dùng nó để thực hành trong quá trình học về machine learning. Đừng chỉ đọc lý thuyết suông, hãy bắt tay vào làm. Các khóa học bằng video Stanford Machine Learning : gồm các bài giảng của Andrew Ng thuộc trường đại học Stanford trên Coursera. Sau khi ghi danh, bạn có thể xem tất cả các bài giảng bất cứ lúc nào và nhận được những kiến thức thực tế như trong khóa học  Stanford CS229.  Khóa học này có cả bài tập về nhà và bài kiểm tra tập trung vào đại số tuyến tính và cách sử dụng Octave. Caltech Learning from Data : được giảng dạy bởi Yaser Abu-Mostafa. Mọi bài giảng và tài liệu của khóa học đã có sẵn trên trang web  CalTech.  Tương tự như Stanford, tùy theo  tốc độ mà bạn muốn,  bạn có thể học và hoàn thành các bài tập được giao. Caltech Learning from Data gồm các chủ đề đi sâu về chi tiết và tập trung nhiều ở mảng toán học. Với các bạn mới bắt đầu, bài tập về nhà sẽ là thách thức không nhỏ. Machine Learning Category on VideoLectures.Net : Hãy tìm những video được nhiều người xem và được đánh giá cao nhất vì có rất nhiều nội dung ở đây. “Getting In Shape For The Sport Of Data Science” – Talk by Jeremy Howard : Bài trao đổi rất có giá trị 1 nhóm users group R. TopDev Techtalk #52: Công nghệ của tương lai: Đột phá gì từ Machine Learning? *Hồ Chí Minh:  18h00 – 21h00 ngày 02/12/2016. [HCM] Big Data Developer | Attractive Salary  — TopDev’s Client [Đà Nẵng] URGENT 10 Java Developers | Lương từ $700 – $1000  — Minori Technology [HN] URGENT !!! Backend Developer | 12 triệu – 18 triệu  — JAMJA Jobs by  Một số tài liệu khác The Discipline of Machine Learning : Đây là một sách trắng (sách trắng là bản báo cáo hoặc bản hướng dẫn của cơ quan có thẩm quyền với mục đích giúp người đọc hiểu về 1 vấn đề, giải quyết 1 vấn đề hoặc ra 1 quyết định) định nghĩa các nguyên tắc của Machine Learning được chấm bút bởi tác giả Tom Mitchell. Đây là một phần trong tài liệu mà Mitchell sử dụng để thuyết phục hiệu trưởng của trường ĐH CMU thành lập khoa Machine Learning độc lập (xem cuộc phỏng vấn ngắn với Tom Mitchell  tại đây ). A Few Useful Things to Know about Machine Learning : Tài liệu tuyệt vời đề cập đến các thuật toán cụ thể và thúc đẩy 1 số vấn đề quan trọng như khả năng khái quát lựa chọn tính năng (feature selection generalizability) và tính đơn giản của mô hình. Đây đều là những kiến thức quan trọng với những bạn mới bắt đầu Sách về Machine Learning cho người mới bắt đầu Programming Collective Intelligence: Building Smart Web 2.0 Applications : Quyển sách được viết cho đối tượng lập trình viên. Nó nhẹ về mặt lý thuyết, nặng về ví dụ lập trình cũng như những giải pháp và vấn đề thực tiễn trên web. Machine Learning for Hackers : Tôi khuyên bạn nên đọc cuốn sách này sau khi đọc cuốn  Programming Collective Intelligence  (ở trên). Machine Learning for Hackers cung cấp các ví dụ thực tế, nhưng thiên về phân tích dữ liệu và sử dụng R. Tôi thực sự thích cuốn sách này! Machine Learning: An Algorithmic Perspective : Cuốn sách này giống như một phiên bản nâng cao của cuốn  Programming Collective Intelligence  (ở trên), nhắm đến các đối tượng lập trình viên bắt đầu nghiên cứu Machine Learning. Sách cũng bao gồm toán học và các ví dụ viết bằng Python. Bạn nên đọc cuốn sách này sau khi đọc cuốn  Programming Collective Intelligence . Data Mining: Practical Machine Learning Tools and Techniques, Third Edition : Tôi đã bắt đầu bằng cuốn sách này với phiên bản đầu tiên xuất bản vào năm 2000. Lúc đó tôi là 1 lập trình viên Java nên cuốn sách này kết hợp với thư viện WEKA trở thành 1 môi trường hoàn hảo để tôi thử nghiệm, thực thi các thuật toán của mình. Machine Learning : Một cuốn sách cũ gồm các công thức và rất nhiều tài liệu tham khảo. Nội dung của sác rõ ràng và dễ dàng tiếp cận thông qua các thuật toán. Tất nhiên ngoài kia cũng có rất nhiều cuốn sách tuyệt vời khác về Machine Learning, nhưng tôi nghĩ những tài liệu trên đây mới thực sự phù hợp với những bạn mới tìm hiểu về lĩnh vực này. Nguồn:  Techmaster  Facebook Twitter Previous article Sản phẩm tốt là phải giải quyết được vấn đề của user Next article Thủ thuật mới sử dụng công nghệ Machine Learning của Google Translate ‘Toát mồ hôi’ phỏng vấn tuyển dụng vào Apple Vue.js vs jQuery: So sánh qua 1 số ví dụ 9 kiểu người tới cuối đời cũng không thể thành công ‘Toát mồ hôi’ phỏng vấn tuyển dụng vào Apple October 23, 2017 \r\n                    Apple là một trong những môi trường làm việc phấn khích nhất, nhưng cũng thách thức nhất thế giới. Vì thế, được làm việc...                 Vue.js vs jQuery: So sánh qua 1 số ví dụ October 22, 2017 \r\n                    Giới thiệu\r\nVue.js là gì? Vue.js khác gì so với jQuery? Tôi có nên khoan hãy đụng đến jQuery nếu đã biết Vue.js? Có thể...                 9 kiểu người tới cuối đời cũng không thể thành công October 22, 2017 \r\n                    Càng ngày, mối liên hệ giữa kinh nghiệm và kiến thức với sự thành công trong công việc càng giảm. Bạn không thể thành...                 Những Visual Studio Code Extensions không thể bỏ qua khi lập trình Angular v2+ October 21, 2017 \r\n                    Visual Studio Code là công cụ editor hiệu quả giúp đời sống coding trở nên dễ dàng hơn, đặc biệt là các developer Javascript.\r\n\r\n\r\n\r\n\r\nĐiều khiến...                 “Ở Việt Nam, cơ hội để thực sự làm về Trí tuệ nhân tạo... October 21, 2017 \r\n                    Sau 2 mùa tuyển sinh, IP (Intelligence Program) vẫn là chương trình đào tạo (miễn phí) hiếm hoi trên thị trường về lĩnh vực...                 “Ở Việt Nam, cơ hội để thực sự làm về Trí tuệ nhân tạo còn quá ít, trong khi những thứ mang hình thù... October 21, 2017 Họp báo ra mắt sự kiện Vietnam Web Summit 2017 October 19, 2017 Tôi đã bắt đầu nghiên cứu AI trong 2 tháng gần đây như thế nào? October 17, 2017 Việt Nam là một trong những nước hàng đầu của thị trường dịch vụ đám mây tại Đông Nam Á October 13, 2017 Sử dụng Machine Learning trên Compute Engine để recommend sản phẩm (phần 2) October 12, 2017 ABOUT US TopDev is a recruitment network and ecosystem in Mobile & IT fields. We are top leading recruitment network in Mobile &  IT fields, in Vietnam, offering Tech talent solution which meets your needs. Our networks cover 95% of Tech communities in Vietnam and our clients come from both Vietnam and South East Asia.\r\n Contact us:  contact@applancer.net FOLLOW US TopDev Jobs TechTalk Devvui MobileDay VietNam Web Summit TopDev Onsite Salary \r\n                    © Applancer TopDev                 MORE STORIES ‘Toát mồ hôi’ phỏng vấn tuyển dụng vào Apple October 23, 2017 Vue.js vs jQuery: So sánh qua 1 số ví dụ October 22, 2017 9 kiểu người tới cuối đời cũng không thể thành công October 22, 2017"
        },
        {
          "title": "Thuật ngữ Machine Learning là gì, tại sao các công ty lớn đều đổ hàng tỷ USD vào đây?",
          "relevance": "1",
          "url": "http://genk.vn/thuat-ngu-machine-learning-la-gi-tai-sao-cac-cong-ty-lon-deu-do-hang-ty-usd-vao-day-20160629105623677.chn",
          "content": "Gamek Kenh14 Cafebiz Mobile Điện thoại Máy tính bảng INTERNET Digital Marketing Media KHÁM PHÁ Lịch sử Tri thức TRÀ ĐÁ CÔNG NGHỆ Tản mạn Ý tưởng sáng tạo TIN ICT THỦ THUẬT Sống APPS - GAMES ĐỒ CHƠI SỐ VIDEO Mobile Tin ICT Internet Khám phá Video Trang chủ › Tri thức \r\nThuật ngữ Machine Learning là gì, tại sao các công ty lớn đều đổ hàng tỷ USD vào đây?\r\n NPQM  ,  Theo  Trí Thức Trẻ Bình luận  0 Chia sẻ  NASA nghiên cứu ứng dụng có thể dự đoán tuổi thọ của máy móc 10 lần chạm trán kinh điển giữa loài người và máy móc: ngoài cờ vây còn có cả StarCraft Có bao giờ bạn đi tìm lời giải đáp cho câu hỏi liệu khả năng thật sự của máy móc có thể đạt tới đâu trong thời đại hiện nay hay không? Một trong những vai trò của công nghệ hiện nay liên quan đến việc phát triển cũng như hoàn thiện những dịch vụ trên nền tảng smartphone và web đó là khả năng học hỏi, nhận thức của hệ thống máy móc (Machine Learning). Đôi khi, khái niệm trên bị hiểu và sử dụng như một cách gọi khác của “trí tuệ nhân tạo” (AI), đặc biệt là khi những tên tuổi lớn trong giới công nghệ muốn quảng bá hình ảnh về phát minh tiên tiến mới nhất của họ. Tuy nhiên, hai phạm trù trên, dù có một mối liên hệ nhất định, nhưng lại thuộc về những khía cạnh hoàn toàn khác biết trong lĩnh vực công nghệ máy tính. Mục đích của những dự án AI là tạo ra một cỗ máy có khả năng mô phỏng lại bộ não của con người, và tất nhiên, để đạt được điều đó, các nhà khoa học phải tìm được ra cách gán cho chúng kỹ năng nhận thức và tiếp thu vốn có của não bộ. Dù vậy, giới hạn của trí thông minh nhân tạo không chỉ dừng lại ở đó, mà còn bao gồm cả lưu trữ và biểu hiện kiến thức, lập luận, hay thậm chí cả tư duy trừu tượng. Mặt khác, Machine Learning chỉ tập trung vào những phần mềm, ứng dụng được viết ra để giúp máy móc “học tập” từ những sự kiện, trải nghiệm và phản hồi nhận được trước đó. Một điều nữa có thể khiến bạn cảm thấy bất ngờ và ngạc nhiên là Machine Learning thực ra lại được áp dụng nhiều hơn trong những phân tích số liệu và thông tin thống kê so với AI. Tại sao lại như vậy? Chẳng phải AI đi kèm với rất nhiều ưu điểm và tiềm năng vượt trội hơn để có thể hỗ trợ con người sao? Để giải đáp cho những thắc mắc, câu hỏi trên, hãy cùng nhìn sâu hơn nữa vào bản chất thật sự của vấn đề này. Một trong số những định nghĩa được lấy làm tiêu chuẩn mẫu mực nhất giải thích cho Machine Learning đã được phát biểu bởi Giáo sư Tom Mitchell tại Đại học Carnegie Mellon (CMU), cho rằng: Một chương trình máy tính được coi là có khả năng học tập từ những “kinh nghiệm” (E) khi thực hiện một số tác vụ (Task) và đối chiếu với thước đo hiệu suất (Performance), KHI hiệu quả công việc tại tác vụ T, được đo bằng thang P đã cải thiện bằng kinh nghiệm E. Nghe có vẻ khá phức tạp và rắc rối phải không? Do đó, giải thích một cách đơn giản, dễ hiểu hơn: Nếu một hệ thống máy tính có thể cải thiện hiệu suất làm việc chỉ bằng cách tận dụng những dữ liệu thu thập trước đó, bạn có thể kết luận rằng nó thực sự đã biết “học hỏi” một cách đúng đắn. Điều này mang những đặc điểm riêng biệt so với một chương trình cũng được lập ra để hỗ trợ hoàn thành những tác vụ tương tự, nhưng đó là vì lập trình viên đã xác định, vạch rõ hoàn toàn từng giới hạn, phạm vi và đường đi nước bước cũng như dữ liệu nó cần để thỏa mãn yêu cầu đặt ra. Chẳng hạn, một ứng dụng máy tính có khả năng chơi cờ caro là do đã được tích hợp những mã code có sẵn liên quan đến việc tìm cách để chiến thắng đối thủ. Nhưng nếu một chương trình khác đơn thuần chỉ được cung cấp và lập trình dữ liệu về luật chơi và cách giành chiến thắng mà không hề có một chiến thuật, nước đi nào được cài sẵn, đó là khi máy móc cần phải tiếp thu và học hỏi qua việc liên tục chơi đi chơi lại ván cờ, tự rút ra những cách thức phù hợp của riêng nó để có thể đạt được mục đích thắng cuộc. Khái niệm này không chỉ được áp dụng trong những trò chơi điện tử, mà còn dính dáng đến cả những phần mềm nắm giữ chức năng phân loại và dự đoán.  Phân loại  là quy trình mà qua đó máy móc có thể nhận biết và phân chia một tập hợp dữ liệu, bao gồm cả những thông tin về hình ảnh và số liệu.  Dự đoán  là phạm vi mà một hệ thống phụ thuộc vào để đưa ra những nhận định về giá trị của một thứ dựa trên những số liệu thống kê thu thập được trước đó. Chẳng hạn như việc nêu lên những đặc điểm lý tưởng mà một ngôi nhà cần có, từ đó đưa ra tiên đoán về số tiền cần bỏ ra để sở hữu ngôi nhà dựa vào những thông tin giao dịch liên quan. Nhìn chung, toàn bộ những dẫn chứng minh họa và cách giải thích như trên đã dẫn đến một định nghĩa khác cho Machine Learning, đó là sự đúc kết kiến thức và kinh nghiệm từ dữ liệu. Tương tự như việc bạn đương đầu với một bài toán hóc búa mà câu trả lời cho nó chắc chắn sẽ nằm trong những giả thiết ban đầu được đưa ra - Đó cũng chính là lý do tại sao công nghệ này lại có một mối liên hệ mật thiết với khía cạnh khai thác thông tin và số liệu thống kê. Các loại hình Machine Learning Ứng dụng công nghệ biết học tập và tiếp thu có thể được chia ra thành ba phạm trù chính: giám sát, không giám sát và củng cố. Loại đầu tiên -  giám sát  - thuộc về những trường hợp mà máy móc sử dụng, truy cập những thông tin có nguồn gốc uy tín, cụ thể. Điều này có nghĩa dữ liệu trên vốn đã được đánh dấu, đính kèm với một kết quả đúng và hợp lý. Chẳng hạn: Đây là một bức ảnh về chữ cái A. Kia là lá cờ của nước Anh, có 3 màu đỏ, xanh trắng .v.v… Càng nhiều dữ liệu, kiến thức và tốc độ máy tính học hỏi càng nhanh. Sau một thời gian trau dồi và “luyện tập”, khi bắt gặp phải một vấn đề chưa từng được thông qua trước đó, máy tính sẽ sử dụng đến những thuật toán đúc rút từ quá trình trước đó để tìm ra câu trả lời chính xác. Kiểu hình học hỏi  không giám sát  liên quan đến những dữ liệu có nguồn gốc không rõ ràng, xác thực so với loại đầu tiên. Thuật toán học hỏi sẽ không thu nhận được bản chất thật sự của thông tin: Đây là một chữ cái, nhưng chấm hết, không có thêm một đầu mối nào cả. Kia là hình ảnh của một lá cờ, nhưng lại không được cung cấp tên gợi ý. Nhìn tổng thể, học hỏi không giám sát cũng giống như nghe một buổi phát sóng radio của nước ngoài mà bạn không hiểu người ta đang nói ngôn ngữ gì. Nực cười là bạn cũng không có bất cứ một quyển từ điển nào trong tay hay một người trợ giúp bên cạnh để giúp bạn hiểu được phần nào những gì mình đang nghe. Ban đầu, nghe một bản radio tất nhiên là không mang lại tác dụng gì cả, thậm chí gần như là vô ích vì không có giá trị gì được ghi lại. Thế nhưng, sau hàng trăm lần như vậy, bộ não của bạn sẽ bắt đầu nhận ra những tín hiệu ban đầu giúp bạn “bắt bài” được một số từ vựng, ngữ pháp nhất định, từ đó dần dần phát triển kỹ năng ngôn ngữ cũng như đọc hiểu. Trong quá trình đó, nếu may mắn nhận được một quyển từ điển hay thuê một gia sư, chắc chắn mức độ tiến triển của bạn sẽ còn tiến xa hơn rất nhiều. Điểm mấu chốt gắn liền với loại hình không giám sát này là một khi kết hợp với các dữ liệu xác thực, chúng sẽ bất ngờ khiến cho toàn bộ những thuật toán trở nên hiệu quả và hoàn thiện. Chẳng hạn như hiện tại đang có hàng ngàn bức ảnh về chữ cái được thu thập, chỉ cần chữ A được xác nhận trong bộ nhớ, nó sẽ tác động đến toàn bộ thông tin còn lại, góp phần phân loại những thông tin được xử lý trước đó. Ưu điểm lớn nhất ở đây là quy trình này chỉ cần đến một tập hợp rất nhỏ những dữ liệu giám sát, dù chúng lại khó thu thập và sáng tạo ra hơn so với nhóm không giám sát. Tóm lại, điều đó cũng giúp biểu hiện tỷ lệ tương quan khá chênh lệch giữa hai loại hình học hỏi đầu tiên. Loại cuối cùng -  học hỏi củng cố  - có phần giống với kiểu thứ hai vì dữ liệu hướng đến cũng không có tính chất xác thực, tuy nhiên, khi xử lý và tìm kiếm lời giải đáp cho một vấn đề, kết quả cho ra sau đó sẽ được đánh giá và xếp hạng. Ví dụ tiêu biểu giải thích cho trường hợp này có thể được gán cho những trò chơi quen thuộc. Nếu máy tính thắng cuộc, kết quả chơi sẽ “chảy” ngược lại dòng xoay chuyển của dữ liệu, trở về với bộ nhớ để học tập, củng cố thêm những nước cờ khôn ngoan, chắc chắn hơn nữa, chuẩn bị cơ sở sẵn sàng cho những lần chơi sau. Khẳng định lại một lần nữa, nỗ lực này sẽ không thực sự tỏ ra hiệu quả nếu quá trình chơi chỉ kéo dài trong vài ván đấu ban đầu, nhưng sẽ là cả một sự tiến bộ đến bất ngờ nếu kết quả của hàng trăm lần chơi được tích lũy và đúc kết dần dần trở thành một “tuyển tập” những chiến thuật chơi đa dạng, phong phú. Cơ chế hoạt động Các kỹ sư và lập trình viên phối hợp với nhau dùng rất nhiều phương pháp để thiết kế nên một hệ thống máy móc có khả năng học hỏi hiệu quả. Như đã đề cập phía trên, hầu hết trong số đó gắn liền mật thiết với khía cạnh khai thác thông tin và số liệu. Chẳng hạn, nếu một tập hợp dữ liệu biểu hiện đặc điểm của nhiều đồng tiền xu, bao gồm trọng lượng và đường kính của chúng, bạn có thể sử dụng những kỹ thuật thống kê như thuật toán “láng giềng gần nhất” (nearest neighbors) để lọc ra thông tin của những đồng xu chưa được xử lý. Về cơ bản, thuật toán trên xem xét và phân tích những cách phân loại được áp dụng dựa vào khoảng cách gần nhất giữa các đối tượng cần xếp lớp. Số lượng “láng giềng” liên quan tới quyết định thực hiện thuật toán được biểu diễn bằng “k”, từ đó tên đầy đủ của phương pháp này là “k-nearest neighbors”. Dù vậy, vẫn còn đó rất nhiều công thức, thuật toán khác cũng đóng vai trò và mục đích tương tự, nhưng cách thức khác nhau. Cùng xem qua biểu đồ dưới đây để hiểu rõ hơn: Bức ảnh trên cùng bên trái là tập hợp dữ liệu ban đầu. Tập hợp đó được phân ra làm hai loại chính, đỏ và xanh. Mặc dù dữ liệu trên chỉ mang tính chất lý thuyết đặt ra, nhưng nó có thể đại diện cho hầu hết mọi thứ: trọng lượng và kích cỡ đồng xu, số cánh hoa của một bông hoa hoặc độ rộng… Bên cạnh đó, cũng có thể thấy dễ dàng thấy được một vài cụm nhóm nhất định, như mọi thứ ở phía trên bên trái đều thuộc nhóm màu đỏ, còn phía dưới bên phải thì ngược lại - màu xanh. Tuy nhiên, ở vị trí giữa lại nổi lên một phần giao thoa. Vậy nếu bạn thu được một mẫu thông tin “mới toanh” thuộc về phần này, làm sao có thể biết được chính xác nó thuộc về màu đỏ hay là xanh? Điều này có thể được giải thích bằng những thuật toán khác được hiển thị ở những biểu đồ còn lại, hoàn thành trọn vẹn vai trò phân loại các dữ liệu mới xuất hiện. Ngoài ra, nếu chẳng may một mẫu nào đó rơi vào vùng có màu trắng, thì xin chia buồn với bạn, phương pháp hiện tại không đủ khả thi để phân loại hết dữ liệu. Con số ở góc dưới bên phải diễn tả xác suất phân loại thành công của thuật toán đó. Mạng lưới thần kinh nhân tạo Một trong những từ ngữ thường thấy ở các công ty như Google và Facebook là “Mạng lưới thần kinh” (Neural Net). Đây là một hệ thống máy tính áp dụng Machine Learning, được mô phỏng theo cách vận hành và hoạt động của các neuron thần kinh trong bộ não con người. Xét về khía cạnh sinh học, các neuron sẽ truyền đi tín hiệu về bộ máy xử lý trung tâm dựa trên cách mà nó xử lý thông tin tiếp nhận ban đầu. Còn về phạm trù máy móc, tác vụ trên được thực hiện phụ thuộc vào những cấp số ma trận đi kèm với hàm số chuyển đổi. Ứng dụng của mạng lưới này trong những năm gần đây đã tăng lên với tốc độ chóng mặt, cụ thể là xu hướng sử dụng những hệ thống chuyên sâu tích hợp nhiều lớp neuron liên kết. Trong sự kiện Google I/O 2015, Phó Chủ tịch Phát triển Sản phẩm Sundar Pichai đã giải thích cơ chế học hỏi của máy móc, đồng thời giới thiệu về mạng lưới chuyên sâu đang làm nhiệm vụ hỗ trợ Google hoàn thành sứ mệnh “tổ chức, điều hành và phổ biến thông tin trên toàn thế giới”. Từ đó, ông lớn công nghệ đã cho ra mắt hàng loạt những phát kiến đáp ứng kỳ vọng của những tín đồ công nghệ như Google Now. Hơn nữa, nhờ có DNN - nền tảng mã nguồn mở cho phép phát triển các phần mềm cổng thông tin điện tử - Google như có thêm cánh tay phải đắc lực trong những dự án xây dựng hệ thống nhận diện giọng nói, tiếp nhận và xử lý ngôn ngữ cũng như phiên dịch. Hiện nay số lượng mạng lưới kết nối trong tay Google đã lên đến 30, một con số vô cùng ấn tượng. Tỉ lệ sai sót và nhầm lẫn trong những tính năng nhận diện giọng nói của công ty cũng giảm đi đáng kể: từ 23% trong năm 2013 xuống còn 8% trong năm 2015. Một vài ví dụ tiêu biểu Đúng vậy, việc các hãng công nghệ hàng đầu như Google và Facebook sử dụng Machine Learning để cải thiện chất lượng dịch vụ đã không còn là điều quá xa lạ. Vậy làm như thế có tác dụng gì, hay giúp họ có được thành quả ra sao? Một trong những lĩnh vực khá thú vị mà hệ thống này đảm nhận là công việc chú thích cho ảnh, hay nói đúng hơn là diễn tả lại nội dung được miêu tả và truyền tải bên trong đó. Dưới đây là một vài minh chứng cụ thể, giúp bạn có một hình dung rõ nét hơn về chức năng này: Hai bức ảnh đâu tiên khá chân thực và dễ nhìn, với chú thích chính xác và không có gì đáng phàn nàn. Nhưng hình ảnh cuối cùng thì lại khá khó hiểu khi máy tính, dù không khó khăn khi nhận ra được hộp bánh donut, nhưng lại nhầm lẫn cụm bánh ngọt kia là một cốc cafe. Không chỉ dừng ở đó, thuật toán nhận biết thậm chí còn nhầm lẫn có phần… quá đà một chút: Một công việc nữa cũng thú vị không kém đó là hệ thống này học cách viết chữ như một cá thể con người thực sự. Cleveland Amory, nhà văn, nhà báo và bình luận viên người Mỹ, từng viết: “In my day the schools taught two things, love of country and penmanship — now they don’t teach either” (tạm dịch: “Trong quá khứ, trường học dạy tôi hai điều: tình yêu nước và nghệ thuật thư pháp. Không còn gì trong số đó tồn tại đến ngày nay”). Không biết ông ấy sẽ nghĩ gì khi thấy áng văn trên của mình giờ có thể được viết lại bởi… một cỗ máy như dưới đây: Đây là sản phẩm của máy móc, tạo ra bởi một mạng lưới thần kinh nhân tạo. Để thực hiện được điều này, cần tới 221 người tham gia vào quá trình sử dụng một tấm bảng thông minh để viết câu văn trên. Trong thời gian đó, dữ liệu vị trí và cách di chuyển của ngòi bút được ghi lại dựa trên cảm biến hồng ngoại, từ đó cho ra kết quả là một tập hợp những tọa độ x và y sẽ được dùng để cung cấp thông tin cho công đoạn “học hỏi giám sát”. Qua đó, thành quả thu được thật sự không tồi đối với một chiếc máy tính. Thậm chí, nhiều phong cách viết khác nhau cũng có thể được biểu hiện, và cả những nét nguệch ngoạc có phần khá tự nhiên nữa. Mới đây, Google đã đưa ra công bố chính thức về việc áp dụng những hệ thống mạng thần kinh nhân tạo vào lĩnh vực mô phỏng, bắt chước các hình thức giao tiếp giữa con người. Cụ thể, các nhà nghiên cứu và lập trình đã “huấn luyện” cho máy móc của họ biết cách tận dụng 62 triệu mẫu câu khai thác từ phần phụ đề của các bộ phim. Kết quả thu được thật kinh ngạc! Trong một lần thử nghiệm, máy tính đã tự khẳng định mình  “không cảm thấy xấu hổ gì khi phải làm một nhà triết học” . Trong khi đó, khi đề cập đến khía cạnh luân thường đạo lý trong cuộc sống, hệ thống lại nhắc đến cách mà nó  “không thích tham gia vào những cuộc thảo luận mang tính triết học” . Có vẻ như nếu lỡ tay nhồi nhét vào “não” những kịch bản và khuôn mẫu phim của Hollywood, máy móc sẽ có xu hướng trở thành một triết gia tính khí thất thường như vậy đó! Kết luận Khác hẳn so với những lĩnh vực liên quan đến trí tuệ nhân tạo, Machine Learning không phải là một thứ gì đó trừu tượng, mơ hồ, mà thực sự là một công cụ hữu hình đóng vai trò quan trọng trong những dịch vụ mà con người sử dụng hằng ngày. Nói một cách khác, nó như một người hùng thầm lặng, một ngôi sao không được công nhận tài năng thực sự của mình, nỗ lực cố gắng phía sau bức màn sân khấu, kiểm tra và rà soát lại tất cả những công đoạn, thông tin để giúp mang lại kết quả tốt nhất cho màn diễn. Và đúng như những lời tâm sự sâu sắc của tác giả Douglas Adam trong Hitchhiker’s Guide to the Galaxy:  Đôi khi chúng ta cần phải hiểu bản chất thực sự của vấn đề trước khi có thể tiếp tục bước đi trên con đường tìm kiếm lời giải đáp thích đáng nhất! Tham khảo: AndroidAuthority Các nhà nghiên cứu tạo ra bàn tay robot linh hoạt như tay người, có thể học tập để tiến bộ như trẻ em mẫu giáo Tags: lập trình viên Trò chơi điện tử trí thông minh nhân tạo trí tuệ nhân tạo lĩnh vực công nghệ hiệu quả công việc Xem theo ngày Ngày 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 Tháng Tháng 1 Tháng 2 Tháng 3 Tháng 4 Tháng 5 Tháng 6 Tháng 7 Tháng 8 Tháng 9 Tháng 10 Tháng 11 Tháng 12 Năm 2017 2016 2015 2014 2013 Xem Scotland khánh thành trang trại năng lượng gió trên biển đầu tiên trên thế giới Dự án này của Scotland có thể cung cấp năng lượng cho khoảng 20.000 hộ gia đình. Tổ chức chống doping thế giới đề xuất cấm chỉnh sửa gen, bắt đầu vào năm 2018 Cơ quan Phòng chống doping Thế giới (WADA) tin rằng việc chỉnh sửa gen là một hành vi gian lận trong thi đấu thể thao khi nó có thể cải thiện thành tích của vận động viên. Phát minh ra loại vải tự làm sạch có thể dùng may đồ cho phi... 3 tuần trước Satya Nadella và những lần phá vỡ \"luật bất thành văn\" trên con... 3 tuần trước 13 sự thật ít người biết về cà phê 4 tuần trước Dự án mô phỏng môi trường sống trên sao Hỏa sẽ được khởi công... 4 tuần trước TIN NỔI BẬT Thiết bị phần cứng của Google chỉ là \"Con ngựa thành Troy\" Đây là nhà sản xuất smartphone lớn thứ hai thế giới sau Samsung mà có thể bạn chưa từng biết tới Không cần phải tốn thời gian cài đặt lại, Windows 10 Fall Creators sẽ cung cấp cho bạn một giải pháp hay hơn rất nhiều Đè bẹp cả Samsung lẫn Google, Apple đang thống trị \"bãi tha ma\" smartwatch/wearable như thế nào? Hãy biến thanh điều hướng ảo nhàm chán của Android trở nên sinh động hơn với hiệu ứng ảnh động vui mắt mà không cần root Video Mobile Tin ICT Internet Khám phá Trà đá công nghệ Thủ thuật Apps - Game Đồ chơi số \r\nChịu trách nhiệm quản lý nội dung: Bà Nguyễn Bích Minh \r\nHà Nội: Tầng 20, Tòa nhà Center Building - Hapulico Complex, Số 1 Nguyễn Huy Tưởng, Thanh Xuân, Hà Nội. \r\nEmail:  info@genk.vn \r\nĐiện thoại: 024.73095555, máy lẻ 62374 \r\nVPĐD tại TP.HCM: Tầng 4, Tòa nhà 123\r\n \r\nVõ Văn Tần, Phường 6, Quận 3, Tp. Hồ Chí Minh\r\n © Copyright 2010 - 2017 - Công ty Cổ phần VCCorp \r\nTầng 17, 19, 20, 21 Toà nhà Center Building - Hapulico Complex, Số 1 Nguyễn Huy Tưởng, Thanh Xuân, Hà Nội. \r\nTrang tin điện tử trên internet: Giấy phép số 460/GP-TTĐT do Sở Thông tin và Truyền thông Hà Nội cấp ngày 03/02/2016 Liên hệ quảng cáo \r\nHotline hỗ trợ quảng cáo: 0942 86 11 33  \r\nEmail:  giaitrixahoi@admicro.vn \r\nHỗ trợ & CSKH: Admicro\r\n  \r\n \r\nAddress: Tầng 20, Tòa nhà Center Building - Hapulico Complex, Số 1 Nguyễn Huy Tưởng, Thanh Xuân, Hà Nội."
        },
        {
          "title": "Menu",
          "relevance": "1",
          "url": "https://ongxuanhong.wordpress.com/2015/10/05/cach-xac-dinh-dang-bai-toan-trong-machine-learning/",
          "content": "Ông Xuân Hồng Chia sẻ kiến thức và thông tin về Machine learning Menu Skip to content Data Science Big Data Deep learning Exploratory Data Analysis Getting and cleaning data Machine learning Lập trình Python Spark R Weka Kiến thức Toán Statistical Inference Xử lý ngôn ngữ tự nhiên – Natural Language Processing (NLP) Chia sẻ Dự án About Cách xác định bài toán trong Machine Learning Tháng Mười 5, 2015 Tháng Sáu 7, 2017 Ông Xuân Hồng 25 phản hồi machine-learning-cheet-sheet Nếu tôi hỏi khách hàng xem họ muốn gì, có lẽ họ sẽ nói rằng họ muốn có một con ngựa biết chạy nhanh hơn \n– Henry Ford Cat or dog Rada signature Are these voltages normal for this season and time of day Regression Van Slot machine Constellations GPA Auto adjust temperature Facebook Twitter Google LinkedIn Like this: Số lượt thích Đang tải... Liên quan Điều hướng bài viết ←  Kỹ năng làm việc với Machine Learning Góp nhặt kinh nghiệm làm nghề Data scientist  → Duy Tuấn nói: \n\t\t\t\t\t\t\t\tTháng Mười 10, 2016 lúc 3:57 chiều\t\t\t\t\t\t\t anh có bài viết nào cụ thể về thuật toán reinforcement learning k a. Hiện tại thì e đang k biết xác định thuật toán nào cho project của mình. project của e là tự động nhận biết được nhiệt độ hiện tại và thực hiện một hành động gì đó thì lúc đầu e nghĩ là regression + reinforcement. Nhưng sau đó e lại nghĩ là chỉ mỗi reinforcement vì mình k cần tính trước nhiệt độ là bao nhiêu để hành động Số lượt thích Số lượt thích Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Mười 11, 2016 lúc 1:07 sáng\t\t\t\t\t\t\t Hi, cám ơn bạn đã quan tâm đến blog ML. Tôi sẽ cố gắng chia sẻ một bài viết liên quan đến đề tài này. Số lượt thích Số lượt thích Phản hồi Duy Tuấn nói: \n\t\t\t\t\t\t\t\tTháng Mười 11, 2016 lúc 4:43 sáng\t\t\t\t\t\t\t e muốn hỏi là loại thuật toán nào có thể thu thập thói quen người dùng một cách tự động, bởi vì các thuật toán như hồi quy, phân lớp đều có bước tiền xử lý dữ liệu đầu vào cả Số lượt thích Số lượt thích Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Mười 11, 2016 lúc 2:40 chiều\t\t\t\t\t\t\t anh nghĩ paper này liên quan đến đề tài của em  https://dspace.jaist.ac.jp/dspace/bitstream/10119/12984/4/paper.pdf Số lượt thích Số lượt thích Phản hồi Duy Tuấn nói: \n\t\t\t\t\t\t\t\tTháng Mười 12, 2016 lúc 2:57 sáng\t\t\t\t\t\t\t ôi trời đúng thứ e cần, thanks anh nhiều Số lượt thích Số lượt thích Phản hồi Duy Tuấn nói: \n\t\t\t\t\t\t\t\tTháng Mười 12, 2016 lúc 3:08 sáng\t\t\t\t\t\t\t ơ cơ mà nó giống với data mining hơn machine learning nhỉ Số lượt thích Số lượt thích Phản hồi Tuan nói: \n\t\t\t\t\t\t\t\tTháng Một 12, 2017 lúc 8:53 sáng\t\t\t\t\t\t\t Em chào anh! \nEm muốn hỏi về dạng bài toán “Nhận dạng”. Như trong nội dung bài viết của anh có đề cập tới việc nhận dạng loại máy bay từ radar. Giả sử trong trường hợp mình có thông tin về đặc trưng của các loại radar hoặc máy bay mang loại radar đó. Nếu mình có 1 bộ tham số đo đạc được thông số của radar nhằm mục đích xác định đây là loại nào thì dạng bài toán này thuộc loại nào ạ? Có tài liệu nào mô tả về dạng bài toán “Nhận dạng” kiểu này không ạ? Em cám ơn anh! Số lượt thích Số lượt thích Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Một 12, 2017 lúc 8:58 sáng\t\t\t\t\t\t\t Đây là bài toán multiclass-classification, trong đó tập nhãn sẽ là kết quả nhận dạng máy bay, feature là các thông số từ radar như tốc độ bay, hướng bay, kích thước vật thể, … \nEm có thể tìm đọc sách nói về multiclass-classification  https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738 Số lượt thích Số lượt thích Phản hồi Tuan nói: \n\t\t\t\t\t\t\t\tTháng Một 16, 2017 lúc 8:27 sáng\t\t\t\t\t\t\t Cám ơn anh về câu trả lời. \nAnh có thể cho em xin một vài keyword liên quan tới bài toán multi-classification như tên thuật toán có thể được áp dụng trong trường hợp này được không ạ! Số lượt thích Số lượt thích Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Một 16, 2017 lúc 8:33 sáng\t\t\t\t\t\t\t một số keyword: a multi-class version of SVMs, boosting type multi-class algorithm, decision trees, Neural networks multi-class Số lượt thích Số lượt thích Phản hồi Vũ Anh nói: \n\t\t\t\t\t\t\t\tTháng Một 16, 2017 lúc 2:44 chiều\t\t\t\t\t\t\t Tuấn có thể tham khảo ở đây http://scikit-learn.org/stable/modules/multiclass.html Số lượt thích Số lượt thích Phản hồi Ngọc Lưu nói: \n\t\t\t\t\t\t\t\tTháng Ba 7, 2017 lúc 4:00 chiều\t\t\t\t\t\t\t Em chào anh Hồng, Em đang tìm hiểu để làm chatbox tự động trả lời câu hỏi về một môn học cho học sinh/ sinh viên, anh có thể cho biết nhận định của anh về loại của bài toán này cũng như thuật toán phù hợp nhất không ạ. Cảm ơn anh. Số lượt thích Số lượt thích Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Ba 8, 2017 lúc 2:37 sáng\t\t\t\t\t\t\t Hi em, đây là bài toán Question and Answers. Tuỳ theo mục tiêu của hệ thống mà em sẽ xây dựng dựa trên truy vấn văn bản ở cấp thấp hoặc phát sinh câu trả lời mang nhiều ngữ nghĩa hơn ở cấp độ cao. \nNote này cũng khá căn bản, em đọc lấy ý tưởng bao quát nhé  https://web.stanford.edu/~jurafsky/slp3/28.pdf Số lượt thích Số lượt thích Phản hồi Ngọc Lưu nói: \n\t\t\t\t\t\t\t\tTháng Ba 8, 2017 lúc 2:13 chiều\t\t\t\t\t\t\t Cảm ơn anh Hồng  nhiều. Số lượt thích Số lượt thích Phản hồi Hoang Asdas nói: \n\t\t\t\t\t\t\t\tTháng Tư 12, 2017 lúc 7:22 chiều\t\t\t\t\t\t\t Chào anh Hồng,  bài viết của anh rất hay.  Hiện tại, trên lớp em đang có 1 project là xây dựng demo một hệ thống đa tác tử, ở đây demo sẽ là một game đá bóng, cầu thù sẽ là những tác tử phối hợp tương tác với nhau. Em  đang nghĩ tới reinforcement learning, anh có tài liệu gì không share e với. Cảm ơn anh rất rất nhiều. Hy vọng sẽ có thêm những bài viết bổ ích từ anh :)) Số lượt thích Số lượt thích Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Tư 13, 2017 lúc 2:13 sáng\t\t\t\t\t\t\t Cái này anh thua, em tham khảo các sách hoặc tutorial làm game nhé https://books.google.com.vn/books?id=gDLpyWtFacYC&pg=PA133&lpg=PA133&dq=soccer+game+programming&source=bl&ots=v0zj5V0n4l&sig=z9DCAsG_ikVAcnUb-w0w-5ulNNc&hl=en&sa=X&ved=0ahUKEwiA64qrraDTAhWIH5QKHe4ODywQ6AEINzAF#v=onepage&q=soccer%20game%20programming&f=false https://ss2d.wordpress.com/ Số lượt thích Số lượt thích Phản hồi Trần Thanh Tùng nói: \n\t\t\t\t\t\t\t\tTháng Năm 19, 2017 lúc 7:37 sáng\t\t\t\t\t\t\t Xin chào anh Hồng! Rất mừng và cám ơn những bài viết rất hữu ích của anh trên Blog cá nhân của anh. Mình đang gặp phải khó khăn trong việc xác định bài toán mà cụ thể là mô hình Deep Learning khi nghiên cứu Lab về ” Phát hiện bất thường”, sử dụng tensor flow để trainning tập dữ liệu KDD99, bài toán vừa xuất hiện phân lớp(classification) vừa xuất hiện hồi qui (regression) vậy mô hình kết hợp của cả 2 dạng này là gì? và cơ sở toán học của nó? Cám ơn Anh! Số lượt thích Số lượt thích Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Năm 19, 2017 lúc 8:41 sáng\t\t\t\t\t\t\t Hi, theo mình biết thì đây là bài toán Anomaly detection/Intrusion detection/Outlier detection. Có 2 hướng tiếp cận là classification và clustering. Hướng 1 thì ít được áp dùng do vấn đề class-unbalancing: các điểm bất thường xuất hiện quá ít nên không đủ dữ liệu để train. Hướng 2 được áp dụng nhiều hơn: sử dụng outlier detection, clustering, distribution… cơ sở toán ở đây chủ yếu áp dụng thống kê để phân tích. Nếu bạn áp dụng Deep learning để phân lớp dữ liệu đầu vào có bất thường không thì bạn có thể tham khảo OneClassSVM. Một số link bạn có thể tham khảo: https://www.datascience.com/blog/intro-to-anomaly-detection-learn-data-science-tutorials https://anomaly.io/blog/ http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/tkdd11.pdf http://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/ Số lượt thích Số lượt thích Phản hồi Tùng nói: \n\t\t\t\t\t\t\t\tTháng Năm 23, 2017 lúc 3:18 sáng\t\t\t\t\t\t\t Rất cám ơn anh đã hỗ trợ rất nhanh chóng, vấn đề khó khăn của mình của mình đó là trong quá trình làm việc với tensorflow để trainning Data theo phương pháp supervised learning thì mình cần nghiên cứu kỹ vấn đề lý thuyết liên quan cụ thể nào vậy Anh? Về Phân lớp hay Hồi qui hay một phương pháp thống kê phân tích nào vậy Anh? Cám ơn Anh! Số lượt thích Số lượt thích Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Năm 23, 2017 lúc 3:27 sáng\t\t\t\t\t\t\t Lý thuyết thì bạn có thể bắt đầu từ logistics regression để hiểu hàm phân lớp sigmoid hoạt động như thế nào trên tập dữ liệu đơn giản, hiểu được hàm này rồi bạn sẽ biết rằng thực tế người ta dùng hàm tanh là chủ yếu, sau đó bạn nghiên cứu mô hình ANN phân lớp nhị phân ra sao, từ đó nâng cao lên phân lớp cho multi-class bằng cách nào. Trong ANN, bạn nên nghiên cứu kĩ cách hoạt động của backpropagation, đây là kĩ thuật giúp bạn update tham số cho các layer lúc training và đầu ra của bạn là gì, ma trận tham số hay vector tham số. Lúc đem testing thì sử dụng model đã trained ra sao… \nBạn nên cài đặt tất cả các tiến trình trên bằng ngôn ngữ bất kỳ để hiểu rõ bản chất như Python hay Matlab đều được. Đến đây, bạn đã có khả năng nghiên cứu các biến thể của ANN như CNN, RNN, LSTM, … \nTensorflow là công cụ giúp abstract tất cả các quá trình trên, bạn không nên phụ thuộc vào framework này. Nếu hiểu nguyên lý rồi, bạn có thể áp dụng lên bất kỳ framework tương tự mà hiệu suất lại cao hơn như Torch, Theano, Caffe, … Số lượt thích Số lượt thích Phản hồi Tùng nói: \n\t\t\t\t\t\t\t\tTháng Sáu 2, 2017 lúc 1:30 sáng\t\t\t\t\t\t\t Thanks A! Số lượt thích Số lượt thích Phản hồi Trang Do nói: \n\t\t\t\t\t\t\t\tTháng Tám 18, 2017 lúc 3:18 chiều\t\t\t\t\t\t\t Em chào anh. Cảm ơn về bài viết của anh. \nAnh cho em hỏi là nếu muốn học cơ bản về phân tích data thì nên học như nào ạ? Học từ cái gì và học ở đâu ạ. Anh có thể cho em xin ít lời khuyên được không ạ? \nEm chưa biết gì về phân tích Data cả, nhưng em thấy nó rất có ích cho công việc kinh doanh trong lĩnh vực Start-up của em, nên em rất muốn học mà chưa biết bắt đầu từ đâu. Rất mong sẽ nhận được sự tư vấn của anh ạ. Số lượt thích Số lượt thích Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Tám 18, 2017 lúc 3:26 chiều\t\t\t\t\t\t\t Phân tích data một phần cũng do cảm nhận và năng lực của mỗi người. Nhưng em có thể tìm học các môn liên quan đến xác suất thống kê. \nCác từ khoá để bắt đầu: \n– Descriptive analysis: trả lời cho câu hỏi điều gì đang diễn ra? Lấy sum, avg, median, mod. Show histogram, bar-chart, pie chart, box chart, line chart, … \n– Diagnostic analysis: trả lời cho câu hỏi tại sao điều đó diễn ra? Hypothesis testing, z-statistic, p-value, A/B testing, … \n– Predictive analysis: trả lời cho câu hỏi điều gì sắp diễn ra? Linear regression, Logistic regression, Pattern recognition, … \n– Prescriptive analysis: trả lời cho câu hỏi làm gì nếu điều đó diễn ra? Recommender system, fraud detection, … Số lượt thích Số lượt thích Phản hồi Hoàng Minh nói: \n\t\t\t\t\t\t\t\tTháng Chín 27, 2017 lúc 10:51 chiều\t\t\t\t\t\t\t Chào bạn! Bài viết của bạn rất hay, tuy nhiên mình đã thử áp dụng để xác định bài toán ML cho vấn đề của mình mà không ra, rất mong bạn tư vấn giúp. \nVấn đề của mình như sau: mình có dữ liệu chơi game của 1 dịch vụ game online như: thời gian đăng nhập của người chơi, quá trình chơi game, lịch sử nạp tiền, lịch sử giao dịch,… Cty dịch vụ game muốn biết “tại sao số lượng người chơi đang giảm dần”? \nHoặc bạn có thể gợi ý cho mình các bài toán ML có thể làm trên tập dữ liệu này hay giới thiệu cho mình các paper, ví dụ tương tự về bài toán hoặc tập dữ liệu. \nXin cảm ơn! Số lượt thích Số lượt thích Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Chín 28, 2017 lúc 5:37 sáng\t\t\t\t\t\t\t “Tại sao số lượng người chơi đang giảm dần?”. Đây là bài toán “Dianogstic” trong thống kê. Bạn cần xây dựng report từ mức Descriptive. Machine learning chỉ giúp bạn predict được future, phát hiện pattern, hoặc tự động thực hiện một tác vụ nào đó khi đã biết trước training data. Việc trả lời câu hỏi là do người Data analyst thực hiện. http://www.kdnuggets.com/2017/07/4-types-data-analytics.html Số lượt thích Số lượt thích Phản hồi Nhập bình luận của bạn tại đây... Mời bạn điền thông tin vào ô dưới đây hoặc kích vào một biểu tượng để đăng nhập: Thư điện tử  (bắt buộc) (Địa chỉ của bạn được giấu kín) Tên  (bắt buộc) Trang web  Bạn đang bình luận bằng tài khoản WordPress.com  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Twitter  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Facebook  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Google+  (  Đăng xuất  /  Thay đổi  ) Hủy bỏ Connecting to %s Notify me of new comments via email. Thông báo cho tôi bằng email khi có bài đăng mới. Bạn Không Thể Sống Một Mình Nếu ngày mai tớ trở thành người nổi tiếng. Gặp tớ giữa đường bạn đừng ngại tớ nghe. Cứ nhìn tớ như ngày xưa bạn thấy. Đừng nên nghĩ tớ quá xa vời.  Nếu một ngày tớ là kẻ ăn xin. Thì bố thí cho tớ vài kỷ niệm. Dù rất nhỏ nhưng đừng nên khinh tớ. Vì tớ vẫn là tớ của ngày xưa.  Nếu ngày mai tớ chẳng biết đường về. Hãy chỉ giùm tớ con đường đi đúng đắn. Đừng để tớ sa chân nơi bùn nhớp. Lúc rút chân lên thì đã bẩn mất rồi.  Nếu ngày mai tớ gặp bạn giữa đường. Thì ta sẽ gặp nhau và cười nhé. Đừng nên trách và đừng nên đánh mất. Đôi khi có một số người lướt qua cuộc đời bạn và ngay tức khắc bạn nhận ra rằng sự có mặt của họ ý nghĩa như thế nào. Họ đã dạy bạn những bài học, đã giúp bạn nhận ra giá trị của chính mình hoặc trở thành con người mà bạn từng mơ ước. Có lẽ bạn sẽ không biết được những con người này từ đâu đến ( bạn cùng phòng, người hàng xóm, vị giáo sư, người bạn mất liên lạc từ lâu hay thậm chí là một người hoàn toàn xa lạ ). Nhưng khi bạn thờ ơ với họ, hãy nhớ rằng trong từng khoảnh khắc họ sẽ ảnh hưởng rất sâu sắc đến cuộc đời bạn.  …Cuộc sống không hề làm khó bạn mà chính bạn đã tự làm khó mình bằng cách đặt ra những câu hỏi đại loại như: “Tại sao lại thế này?… Tại sao lại thế kia?…”. Không ai có thể trả lời giúp bạn mà chính bạn phải tự đi tìm lời giải đáp cho mình. Hãy luôn nghĩ về người khác trước khi nghĩ đến bản thân, chắc chắn lúc đó bạn sẽ hài lòng với câu trả lời của chính mình. Và đừng quên luôn đặt câu hỏi: “Mình đã làm được gì?” trước khi tự hỏi: “Mình đã nhận được gì?” nhé! Tôi tin là bạn sẽ thành công!   Thật ra trên đời, con người luôn cần có nhau để giải quyết rất nhiều vấn đề. Nhờ những lúc qua lại giúp đỡ đó mà hoá giải bao muộn phiền, nảy sinh các tình cảm, cùng trải nghiệm biết bao điều thú vị, hiểu biết thêm nhiều tri thức mới. Đây chính là cuộc sống! Vì thế hãy trân trọng những người đang “làm phiền” bên cạnh bạn nhé! Đừng để đến khi mất đi rồi mới nghẹn ngào lại bảo “giá như” … ĐĐ. GS. Thích Phước Tiến \n__(())__ Namo Bụt Sakyamuni Nhận Email khi có bài viết mới Liên hệ Email (bắt buộc) Comment (bắt buộc) Top bài viết Bài viết mới FAQ: Big data cho người mới bắt đầu Tháng Mười 2, 2017 Làm việc với Spark DataFrames – Truy vấn nâng cao (Subqueries, Cube, Rollup) Tháng Chín 15, 2017 Thống kê ứng dụng 3: Các vấn đề trong thống kê Tháng Chín 14, 2017 Thống kê ứng dụng 2: Suy luận thống kê Tháng Chín 12, 2017 Thống kê ứng dụng 1: Quan sát dữ liệu Tháng Chín 7, 2017 AI, Machine Learning, Deep Learning phân biệt như thế nào cho đúng Tháng Chín 4, 2017 Information extraction – Bài toán rút trích thông tin trong văn bản Tháng Tám 28, 2017 Bắt đầu nghiên cứu big data từ đâu và như thế nào Tháng Tám 3, 2017 Data Science – Mỏ vàng của Kỉ nguyên số Tháng Tám 3, 2017 SMA 2017 – Lý thuyết ra quyết định Tháng Sáu 17, 2017 Big Data Chia sẻ Data Science Deep learning Dự án Exploratory Data Analysis Getting and cleaning data Kiến thức Lập trình Machine learning Python R Spark Statistical Inference Toán Weka Xử lý ngôn ngữ tự nhiên - Natural Language Processing (NLP) This slideshow requires JavaScript. Thư viện Tháng Mười 2017  (1) Tháng Chín 2017  (5) Tháng Tám 2017  (3) Tháng Sáu 2017  (3) Tháng Năm 2017  (3) Tháng Ba 2017  (1) Tháng Một 2017  (3) Tháng Mười Hai 2016  (1) Tháng Mười Một 2016  (2) Tháng Mười 2016  (2) Tháng Chín 2016  (1) Tháng Tám 2016  (2) Tháng Bảy 2016  (2) Tháng Năm 2016  (1) Tháng Tư 2016  (1) Tháng Ba 2016  (2) Tháng Hai 2016  (2) Tháng Một 2016  (1) Tháng Mười Hai 2015  (6) Tháng Mười Một 2015  (5) Tháng Mười 2015  (6) Tháng Chín 2015  (11) Tháng Tám 2015  (16) Tháng Bảy 2015  (25) Tháng Sáu 2015  (4) Thống kê 360,709 hits Tháng Mười 2015 H B T N S B C « Th9   Th11 »   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31   Data Science Lập trình Kiến thức Chia sẻ Dự án About Blog tại WordPress.com. Post to Hủy bỏ %d  bloggers like this:"
        },
        {
          "title": "Một số thuật toán cơ bản trong Machine Learning",
          "relevance": "0",
          "url": "https://phamhuudanh.com/2017/02/18/mot-so-thuat-toan-trong-machine-learning/",
          "content": "Search Search for: Phạm Hữu Danh Let's share the passion for technology! Menu Home About Me Contact Open Search Một số thuật toán cơ bản trong Machine Learning Đây là bài post mình dùng đe ôn tập và chia sẻ kiến thức đã học. Mình rất vui khi nhận được chia sẻ, góp ý nội dung từ các bạn. – Hữu Danh 1. Machine Learning là gì? Theo mình, điểm khác nhau giữa người và máy tính chính là người có thể học tập kiến thức mới từ những thứ trong cuộc sống. \nTrước khi biết đến Machine Learning, mình chỉ nghĩ lập trình là xây dựng thuật toán để giải quyết một vấn đề cụ thể. \nCòn đối với Machine Learning, thì chúng ta lập trình để máy tính học và và giải quyết các yêu cầu từ “kiến thức đã có”. Vốn kiến thức này chính là dữ liệu (dữ liệu dùng để dạy cho máy tính học trước khi suy ra kết quả gọi là training data). 2. Các thuật toán cơ bản Trước khi chúng ta học giải Phương trình, Vi phân, Toán cao cấp, … thì chúng ta phải học những phép tính cơ bản trước như: cộng, trừ, nhân, chia, lấy căn, … \nĐối với Machine Learning cũng vậy, khi mình tìm hiểu về nó qua các khóa học, mình được ho những thuật toán cơ bản nhất, để từ đó kết hợp và giải quyết các vấn đề phức tạp hơn. Tổng hợp lại thì ta có: \n– Decision Trees \n– Naive Bayes \n– Gradient Descent \n– Linear Regression \n– Logistic Regression \n– Neural Networks \n– Clustering Tất nhiên còn rất nhiều thuật toán khác, nếu bạn thấy thuật toán nào hay muốn chia sẻ, hãy comment phía dưới nhé! Decision Trees / Random Forest Decision Trees là một thuật toán mà trong đó chúng ta xét từng thuộc tính của đối tượng so với điều kiện mà chúng ta đặt ra trước để phân loại đối tượng vào các nhóm. Xem hình phía dưới, bạn sẽ hiểu tại sao nó được gọi là Decision Trees (cây quyết định). ví dụ kinh điển: tính khả năng sống sót của một người trên tàu Titanic Theo mình học được thì thuật toán này có các điểm mạnh sau: Dễ hiểu và dễ giải thích Áp dụng cho cả dữ liệu số và dữ liệu phân loại  (có thể tạm hiểu là kiểu dữ liệu enum) . Xử lí tốt với dữ liệu lớn. Hạn chế thì có: Thay đổi  dữ liệu dạy học  (training data) sẽ thay đổi cây cấu trúc. Không chính xác bằng các thuật toán khác. Có nhiều kiểu dữ liệu mà sẽ làm cho thuật toán này bối rối, ví dụ:  XOR Naive Bayes Naive Bayes là thuật toán dùng để phân nhóm dựa vào tính xác suất có điều kiện. Nếu ai đã học xác xuất thống kế thì có lẽ đã nghe đến công thức này. Ví dụ kinh điển: detecting spam emails Dựa vào training data đã phân loại, ta sẽ xác định các thuộc tính của chúng, ví dụ ở đây là từ “cheap”, ta sẽ tính tỉ lệ email là spam khi có tư “cheap”. Sau đó ta xác định với các thuộc tính khác. \nKhi có email mới sẽ xác định tỉ lệ email mới có phải spam không dựa theo việc xác định theo thuộc tính, thường là lấy tích các thuộc tính. Xem thêm về công thức Naive Bayes để hiểu cách tính. Ưu điểm mình nhận thấy là: tạo ra các bộ lọc khác nhau cho các trường hợp khác nhau, ví dụ email bị cho là spam khác nhau với mỗi người. Nhươc điểm là: Khi quá nhiều thuộc tính, dataset quá lớn thì sẽ chạy chậm. Và sẽ không chính xác khi các thuộc tính có nhiều mối tương quan. Gradient Descent Đây là một dạng thuật toán tìm thành phần ưu tiên. Thuật toán này tương tự cách chúng ta đi xuống đồi, chúng sẽ tìm nơi có độ cao thấp nhất và đi về hướng đó. Thuật toán này sẽ tìm biến để cho biểu thức xác định đạt giá trị nhỏ nhất. Thuật toán này chủ yếu được ứng dụng để xây dựng các thuật toán khác. Linear Regression & Logistic Regression Với mình thì các thuật toán dạng Regression giố với các bài phương trình hàm: với biểu thức  y  = a  x  + b  mà ta có rất nhiều cặp số (x, y) và nhiệm vụ là tìm ngược lại a và b Linear Regression đúng như tên gọi của nó, ta cố gắng vẽ một đường thẳng sao cho tổng khoảng cách từ các điểm đến nó là ngắn nhất. \nKhi có một dữ liệu mới, ta sẽ dựa vào đường thẳng này để dự đoán. \nVí dụ, giá nhà sẽ tăng khi diện tích căn nhà tăng và ta sẽ ước tính sự liên quan này qua một đường thẳng trên biểu đồ. Tất nhiên, trong thực tế thì sẽ ít khi xuất hiện đường thẳng. Lúc đó ta cần đến Logistic Regression. Lúc này chúng ta sẽ cố gắng dự đoán bằng cách phân chia hai miền giá trị bằng một hàm đặc biệt. Regression đều ứng dụng Gradient Descent để tìm được hàm chính xác. Support Vector Machines Vấn đề đặt ra là khi mà dữ liệu mẫu phân nhóm, không bao phủ thì độ chính xác của Regression không cao nữa. Ví dụ như hình sau: Bạn thấy đấy, các đường kẻ chia cắt 2 nhóm đều đúng, vậy thì đường nào là đúng nhất? \nLúc này thì ta áp dụng Support Vector Machines (SVM). Với SVM thì ta sẽ cố gắng mở rộng mỗi miền giá trị nhất có thể. \nCó thể hiểu thuật toán này qua hình trên, chúng ta cần tìm đường thẳng chia ở giữa làm sao cho  khoảng cách ngắn nhất  của các điểm đến đường thẳng là  dài nhất . I will update later Share this: Facebook Google Twitter Like this: Like Loading... Published by  Phạm Hữu Danh \n\t\t\tA young and passionate student who loves learning new technologies.\t\t\t \n\t\t\t\tView all posts by Phạm Hữu Danh\t\t\t \n\t\tFebruary 18, 2017\t Machine Learning Post navigation C++ – Rounding errors with double type Enter your comment here... Fill in your details below or click an icon to log in: Email  (required) (Address never made public) Name  (required) Website  You are commenting using your WordPress.com account.  (  Log Out  /  Change  )  You are commenting using your Twitter account.  (  Log Out  /  Change  )  You are commenting using your Facebook account.  (  Log Out  /  Change  )  You are commenting using your Google+ account.  (  Log Out  /  Change  ) Cancel Connecting to %s Notify me of new comments via email. Create a website or blog at WordPress.com Up ↑ Post to Cancel %d  bloggers like this:"
        },
        {
          "title": "Các phương pháp học máy (Machine Learning)",
          "relevance": "1",
          "url": "http://bis.net.vn/forums/t/619.aspx",
          "content": "\r\n\t\t\t                            \r\n        Chào mừng đến với BIS\r\n         Đăng nhập  \r\n         |  Đăng ký \r\n        |  Trợ giúp \r\n\t\t\t\t\t                                    \r\n\t\t\t\t\t\t                                         trong \r\n\t\t\t\t\t\t                                         Data Mining and Business Intelligence... Data Mining and Business Intelligence... (Entire Site) Tìm kiếm BIS  »  Data Mining and Business Intelligence  »  Data Mining and Business Intelligence  »  Các phương pháp học máy (Machine Learning)  Các phương pháp học máy (Machine Learning)  Bài cuối 06-25-2013 11:27 AM của  thanhthi . 5 trả lời. Trang 1 trong số 1 (6 nội dung)  \r\n\t\t\t\t        \r\n\t\t\t\t                Sắp xếp bài viết:\r\n\t\t\t\t                 Cũ đến mới Mới đến cũ Trước Tiếp theo \r\n\t\t\t\t\t\t\t\t        04-10-2012 05:51 PM    \r\n\t\t\t\t\t\t\t\t     TienPhong Tham gia 03-04-2012 Điểm 110 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Các phương pháp học máy (Machine Learning)\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Trong lĩnh vực Học máy có các phương\r\npháp học sau: 1) Học có giám sát (supervised\r\nlearning) 2) Học không có giám sát\r\n(unsupervised learning) 3) Học bán giám sát\r\n(semi-supervised learning) 4) Học tăng cường (reinforcement\r\nlearning) Không phải ngẫu nhiên người ta lại\r\nphân chia và đặt tên cho các phương pháp học như vậy. Sau một thời gian tìm đọc tài liệu\r\nvề học máy, cố gắng hiểu được bản chất và phân biệt được sự giống và khác nhau\r\ncủa các phương pháp học này, tôi đưa ra một bài toán ví dụ sau: Cho một công-ten-nơ chứa đầy hoa\r\nquả. Nhiệm vụ phải chia số quả này thành các nhóm đúng với loại quả đó. (xoài\r\nra xoài, cam ra cam, táo ra táo,…) 1) Với Học có giám sát\r\n(supervised learning) Là kỹ thuật học sử dụng cho các bài\r\ntoán phân lớp (Classification) Để thực hiện được bài toán trên,\r\ntrước tiên cần phải có 2 điều kiện: Điều kiện 1: phải biết trước số\r\nnhãn lớp cần phân loại, tức là phải biết trong công-ten-nơ đó có nhưng loại quả\r\ngì. Giả sử trong công-ten-nơ đó có 5 loại quả là xoài, cam, táo, ổi, đào (đây\r\nchính là 5 loại nhãn lớp). Điều kiện 2: phải có tập đặc trưng\r\ncủa mỗi loại quả, ví dụ các đặc trưng là: hình dáng, màu sắc, trọng lượng, độ cứng\r\nmềm, v.v… Tập đặc trưng này có được thông qua học một tập dữ liệu huấn luyện\r\n(chính là các công-ten-nơ của các chuyến hàng trước đó) Khi thực hiện phân loại các loại\r\nquả trong công-ten-nơ đang xét, dựa vào đặc trưng của các loại quả (điều kiện\r\n2), quả sẽ được đưa vào 1 trong 5 nhóm đã biết (điều kiện 1). 2) Học không có giám sát\r\n(unsupervised learning) Là kỹ thuật học sử dụng cho các bài\r\ntoán phân cụm, gom cụm (Clustering)   Để thực hiện được bài toán trên,\r\ncần phải có tập đặc trưng của mỗi loại quả. Tập đặc trưng này có được cũng thông\r\nqua học một tập dữ liệu huấn luyện (như điều kiện 2 của Học có giám sát).   Điểm khác của Học không giám sát\r\nso với Học có giám sát là: trước khi phân cụm, không biết trong công-ten-nơ đang\r\nxét có bao nhiêu loại quả và đó là những loại quả gì. Khi thực hiện phân cụm, dựa vào đặc\r\ntrưng của mỗi loại quả, sẽ đưa quả đang xét vào nhóm (cụm) có đặc trưng tương đồng\r\nvới nó nhất. Khi đó, 2 quả bất kỳ ở cùng cụm sẽ tương đồng nhau, 2 quả khác cụm\r\nsẽ khác biệt nhau. * Nhận xét Giống nhau: Cả\r\nhai phương pháp học 1) và 2) đều cần phải có một tập huấn luyện (training data\r\nset) để hệ thống có thể “học” và rút ra được các đặc trưng dùng cho việc gán nhãn. Khác nhau: Phương\r\npháp 1) cần biết trước đầu ra chính là số nhãn lớp. Phương pháp 2) không cần biết\r\ntrước đầu ra  (là số cụm và nhãn) để phân\r\ncụm.   Trên đây là cách hiểu của tôi về\r\nphương pháp Học có giám sát và Học không giám sát. Nhân đây tôi xin đưa một thắc\r\nmắc để các thành viên và những ai quan tâm đưa ra ý kiến để cùng thảo luận. Thắc mắc: Học bán giám sát là gì?\r\nHọc tăng cường là gì? Phân biệt sự khác nhau và giống nhau giữa các phương pháp\r\nhọc bán giám sát và học tăng cường. Cho ví dụ minh hoạ? Từ khóa đại diện:  Machine Learning ,  Phân cụm dữ liệu. ,  Học máy ,  Phân lớp dữ liệu ,  Khai phá dữ liệu Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        06-18-2012 09:41 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  phamvu Tham gia 06-18-2012 Điểm 35 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Các phương pháp học máy (Machine Learning)\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ \r\n\t\t\t\t\t\t\t\t\t\t    Em thấy k - means là học ko giám sát, trong khi đó xác định k cụm là do người dùng xác định hoặc có thể default, vậy số k đã được biết trước.Vậy thuật toán k - means phải là học nửa giám sát?\r\n\t\t\t\t\t\t\t\t\t\t    \r\n\t\t\t\t\t\t\t\t\t     phamvu Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        09-02-2012 01:19 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  TienPhong Tham gia 03-04-2012 Điểm 110 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Các phương pháp học máy (Machine Learning)\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Đúng k means là thuật toán học không giám sát vì nó được dùng để phân cụm cho các dữ liệu chưa biết nhãn. Để dễ phân biệt bạn chú ý là việc biết (cho) trước k cụm không liên quan gì đến dữ liệu đầu vào (hay dữ liệu huấn luyện - training data set). Với học có giám sát dữ liệu đầu vào đều đã được gán nhãn trước, phương pháp học này dùng giải quyết bài toán phân lớp (classification). Với học không giám sát, không biết trước nhãn của dữ liệu (hay dữ liệu đầu vào đều chưa được gán nhãn), phương pháp này dùng để giải quyết bài toán phân cụm (clustering). Còn với học nửa giám sát, sử dụng tập dữ liệu đầu vào gồm cả dữ liệu đã gán nhãn và dữ liệu chưa gán nhãn, trong đó dữ liệu gán nhãn thường (rất) ít và dữ liệu chưa gán nhãn thường (rất) nhiều. Hai kỹ thuật tiêu biểu cho học nửa giám sát là Self-training và Co-training, bạn có thể tìm kiếm trên internet. Chúc vui vẻ! Điểm chủ đề: 20 \r\n\t\t\t\t\t\t\t\t        05-17-2013 12:49 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  TienPhong Tham gia 03-04-2012 Điểm 110 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Các phương pháp học máy (Machine Learning)\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Có nhiều bạn gửi mail hỏi mình về Machine Learning, các bạn thông cảm vì đi làm bận nên không trả lời riêng các bạn, và cũng để mọi người có thể cùng trao đổi mình sẽ post lên đây để mọi người chia sẻ những kiến thức về ML.   Trước tiên mình nêu một số định nghĩa rất quan trọng trong ML.   Định nghĩa học máy Học máy (hay máy học – Machine learning) là một thành phần quan trọng của trí tuệ nhân tạo nhằm nghiên cứu và phát triển các phương pháp, kỹ thuật giúp cho các hệ thống hay máy tính có khả năng học (Tiến Phong).   Vấn đề quá vừa dữ liệu (Over-fitting) Thuật ngữ over-fitting ra đời dùng để chỉ một hiện tượng xuất hiện trong quá trình khai phá dữ liệu sử dụng phương pháp học máy. Hiện tượng này gây khó khăn đáng kể cho việc thiết kế, xây dựng hệ thống ngay từ bước chuẩn bị dữ liệu cho đến bước kiểm thử hệ thống. Khi hiện tượng over-fitting xảy ra sẽ làm cho hiệu quả của hệ thống giảm xuống và kết quả thu được từ hệ thống không còn độ tin cậy cao. Có thể định nghĩa hiện tượng over-fitting như sau:   Định nghĩa quá vừa dữ liệu Một hàm mục tiêu hay một giả thiết học được  h , sẽ được gọi là over-fitting (quá vừa dữ liệu) với một tập dữ liệu huấn luyện nếu tồn tại một hàm mục tiêu khác là  h’  sao cho: h’  kém phù hợp hơn, đạt độ chính xác kém hơn so với  h  trên tập dữ liệu huấn luyện, nhưng  h’  lại đạt độ chính xác cao hơn  h  đối với toàn bộ tập dữ liệu (bao gồm cả tập dữ liệu liệu huấn luyện và tập dữ liệu kiểm tra)   Ví dụ quá vừa dữ liệu Giả sử gọi D là tập toàn bộ các dữ liệu có thể có, Training_D là tập các dữ liệu huấn luyện Giả sử Err_D(h) là mức lỗi mà giả thiết h sinh ra đối với tập D, và Err_Training_D(h) là mức lỗi mà giả thiết h sinh ra đối với tập Training_D. Nếu tồn tại một giả thiết khác là h’ sao cho: Err_Training_D(h) < Err_Training_D(h’) và Err_D(h) > Err_D(h’) Thì khi đó h được coi là quá vừa dữ liệu trên tập huấn luyện Training_D.   Nguyên nhân quá vừa dữ liệu Vấn đề over-fitting thường do các nguyên nhân: - Lỗi (nhiễu) trong tập huấn luyện phát sinh trong quá trình thu thập, xây dựng tập dữ liệu. - Số lượng dữ liệu của tập huấn luyện quá nhỏ, không đại diện cho toàn bộ tập dữ liệu có thể có hay toàn bộ phân bố dữ liệu của bài toán.   Bản chất của học máy dưới góc nhìn của xác suất thống kê. Có thể mô hình hoá một vấn đề máy học như sau: Cho một dãy  l  quan sát:  (x 1 , y 1 ), (x 2 , y 2 ), … , (x l , y l ) .  Trong đó: - x 1 , x 2 , …, x l  là các mẫu, x i R n . Các mẫu x i  được phát sinh ngẫu nhiên theo một hàm phân bố xác suất F(x) nào đó mà ta không biết. - y i  là các kết quả học tương ứng với mẫu x i , y i R. Thường thì y là kết quả của một hàm f(x) nào đó – đơn trị. Tuy nhiên trong trường hợp tổng quát thì y không đơn trị. Do đó y được xác định theo một hàm phân bố điều kiện F(y|x) mà ta cũng không biết. - Bây giờ cho một mẫu x*, vấn đề của máy học là xác định một hàm f 0 (x) mà có thể ước lượng tốt nhất giá trị y* tương ứng. Như vậy theo lý thuyết tương quan trong thống kê thì f 0 (x) tốt nhất theo lý thuyết phải là kỳ vọng của y theo x theo phân bố F(y|x).f 0 (x) còn được gọi là phương trình hồi quy. Với x tuân theo phân bố F(x), y tuân theo phân bố có điều kiện F(y|x) thì hàm phân bố của cặp (x, y) là F(x, y) = F(x)F(y|x). Có thể thấy xác suất để có dãy  (x 1 , y 1 ), (x 2 , y 2 ), … , (x l , y l )  là tích  F(x 1 , y 1 )F(x 2 , y 2 )…F(x l , y l ). Tuy nhiên, ở đây ta không biết F(x) lẫn F(y|x) nên không thể xác định chính xác kỳ vọng này. Tất cả dữ liệu mà ta biết chỉ là dãy hữu hạn các mẫu quan sát  (x 1 , y 1 ), (x 2 , y 2 ), … , (x l , y l ) . Nhiệm vụ của máy học là xác định chính xác nhất có thể được hàm f 0 (x) dựa trên các dữ liệu hữu hạn này. Trong trường hợp khi y   R, tức đây là vấn đề hồi quy (regression).  Trong trường hợp bài toán phân lớp (classification) thì y  {-1, 1} là trường hợp nhận dạng hai lớp, nếu y i  = -1 thì x i  thuộc lớp thứ nhất (không được quan tâm), còn y i  = 1 thì x i  thuộc lớp thứ 2 (lớp được quan tâm)   Một số phương pháp học máy Trong lĩnh vực học máy có nhiều phương pháp học khác nhau, trong phần này đề cập đến 3 phương pháp học được sử dụng phổ biến nhất, gồm có: học không giám sát, học bán/ nửa giám sát và học có giám sát.   Phương pháp học không giám sát (Unsupervised Learning) *  Khái niệm học không giám sát Học không giám sát là học với tập dữ liệu huấn luyện ban đầu hoàn toàn chưa được gán nhãn. Học không giám sát là phương pháp học sử dụng cho lớp bài toán gom cụm, phân cụm (Clustering).   *  Nội dung phương pháp học không giám sát - Để thực hiện phân cụm, trước tiên cần một tập dữ liệu huấn luyện (training dataset) – là một tập các ví dụ học (training examples/instances). Trong đó, mỗi ví dụ học chỉ chứa thông tin biểu diễn (ví dụ: một vector các giá trị thuộc tính), mà không có bất kỳ thông tin gì về nhãn lớp hoặc giá trị đầu ra mong muốn (expected output). - Áp dụng một thuật toán học không có giám sát (ví dụ k-Means) để học hàm/mô hình mục tiêu (trong trường hợp này là hàm phân cụm ứng với thuật toán được chọn). - Sử dụng một phương pháp thử nghiệm (có thể kết hợp với một tập dữ liệu có gán nhãn) để đánh giá hiệu năng/chất lượng của hàm mục tiêu học được.   *  Một số thuật toán học không giám sát Có rất nhiều thuật toán học không giám sát được ra đời và phát triển nhằm giải quyết bài toán phân cụm phục vụ khai thác hiệu quả nguồn dữ liệu chưa gán nhãn nhiều và rất đa dạng. Việc lựa chọn sử dụng thuật toán nào tuỳ thuộc vào dữ liệu và mục đích của từng bài toán. Trong đó các thuật toán thường được sử dụng như: k-means, HAC  (Hierarchical Agglomerative Clustering) , SOM  (Self-Organizing Map) , DBSCAN, FCM,... (chi tiết các thuật toán này có thể tìm kiếm trên Internet)   Phương pháp học bán giám sát (Semi-Supervised Learning) Trong thực tế, để có được một tập dữ liệu có chất lượng và đã được gán nhãn của một lĩnh vực, thường được thực hiện thủ công bằng tay bởi người có nhiều kinh nghiệm về lĩnh vực đó. Vì vậy, dữ liệu đã được gán nhãn thường ít và đắt. Trong khi đó, dữ liệu chưa được gán nhãn lại rất nhiều và phong phú. Phương pháp học bán giám sát (hay học nửa giám sát) được đặt ra để tận dụng cả hai nguồn dữ liệu này.   *  Khái niệm học bán giám sát Học bán giám sát là học với tập dữ liệu huấn luyện gồm cả dữ liệu đã được gán nhãn và dữ liệu chưa được gán nhãn. Tuỳ vào từng mục đích cụ thể, học bán giám sát có thể được áp dụng cho bài toán phân lớp hoặc phân cụm.   *  Nội dung phương pháp học bán giám sát -  Nội dung chính của học bán giám sát là hệ thống  sử dụng  một tập học (training set) gồm 2 phần: các ví dụ học có nhãn, thường với số lượng (rất) ít, và các ví dụ học không có nhãn, thường với số lượng (rất) nhiều. Thực tế cho thấy khi sử dụng kết hợp dữ liệu không có nhãn với một lượng nhất định dữ liệu có nhãn có thể tăng độ chính xác đáng kể. -  Một thuật toán học bán giám sát được sử dụng (ví dụ Self-training) sẽ học các ví dụ có nhãn, sau đó  tiến hành gán nhãn cho một số (có lựa chọn) các ví dụ không có nhãn - một cách hợp lý, có đánh giá chất lượng công việc  hay  độ chính xác. Tiếp theo, chọn các ví dụ vừa được gán nhãn có độ tin cậy cao (vượt trên một ngưỡng chọn trước) đưa vào kết hợp với tập dữ liệu có nhãn, tạo thành một tập dữ liệu huấn luyện mới. -  Áp dụng một phương pháp kiểm thử (có thể kết hợp với một tập dữ liệu đã biết trước nhãn) để đánh giá hiệu năng/độ chính xác của mô hình.   *  Một số thuật toán học bán giám sát Một số thuật toán thường được sử dụng gồm có: thuật toán Cực đại kỳ vọng (EM - Expectation Maximization), SVM truyền dẫn (TSVM - Transductive Support Vector Machine), Self-training, Co-training và các phương pháp dựa trên đồ thị (graph-based). Việc lựa chọn thuật toán nào dựa trên một số định hướng: nếu các lớp dữ liệu có tính phân cụm cao thì nên dùng EM với mô hình hỗn hợp sinh; nếu đã sử dụng SVM thì mở rộng thành TSVM; khi khó nâng cấp mô hình học có giám sát đã có, thì nên dùng self-training; nếu các đặc trưng của dữ liệu phân chia tự nhiên thành hai phần riêng rẽ thì nên dùng Co-training; còn nếu hai mẫu dữ liệu có đặc trưng tương tự nhau hướng tới một lớp thì sử dụng phương pháp dựa trên đồ thị.   Trong số các thuật toán học bán giám sát thông dụng, có 2 thuật toán tiêu biểu là Self-training và Co-training. - Thuật toán Self-training: Self-training là kỹ thuật học bán giám sát được sử dụng khá phổ biến do tận dụng được nguồn dữ liệu chưa gán nhãn lớn và ban đầu chỉ cần lượng nhỏ dữ liệu đã gán nhãn. Nội dung chính của Self-training là lặp nhiều lần phương pháp học có giám sát. Gọi     D: là tập các dữ liệu đã được gán nhãn.           C : là tập các dữ liệu chưa gán nhãn. Thuật toán Self-training thực hiện như sau: Lặp  (cho đến khi C =  Æ ): i. Huấn luyện bộ phân lớp có giám sát  h  trên tập D ii. Sử dụng  h  để phân lớp dữ liệu trong tập C iii. Tìm tập con C’  Í  C có độ tin cậy cao nhất: D + C’  Þ  D ; C – C’  Þ  C. Ban đầu huấn luyện bộ phân lớp bằng cách cho bộ phân lớp học một tập dữ liệu huấn luyện đã được gán nhãn (tập này thường nhỏ so với tập dữ liệu chưa gán nhãn). Dùng bộ phân lớp đã được huấn luyện, phân lớp cho các dữ liệu chưa được gán nhãn. Trong số dữ liệu mới được gán nhãn, chọn các dữ liệu có độ tin cậy cao (lớn hơn một ngưỡng nào đó) kèm với nhãn vừa gán, đem bổ sung vào tập dữ liệu huấn luyện ban đầu. Sau đó, bộ phân lớp được học lại trên tập huấn luyện mới (gồm dữ liệu đã gán nhãn ban đầu và dữ liệu do bộ phân lớp mới gán nhãn) và thuật toán được lặp lại. Sau mỗi vòng lặp, bộ phân lớp sẽ bổ sung một số mẫu dữ liệu có độ tin cậy cao nhất cùng với dự đoán phân lớp của chúng vào tập dữ liệu huấn luyện. Tên gọi Self-training xuất phát từ việc sử dụng dự đoán của nó để huấn luyện chính nó.   - Thuật toán Co-training: Thuật toán Co-training dựa trên giả thuyết rằng các đặc trưng của tập dữ liệu huấn luyện có thể được phân chia thành 2 tập con (trường hợp lý tưởng là hai tập con này thoả mãn điều kiện độc lập nhau - conditional independent). Nội dung chính của thuật toán như sau: + Dùng 2 bộ phân lớp phù hợp để học 2 tập con tương ứng (mỗi tập con huấn luyện một bộ phân lớp). + Mỗi bộ phân lớp thực hiện phân lớp cho các dữ liệu chưa gán nhãn, thu được kết quả là tập dữ liệu chưa gán nhãn kèm theo nhãn dự đoán của chúng. Trong tập kết quả của bộ phân lớp 1, chọn ra những mẫu dữ liệu (kèm nhãn đã dự đoán) có độ tin cậy cao nhất bổ sung vào tập huấn luyện của bộ phân lớp 2 và ngược lại. + Mỗi bộ phân lớp được học lại tập dữ liệu huấn luyện (gồm dữ liệu gán nhãn ban đầu và dữ liệu gán nhãn mới bổ sung từ kết quả của bộ phân lớp kia). Quá trình được lặp lại cho đến khi tập dữ liệu chưa gán nhãn rỗng hoặc số vòng lặp đạt tới một ngưỡng được xác định trước.   Thuật toán Co-training: (1). Huấn luyện hai bộ phân lớp:   f  (1)  từ (X l  (1) , Y l ), f  (2)  từ (X l  (2) , Y l ).  (2). Phân lớp các mẫu dữ liệu chưa gán nhãn X u  với f  (1)  và f  (2)  tách biệt nhau. (U là tập các mẫu dữ liệu chưa gán nhãn) (3).  Chèn thêm vào f  (1)  k-most-confident (x, f  (1) (x)) tới các dữ liệu đã gán nhãn của f  (2) . (4). Chèn thêm vào f  (2)  k-most-confident (x, f  (2)  (x)) tới các dữ liệu đã gán nhãn của f  (1) . (5). Lặp lại các quá trình trên.   Thuật toán Co-training trên có thể viết như sau: L: là tập các mẫu dữ liệu đã gán nhãn U: là tập các mẫu dữ liệu chưa gán nhãn (1). L có thể phân chia thành hai tập con L 1  và L 2  (trường hợp lý tưởng thì L 1  và L 2  độc lập nhau). (2). Cho bộ phân lớp h 1  học L 1  (hay L 1  huấn luyện bộ phân lớp h 1 ) Cho bộ phân lớp h 2  học L 2  (hay dùng L 2  huấn luyện bộ phân lớp h 2 ) (3). Dùng h 1  phân lớp cho U thu được tập U 1 ’ kèm nhãn dự đoán của chúng. Dùng h 2  phân lớp cho U thu được tập U 2 ’ kèm nhãn dự đoán của chúng. (4). Từ U 1 ’ chọn ra u 1  mẫu dữ liệu kèm theo nhãn của nó, có độ tin cậy cao nhất. Bổ sung u 1  vào L 2 . Khi đó, L 2  + u 1  => L 2 . Từ U 2 ’ chọn ra u 2  mẫu dữ liệu kèm theo nhãn của nó, có độ tin cậy cao nhất. Bổ sung u 2  vào L 1 . Khi đó, L 1  + u 2  => L 1 . (5). Dùng L 1  mới huấn luyện bộ phân lớp h 1  (hay h 1  học L 1 ) Dùng L 2  mới huấn luyện bộ phân lớp h 2  (hay h 2  học L 2 ) (6). Lặp lại từ bước (3). cho đến khi tập U rỗng hoặc số vòng lặp đạt đến ngưỡng xác định trước.   Có thể viết rút gọn bằng cách bỏ bước (5). ở trên. Bước (6). đổi thành bước (5): Lặp lại từ bước (2). cho đến khi tập U rỗng hoặc số vòng lặp đạt đến ngưỡng xác định trước.   Phương pháp học có giám sát (Supervised Learning) *  Khái niệm học có giám sát : Học có giám sát là học với tập dữ liệu huấn luyện ban đầu hoàn toàn được gán nhãn từ trước. Học có giám sát là phương pháp học sử dụng cho lớp bài toán phân lớp, phân loại (Classification).   *  Nội dung phương pháp học có giám sát : -  Để thực hiện phân lớp, trước tiên phải chuẩn bị một tập dữ liệu huấn luyện (trainning data set), để có  tập dữ liệu huấn luyện  phải thực hiện gán nhãn cho dữ liệu ban đầu, đây được gọi là quá trình thu thập tập huấn luyện.  -  Lựa chọn một thuật toán phân lớp (ví dụ SVM) xây dựng bộ phân lớp để  học  tập dữ liệu huấn luyện. Hay nói cách khác, dùng tập dữ liệu huấn luyện để huấn luyện bộ phân lớp. Thuật ngữ  học có giám sát  được hiểu là  học  tập dữ liệu đã được gán nhãn trước (các dữ liệu kèm theo nhãn tương ứng này coi như đã được giám sát bởi người thực hiện gán nhãn). -  Sử dụng một tập dữ liệu kiểm tra (test data set) đã được gán nhãn trước, để kiểm tra tính đúng đắn của bộ phân lớp. Sau đó, có thể dùng bộ phân lớp để phân lớp cho các dữ liệu mới.   *  Một số thuật toán học có giám sát : Một số thuật toán thường được lựa chọn khi xây dựng bộ phân lớp gồm có: máy vector hỗ trợ (Support Vector Machine – SVM); k láng giềng gần nhất (K Nearest Neighbours – KNN); tiếp cận xác suất thống kê (Naïve Bayes – NB); Cây quyết định (Decision Tree – DT); sử dụng mạng nơron (Neural Network – Nnet); dựa trên vector trọng tâm (Centroid–base vector); hay tuyến tính bình phương nhỏ nhất (Linear Least Square Fit – LLSF). (Chi tiết các thuật toán này có thể tham khảo trên Internet). Từ khóa đại diện:  Machine Learning ,  Học máy ,  máy học Điểm chủ đề: 20 \r\n\t\t\t\t\t\t\t\t        05-17-2013 01:16 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  TienPhong Tham gia 03-04-2012 Điểm 110 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Các phương pháp học máy (Machine Learning)\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Mình đính chính một vài lỗi soạn thảo nhỏ: Trong phần “Bản chất của học máy dưới góc nhìn của xác suất thống kê”  - x 1 , x 2 , …, x l  là các mẫu,  x i  thuộc R n .  Các mẫu x i  được phát sinh ngẫu nhiên theo một hàm phân bố xác suất F(x) nào đó mà ta không biết. - y i  là các kết quả học tương ứng với mẫu  x i , y i  thuộc R . Thường thì y là kết quả của một hàm f(x) nào đó – đơn trị. Tuy nhiên trong trường hợp tổng quát thì y không đơn trị. Do đó y được xác định theo một hàm phân bố điều kiện F(y|x) mà ta cũng không biết. … Trong trường hợp khi  y thuộc R , tức đây là vấn đề hồi quy (regression).  Trong trường hợp bài toán phân lớp (classification) thì  y thuộc{-1, 1}  là trường hợp nhận dạng hai lớp, nếu y i  = -1 thì x i  thuộc lớp thứ nhất (không được quan tâm), còn y i  = 1 thì x i  thuộc lớp thứ 2 (lớp được quan tâm).   Phần “ Thuật toán Self-training thực hiện như sau:” Lặp (cho đến khi C =  rỗng ): … iii. Tìm tập con C’  <ký hiệu là con thực sự của>  C có độ tin cậy cao nhất: D + C’  =>  D; C – C’  =>  C. Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        06-25-2013 11:27 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  thanhthi Tham gia 06-25-2013 Điểm 20 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Các phương pháp học máy (Machine Learning)\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ  Em có bài toán thế này mong các anh giúp đỡ - Dữ liệu đầu vào là các chỉ số của 1 mã chứng khoán theo ngày: Giá mở đóng cửa, chỉ số trung bình trượt 5 ngày, 25 ngày ... (đầu vào khoảng 10 chiều ) - và được gán nhãn là Tăng hoặc giảm - Em muốn dùng SVM để dự đoán ngày tiếp theo sẽ tăng hay giảm thì em cần phải làm như nào ạ, có thể dùng Weka với libsvm được không ạ? hoặc tool nào có thể giải quyết được bài toán này ạ ?   Các anh, các chị nào biết vui lòng chỉ giáo cho em với, em đang cần rất gấp  Cảm ơn các anh chị nhiêu ạ ! Nếu có tài liệu mong anh chị gửi cho em vào mail giúp em ạ    Trần Thị Thanh:  thanhtd112@gmail.com Điểm chủ đề: 20 Trang 1 trong số 1 (6 nội dung)  ©2008-2017 Business Intelligence Solution. All rights reserved."
        },
        {
          "title": "Học máy",
          "relevance": "1",
          "url": "https://vi.wikipedia.org/wiki/H%E1%BB%8Dc_m%C3%A1y",
          "content": "Bách khoa toàn thư mở Wikipedia Bạn có  tin nhắn mới  ( thay đổi gần đây ). \n\t\t\t\t\tBước tới:\t\t\t\t\t menu , \t\t\t\t\t tìm kiếm Học máy , có tài liệu gọi là  Máy học , ( tiếng Anh :  machine learning ) là một lĩnh vực của  trí tuệ nhân tạo  liên quan đến việc nghiên cứu và xây dựng các kĩ thuật cho phép các hệ thống \"học\" tự động từ dữ liệu để giải quyết những vấn đề cụ thể. Ví dụ như các máy có thể \"học\" cách phân loại  thư điện tử  xem có phải  thư rác (spam)  hay không và tự động xếp thư vào thư mục tương ứng. Học máy rất gần với  suy diễn thống kê  (statistical inference) tuy có khác nhau về thuật ngữ. Học máy có liên quan lớn đến  thống kê , vì cả hai lĩnh vực đều nghiên cứu việc phân tích dữ liệu, nhưng khác với thống kê, học máy tập trung vào sự phức tạp của các giải thuật trong việc thực thi tính toán. Nhiều bài toán suy luận được xếp vào loại bài toán  NP-khó , vì thế một phần của học máy là nghiên cứu sự phát triển các giải thuật suy luận xấp xỉ mà có thể xử lý được. Học máy có hiện nay được áp dụng rộng rãi bao gồm  máy truy tìm dữ liệu ,  chẩn đoán y khoa , phát hiện  thẻ tín dụng giả , phân tích  thị trường chứng khoán , phân loại các  chuỗi DNA ,  nhận dạng tiếng nói  và  chữ viết ,  dịch tự động ,  chơi trò chơi  và  cử động rô-bốt  ( robot locomotion ). Mục lục 1 Định nghĩa 2 Biểu diễn 3 Tính phổ quát 4 Tương tác với con người 5 Tương quan với Khai phá dữ liệu 6 Các loại giải thuật 7 Các chủ đề về máy học 8 Xem thêm 9 Tham khảo 10 Liên kết ngoài 10.1 Tài nguyên chung 10.2 Tạp chí và Hội thảo 10.3 Nhóm nghiên cứu 10.4 Phần mềm Định nghĩa [ sửa  |  sửa mã nguồn ] Dưới góc nhìn của  trí tuệ nhân tạo , động lực chính học máy bởi là nhu cầu thu nhận tri thức (knowledge acquisition). Thật vậy, trong nhiều trường hợp ta cần kiến thức chuyên gia là khan hiếm (không đủ chuyên gia ngồi phân loại lừa đảo thẻ tín dụng của tất cả giao dịch hàng ngày) hoặc chậm vì một số nhiệm vụ cần đưa ra quyết định nhanh chóng dựa trên xử lý dữ liệu khổng lồ (trong mua bán chứng khoán phải quyết định trong vài khoảng khắc của giây chẳng hạn) và thiếu ổn định thì buộc phải cần đến máy tính. Ngoài ra, đại đa số dữ liệu sinh ra ngày nay chỉ phù hợp cho máy đọc (computer readable) tiềm tàng ngưồn kiến thức quan trọng. Máy học nghiên cứu cách thức để mô hình hóa bài toán cho phép máy tính tự động hiểu, xử lý và học từ dữ liệu để thực thi nhiệm vụ được giao cũng như cách đánh giá giúp tăng tính hiệu quả. Tom Mitchell , giáo sư nổi tiếng của Đại học Carnegie Mellon University -  CMU  định nghĩa cụ thể và chuẩn mực hơn như sau: \"Một chương trình máy tính CT được xem là học cách thực thi một lớp nhiệm vụ NV thông qua trải nghiệm KN, đối với thang đo năng lực NL nếu như dùng NL ta đo thấy năng lực thực thi của chương trình có tiến bộ sau khi trải qua KN\" (máy đã học). [1] Biểu diễn [ sửa  |  sửa mã nguồn ] Biểu diễn (tiếng Anh: representation) là một trong những vấn đề quan trọng của học máy. Biểu diễn ở đây có thể hiểu làm sao ghi mã (encode) những thông tin của thế giới thật giúp hoàn thành nhiệm vụ một cách hiệu quả và đầy đủ nhất có thể. Thông tin ở đây bao hàm cả thông tin về dữ liệu đầu vào, đầu ra hay các trạng thái của hệ thống; cũng như cách đánh giá hiệu quả của chương trình. Thông thường, trong học máy người ta hay xây dựng các mô hình sử dụng những  biến ngẫu nhiên  cho việc biểu diễn dữ liệu và nội trạng thái của hệ thống.  Ví dụ:  dùng biến ngẫu nhiên để biểu thị cho tính chất của email là spam (tương ứng giá trị 0) hay là bình thường (tương ứng 1). Mối tương quan giữa các biến ngẫu nhiên này có thể sử dụng ví dụ như  mô hình xác suất dạng đồ thị  để miêu tả. Mặt khác, để đo hiệu quả có thể dùng các  hàm thiệt hại  (hay  hàm tiện ích , trong tiếng Anh là  loss function  và  utility function  tương ứng). Tính phổ quát [ sửa  |  sửa mã nguồn ] Một trong những trọng tâm khác của học máy là đạt được tính phổ quát (tiếng Anh: generalization), nói cách khác là tính chất của chương trình có thể làm việc tốt với dữ liệu mà nó chưa gặp bao giờ (tiếng Anh: unseen data). Một chương trình chỉ hiệu quả với dữ liệu đã gặp nhìn chung không có nhiều tính hữu dụng. Lấy ví dụ về xếp thư điện tử tự động như trên, một hệ thống tự động sau khi trải qua quá trình học từ dữ liệu (\"training\") có thể suy diễn một số nguyên tắc riêng (chẳng hạn như xem xét nội dung: nếu thư được viết bằng tiếng Anh mà chứa một số từ như \"porn\", \"sell\", \"good product\" hoặc người gửi đến từ Somalia trong khi người nhận ở Hà Nội không thân quen nhau) để quyết định xem có phải là  thư rác  hay không. Tuy nhiên, nếu như trong dữ liệu bài giảng ( training data ) có ngôn ngữ khác trong thực tế (tiếng Việt thay vì tiếng Anh) hoặc thậm chí không phải dạng thuần văn bản (dạng ảnh khiến cho bóc tách nội dung khó hơn hoặc không thể) thì rất có thể máy sẽ dự báo không chính xác nữa. Một số chương trình có thể tự động cập nhật trong thời gian thực (ví dụ như người sử dụng có chỉ ra rằng thư bị sắp xếp sai danh mục). Tương tác với con người [ sửa  |  sửa mã nguồn ] Một số hệ thống học máy nỗ lực loại bỏ nhu cầu trực giác của con người trong việc phân tích dữ liệu, trong khi các hệ thống khác hướng đến việc tăng sự cộng tác giữa người và máy. Không thể loại bỏ hoàn toàn tác động của con người vì các nhà thiết kế hệ thống phải chỉ định cách biểu diễn của dữ liệu và những cơ chế nào sẽ được dùng để tìm kiếm các đặc tính của dữ liệu. Học máy có thể được xem là một nỗ lực để tự động hóa một số phần của  phương pháp khoa học . Một số nhà nghiên cứu học máy tạo ra các phương pháp bên trong các khuôn khổ của  thống kê Bayes . Tương quan với Khai phá dữ liệu [ sửa  |  sửa mã nguồn ] Khai phá dữ liệu  và học máy là hai khái niệm hay bị nhầm lẫn. Hai lĩnh vực này nhìn chung gần với nhau và đôi khi dùng chung nhiều phương pháp, công cụ nhưng khác biệt chính là ở mục tiêu: Khai phá dữ liệu: thường mục tiêu là tìm kiếm những thông tin, tri thức hoàn toàn mới tiềm năng có ích trong nguồn dữ liệu. Học máy: dự đoán một số thông tin của dữ liệu dựa trên những đặc tính đã biết. Các loại giải thuật [ sửa  |  sửa mã nguồn ] Các  thuật toán  học máy được phân loại theo kết quả mong muốn của thuật toán. Các loại thuật toán thường dùng bao gồm: Học có giám sát —trong đó, thuật toán tạo ra một hàm ánh xạ dữ liệu vào tới kết quả mong muốn. Một phát biểu chuẩn về một việc học có giám sát là bài toán  phân loại : chương trình cần học (cách xấp xỉ biểu hiện của) một hàm ánh xạ một vector  [ X 1 , X 2 , … X N ] {\\displaystyle [X_{1},X_{2},\\ldots X_{N}]}  tới một vài lớp bằng cách xem xét một số mẫu dữ liệu - kết quả của hàm đó. Học không giám sát —mô hình hóa một tập dữ liệu, không có sẵn các ví dụ đã được gắn nhãn. Học nửa giám sát —kết hợp các ví dụ có gắn nhãn và không gắn nhãn để sinh một hàm hoặc một bộ phân loại thích hợp. Học tăng cường —trong đó, thuật toán học một chính sách hành động tùy theo các quan sát về thế giới. Mỗi hành động đều có tác động tới môi trường, và môi trường cung cấp thông tin phản hồi để hướng dẫn cho thuật toán của quá trình học. Chuyển đổi —tương tự học có giám sát nhưng không xây dựng hàm một cách rõ ràng. Thay vì thế, cố gắng đoán kết quả mới dựa vào các dữ liệu huấn luyện, kết quả huấn luyện, và dữ liệu thử nghiệm có sẵn trong quá trình huấn luyện. Học cách học —trong đó thuật toán học  thiên kiến quy nạp  của chính mình, dựa theo các kinh nghiệm đã gặp. Phân tích hiệu quả các thuật toán học máy là một nhánh của ngành  thống kê , được biết với tên  lý thuyết học điện toán . Các chủ đề về máy học [ sửa  |  sửa mã nguồn ] Danh sách các chủ đề của môn học này: Mô hình hóa  các hàm mật độ xác suất điều kiện :  hồi quy  và  phân loại Mạng nơ-ron Máy học cực độ (Extreme learning machine) Cây quyết định Lập trình biểu thức gen Lập trình di truyền Hồi quy quá trình Gauss Phân tích biệt thức tuyến tính k láng giềng gần nhất Độ dài thông điệp tối thiểu Cảm tri nguyên Hàm cơ sở xuyên tâm Máy vector hỗ trợ (Support Vector Machine) Mô hình hóa các  hàm mật độ xác suất  qua các  mô hình phát sinh :\n Thuật toán cực đại kì vọng Các  mô hình đồ họa  gồm  mạng Bayes  và  mạng Markov Ánh xạ topo phát sinh Các kỹ thuật suy luận xấp xỉ đúng:\n Chuỗi Markov phương pháp Monte Carlo Phương pháp biến thiên Tối ưu hóa : hầu hết các phương pháp trên đều sử dụng tối ưu hóa hoặc là các thể hiện của các thuật toán tối ưu hóa. Xem thêm [ sửa  |  sửa mã nguồn ] Trí tuệ nhân tạo Trí tuệ điện toán Khai phá dữ liệu Nhận dạng mẫu Các ẩn bản quan trọng trong học máy (khoa học máy tính) Các ấn bản quan trọng trong học máy (thống kê) Rô-bốt tự hành Lập trình suy diễn lôgic Tham khảo [ sửa  |  sửa mã nguồn ] Bishop C. M. (1995).  Neural Networks for Pattern Recognition , Nhà in Đại học Oxford.  ISBN 0-19-853864-2 Richard O. Duda, Peter E. Hart, David G. Stork (2001)  Pattern classification  (ấn bản lần 2), Wiley, New York,  ISBN 0-471-05669-3 . MacKay D. J. C. (2003).  Information Theory, Inference, and Learning Algorithms , Nhà in Đại học Cambridge.  ISBN 0-521-64298-1 Sholom Weiss và Casimir Kulikowski (1991).  Computer Systems That Learn , Morgan Kaufmann.  ISBN 1-55860-065-5 ^ * Mitchell, T. (1997).  Machine Learning , McGraw Hill.  ISBN 0-07-042807-7 , p.2. Liên kết ngoài [ sửa  |  sửa mã nguồn ] Tài nguyên chung [ sửa  |  sửa mã nguồn ] UCI description MLnet Mailing List Kmining List of machine learning, data mining and KDD scientific conferences Book \" Intelligent Systems and their Societies \" của  Walter Fritz Links from Open Directory Project Eruditionhome  - nơi chứa nhiều mục đề về Học máy MLpedia  - Từ điển bách khoa wiki dành riêng cho chủ đề Học máy Tạp chí và Hội thảo [ sửa  |  sửa mã nguồn ] Journal of Machine Learning Research Machine Learning Journal Machine Learning papers  tại CiteSeer NIPS: Neural Information Processing Systems ICML: International Conference on Machine Learning Nhóm nghiên cứu [ sửa  |  sửa mã nguồn ] Machine Learning  tại Đại học Hebrew Machine Learning and Natural Language Processing  tại Đại học Freiburg Machine Learning and Data Mining in Bioinformatics Group  tại TU München Machine Learning and Biological Computation Group  tại Đại học Bristol Machine Learning and Applied Statistics  của Microsoft Research Department of Knowledge Technologies  của Học viện Jozef Stefan Statistical Multimedia Learning Group  tại Đại học British Columbia Machine Learning Systems Group  tại Jet Propulsion Laboratory, Học viện Kỹ thuật California Department of Empirical Inference  tại Viện Max Planck về điều khiển học sinh học, Tübingen Machine Learning Group  tại Đại học Toronto Intelligent Data Analysis Group  tại Fraunhofer FIRST, Berlin Machine Learning Group  tại Đại học Tự do Bruxelles Phần mềm [ sửa  |  sửa mã nguồn ] Chương trình mạng nơ ron đa lớp (Multi Layer Neural Network) và mạng nơ ron tự tổ chức (Self Organizing Maps) có giải thích bằng tiếng Việt. Sử dụng phần mềm mạng nơ ron 3 lớp Spice-MLP Sử dụng phần mềm mạng tự tổ chức Spice-SOM Hướng dẫn sử dụng mạng nơ ron trong các ứng dụng thực tế  trong đó có minh họa phân loại ảnh khuôn mặt, ảnh người đi bộ, ảnh xe hơi, dự báo chứng khoán và một số ví dụ khác SPIDER  - một hộp công cụ học máy hoàn chỉnh cho Matlab PRTools  PRTools là một gói phần mềm hoàn chỉnh khác tương tự SPIDER và được cài trong Matlab. SPIDER có vẻ có nhiều hỗ trợ mức thấp, nhưng các công cụ của PRTools có phần đa dạng hơn. PRTools có sách và tài liệu tốt. Cả SPIDER và PRTools được cung cấp miễn phí trên mạng cho các ứng dụng phi thương mại. Orange , bộ chương trình học máy với các script viết bằng Python và giao diện lập trình đồ họa YALE  là một công cụ mạnh miễn phí cho Học máy và  Khai phá dữ liệu Weka Machine Learning Software Matlab  MATLAB có hỗ trợ hộp công cụ cho nhiều công cụ học máy. Hiện giờ hộ công cụ Tin sinh học đã có Support Vector Machines và các bộ phân loại KNN (k láng giềng gần nhất). Hộp công cụ thống kê thực hiện biệt thức tuyến tính và phân loại bằng cây quyết định. Hộp công cụ mạng nơ-ron là một bộ công cụ hoàn chỉnh để cài đặt mạng nơron. Trong thời gian gần đây, các phương pháp mới để đánh giá hiệu quả của các bộ phân loại và để thẩm định chéo đã làm Matlab trở nên hấp dẫn hơn đối với học máy. MLC++  là thư viện lớp C++ dành cho học có giám sát MDR  là một gói phần mềm nguồn mở dành cho việc phát hiện các tương tác thuộc tính bằng phương pháp  rút gọn thứ nguyên đa thừa số  (MDR). x t s Những lĩnh vực chính của  khoa học máy tính Các nền tảng toán học Logic toán học   · Lý thuyết tập hợp   · Lý thuyết số   · Lý thuyết đồ thị   · Lý thuyết kiểu   · Lý thuyết thể loại   · Giải tích số   · Lý thuyết thông tin   · Đại số   · Nhận dạng mẫu   · Nhận dạng tiếng nói   · Toán học tổ hợp   · Đại số Boole   · Toán rời rạc Lý thuyết phép tính Độ phức tạp Kolmogorov   · Lý thuyết Automat   · Lý thuyết tính được   · Lý thuyết độ phức tạp tính toán   · Lý thuyết điện toán lượng tử Các cấu trúc dữ liệu \nvà  các giải thuật Phân tích giải thuật   · Thiết kế giải thuật   · Hình học tính toán   · Tối ưu hóa tổ hợp Các ngôn ngữ lập trình \nvà  Các trình biên dịch Các bộ phân tích cú pháp   · Các trình thông dịch   · Lập trình cấu trúc   · Lập trình thủ tục   · Lập trình hướng đối tượng   · Lập trình hướng khía cạnh   · Lập trình hàm   · Lập trình logic   · Lập trình máy tính   · Lập trình mệnh lệnh   · Lập trình song song   · Lập trình tương tranh   · Các mô hình lập trình   · Prolog   · Tối ưu hóa trình biên dịch Tính song hành , Song song , \nvà các hệ thống  phân tán Đa xử lý   · Điện toán lưới   · Kiểm soát song hành   · Hiệu năng hệ thống   · Tính toán phân tán Công nghệ phần mềm Phân tích yêu cầu   · Thiết kế phần mềm   · Các phương pháp hình thức   · Kiểm thử phần mềm   · Quy trình phát triển phần mềm   · Các phép đo phần mềm   · Đặc tả chương trình   · LISP   · Mẫu thiết kế   · Tối ưu hóa phần mềm Kiến trúc hệ thống Kiến trúc máy tính   · Tổ chức máy tính   · Các hệ điều hành   · Các cấu trúc điều khiển   · Cấu trúc bộ nhớ lưu trữ   · Vi mạch   · Thiết kế ASIC   · Vi lập trình   · Vào/ra dữ liệu   · VLSI design   · Xử lý tín hiệu số Viễn thông \nvà  Mạng máy tính Audio máy tính   · Chọn tuyến   · Cấu trúc liên kết mạng   · Mật mã học Các cơ sở dữ liệu \nvà  Các hệ thống thông tin Hệ quản trị cơ sở dữ liệu   · Cơ sở dữ liệu quan hệ   · SQL   · Các giao dịch   · Các chỉ số cơ sở dữ liệu   · Khai phá dữ liệu   · Biểu diễn và giao diện thông tin   · Các hệ thống thông tin   · Khôi phục dữ liệu   · Lưu trữ thông tin   · Lý thuyết thông tin   · Mã hóa dữ liệu   · Nén dữ liệu   · Thu thập thông tin Trí tuệ nhân tạo Lập luận tự động   · Ngôn ngữ học tính toán   · Thị giác máy tính   · Tính toán tiến hóa   · Các hệ chuyên gia    · Học máy   · Xử lý ngôn ngữ tự nhiên   · Robot học Đồ họa máy tính Trực quan hóa   · Hoạt họa máy tính   · Xử lý ảnh Giao diện người-máy tính Khả năng truy cập máy tính   · Giao diện người dùng   · Điện toán mang được   · Điện toán khắp mọi nơi   · Thực tế ảo Khoa học tính toán Cuộc sống nhân tạo   · Tin sinh học   · Khoa học nhận thức   · Hóa học tính toán   · Khoa học thần kinh tính toán   · Vật Lý học tính toán   · Các giải thuật số   · Toán học kí hiệu Chú ý: khoa học máy tính còn có thể được chia thành nhiều chủ đề hay nhiều lĩnh vực khác dựa theo  Hệ thống xếp loại điện toán ACM . x t s Những Chuyên ngành chính của  Tin học  •   Phần cứng  •   Phần mềm Công nghệ thông tin Cuộc sống nhân tạo Đa xử lý Điện toán lưới Đồ họa máy tính Hệ chuyên gia Hệ thống thông tin quản lý Hoạt họa máy tính Khoa học nhận thức Khoa học tính toán Khoa học thần kinh tính toán Khoa học thông tin Kiểm soát song hành Kiến trúc hệ thống Lập luận tự động Ngôn ngữ hình thức Ngôn ngữ học tính toán Người máy Robot học Thực tế ảo Tính toán song song Tối ưu hóa trình biên dịch Tổ chức máy tính Trí tuệ nhân tạo Từ điển học Tương tranh Vật lý học tính toán Hệ thống thông tin An toàn thông tin Cơ sở dữ liệu đa phương tiện Cơ sở dữ liệu thông minh Dữ liệu lớn Hệ cơ sở tri thức Hệ dựa trên logic Hệ gợi ý Hệ thích nghi dựa trên ngữ cảnh Hệ thống hướng tác tử Hệ thống thông minh Hệ thống thông tin địa lý Hệ trợ giúp quyết định Kỹ nghệ dữ liệu Kỹ nghệ tri thức Logic mờ Phân tích dữ liệu Phân tích và thiết kế hệ thống Quản trị dự án Quản trị tri thức Thiết kế và quản trị dữ liệu Tích hợp dữ liệu Tính toán hiệu năng cao Web ngữ nghĩa Xử lý thông tin mờ Khoa học máy tính Cơ sở dữ liệu phân tán Hệ quản trị cơ sở dữ liệu Hệ thống đa lõi Hệ thống truyền thông Hình học tính toán Hóa học tính toán Học máy Khai phá dữ liệu Lập trình song song Lý thuyết mã hóa Lý thuyết tính toán Ngôn ngữ và phương pháp dịch Nguyên lý ngôn ngữ lập trình Quy hoạch ràng buộc Sinh học tính toán  ( Tin sinh học ) Thiết kế và phân tích thuật toán Tìm kiếm thông tin Tính toán khoa học Tính toán kí hiệu Tính toán phân tán Tính toán tiến hóa Tính toán tự nhiên Tối ưu hoá tổ hợp Xử lý song song Kỹ thuật máy tính Đa phương tiện Định vị vệ tinh  ( GNSS ) Giao diện người dùng Ghép nối máy tính Hệ nhúng Hệ thống thời gian thực Hiệu năng hệ thống Kiến trúc máy tính Lập trình đôi Lập trình đồ họa Lập trình hệ thống Lý thuyết nhận dạng Mạng nơ-ron Nhận dạng tiếng nói Phân tích tín hiệu Thị giác máy tính Thiết kế IC Thoại IP Tổng hợp giọng nói Tương tác người–máy tính Vi xử lý Xử lý ảnh Xử lý dữ liệu đa phương tiện Xử lý ngôn ngữ tự nhiên Xử lý tiếng nói Xử lý tín hiệu số Kỹ nghệ phần mềm Bảo trì phần mềm Các phương pháp hình thức Chất lượng phần mềm Đảm bảo chất lượng phần mềm Đánh giá phần mềm Đo lường và quản trị phần mềm Độ tin cậy và chịu lỗi phần mềm Kiểm thử phần mềm Kiến trúc doanh nghiệp Kiến trúc phần mềm Kinh tế công nghệ phần mềm Kỹ nghệ hướng dịch vụ Lập trình linh hoạt Mẫu thiết kế Mô hình hóa phần mềm Phân tích hệ thống Phân tích thiết kế hướng đối tượng  ( UML ) Phân tích yêu cầu phần mềm Phát triển phần mềm Quản lý cấu hình phần mềm Quản lý dự án phần mềm Quản lý kỹ thuật phần mềm Quy trình phát triển phần mềm  ( Vòng đời phát hành phần mềm ) Thiết kế phần mềm Triển khai phần mềm Tối ưu hóa phần mềm Mạng máy tính An ninh mạng An ninh trong giao dịch điện tử Đánh giá hiệu năng mạng  ( QoS ) Điện toán đám mây Định tuyến Hệ phân tán Kỹ thuật truyền thông Lý thuyết thông tin Mạng không dây Mạng thế hệ mới Mạng thiết bị di động Mạng thông tin quang Mật mã học Mô phỏng mạng Nhận dạng Quản trị mạng Thiết bị truyền thông và mạng Thiết kế mạng Tính toán khắp nơi và di động Trung tâm dữ liệu Truyền thông di động Truyền thông đa phương tiện Truyền thông số Vệ tinh thông tin Viễn thông  ( Mạng viễn thông ) Ước lượng tín hiệu và hệ thống Web thế hệ mới Tin học kinh tế x t s Giám đốc công nghệ thông tin  ·  Tin học kinh tế  ·  Quản lý công nghệ thông tin Quản lý ITIL  &  ITSM Định hướng phát triển Phát triển nhân lực Quản lý bảo mật Quản lý chất lượng Quản lý công nghệ Quản lý dự án Quản lý mua sắm Quản lý ngân sách Quản lý nguồn lực Quản lý phát hành Quản lý rủi ro Quản lý tài sản Quản lý thay đổi Quản lý tích hợp Quản lý tổ chức Quản lý truyền thông Quản lý tuân thủ Quản lý vấn đề Thiết kế giải pháp Xây dựng chiến lược Xây dựng chính sách Quản lý mạng Ảo hóa Mạng campus Mạng diện rộng Mạng nội bộ Mạng riêng ảo STP VLAN IVR VTP Quản trị hệ thống Hoạt động vận hành Bảo trì thiết bị Bảo vệ hệ thống Đối phó sự cố Kế hoạch dự phòng Hoạt động kỹ thuật Hỗ trợ kỹ thuật Kiểm soát truy cập Kiểm tra hệ thống Xác thực người dùng Hoạt động an toàn An ninh nhân sự An ninh hệ thống Nhận thức an toàn Rủi ro hệ thống Quản lý hệ thống Bàn dịch vụ Quản lý cấu hình Quản lý công suất Quản lý dịch vụ Quản lý hạ tầng Quản lý khôi phục Quản lý người dùng Quản lý sự cố Quản lý tính liên tục Quản lý tính sẵn sàng Tổ chức công việc Tổ chức hỗ trợ Kỹ năng lãnh đạo Kỹ năng cộng tác nhóm Kỹ năng đàm phán Kỹ năng giải quyết vấn đề Kỹ năng giao tiếp Kỹ năng gọi thoại Kỹ năng huấn luyện Kỹ năng lắng nghe Kỹ năng phân công ủy thác Kỹ năng phỏng vấn tuyển dụng Kỹ năng quản lý thời gian Kỹ năng tạo động lực Kỹ năng tư duy Kỹ năng thiết kế quy trình Kỹ năng thuyết trình Kỹ năng viết tài liệu kỹ thuật Ứng dụng Chính phủ điện tử Giáo dục trực tuyến Hoạch định tài nguyên doanh nghiệp Kinh doanh điện tử  ( Mua sắm trực tuyến    · Thương mại điện tử    · Tiếp thị trực tuyến ) Kinh doanh thông minh Quản lý quan hệ khách hàng Quản lý tri thức Các lĩnh vực liên quan Kinh tế Luật pháp Tài chính Kế toán Kinh doanh Tổ chức Xã hội Quản lý Quản trị kinh doanh \n\t\t\t\t\t\tLấy từ “ https://vi.wikipedia.org/w/index.php?title=Học_máy&oldid=32001081 ”\t\t\t\t\t Thể loại :  Học máy Trí tuệ nhân tạo Học tập Điều khiển học Thể loại ẩn:  Trang sử dụng liên kết tự động ISBN Trình đơn chuyển hướng Công cụ cá nhân Chưa đăng nhập Thảo luận cho địa chỉ IP này Đóng góp Mở tài khoản Đăng nhập Không gian tên Bài viết Thảo luận Biến thể Các hiển thị Đọc Sửa đổi Sửa mã nguồn Xem lịch sử Khác Tìm kiếm Xem nhanh Trang Chính Bài viết chọn lọc Tin tức Bài viết ngẫu nhiên Thay đổi gần đây Phản hồi lỗi Quyên góp Tương tác Hướng dẫn Giới thiệu Wikipedia Cộng đồng Thảo luận chung Giúp sử dụng Liên lạc Công cụ Các liên kết đến đây Thay đổi liên quan Các trang đặc biệt Liên kết thường trực Thông tin trang Khoản mục Wikidata Trích dẫn trang này In/xuất ra Tạo một quyển sách Tải về dưới dạng PDF Bản để in ra Tại dự án khác Wikimedia Commons Ngôn ngữ khác العربية অসমীয়া Azərbaycanca Bahasa Indonesia Български Català Čeština Dansk Deutsch Eesti Ελληνικά English Español Euskara فارسی Français 한국어 Հայերեն हिन्दी Íslenska Italiano עברית ಕನ್ನಡ Latviešu Lietuvių Magyar Македонски മലയാളം मराठी Nederlands 日本語 Norsk Norsk nynorsk Polski Português Русский Shqip Simple English Slovenščina Српски / srpski Srpskohrvatski / српскохрватски Suomi Svenska Tagalog தமிழ் ไทย Türkçe Українська 中文 Sửa liên kết  Trang này được sửa đổi lần cuối lúc 07:53 ngày 14 tháng 10 năm 2017. Văn bản được phát hành theo  Giấy phép Creative Commons Ghi công–Chia sẻ tương tự ; có thể áp dụng điều khoản bổ sung. Với việc sử dụng trang web này, bạn chấp nhận  Điều khoản Sử dụng  và  Quy định quyền riêng tư . \nWikipedia® là thương hiệu đã đăng ký của  Wikimedia Foundation, Inc. , một tổ chức phi lợi nhuận. Quy định quyền riêng tư Giới thiệu Wikipedia Lời phủ nhận Nhà phát triển Tuyên bố về cookie Phiên bản di động"
        },
        {
          "title": "Tôi là Duyệt",
          "relevance": "1",
          "url": "http://blog.duyet.net/2017/08/machine-learning-is-fun.html",
          "content": "Tôi là Duyệt \nMachine Learning is Fun! (Vietnamese version)\n \nChuỗi bài viết  \"Machine Learning is Fun!\"  này mình lược dịch từ bài viết gốc của tác giả ageitgey. Mình tin chắc có rất nhiều bạn đã và đang quan tâm đến Machine Learning hiện nay. \"Machine Learning is Fun!\" chắc chắn sẽ mang cho bạn đến cho bạn cái nhìn từ cơ bản đến chuyên sâu nhất về thế giới Machine Learning. Bài gốc:  https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471 \nBạn đã từng nghe rất nhiều người nói về Machine Learning, nhưng chúng chỉ là các thông tin rất mù mờ? \nChuỗi bài viết này dành cho những ai muốn tìm hiểu về Machine Learning nhưng chưa biết bắt đầu từ đâu. Tôi cá rằng có rất nhiều người đã cố gắng đọc  bài viết về Machine Learning này trên Wikipedia , và họ phải rất vọng và bỏ cuộc vì những định nghĩa giải thích trong đó. \nMachine Learning là gì? Machine Learning  là một ý tưởng về một thuật toán tổng quát chung có thể nói cho bạn biết vài điều về các khía cạnh khác nhau của bộ dữ liệu, mà bạn không cần phải viết bất cứ dòng code đặc biệt nào để giải quyết vấn đề. Thay vì bạn viết code, bạn đổ dữ liệu vào các thuật toán và chúng sẽ tự xây dựng các logic dựa vào dữ liệu đó. \nVí dụ, một loại thuật toán cơ bản đó là phân lớp (classification algorithm), thuật toán này cho phép chia dữ liệu thành nhiều nhóm khác nhau. Một thuật toán dùng để nhận dạng chữ số viết tay (recognize handwritten numbers) có thể được sử dụng để phân loại email (thành  spam  và  không spam ), mà không cần phải code lại. \nHai bài toán trên không một thuật toán, nhưng khác dữ liệu. Thuật toán Machine Learning này là black-box, có thể được sử dụng để giải quyết nhiều bài toán phân lớp khác nhau. \n\"Machine Learning\" là một thuật ngữ chung bao hàm rất nhiều thuật toán như trên. \nHai loại thuật toán Machine Learning \nThuật toán của Machine Learning được chia thành hai nhóm lớn - học giám sát ( supervised learning ) và học không giám sát ( unsupervised learning ). Sự khác nhau giữa hai nhóm này đơn giản, nhưng rất quan trọng. \nSupervised Learning \nGiả sử bạn là một người làm về bất động sản. Công ty của bạn phát triển nhanh, bạn tuyển hàng loạt thực tập. Nhưng có một vấn đề - bạn có thể dễ dàng nhìn lướt qua và đánh giá chính xác giá trị của một ngôi nhà, nhưng với các thực tập viên không có kinh nghiệm, họ không biết mỗi căn trị giá bao nhiêu. \nĐể giúp đỡ các bạn thực tập này, bạn quyết định viết một ứng dụng nhỏ có thể ước lượng được giá của một ngôi nhà dựa vào  diện tích, khu vực lân cận, ...  \nVà bạn ghi lại thông tin của mọi căn nhà được bán trong thành phố, trong vòng 3 tháng. Với mỗi ngôi nhà, bạn ghi lại mọi thông tin:  số phòng ngủ, diện tích (feet vuông), neighborhood, ...  Nhưng quan trọng nhất là  giá (price)  cuối cùng của căn nhà được bán: Chúng ta có được \"training data\" \nVới dữ liệu \"training data\" như trên, chúng ta muốn viết một ứng dụng có thể ước tính được giá của một căn nhà tương tự khác: Chúng ta muốn sử dụng training data để dự đoán giá của những ngôi nhà khác. \nĐây được gọi là  supervised learning . Bạn biết được giá mỗi căn nhà được bán đi, nói cách khác, bạn biết được câu trả lời của bài toán, và có thể từ đây suy ra được logic của vấn đề. \nĐể xây dựng ứng dụng này, bạn cho training data về mỗi ngôi nhà này vào một thuật toán machine learning. Thuật toán sẽ cố gắng tìm ra loại tính toán nào để các con số có thể work. \nNó giống như việc tìm các toán tử trong bài tập toán hồi lớp 1 chúng ta vẫn hay được học: \nTừ bảng trên, bạn có thể tìm ra được các phép toán nào để có được kết quả bên phải? Bạn biết bạn có nghĩa vụ phải \"làm gì đó\" với những con số ở bên trái để có được câu trả lời ở bên phải. \nVới  supervised learning , bạn đang để máy tính phải tìm ra những quan hệ đó cho bạn. Và một khi bạn biết những phép toán nào cần để giải một số bài toán trên, bạn có thể giải bất kỳ bài toán khác cùng loại! \nUnsupervised Learning \nQuay lại với ví dụ bất động sản, giả sử bạn  không biết  thông tin gì về  giá . Chỉ với thông tin về diện tích, vị trí, ... bạn vẫn có thể làm được vài thứ hay ho. Đây được gọi là  unsupervised learning . Mặc dù bạn không thể dự đoán được giá nhà, bạn vẫn có thể làm vài chuyện hay khác với Machine Learning \nBài toán này giống như ai đó đưa cho bạn một đống hồ sơ và bảo rằng  \"Tao không biết mấy con số này có nghĩa gì, nhưng mà tao nghĩ mày có lẽ sẽ tìm ra được một pattern nào đó, hoặc chia thành các nhóm, hoặc là một cái gì khác. OK?\" \nVậy chúng ta có thể làm gì với data này? Với người mới bắt đầu, bạn có thể có một thuật toán để tự động chia dữ liệu thành các nhóm thị trường khác nhau.  Có thể bạn sẽ tìm ra rằng người mua nhà ở khu vực gần trường đại học thích mua nhà nhỏ với nhiều phòng ngủ, nhưng người mua nhà ở vùng ngoại ô thích nhà có 3 phòng ngủ hơn, diện tích lớn hơn.  Biết được những loại khách hàng này sẽ giúp ích rất nhiều tới chiến lược kinh doanh.  \nMột điều thú vị nữa là bạn có thể làm là tự động xác định các ngôi nhà đặc biệt khác biệt (outlier). Bạn có thể tập trung những người bán hàng giỏi nhất vào những ngôi nhà thuộc dạng outlier này (biệt thự lớn, giá trị cao), vì mức hoa hồng sẽ cao hơn.   \nSupervised learning sẽ được tập trung giới thiệu ở những phần còn lại của bài viết này, nhưng không có nghĩa là unsupervised learning vô dụng hơn hoặc không thú vị bằng. Thực tế, unsupervised learning ngày càng quan trọng hơn bởi vì nó có thể được áp dụng mà không cần có nhãn (label) mà vẫn cho được kết quả đúng. Note: có rất  nhiều loại thuật toán machine learning khác . Nhưng supervised learning và unsupervised learning là hai nhóm cơ bản tốt nhất khi bắt đầu nghiên cứu vào machine learning. \nCó đúng là việc ước lượng (estimate) được giá của một ngôi nhà là \"learning\" hay không? \nVới con người, bộ não của chúng ta có thể tiếp cận mọi vấn đề và học cách giải quyết nó mà không cần chỉ dẫn chi tiết nào. Nếu bạn bán nhà trong một thời gian dài, theo bản năng bạn có thể cảm giác được đâu là giá chính xác của một căn nhà. Mục tiêu của ngành AI là lặp lại được điều đó với máy tính. \nNhưng hiện tại các thuật toán của machine learning của máy tính vẫn chưa đủ tốt, chúng chỉ giải quyết được những vấn đề rất cụ thể và giới hạn. Chúng ta có lẽ nên định nghĩa  \"learning\"  ở đây là  \"tìm ra được cách giải một bài toán cụ thể dựa vào vài ví dụ\". \nKhông may  \"tìm ra được cách giải một bài toán cụ thể dựa vào vài ví dụ\"  không phải là cái tên hay, thay vào đó mọi người gọi ngắn gọn là  \"Machine Learning\" . \nLet’s write that program! \nVậy, làm thế nào để có thể code được chương trình cho ví dụ trên? Bạn có thể suy nghĩ một chút trước khi đọc phần tiếp theo. \nNếu bạn không biết gì về machine learning, theo cách thông thường bạn có thể cố viết ra một số quy luật cơ bản để ước lượng giá nhà như sau: def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):\n  price = 0\n  # In my area, the average house costs $200 per sqft\n  price_per_sqft = 200\n  if neighborhood == \"hipsterton\":\n    # but some areas cost a bit more\n    price_per_sqft = 400\n  elif neighborhood == \"skid row\":\n    # and some areas cost less\n    price_per_sqft = 100\n  # start with a base price estimate based on how big the place is\n  price = price_per_sqft * sqft\n  # now adjust our estimate based on the number of bedrooms\n  if num_of_bedrooms == 0:\n    # Studio apartments are cheap\n    price = price — 20000\n  else:\n    # places with more bedrooms are usually\n    # more valuable\n    price = price + (num_of_bedrooms * 1000)\n return price \nCho dù bạn bỏ ra thêm hàng giờ nữa để code chương trình này, chương trình của bạn vẫn sẽ không bao giờ hoàn hảo và khó để bảo trì. \nTại sao bạn không để máy tính giúp bạn tự tìm ra kết quả? def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):\n  price = <computer, please do some math for me>\n  return price \nMột ý tưởng để giải quyết vấn đề này là bạn biết được  price  được tính bằng tổ hợp của  number of bedrooms ,  square footage  và  neighborhood . Nếu bạn có thể chỉ ra mỗi thành phần tác động đến price cuối cùng như thế nào, vậy có lẽ sẽ tồn tại một tỉ lệ nào đó để kết hợp 3 chỉ số trên để có được  price . \nVới suy nghĩ trên, bạn có thể rút lại chương trình ban đầu thành như này: def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):\n price = 0\n # a little pinch of this\n price += num_of_bedrooms * .841231951398213\n # and a big pinch of that\n price += sqft * 1231.1231231\n # maybe a handful of this\n price += neighborhood * 2.3242341421\n # and finally, just a little extra salt for good measure\n price += 201.23432095\n return price \nChúng ta có những con số đặc biệt sau:  0.841231951398213 ,  1231.1231231 ,  2.3242341421  và  201.23432095 . Đây sẽ là các trọng số  weights . Nếu bạn có thể tìm ra được weights hoàn hảo vừa khớp cho mọi căn nhà, chương trình của chúng ta có thể predict được giá nhà! \nMột cách ngớ ngẩn để tìm ra weight tốt nhất mà mình có thể làm là: Bước 1:  Đặt tất cả weight là  1.0 def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):\n  price = 0\n  # a little pinch of this\n  price += num_of_bedrooms * 1.0\n  # and a big pinch of that\n  price += sqft * 1.0\n  # maybe a handful of this\n  price += neighborhood * 1.0\n  # and finally, just a little extra salt for good measure\n  price += 1.0\n  return price Bước 2:  Với dữ liệu training data mọi căn nhà mà bạn biết giá, sử dụng chương trình trên để đoán giá và xem thử giá này cách xa với giá thực tế bao nhiêu: Sử dụng function để predict giá nhà \nVí dụ với dòng đầu tiên, thực tế nó được bán với giá $250,000, nhưng function của chúng ta đoán nó có giá $178,000, chênh lệch $72,000. \nBây giờ bạn cộng tổng các bình phương giá chênh lệch của mỗi căn nhà trong tập dữ liệu. Giả sử như bạn có 500 căn nhà, và tổng bình phương chênh lệch từ hàm đoán giá của chúng ta so với thực tế là $86,123,373 - con số này thể hiện mức độ \"lỗi\" của chương trình của chúng ta. \nTiếp đến ta lấy tổng đó chia cho 500 để lấy trung bình độ chênh lệnh của mỗi ngôi nhà. Gọi nó là độ lỗi trung bình  cost function . \nNếu bạn có được cost bằng 0 nếu thay đổi các weights, function của bạn sẽ hoàn hảo. Có nghĩa là trong mọi trường hợp, function sẽ đoán chính xác giá của mọi ngôi nhà được đưa vào theo dữ liệu. \nVà đây cùng là nhiệm vụ của chúng ta: làm sao để có được cost thấp nhất có thể bằng cách thử các weights khác nhau. Bước 3:  Lặp lại bước 2 với  mọi tổ hợp weights có thể có . Tìm ra tổ hợp nào giúp ta có được cost gần 0 nhất. Một khi bạn tìm ra được tập weights này, bài toán được giải quyết! \nThật đơn giản đúng không? Bạn lấy một vài dòng dữ liệu, thực hiện 3 bước đơn giản và bạn có được chương trình có thể đoán được giá của mọi căn nhà. \nNhưng khoan, hay xem lại. Có một vài điều sẽ khiến bạn ngạc nhiên: Phương pháp \"ngớ ngẫn\" trên có thể vượt lên đánh bại các chuyên gia. Function của bạn có được một cách ngớ ngẫn. Nó không biết \"square feet\" hay \"bedrooms\" là gì. Tất cả những gì nó biết là tìm ra con số để có được kết quả đúng. Bạn cũng sẽ không biết tại sao các weights đó lại giúp function trả về kết qủa đúng. Vì thế bạn cũng sẽ không thể hiểu để chứng minh rằng nó hoạt động chính xác.  Tưởng tượng rằng thay vì bạn truyền giá trị cho các tham số \"sqft\" hay \"num_of_bedrooms\", predict function có thể nhận một mảng các con số. Ví dụ như mỗi con số là độ sáng của pixel ảnh được chụp từ camera gắn ở trước xe hơi của bạn. Chương trình thay vì dự đoán giá nhà, bạn có thể sẽ có chương trình \"degrees_to_turn_steering_wheel\" (điều chỉnh vô lăng xe hơi). Bạn mới vừa viết ra chương trình để xe hơi tự lái!! \nThật điên rồi đúng không :v  \nThử \"mọi tổ hợp weights có thể có\" ở bước 3 \nDĩ nhiên bạn hoàn toàn có thể thử mọi tổ hợp số, hoặc là bạn sẽ có kết quả tốt, hoặc là bạn sẽ thử suốt đời mới xong.  \nĐể tránh điều này, toán học có nhiều  cách làm \"thông minh\"  để nhanh chóng tìm ra các weights này mà không cần phải thử quá nhiều lần. Đây là một cách: \nĐầu tiên, viết một biểu thức đơn giản để biểu diễn cost ở bước 2 ở trên: cost function. \nBây giờ chúng ta viết lại dưới dạng một biểu thức với ký hiệu toán machine learning (không hiểu không sao, có thể bỏ qua): θ biểu diễn cho weights, J(θ) có nghĩa là cost của weight hiện có. \nCông thức trên biểu diễn độ sai của function ước lượng price của chúng ta với tập weights θ.  \nNếu chúng ta vẽ hết tất cả giá trị của biểu thức J(θ) với các weights có thể có ứng với number_of_bedrooms và sqft, biểu đồ sẽ có thể có dạng như sau: Trục đứng thể hiện cost. Đồ thị của cost function sẽ có dạng hình cái bát. \nTrên hình trên, điểm thấp nhất màu xanh ứng với nơi có cost thấp nhất - vì thế chương trình sẽ có độ lỗi thấp nhất. Điểm càng cao sẽ có độ lệch/lỗi càng cao. Vì thế nếu chúng ta có thể tìm được weights đưa chúng ta đến điểm thấp nhất trên đồ thị, chúng ta sẽ tìm ra được câu trả lời! \nVì thế, chúng ta cần điều chỉnh weights, giống như việc \"đi xuống thung lũng\" trong đồ thị để tìm được điểm thấp nhất. Nếu như chúng ta điều chỉnh từng chút một và luôn đi xuống, ta sẽ tìm được điểm cực tiểu mà không cần phải thử quá nhiều weights. \nNếu bạn còn nhớ trong môn Đại số, nếu bạn đạo hàm một hàm số, bạn sẽ biết được hàm số đồng biến (hướng lên) hay nghịch biến (hướng xuống) tại mọi điểm tiếp tuyến, có nghĩa là bạn sẽ biết được độ dốc của mọi điểm trên đồ thị. \nVì vậy, nếu chúng ta đạo hàm cost function, tính giá trị tại điểm đạo hàm đó, trừ giá trị này với từng weight, từng bước như vậy sẽ giúp ta di chuyển dần đến điểm thấp nhất của đồ thị. \nỞ trên là một cách tốt nhất để tìm weights tối ưu, người ta gọi nó là  batch gradient descent . Nếu bạn quan tâm muốn đi sâu hơn, có thể đọc tiếp bài viết này:  http://hbfs.wordpress.com/2012/04/24/introduction-to-gradient-descent/ . \nNhững thứ có thể bị skip qua nhanh \nThuật toán 3 bước được giới thiệu ở trên được gọi là  multivariate linear regression . Bạn có thể ước lượng được biểu thức để có thể fit tất cả dòng dữ liệu đang có. Sau đó bạn dùng biểu thức để đoán giá của những căn nhà khác, dựa vào giá của những căn có trong dữ liệu qúa khứ. Đây là một ý tưởng rất tuyệt, và có thể triển khai được trong thực tế. \nNhưng ý tưởng tôi đã trình bày chỉ đúng với trường hợp đơn giản, sẽ có vài ngoại lệ. Vì giá nhà đất không phải lúc nào cũng theo một đường, một quy luật cố định. \nMay mắn là có nhiều thuật toán khác có thể giải quyết vấn đề này, xử lý được dữ liệu phi tuyến tính như  neural networks  hoặc  SVMs  với  kernels . Hoặc cũng có thể sử dụng linear regression khéo léo hơn, cho phép tạo ra đường fit nhất với dữ liệu. \nTương tự, tôi cũng bỏ qua trường hợp  overfitting . Đây là trường hợp các weights khớp và đoán đúng mọi căn nhà trong dữ liệu training, nhưng không đúng với những căn khác ngoài tập dữ liệu. \nCó nhiều cách để giải quyết vấn đề này như  regularization  và sử dụng  cross-validation  trên data set. Học cách để giải quyết những vấn đề trên cũng là một mục tiêu để ứng dụng thành công machine learning.  \nMachine Learning toàn năng? \nMột khi nhìn thấy cách mà machine learning giải quyết những vấn đề rất phức tạp một cách dễ dàng (như nhận diện khuôn mặt, chữ viết), người ta thường sẽ nghĩ rằng machine learning sẽ giải quyết được mọi vấn đề nếu như có đủ data. \nNhưng, hãy nhớ rằng machine learning chỉ có thể hoạt động nếu vấn đề có-thể-giải-quyết được với dữ liệu bạn đang có. \nVí dụ, bạn xây dựng mô hình dự đoán giá nhà dựa vào các loại chậu cây trồng trước nhà, nó sẽ không bao giờ hoạt động. Giá nhà và chậu cây chẳng liên quan gì nhau cả. \nVì vậy hãy nhớ nếu một chuyên gia  không  thể sử dụng dữ liệu của giải quyết vấn đề, thì máy tính cũng vậy. \nLàm thế nào để tìm hiểu nhiều hơn về Machine Learning \nTác giả đã tạo một khóa học  từng bước một về chuỗi bài viết này, kể cả viết code . \nNếu bạn muốn đi sâu hơn, hãy thử khóa học  Machine Learning trên Coursera của Andrew Ng . Ngoài ra, bạn cũng có thể thử đủ loại thuật toán trong thư viện  Scikit-learn  của Python. Đăng ký nhận bài viết mới qua email ❤  by  duyetdev"
        },
        {
          "title": "Machine learning là gì?",
          "relevance": "0",
          "url": "http://trainghiemso.vn/machine-learning-la-gi/",
          "content": "HUB ĐÁNH GIÁ HỎI ĐÁP – TƯ VẤN THỦ THUẬT ỨNG DỤNG iOS ANDROID WINDOWS PHIM GAME TREND #iPHONEX #iOS11 #iPhone 8 Connect with us TRAINGHIEMSO.VN HUB ĐÁNH GIÁ HỎI ĐÁP – TƯ VẤN THỦ THUẬT ỨNG DỤNG iOS ANDROID WINDOWS PHIM GAME TREND #iPHONEX #iOS11 #iPhone 8 Machine learning là gì? Share Tweet Machine learning là gì? HỎI ĐÁP - TƯ VẤN Machine learning là gì? Tác giả:  An Nhiên Ngày đăng:  02/09/2017 Share Tweet Khuyến mại nóng! Giảm 20% (tối đa 50.000 đồng) với coupon  THANG9 . Mua Thẻ nhớ Samsung Evo Plus 32G 95M/s chỉ còn 205.000 đồng sau khi áp coupon, nếu không dùng được bạn hãy đăng ký tài khoản mới. Cũng với coupon trên bạn có thể mua loa Xiaomi bluetooth square-box chỉ còn 250.000 đồng. You may also like... Machine learning là gì? HỎI ĐÁP - TƯ VẤN Power Throttling trên Windows 10 là gì? Cách bật Power Throttling Tác giả:   An Nhiên 22/10/2017 Power Throttling là một tính năng mới xuất hiện trong bản cập nhật Fall Creator Update... HỎI ĐÁP - TƯ VẤN DDOS là gì? Tác giả:   An Nhiên 21/10/2017 DDOS (viết tắt của Distributed Denial of Service) là hình thức tấn công từ chối dịch... HỎI ĐÁP - TƯ VẤN WPS là gì? Tác giả:   An Nhiên 20/10/2017 WPS (Wi-Fi Protected Setup) là một tiêu chuẩn mới cho việc thiết lập mạng không dây... HỎI ĐÁP - TƯ VẤN Hashtag là gì? Tác giả:   An Nhiên 19/10/2017 Hastag là tính năng giúp người dùng Facebook có thể dễ dàng liên kết với nhau với... HỎI ĐÁP - TƯ VẤN Chromium là gì? Tác giả:   An Nhiên 18/10/2017 Chromium là một dự án mã nguồn mở dựa trên nền tảng là trình duyệt Google... Điện thoại trôi bảo hành là gì? Facebook On This Day là gì? XEM NHIỀU TRONG NGÀY… GAME Tổng hợp 9 ứng dụng iOS giảm giá miễn phí ngày 22.10 trị giá 19USD ĐÁNH GIÁ Game cũ mà hay: Sid Meier’s Civilization III ĐÁNH GIÁ Đánh giá phim Oan Gia Đổi Mệnh (Never Say Die) HỎI ĐÁP - TƯ VẤN DDOS là gì? ỨNG DỤNG Cách vừa duyệt web vừa làm việc trên Windows 10 bằng cửa sổ nổi HUB ĐÁNH GIÁ HỎI ĐÁP – TƯ VẤN THỦ THUẬT ỨNG DỤNG iOS ANDROID WINDOWS PHIM GAME TREND #iPHONEX #iOS11 #iPhone 8 Copyright © 2013-2016 Trải Nghiệm Số.\r\nGiấy phép số 544/GP-BTTTT cấp ngày 2-12-2016.\r\nChịu trách nhiệm: Võ Thị Quỳnh Loan."
        },
        {
          "title": "Machine Learning cho nông dân (p. 1)",
          "relevance": "1",
          "url": "https://www.facebook.com/notes/nguy%E1%BB%85n-%C4%91%E1%BB%A9c-anh/machine-learning-cho-n%C3%B4ng-d%C3%A2n-p-1-d/10154954912798072/",
          "content": "Chuyển tới Các phần của trang này Trợ giúp về trợ năng Nhấn  alt  +  /  để mở menu này Facebook Tham gia hoặc đăng nhập Facebook    Email hoặc điện thoại Mật khẩu Quên tài khoản? Đăng nhập Bạn có muốn tham gia Facebook không? Đăng ký Đăng ký Machine Learning cho nông dân (p. 1)  :D Nguyễn Đức Anh · 8 Tháng 12 2016 Một trong những quyết định mình cảm thấy may mắn và sáng suốt nhất là học chuyên ngành Information System/Data Analyst. 5-6 năm từ khi mình ra trường thì thế giới hiện nay đang phát cuồng với AI/Machine Learning. Ứng dụng của AI/Machine Learning (AI/ML) là vô tận nhưng chắc chắn AI/ML không phải là một phép thuật, hoặc nếu theo một chiều nào đó là phép thuật thì mình chắc chắn đây là một phép thuật có thể tự học được nếu bạn đủ đam mê. Đấy là lý do mình quyết định viết tất cả những gì mình biết về AI/ML ra để chia sẻ với mọi người. Hôm trước mình có đọc ở đâu đấy về cách để có thể học nhanh nhất, đó chính là chia sẻ điều mình đã học được với người khác. Tạm thời mình nghĩ ra cái gì sẽ chia sẻ cái đấy, thường thì sẽ là những thứ mang tính thực hành (practical) chứ rất ít lý thuyết (lecture) ở đây  :D Nhiều bạn băn khoăn không biết nếu muốn tìm hiểu về AI/ML thì nên bắt đầu từ đâu. Đối với các bạn trẻ còn đang đi học thì cách tốt nhất là học và tìm hiểu về Xác suất thống kê càng nhiều càng tốt. Mọi nguyên lý của AI/ML đều bắt nguồn từ môn Xác suất thống kê, nếu yêu thích môn này thì đường đến với AI/ML của bạn rất gần rồi. Đối với các bạn đã đi làm thì cách tốt nhất là nên đăng ký các khoá học cơ bản về AI/ML trên các trang web MOOC lớn như Cousera, Udacity, Udemy v.v.. Và một cách nữa là đọc blog của mình  :D Mở bài dài quá, chúng ta bắt đầu nhé. Trong bài đầu tiên này mình sẽ cố gắng giải thích cho các bạn 10 thuật toán Machine Learning quan trọng cũng như phổ biến nhất hiện nay theo ngôn ngữ nông dân nhất có thể  :D  Mình sẽ cố gắng viết thật đơn giản, dễ hiểu, không sử dụng quá nhiều thuật ngữ khó hiểu với đa số mọi người.  Disclaimer : Có một điều mong mọi người lượng thứ là do mình học và nghiên cứu về AI/ML hoàn toàn bằng tiếng Anh nên thuật ngữ tiếng Việt thú thực là mình cũng không biết :D Theo mình thấy thì thuật ngữ tiếng Việt hiện nay cũng không thống nhất và có thể gây khó hiểu cho người học/tìm hiểu. Ví dụ thuật toán clustering thì mình thấy tiếng Việt gọi là thuật toán gom nhóm, phân cụm, gom cụm v.v..  I. 3 dạng Machine Learning chính 1/ Supervised Learning: Nôm na nghĩa là máy sẽ đưa ra suy đoán dựa trên một tập dữ liệu cho trước đã được training.  Ví dụ: Chúng ta có sẵn 1000 status facebook (gọi là dataset A) được phân loại theo các trạng thái như vui, buồn, giận dữ v.v.. (ứng với mỗi trạng thái là một tập hợp các đặc tính có liên quan, ví dụ các trạng thái status vui thì thường xuất hiện các cụm từ như “đáng yêu quá\", “thích quá\", “hay quá\"...), supervised learning là phương pháp sử dụng dataset A này để xác định 100 status facebook khác chưa được gắn trạng thái. 2/ Unsupervised Learning: Nôm na nghĩa là tay không bắt giặc  :D  máy sẽ tự tìm ra các mối quan hệ hoặc các pattern từ một dataset. Ví dụ: Chúng ta có thể sử dụng phương pháp unsupervised learning để tìm ra các chủ đề được đề cập tới từ 1000 status facebook, ví dụ các status có chủ đề về bóng đá (vì trong những status này xuất hiện nhiều từ, cụm từ, hashtag có liên quan đến bóng đá chẳng hạn). 3/ Reinforcement Learning: Nôm na nghĩa là máy có thể tự cập nhật cách giải quyết vấn đề  theo kiểu thử và sửa liên tục. Ví dụ: DeepMind AlphaGo với việc chiến thắng Lee Sedol trong bộ môn cờ vây là ví dụ nổi tiếng nhất về RL (thực tế DeepMind gọi là Deep Reinforcement Learning). II. 10 thuật toán Machine Learning quan trọng nhất Để kết luận 10 thuật toán ML nào quan trọng nhất thực sự là việc rất phức tạp vì chúng ta không biết xếp theo tiêu chí nào? Ví dụ độ phổ biến, tính hữu dụng, tính đơn giản, tính chính xác v.v.. Vì thế mình chỉ giả định nếu mình phỏng vấn một người cho vị trí Data Scientist / Data Engineer thì mình cần họ phải nắm được 10 thuật toán sau (theo thứ tự ưu tiên): 1/ Linear Regression và Logistic Regression (Hồi quy tuyến tính và Hồi quy Logistic) : May quá, cái này mình biết thuật ngữ tiếng Việt  :D Dùng làm gì nhỉ?  Thuật toán Linear Regression mô tả dữ liệu và thể hiện mối quan hệ giữa một biến phụ thuộc với một hoặc nhiều biến độc lập. Tức là nếu biến độc lập thay đổi thì ảnh hưởng đến biến phụ thuộc như thế nào. Từ từ, biến phụ thuộc với biến độc lập là cái gì??  Biến phụ thuộc (dependent variable - DV) nôm na là cái đề bài, mục tiêu chính cần phải giải được; biến độc lập (independent variable - IV) là những dữ kiện ảnh hưởng đến biến phụ thuộc. Khó hiểu vãi, cho ví dụ đi??  Ok... Giả sử chúng ta muốn tìm hiểu lý do sử dụng Facebook của đàn ông hay phụ nữ ảnh hưởng như thế nào đến việc tương tác của họ lên Facebook.  Ở đây, việc tương tác lên Facebook là một DV và các lý do sử dụng Facebook là các IV. Ví dụ chúng ta liệt kê được một số lý do sử dụng Facebook như giải trí, tìm hiểu thông tin, tương tác với bạn bè, đi soi facebook người lạ, hóng biến v.v.. Sử dụng Linear Regression ta có thể biết được phụ nữ thường tương tác nhiều nhất khi họ đi soi facebook người lạ chẳng hạn => Facebook sẽ đẩy nhiều thông tin kiểu này lên news feed của các bạn nữ để tăng tương tác  :) ) Ok, tạm hiểu, thế sao người ta lại dùng Linear Regression mà không dùng thuật toán khác?  Một số ưu điểm của Linear Regression là tính dễ hiểu của thuật toán (ví dụ trên chẳng hạn, những thuật toán sau ví dụ không dễ hiểu như vậy đâu  :) ); dễ chỉnh sửa (tuning); độ phổ biến cao và tốc độ giải thuật nhanh. Nghe có vẻ dễ đúng không? Cái khó nhất của các Generalized Linear Model (GLM - Mô hình tuyến tính tổng quát) là việc chọn independent variables chứ không phải là thuật toán.  Thế Linear Regression và Logistic Regression thì khác nhau cái gì?  Cái khác nhau nhất chính là ở dependent variable, đối với Linear Regression thì DV có dạng liên tục (continuous) còn đối với Logistic Regression thì DV có thể chỉ là Có/Không, Sống/Chết, 0/1 (mô hình nhị phân) hoặc DV có thể ở dạng rời rạc (discrete). Continuos và discrete là hai khái niệm rất lớn trong toán học, các bạn có thể tìm hiểu thêm ở trên mạng, trong khuôn khổ bài này thì hơi khó để giải thích ngắn gọn được. Ứng dụng trong đời thường như thế nào?  Một trong những ứng dụng lớn nhất của GLM model (Mô hình tuyến tính tổng quát) là hệ thống quản lý rủi ro hoặc chấm điểm tín dụng của các ngân hàng. Chẳng hạn như để quyết định hạn mức thẻ Credit card của bạn, hệ thống sẽ sử dụng GLM để tính toán dựa trên các thông tin bạn cung cấp như mức lương hiện tại, độ tuổi, giới tính, công việc đang làm v.v.. 2/ Decision Trees (Cây quyết định) Dùng làm gì nhỉ?  Cây quyết định là một công cụ dùng để hỗ trợ trong việc ra quyết định dựa trên một hoặc nhiều  biểu đồ dạng cây. Biểu đồ này thể hiện số lượng câu hỏi yes/no tối thiểu mà một người cần hỏi, để đánh giá về khả năng ra được một quyết định đúng. Ví dụ nhé,  chúng ta có một dataset về dữ liệu về các cầu thủ trẻ trong khoảnng 10-16 tuổi. Chúng ta có thể collect được những dữ liệu như vị trí thi đấu, chiều cao, cân nặng, tỷ lệ sút bóng thành công, tỷ lệ xoạc bóng thành công, tỷ lệ chuyền bóng thành công, tốc độ, tỷ lệ đánh đầu thành công v.v... Giả sử bạn là một HLV và đang có một lứa cầu thủ trẻ mới vào. Làm thế nào để quyết định được một cầu thủ trẻ nên chơi ở vị trí thi đấu nào là phù hợp nhất? Sử dụng Cây quyết định sẽ giúp bạn làm được điều này, bằng cách thu thập những dữ liệu của lứa cầu thủ trẻ mới vào và so sánh dữ liệu mới này với dữ liệu có sẵn trong dataset, Cây quyết định sẽ giúp bạn xác định được vị trí thi đấu nào thích hợp nhất với một cầu thủ trẻ mới tuyển. Nghe cool vãi, nhưng theo tôi biết thì đếch có thuật toán nào gọi là Cây quyết định cả  :) )  Oh đúng, xin lỗi  :) ) Cây quyết định thực ra là một họ tập hợp các thuật toán mà outcome của nó có thể biểu diễn dưới dạng cây. Một số thuật toán họ này rất phổ biến như C4.5 hoặc Random Forest v.v... Trong khuôn khổ bài mở đầu này, mình chỉ có thể giới thiệu khái quát như vậy, sẽ có những bài khác đi sâu hơn vào từng thuật toán trên. Oh thế Cây quyết định thì có gì hay?  Cái hay nhất của Cây quyết định là tính đơn giản và dễ interpret của kết quả. Hầu như bất kỳ ai nhìn vào một decision tree cũng có thể hiểu được mà không cần có kiến thức gì về Machine Learning cả. Ứng dụng lớn nhất của Decision Trees là gì?  Ứng dụng của Decision Trees cực nhiều, cực phổ biến, vì tính thực tiễn của nó trong việc hỗ trợ ra quyết định (mà trong thực tế thì công việc gì cũng cần quyết định). Ví dụ: quyết định cho vay, quyết định về chiến lược giá bán v.v.. Mình tạm kết thúc Phần 1 ở đây, Phần 2 mình sẽ giới thiệu 8 thuật toán khác cũng rất thú vị và hiện hữu quanh ta mà chúng ta không để ý bấy lâu   :D Mình muốn kết thúc phần này bằng một câu nói của Andrew Ng - Chief Scientist của Baidu và Founder của Coursera:  “Nếu bạn thành thạo thuật toán hồi quy tuyến tính, hồi quy logistic và nguyên tắc kiểm soát (regularization) thì bạn đã biết nhiều về Machine Learning hơn phần lớn các kỹ sư Machine Learning ở Silicon Valley rồi\" . Để biết về các thuật toán này thì không khó nhưng để master nó thì không hề dễ chút nào (nhất là về regularization). Điều mà mình muốn nói ở đây chính là Machine Learning không hề khó, không phải là một black-box, ai cũng có thể học và master được nếu họ thực sự yêu thích và muốn nắm được một trong những vũ khí mạnh nhất của nhân loại hiện nay. Tiếng Việt English (UK) 中文(台灣) 한국어 日本語 Français (France) ภาษาไทย Español Português (Brasil) Deutsch Italiano Đăng ký Đăng nhập Messenger Facebook Lite Di động Tìm bạn bè Danh bạ Trang Địa điểm Trò chơi Vị trí Người nổi tiếng Marketplace Nhóm Công thức nấu ăn Thể thao Look Moments Instagram Giới thiệu Tạo quảng cáo Tạo Trang Nhà phát triển Tuyển dụng Quyền riêng tư Cookie Lựa chọn quảng cáo Điều khoản Trợ giúp Cài đặt Nhật ký hoạt động  Facebook © 2017"
        },
        {
          "title": "Machine Learning là gì?",
          "relevance": "1",
          "url": "https://kipalog.com/posts/Bat-dau-voi-Machine-Learning-thong-qua-Tensorflow--Phan-I-I",
          "content": "\n              Đăng nhập\n \n              Đăng ký\n \n    Chú ý\n   \n      Ok\n     \n    Xóa bài viết\n   \n        Bạn có chắc chắn muốn xóa bài viết này không ?\n       \n      Không\n     \n      Có\n     \n    Xóa bình luận\n   \n        Bạn có chắc chắn muốn xóa bình luận này không ?\n       \n      Không\n     \n      Có\n     \n    Chào bạn! Hãy đăng ký hoặc đăng nhập để tiếp tục nhé!\n   \n    Kipalog\n   \n          or\n         \n          or\n         \n            facebook\n \n            twitter\n \n            github\n \n        Bạn luôn có thể lựa chọn email hoặc các tài khoản mạng xã hội để đăng nhập hoặc đăng ký.\n       Bắt đầu với Machine Learning thông qua Tensorflow (Phần I.I) \n              Machine Learning\n               7 \n              tensorflow\n               4 \n            Trần Đức Tâm viết ngày 02/07/2017\n           Phần I.I Machine Learning là gì? TL;DR Mình nghiên cứu  Machine Learning [Coursera]  bắt đầu từ khoảng một năm về trước, mất khoảng sáu tháng để tự luyện lại bản thân kiến thức toán từ lớp 9 để làm quen lại với các dạng toán từ giải phương trình đến đạo hàm, tích phân... Cảm giác đó là một quãng thời gian tuy không hẳn là phí phạm lắm nhưng kết quả đạt được để hiểu về Machine Learning lại không nhiều. Nhất là với một lập trình viên Mobile như mình. Vậy nên mình tổng hợp lại một vài kiến thức cơ bản của Machine Learning dưới góc nhìn của một người ứng dụng lại các thành tựu có sẵn nhằm giải quyết các bài toán Machine Learning dành cho bạn nào cũng mới bắt đầu giống mình mà đã quên đi khá nhiều kiến thức toán phổ thông. Để theo dõi bài viết được hiệu quả, cách tốt nhất là kết hợp đọc cùng với bai tài liệu sau: Khóa học Machine Learning của thầy Andrew Ng trên Coursera Blog của anh Vũ Hữu Tiệp với các thuật toán ML được viết rất dễ hiểu Hướng dẫn nhập môn của Tensorflow Ngoài ra thì blogs của  Huy Trần  và bạn  Khánh Nguyễn  cũng là những blogs rất đáng được tham khảo. Machine Learning là gì? Theo như  khóa học Machine Learning  trên Coursera thì khái niệm Machine Learning có hai định nghĩa cổ đại và hiện đại. Định nghĩa cổ đại : Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed. ( Machine Learning là một ngành học nghiên cứu về cách giúp cho máy tính điện tử có được khả năng có thể học tập mà không cần phải được lập trình một cách tỉ mỉ trước đó. ) (Arthur Samuel – 1959) Theo như định nghĩa này thì mình cảm thấy là nó giống như việc dịch sát nghĩa từ Machine Learning vậy. Hoàn toán bị bối rồi về việc Machine Learning là gì? Làm sao để thực hành Machine Learning. Vậy nên một định nghĩa khác mở hơn được đưa ra giúp cho mọi người có cái nhìn dễ dàng hơn về Machine Learning. Định nghĩa hiện đại : A computer program is said to learn from experience  E  with respect to some class of tasks  T  and performance measure  P , if its performance at tasks in  T , as measured by  P , improves with experience  E . ( Một chương trình máy tính được gọi là học cách thực thi 1 nhóm nhiệm vụ tương đồng  T  thông qua trải nghiệm  E  dựa trên tiêu chí đánh giá  P  khi kết quả thực thi các nhiệm vụ trong nhóm  T  của chương trình đó, tăng lên theo trải nghiệm  E  thông qua đánh giá bằng tiêu chí  P . ) (Tom Mitchell – 1998) Dưới góc nhìn của Machine Learning, mọi bài toán đều có input và output, ở giữa của quy trình đó là một hàm. Hàm là một khái niệm toán học khá quen thuộc với chúng ta. Hàm là một tập hợp các quy tắc được dùng để các phần tử ở tập nguồn đến các phần tử ở tập đích sao cho mỗi một phần tử ở tập nguồn chỉ có một và chỉ một phần tử ở tập đích. Trong đó, một bộ phận nhỏ của hàm, khi tất cả các phần tử của cả hai tập đều là dạng số học, người ta gọi đó là hàm sô. \nHình 1.1: Hàm số Cùng với sự phát triển của  Computer Vision [Wiki] , nhiệm vụ của người làm Machine Learning ở một mức cơ bản nào đó là giúp máy tính điện tử tìm ra được hàm quan hệ giữa tập input và tập output để từ đó máy tính có khả năng cải tiến hiệu xuất P của nhiệm vụ T thông qua trải ngiệm E. Hàm này được gọi là  Hypothesis [Wiki]  hoặc  Model [Wiki] Cũng theo  khóa học Machine Learning  trên Coursera thì Machine Learning chia các bài toán ra làm hai loại chính là  Supervised Learning ( Học có giám sát ) [SL]  và  Un-Supervised Learning ( Học không có giám sát ) [USL] . Supervised Learning Học có giám sát là việc có sẵn một tập nguồn và một tập đích tương ứng để làm cơ sở xây dựng ra model mong muốn. Tập hợp kết hợp bởi hai tập này được gọi là  tập Train [Wiki] .  (Test sets và validate sets không được đề cập đến trong khuôn khổ bài viết này) . Theo thời gian, khi đưa bài toán vào áp dụng thực tế, dữ liệu mới lại được sử dụng để  train ( Huấn luyện )  lại nhằm cải tiến model hiện tại. Điều này chính là mô tả cho khái niệm học có giám sát. Các thuật toán nằm trong nhóm các bài toán học có giám sát được chia nhỏ thành hai phần dựa trên đặc tính của tập đích trong tập train. Classification Khi dữ liệu của tập đích là một nhóm hữu hạn và có thể  labled ( Gắn nhãn )   được, bài toán được xếp vào dạng  classification ( Bài toán phân loại ) . Ví dụ hay được sử dụng cho bài toán dạng này là bài toán gắn nhãn xem một email có phải là spam hay không, hoặc phân loại giữa cam và táo hay phân biệt chữ viết tay... Regression Dạng còn lại được gọi là  regression ( Bài toán hồi quy ) . Hồi quy là một thuật ngữ rất khó hiểu. Khi tìm hiểu cả bằng tiếng Trung và tiếng Nhật thì từ này trùng hợp là đều được dịch là  hồi quy ( 回帰 )  cả. Thì ra là những cái gì có tính liên tục và tiếp nối với nhau thì người ta gọi là là hồi quy. Vậy khi tập đích trong tập train là một tập dữ liệu có dạng liên tục không thể phân thành nhóm mà là một dữ liệu cụ thể thì bài toán được xếp vào dạng hồi quy. Bài toán tiêu biểu cho dạng này thường được ví dụ là tính toán giá cả của sản phẩm dựa trên thông số trước đó, hoặc dự đoán biến động tài chính... \nHình 1.3: Hàm số regression Lưu ý:  Có những bài toán, khái niệm hồi quy và phân loại khá khó xác định. Ví dụ như bài toán về dự đoán tuổi. Nếu ta coi tuổi là một đại lượng liên tục, bài toán thuộc về dạng hồi quy. Tuy nhiên ở một mặt khác khi coi tuổi là một số nguyên dương lớn hơn 0 và không vượt quá 150 thì ta lại quy bài toán về dạng phân loại. [Machine learning cơ bản] Un-Supervised Learning Được gọi là học không giám sát, chúng ta không hề có tập đích mà chỉ có tập nguồn, khi dữ liệu đủ lớn, chúng ta mặc dù không thể dự đoán được tấp đích nhưng bù lại thì có thể thực hiện phân nhóm hay làm giảm độ phức tạp của dữ liệu. Điều này đặc biệt có ích trong việc lưu trữ hoặc sử lí dữ liệu trong tương lai ( Đưa về bài toán học có giám sát khi đã có dữ liệu thực tế ) Các bài toán học không giám sát cũng được chia làm hai loại dựa vài tập nguồn của chúng. Clustering Với bài toán dạng này, ta buộc phải có một tập hợp các categories cho trước. Dữ liệu nguồn sẽ được  Clustering ( Bài toán gom nhóm )  dựa trên các yếu tố có trong tập hợp categories cho trước. Điều này có vẻ tương tự với bài toán ở dạng Classification bị bỏ đi tập đichs nhưng trên thực tế thì không chính xác. Ví dụ như với bài toán phân loại email là spam nhưng hoàn toàn không có bất kìa ai đánh dấu mail là spam hay không. Việc phân loại lúc này đơn thuần chỉ dự trên những nhóm email được cho là giống nhau về cách đặt tên, nguồn gửi, tần suất xuất hiện, nội dung... Dimensionality reduction Bài toán ở dạng này được gọi là  Dimensionality reduction ( Bài toán giảm chiều của dữ liệu ) . Bài toán ở dạng này không được quy về các dạng trên mà được thực hiện dưới hình thức làm giảm đi số lượng các biến số đầu vào nhằm hạn chế độ phức tạp của bài toán. Hãy tưởng tượng với bài toán  \nHình 1.4: Sơ đồ lựa chọn thuật toán Tổng kết Trên đây là một số khái niệm mở đầu khá quan trọng của Machine Learning. Để đi sâu hơn vào những khái niệm này, các bạn có thể tìm hiểu thêm trong phần Readmore bên dưới. Lượng khái niệm được đề cập tới chỉ mang tính chất giới thiệu từ khóa tra cứu cũng như làm tiền đề cho các bài viết tiếp theo. Readmore Khóa học Machine Learning của thầy Andrew Ng trên Coursera Blog của anh Vũ Hữu Tiệp Blog của Huy Trần Blog của Nguyễn Khánh \n        Chia sẻ bài viết với bạn bè nữa nhé!\n       \n          facebook\n         \n          google plus\n         \n          twitter\n         \n        Bình luận\n       {{ comment.user.name }} {{ comment.updated_at }} \n                  Xoá bình luận\n                 Bỏ hay Hay \n              {{comment.like_count}}\n             {{ comment_error }} Hủy \n               \n             Hiển thị thử Chỉnh sửa \n          Trần Đức Tâm\n         3 \n            bài viết.\n           37 \n            người follow\n           Kipalog {{userFollowed ? 'Following' : 'Follow'}} \n        Cùng một tác giả\n       \n            18\n             \n            7\n             Bắt đầu với Machine Learning thông qua Tensorflow (Phần I.2) \n                  Machine Learning\n                 \n                  tensorflow\n                 \n              Phần I.2 Tensorflow và bài toán Hồi quy đơn giản đầu tiên   TL;DR  Qua bài viết trước, chúng ta đã biết được đến sự tồn tại của một vài khái niệm c...\n             \n                Trần Đức Tâm\n               \n              viết\n              4 tháng\n              trước\n             \n              18\n               \n              7\n               \n            1\n             \n            2\n             Default Parameter Values in Kotlin \n                  TIL\n                 \n                  Kotlin\n                 \n              kotlin  fun main(args: Array) {   find(\"a\")   find(\"b\")   find(\"c\")   find(\"d\")  }  val a : MutableList = mutableListOf()  fun find(name: String, a...\n             \n                Trần Đức Tâm\n               \n              viết\n              5 tháng\n              trước\n             \n              1\n               \n              2\n               \n        Bài viết liên quan\n       \n            18\n             \n            7\n             Bắt đầu với Machine Learning thông qua Tensorflow (Phần I.2) \n                  Machine Learning\n                 \n                  tensorflow\n                 \n              Phần I.2 Tensorflow và bài toán Hồi quy đơn giản đầu tiên   TL;DR  Qua bài viết trước, chúng ta đã biết được đến sự tồn tại của một vài khái niệm c...\n             \n                Trần Đức Tâm\n               \n              viết\n              4 tháng\n              trước\n             \n              18\n               \n              7\n               \n            34\n             \n            13\n             Thử code CycleGAN với TensorFlow \n                  Deep Learning\n                 \n                  tensorflow\n                 \n              Giới thiệu về CycleGAN  Trước hết mời các bạn xem video này:  https://www.youtube.com/watch?v=9reHvktowLY  Đây là thuật toán sử dụng Deep Learning ...\n             \n                Van Phu Quang Huy\n               \n              viết\n              6 tháng\n              trước\n             \n              34\n               \n              13\n               {{like_count}} kipalog {{ comment_count }} bình luận {{liked ? \"Đã kipalog\" : \"Kipalog\"}} {{userFollowed ? 'Following' : 'Follow'}} \n              Trần Đức Tâm\n               3 \n            bài viết.\n             37 \n            người follow\n           \n           Đầu mục bài viết\n         \n        Vẫn còn nữa!\n           x \n          Kipalog vẫn còn rất nhiều bài viết hay và chủ đề thú vị chờ bạn khám phá!\n         \n          Khám phá\n         \n          Đăng nhập\n         \n          Điều khoản\n         \n          Phản hồi\n         \n          Yêu cầu\n         \n          Fanpage\n         \n          Copyright © 2017 Kipalog\n        "
        },
        {
          "title": "Khoa Khoa Học và Kỹ Thuật Máy Tính & TRƯỜNG ĐẠI HỌC BÁCH KHOA TP.HỒ CHÍ MINH",
          "relevance": "0",
          "url": "http://www.cse.hcmut.edu.vn/site/vi/Page?item=35",
          "content": "Đại học\r\n        Bách Khoa \r\n        Khoa Khoa Học và Kỹ Thuật Máy Tính & TRƯỜNG ĐẠI HỌC BÁCH KHOA TP.HỒ CHÍ MINH\r\n     Cán Bộ \r\n                Đăng nhập English Trang chủ Giới thiệu Lịch sử hình thành Nhân sự Trang thiết bị Liên hệ                   Đào tạo Đại học Sau đại học Kiểm định chất lượng Đào tạo trực tuyến Cổng thông tin Sakai                   Nghiên cứu Lĩnh vực nghiên cứu Các bài báo khoa học                   Tin tức Tất cả tin Tin đồ án, luận văn Thông báo môn học Tin quản lý sinh viên Hướng dẫn - Biểu mẫu Thông báo học vụ Tin học bổng Tuyển dụng Tin thư viện Sự kiện                   Tuyển sinh 2017 CHỨNG NHẬN KIỂM ĐỊNH ABET THÔNG BÁO Hội nghị ACOMP 2017 Hội nghị FDSE 2017 Tuyển sinh 2017 Doanh Nghiệp Hoạt động Nghiên cứu \r\n\t  Cùng với đào tạo, nghiên cứu và phát triển là hoạt động chính yếu của khoa, tập trung vào các đề tài tính toán cấp cao. Các nhóm nghiên cứu của khoa hiện đang làm việc trong các lĩnh vực sau: Hệ thống cơ sở dữ liệu cấp cao Khi chúng ta bước vào thời đại thông tin, cơ sở dữ liệu trở thành một công nghệ chính yếu cho sự thành công trong thương mại và công nghiệp. Mục tiêu của nhóm là: 1. Nắm bắt những phát triển mới và nâng cao những thành tựu mới trong công nghệ 2. Ứng dụng những kết quả nghiên cứu vào các doanh nghiệp và tổ chức bên ngoài 3. Chuẩn bị cho các sinh viên để vượt qua những thử thách phía trước. Việc nghiên cứu trong nhóm này nhấn mạnh đến các lĩnh vực nghiên cứu cổ điển cũng như hiện đại Các chủ đề nghiên cứu ·         Temporal Databases ·         Data Warehousing ·         Data Mining Database ·         Implementation Techniques \r\n\t  Trí tuệ nhân tạo Nhóm AI đi theo hướng nghiên cứu liên ngành để phát triển và kết hợp các phương pháp và kĩ thuật từ ngành Trí tuệ nhân tạo cổ điển và ngành Tính toán mềm để giải quyết các vấn đề trong thế giới thực. Theo quan điểm của nhóm, việc kiểm soát sự không chắc chắn và không chính xác là một nhu cầu không chỉ bởi chúng luôn hiện hữu trong mọi vấn đề mà còn vì giải quyết được chúng sẽ giúp đạt được nhiều giải pháp thực tế. Với niềm tin một hệ thông tốt luôn cần một nền tảng vững chắc, công việc nghiên cứu được thực hiện cả về lý thuyết và ứng dụng. Hiện tại, các hoạt động nghiên cứu trong nhóm tập trung vào Xử lý ngôn ngữ tự nhiên, Biểu diễn và suy luận tri thức không chắc chắn và không chính xác, cho các ứng dụng trong các miền khác nhau như xử lý văn bản và World Wide Web. Nhóm đang đảm nhiệm dự án Vietnamese Semantic Web, một dự án trọng điểm cấp quốc gia. Các chủ đề nghiên cứu: ·         Semantic Web ·         Machine Translation ·         Computing with Words ·         Information Extraction ·         Machine Learning \r\n\t  Constraint Programming and Meta-heuristics Việc nghiên cứu trong nhóm tập trung vào các giải thuật thỏa mãn ràng buộc, lập trình logic ràng buộc và meta-heuristics, ví dụ như các phương pháp tìm kiếm cục bộ, giải thuật genetic và cách tiếp cận hybrid. Một trong những ứng dụng quan trọng nhất của hướng nghiên cứu này là lên lịch (định thời) tự động, như thời khóa biểu ở các viện giáo dục, lên lịch làm việc ở các bệnh viện và ở khu công nghiệp, v.v… Các chủ đề nghiên cứu: ·         Constraint Programming ·         Meta-heuristics ·         Automated Timetabling \r\n\t  Hệ thống nhúng Hệ thống nhúng là các thiết bị có gắn kèm các bộ xử lý khả lập trình nhưng không phải là các máy tính đa dụng. Các ví dụ của các hệ thống nhúng là máy in, modem, PDA (Personal Digital Assistant), các máy robot, v.v…Các hệ thống nhúng phức tạp như hệ thống robotic tương tác với thế giới thực, và có thể bao gồm các cảm biến phân tán, actuator, và các bộ vi xử lý. Nghiên cứu của nhóm tập trung vào lập mô hình, thiết kế, và mô phỏng các hệ thống phức tạp, tức là các hệ thống bao gồm nhiều kiểu hoạt động khác nhau như xử lý tín hiệu, kiểm soát feedback, và giao diện người dùng. Các vấn đề như nhận dạng và xác định các mô hình tính toán đồng thời cũng như các ràng buộc về hình học và vật lý cần cho những mô phỏng dựa trên vật lý đang được nghiên cứu. Việc nghiên cứu cũng bao gồm việc hiện thực các hệ thống nhúng dựa trên các cách tiếp cận như đồng thiết kế phần cứng/ phần mềm hay thiết kế dựa trên thành phần. Các chủ đề nghiên cứu: ·         Các phương pháp lập mô hình, thiết kế, mô phỏng và hiện thực hệ thống nhúng ·         Các công cụ cho việc lập mô hình, thiêt kế và mô phỏng cho các hệ thống nhúng. ·         Lập trình hệ thống nhúng ·         Kiến trúc phần mềm cho các hệ thống robotic \r\n\t  Microprocessor-based Applications and System-on-a-Chip Design Từ những năm 1980, việc phát minh ra bộ vi xử lý đã tạo ra một cuộc cách mạng cho công việc thiết kế các hệ thống số. Nhờ đó, không chỉ các hệ thống truyền thống trở nên thông minh hơn, mà còn xuất hiện các loại hệ thống hoàn toàn mới. Với khả năng tính toán cực lớn đạt được trong một IC (integrated circuit) nhỏ, các bộ vi xử lý đã dần trở thành quan tâm hàng đầu đối với các kĩ sư thiết kế. Khi kích thước và độ phức tạp của các hệ thống số tăng lên, nhiều công cụ CAD (computer-aided design) đã được giới thiệu cho quá trình thiết kế phần cứng. Sự gia tăng mới nhất cho phương pháp thiết kế này là HDL (hardware description language). Dựa trên HDLs, những công cụ CAD mới đã được phát triển và hiện đang được sử dụng bởi những nhà thiết kế phần cứng. Thiết kế SoC (system-on-a-chip) được định nghĩa như là sự thiết kế một IC phức tạp tích hợp các phần tử chức năng chính của một sản phẩm cuối hoàn thiện vào trong một con chip đơn hay một chipset. Các hoạt động của nhóm hầu như tập trung vào các ứng dụng trong công nghiệp, y tế, hóa học và thông tin liên lạc. Những ứng dụng này hoặc dựa trên hệ thống vi xử lý hoặc liên quan đến thiết kế SoC. Các chủ đề nghiên cứu: ·         Sensor-based Applications ·         IC Design Automation ·         Advanced Computer Architectures ·         Microprocessor-based Applications ·         VHDL ·         System-on-a-Chip Design \r\n\t  Xử lý song song và tính toán mạng Xử lý song song đang trở thành một công nghệ trọng tâm của thế kỉ 21 không chỉ cho nghiên cứu khoa học mà còn cho các ứng dụng thương mại và công nghiệp. Vấn đề gia tăng những yêu cầu về sức mạnh tính toán chỉ có thể giải quyết bằng cách sử dụng những kíên trúc song song và phân bố như những hệ thống thống đa xử lý hay mạng các hệ thống máy tính. Hoạt động nghiên cứu chính trong nhóm tập trung vào hai khía cạnh quan trọng: thứ nhất là công cụ và môi trường lập trình song song, thứ hai là các giải thuật song song và các ứng dụng trong khoa học và kĩ thuật. Trong khía cạnh đầu tiên, mục tiêu của công việc nghiên cứu là cung cấp các middleware và các công cụ giúp người dùng phát triển các ứng dụng song song của họ dễ dàng và hiệu quả trên các kiến trục song song dựa trên mạng như cluster, mạng các workstations và lưới (grid). Các công cụ được cung cấp ở các mức độ khác nhau, bao gồm công cụ thiết kế chương trình, công cụ giám sát, debugging, và các công cụ đánh giá hiệu năng. Các công cụ chỉ có thể đánh giá đúng khi chúng có người dùng, và do đó, nhóm nghiên cứu đã phối hợp với các đối tác nghiên cứu khác để phát triển các giải thuật song song cũng như các phương pháp để có thể được sử dụng trong các ứng dụng tính toán khoa học như lập mô hình hay mô phỏng. Các chủ đề nghiên cứu: ·         Parallel and distributed computer architecture and grid architecture ·         Parallel programming environments ·         Resource management and scheduling ·         Task partitioning and mapping algorithms ·         Load balancing ·         Fault tolerance Parallel and grid computing ·         Thread-safety message passing environments ·         Networking with embedded systems \r\n\t  Bảo mật trong hệ thống và ứng dụng quản trị thông tin hiện đại Hệ thống quản trị thông tin (DMSs - Data management systems) là những hệ thống quản trị dữ liệu, rút trích thông tin có nghĩa từ dữ liệu, và đưa những thông tin này vào sử dụng. DMSs bao gồm các hệ thống cơ sở dữ liệu , data warehouses, và hệ thống data mining. Dữ liệu có thể là những dữ liệu có cấu trúc như được tìm thấy trong cơ sở dữ liệu quan hệ, hay dữ liệu bán cấu trúc và XML, hoặc thậm chí là dữ liệu không có cấu trúc như dữ liệu multimedia. DMSs là thành phần chính của các hệ thống thông tin và ứng dụng, một trong những tài sản giá trị nhất của các tổ chức ngày nay. Những tiến bộ trong công nghệ Internet và sự phát triển liên tục của Web đã tạo ra yêu cầu ngày càng lớn cho việc quản lí dữ liệu và thông tin. Điều này dần tạo ra những đòi hỏi thiết yếu cho việc duy trì tính bảo mật của của cơ sở dữ liệu, ứng dụng và hệ thống thông tin. Nhóm bảo mật hệ thống thông tin (ISS- Information Systems Security) đang nghiên cứu các vấn đề bảo mật xuất hiện trong các ứng dụng và DMSs hiện đại. Đây là một trong những nhóm tiên phong trong lĩnh vực này ở Việt Nam. Các đề tài nghiên cứu: ·         Database security technologies ·         Security issues in outsourced database services ·         Digital copyright protection ·         Digital signature ·         User and data privacy management ·         Digital forensics ·         Security models for modern information systems \r\n\t  \r\n            C.S.E Computer Science & Engineering - University of Technology RSS Block\r\n                A3, Ho Chi Minh City University of Technology Address:\r\n                268 Ly Thuong Kiet Street, District 10, Hochiminh City, Vietnam Tel: (+84\r\n                8) 865-8689 Fax: (+84 8) 864-5137 636.227 \r\n                Lượt truy cập Copyright ©2013 Computer Science & Engineering"
        },
        {
          "title": "Muốn tồn tại, doanh nghiệp phải có công nghệ",
          "relevance": "0",
          "url": "https://blog.perkfec.com/vi/2017/06/28/big-data-va-machine-learning-trong-hr/",
          "content": "Perkfec | Workplace revolution Menu Trang chủ Nghệ thuật quản lý Thị trường & Xu hướng Văn hóa tổ chức  Chat với chuyên gia SIOP LEC 2016 – Hội nghị tâm lý học tổ chức nhân sự Máy tính được kì vọng sẽ thực hiện các nhiệm vụ thông thường tốt hơn các chuyên gia nhân sự. “Thời đại siêu nhân: Sự đổi mới của khoa học dữ liệu nhân sự. “  “Tận dụng dữ liệu sẵn có để nâng cao kinh nghiệm của nhân viên” “Từ sự lạc quan đến tác động” About The Author Kim Nguyen Related Posts Tháng Chín 7, 2017 Nhận diện nhân viên gắn kết Tháng Tám 22, 2017 Robot và AI có thực sự thay thế được các giám đốc nhân sự? Tháng Tám 30, 2017 Xu hướng HR mới 2017: khảo sát “xung nhịp” nhân viên? Leave a Reply Hủy Get more stuff Subscribe to our mailing list and get interesting stuff and updates to your email inbox. Thank you for subscribing. Something went wrong. we respect your privacy and take protecting it seriously Popular Recent Comments Tags Chuyên mục Chat với chuyên gia Gắn kết nhân viên Nghệ thuật quản lý Perkfec Thị trường & Xu hướng Văn hóa tổ chức Copyright © 2017 by  Perkfec | Workplace revolution . Theme Designed by  MyThemeShop.com  "
        },
        {
          "title": "[ML] Giải thích 10 thuật ngữ chính trong Machine Learning",
          "relevance": "0",
          "url": "http://tech.3si.vn/2016/04/04/ml-giai-thich-10-thuat-ngu-chinh-trong-machine-learning/",
          "content": "3SI SERVICES COMMUNITY CONTACT Blog_check10 b_c_10 AUTHOR : Nam Vu Tags:  machine-learning «  [ML] Classification – Part 3 [Microsoft Azure] Deploy PHP website lên Azure App Service  »  Comments are closed. Administration Login Recent Posts THIẾT BỊ BAY KHÔNG NGƯỜI LÁI: CÔNG NGHỆ & CÁC HƯỚNG ỨNG DỤNG MỚI Giải pháp thúc đẩy ứng dụng Công nghệ Thông tin trong sản xuất nông nghiệp thông minh tại Thanh Hóa [NodeJS] Lựa chọn mới cho nền tảng server side stdClass LÀ GÌ? LÀM THẾ NÀO ĐỂ CÓ PROPERTIES ĐỘNG TRONG PHP? What is stdClass? And Dynamic Properties in PHP? DevOps Thiết kế RESTful APIs CodeFights – Hướng dẫn chi tiết cách giải các dạng bài tập CodeFights – Nơi những tráng sĩ coder hội tụ Database versioning Categories Advertising  (2)\n Agile  (1)\n Algorithm  (21)\n Android Development Patterns  (5)\n Bản tin iOS  (30)\n Business  (5)\n Coding  (45)\n Entertainement  (15)\n Framework  (27)\n Fun  (9)\n Journal  (4)\n Lesson & Practice  (14)\n Machine Learning  (17)\n Marketing  (6)\n Media  (1)\n Mobility  (14)\n Photography  (1)\n Primary  (11)\n Process  (6)\n Social Marketing  (1)\n Uncategorized  (8)\n Web Applications  (6)\n Web Design  (2)\n WordPress  (1)\n Archives October 2017  (2) July 2017  (1) May 2017  (2) April 2017  (3) March 2017  (3) January 2017  (3) December 2016  (4) November 2016  (1) July 2016  (2) June 2016  (2) May 2016  (3) April 2016  (10) March 2016  (6) February 2016  (3) January 2016  (12) December 2015  (8) November 2015  (6) October 2015  (4) September 2015  (6) August 2015  (10) May 2015  (2) April 2015  (7) March 2015  (22) February 2015  (5) Tag #react-native Ajit Johnson Android AnimatedVectorDrawable Apple TV asynchronous backend battery CSS Database Encode frontend Fuji Fujifilm hiếp ảnh iOS Java javascript machine-learning Microsoft Azure Nodejs package-by-feature photography PHP Process Realm Retro runtime Rxjava shopapp smart agriculture SQL Server structure SVG Swift Tip tvOS VectorDrawable Vintage Web X-E1 X-E2 X-T1 X100T Xephinh Meta Log in Entries  RSS Comments  RSS WordPress.org Copyright © 2015 3S Intersoft JSC. All Rights Reserved\r\n"
        },
        {
          "title": "Bài toán phân lớp trong Machine Learning (Classification in Machine Learning)",
          "relevance": "0",
          "url": "http://eitguide.net/bai-toan-phan-lop-trong-machine-learning-classification-machine-learning/",
          "content": "Facebook Twitter Behance Instagram Vkontakte SoundCloud DeviantArt Eitguide Machine Learning Android C/C++ Linux OpenCV C# Java Game SDL Liên Hệ Bài toán phân lớp trong Machine Learning (Classification in Machine Learning) January 4, 2017 Nguyễn Nghĩa Machine Learning No comments Tags: classification ,  machine learning ,  neural network ,  support vector machine Nguyễn Nghĩa Kiến Thức Nền Tảng- Tư duy tốt về hướng đối tượng. Không ràng buộc ở ngôn ngữ mà căn bản ở giải thuật và tư duy con người.\r\n- Lập trình trên mobile: Windows Phone 8/8.1, Android.\r\n- Xây dựng game engine sử dụng thư viện SDL2.\r\n- Kiến thức căn bản về đồ họa máy tính, trí tuệ nhân tạo. Previous post Tool hổ trợ nghiên cứu các thuật toán Machine Learning và Data Science Bài viết liên quan Tool hổ trợ nghiên cứu các thuật toán Machine Learning và Data Science December 8, 2016 Nguyễn Nghĩa Hướng dẫn cài đặt LibSVM Trên Linux và MacOS November 25, 2016 Nguyễn Nghĩa Leave a Reply  Cancel reply Your email address will not be published.  Required fields are marked  * Bình luận  * Tên  * Địa chỉ Email  * Website của bạn Bài viết gần đây Material Design CSS Bài 3: Overflow menu, checkbox, radio inputs Asynchronous operations Firebase Q&A, #1 Material Design CSS Bài 2: Toolbar và Input Material Design CSS Bài 1: Nền, giao diện card và hiệu ứng đổ bóng Cách đặt tên instance cho đúng khi code Java và Android Tìm hiểu về Material Design Launch screen cho iOS Splash screen cho Android Một số điểm cần lưu ý khi thao tác với Fragment Bình Luận Mới Nhất Công  on  Hướng dẫn submit ứng dụng Android lên Google Play (phần 2) Tan  on  Cách build một chương trình C/C++ bằng Terminal Bảo Huy  on  Lưu trữ dữ liệu với SQLite trong Android Tuân  on  Lưu trữ dữ liệu với SQLite trong Android Bảo Huy  on  Hướng dẫn submit ứng dụng Android lên Google Play (phần 1) Giới thiệu Eitguide là website chia sẻ kiến thức hoàn toàn miễn phí về lập trình, khoa học máy tính và hướng đến những bạn đam mê tìm tòi học hỏi về lĩnh vực lập trình. Hệ thống Website bao gồm các bài viết về các ngôn ngữ lập trình như C/C++, Java, C#, Swift. Và các bài viết về lập trình trên các thiết bị di động thông mình Android, iOS. Các bài viết ở Eitguide được biên tập một cách chi tiết, linh hoạt, và biên giúp cho người đọc có thể tiếp thu một các dễ dàng nhất. © 2016 All rights reserved. Eitguide.com"
        },
        {
          "title": "Machine learning – Trí Tuệ Nhân Tạo ảnh hưởng tới làm SEO thế nào?",
          "relevance": "1",
          "url": "http://trungvanhoang.com/2016/11/05/machine-learning-anh-huong-toi-seo/",
          "content": "Dự Án Đầu Tư HOT Trong Tháng 10: Etherbanking Home Kiếm Tiền Online Kiếm Tiền Điện Tử Kiếm tiền YouTube Kiếm Tiền với ClickBank Kiếm Tiền Bitcoin Kiếm Tiền Bitconnect Kiếm Tiền Ethereum Kiếm Tiền Litecoin Đầu Tư Questra Holdings Giới Thiệu Questra Holdings Hướng Dẫn Tạo Tài Khoản Nạp Tiền Đầu Tư Chọn Gói Đầu Tư Qũy Đầu Tư An Toàn Tái Đầu Tư Và Lãi Kép Bằng Chứng Thanh Toán Mua Bán Tiền Điện Tử Đầu Tư Bitconnect ICO Liên Hệ Home Kiếm Tiền Online Kiếm Tiền Điện Tử Kiếm tiền YouTube Kiếm Tiền với ClickBank Kiếm Tiền Bitcoin Kiếm Tiền Bitconnect Kiếm Tiền Ethereum Kiếm Tiền Litecoin Đầu Tư Questra Holdings Giới Thiệu Questra Holdings Hướng Dẫn Tạo Tài Khoản Nạp Tiền Đầu Tư Chọn Gói Đầu Tư Qũy Đầu Tư An Toàn Tái Đầu Tư Và Lãi Kép Bằng Chứng Thanh Toán Mua Bán Tiền Điện Tử Đầu Tư Bitconnect ICO Liên Hệ Dự Án Đầu Tư HOT Trong Tháng 10: Etherbanking lending voi bitconnect SEO / SEO Chuyên Nghiệp 3 Machine learning – Trí Tuệ Nhân Tạo ảnh hưởng tới làm SEO thế nào? by    Linh Vũ  ·\r\n                            Published  November 5, 2016 \r\n              · Updated  August 6, 2017 Cho Đi Để Nhận Lại Facebook 10 Google+ 1 Machine learning – trí tuệ nhân tạo  tưởng chừng là một vấn đề gì đó xa vời. Nhưng không phải vậy, nó đang diễn ra trong thực tế và được áp dụng ở những lĩnh vực mà bạn có thể không nghĩ tới. Và một ứng dụng của nó mà các bạn đang tiếp xúc hằng ngày đó chính là Google. Hôm nay, do có thời gian rỗi rãi nên tôi sẽ đào sâu về vấn đề này thêm một chút về cơ chế hoạt động của nó và cách nó đã được vận dụng vào trong SEO và digital marketing. Cơ duyên của tôi với Machine Learning – Trí Tuệ Nhân Tạo \nTôi được biết đến Machine learning và các lý thuyết trí tuệ nhân tạo từ khá sớm do hay theo dõi Discovery và các chương trình khoa giáo trên VTV2. Tuy nhiên trong quá trình học tập và làm việc tôi lại không có dịp tiếp cận sâu về các vấn đề này dù nó cũng đã được nhắc đến đôi lần. Thế nhưng gần đây, khi tham gia hội thảo về việc ứng dụng trí tuệ nhân tạo vào Google Dịch bởi chính kỹ sư đến từ Google cùng với việc trao đổi với một số bạn sinh viên trẻ thì hứng thú của tôi về vấn đề này lại tăng lên. Về cơ bản, Google đã thực hiện quét tất cả các văn bản đã được dịch mà họ có thể tìm thấy trên web và khai thác chúng. Đây là một ví dụ tiêu biểu và phức tạp về machine learning mà Google đã bắt đầu triển khai từ 2011. Không quá khi nói rằng tất cả những ông lớn công nghệ trên thế giới hiện nay như Google, Apple, Microsoft, Facebook đều đã và đang ứng dụng Machine learning theo nhiều cách khác nhau. Trong quá trình  làm SEO  của mình, tôi cũng đã có nghiên cứu các tài liệu trên mạng và thực hiện nhiều truy vấn để tìm ra quy luật hoạt động của cỗ máy tìm kiếm. Dựa trên kiến thức cơ bản về công nghệ thông tin của mình mà tôi đã dần hiểu ra nguyên tắc hoạt động và đã áp dụng thành công vào dự án SEO của mình. Dù hiểu biết còn khiêm tốn, nhưng như người ta vẫn nói, cho đi là nhận lại. Vậy nên tôi sẽ cố gắng diễn đạt những hiểu biết của mình về mối liên quan giữa Machine learning và  công cụ tìm kiếm  một cách dễ hiểu nhất cho những người chưa có bất kì khái niệm nào về Machine Learning và những ai đang có nhu cầu tìm hiểu về SEO và Digital Marketing trong giai đoạn hiện nay. Dành cho những bạn nào muốn tìm hiểu sâu hơn về Machine learning, các bạn có thể tham gia khóa học online về nó tại Coursera – website giáo dục đại học online nổi tiếng. Khóa học được truyền đạt qua 19 phần với các video dài hơn 1 tiếng. giảng dạy bởi Andrew Ng của trường đại học Standford. Và lưu ý thêm là bạn cần có một nền tảng toán học khá tốt để tham gia khóa học này vì sẽ có rất nhiều kiến thức toán học được nhắc đến trong suốt khóa học. Thêm vào đó, bạn sẽ được thấy nhiều ví dụ về ứng dụng trí tuệ nhân tạo dựa trên ngôn ngữ Octave – một ngôn ngữ rất mới mẻ đối với nhiều người, không chỉ mình tôi. Nếu các bạn vẫn còn gặp trở ngại về ngôn ngữ nhưng có nền tảng toán học và công nghệ thông tin vừa đù thì có thể tham khảo khóa học tương tự do TechMaster tổ chức . Cũng nhờ trí tuệ nhân tạo của Google mà tôi và bạn đều có thể tìm ra khóa học này nếu muốn. Bản thân tôi có lẽ cũng sẽ dành thời gian tham gia các khóa học trên để nâng cao kiến thức cho bản thân mình. Các khái niệm cơ bản về machine learning – Trí Tuệ Nhân Tạo Bạn có thể chia các hệ thống Machine learning hiện nay ra làm 2 loại chính : hệ học tập có giám sát và hệ học tập không giám sát. Học có giám sát – supervised machine learning Ở một cấp độ đơn giản nhất, bạn có thể hiểu hệ học tập có giám sát là tìm ra các quy luật của một tập dữ liệu xác định. Chẳng hạn bạn tìm một phương pháp ( giải thuật ) để dự đoán giá nhà. Bạn sẽ cần phải có một số dữ liệu giả định như sau. \nTrong ví dụ này, chúng ta đã có dữ liệu trước đây về giá nhà dựa trên diện tích của chúng. Và như bạn có thể thấy , giá nhà có xu hướng tăng cùng với diện tích, thế nhưng mức tăng của chúng không giống nhau và trên đồ thị giá nhà không năm trên đường thẳng. Tuy nhiên bạn có thể tính toán ra được đường thẳng này và sử dụng nó để làm tham chiếu dự đoán cho giá nhà đất. Như vậy, diện tích nhà được xem như dữ liệu đầu vào để sẽ đưa ra mức giá tức dữ liệu đầu ra thông qua giải thuật. Tuy nhiên, ví dụ này đã được tối giản hóa đi rất nhiều. Còn có rất nhiều yếu tố khác có thể ảnh hưởng tới giá nhà đất như số lượng phòng, số phòng ngủ, số phòng tắm và diện tích khuôn viên. Với những số liệu trên thì ta có thể có một mô hình phức tạp hơn chút xíu, với các bảng dữ liệu như sau. Lập tức bạn có thể thấy ngay một biểu đồ đường thẳng sẽ không thể hiện được một tương quan cần thiết, và bạn sẽ phải xem xét mức độ quan trọng của các yếu tố đầu vào để đánh giá giá nhà đất. Có lẽ yếu tố quan trọng nhất vẫn là diện tích nhà và diện tích khuôn viên nhưng số phòng, số phòng ngủ và phòng tắm có lẽ cũng nên có một mức độ quan trọng nhất định. Bộ dữ liệu sau khi đã được xem xét về mức độ ảnh hưởng sẽ được xem như dữ liệu đầu vào mới. Nhưng ngay cả sau khi đã thêm các yếu tố này thì mọi thứ vẫn còn khá đơn giản. Trên thực tế chúng ta vẫn cò một yếu tố lớn cần tính đến đó là vị trí. Giá một căn nhà ở Hồ Tây chắc chắn phải khác so với giá một căn nhà ở ngoại thành. Nếu bạn muốn xây dựng giải thuật cho bài toán trên ở tầm cỡ quốc gia, và có xét đến vị trí làm dữ liệu đầu vào thì mọi thức sẽ thực sự rất phức tạp. Dù vậy, qua ví dụ trên, ta có thể thấy rằng machine learning hoàn toàn có thể được ứng dụng để giải quyết các vấn đề như thế. Qua từng ví dụ, bạn đã thực hiện việc tạo dựng bộ dữ liệu mẫu (gọi là training samples – dữ liệu tập huấn) và chạy các chương trình để có thể tìm ra giải thuật đáp ứng được bộ dữ liệu. Từ đó bạn có thể đưa vào dữ liệu mới và sử dụng giải thuật đó để dự đoán kết quả đầu ra (trong trường hợp này là giá). Việc có sử dụng các tập mẫu như vậy được gọi là “học có giám sát”. Vấn đề với việc phân loại Trường hợp này sẽ cho thấy các vấn đề thường gặp khi xác định tập mẫu chuẩn nếu mục đích của bạn là muốn dự đoán một điều xác định nào đó. Ví dụ, chúng ta muốn dự đoán khả năng một đứa trẻ mới sinh sẽ có tiềm năng đạt chiều cao 1,82 m khi lớn lên (chuẩn chiều cao Âu Mỹ). Dưới đây là bộ dữ liệu đã thu thập được. Kết quả của giải thuật này có thể là 0% nếu tất cả người được lấy đữ liệu đều có chiều cao thấp hơn 1,82 m hoặc 100% nếu họ tất cả họ đều cao hơn hoặc bằng 1,82 m. Đây chính là vấn đề với việc phân loại vì bạn đang lấy dữ liệu từ một nhóm đặc thù thay vì một bộ dữ liệu tiêu chuẩn đại diện cho nhiều nhóm với đặc tính khác nhau. Và nên nhớ, ở đây chúng ta không cố gắng xác định chiều cao chính xác của đứa bé sau này, chỉ đơn giản là tính xắc suất để đứa bé có thể cao hơn hoặc thấp hơn 1,82 m. Một số ví dụ khác về độ phức tạp của việc phân loại để tìm ra tập mẫu là việc nhận diễn chữ viết tay và nhận diện email spam. Học không giám sát Việc học không giám sát áp dụng trong trường hợp bạn không có một tập dữ liệu mẫu chuẩn . Về cơ bản, bạn đang muốn tìm cách lọc và xác định các nhóm đối tượng với một số thuộc tính giống nhau. Giả sử bạn đang có một tập dữ liệu như thế này Sau đó giải thuật sẽ phải phân tích những dữ liệu này và tìm ra cách để nhóm chúng lại dựa trên những đặc điểm chung. Cụ thể đó là việc tất cả những hình chữ X màu đỏ sẽ được nhóm lại thành nhóm vì chúng có đặc tính tương đồng. Tuy nhiên, có thể một giải thuật đơn giản sẽ khó có thể nhận diện được một số điểm nằm ngoài ranh giới và có thể chỉ nhóm các dữ liệu lại theo cách dưới đây.     Điều mà giải thuật làm là tìm ra cách nhóm dữ liệu tự nhiên nhất, nhưng không giống như việc học có giám sát, nó phải xác định được đặc tính gì là riêng nhất của mỗi nhóm. Một dịch vụ đã đi vào hoạt động của Google là Google News là một ứng dụng của việc học không giám sát. Ví dụ bạn có thể nhìn vào ảnh dưới đây. Bạn có thể thấy rằng câu chuyện chính là về việc Iran giam giữ 10 thủy thủ Mỹ, ngoài ra còn có các tin bài liên quan lâý từ Reuters và Bloomberg được đánh dấu trong vùng màu đỏ. Việc nhóm các tin bài này lại với nhau chính là bài toán mà việc học không giám sát cần giải quyết. Các sản phẩm khác đã ứng dụng Machine Learning – Trí Tuệ Nhân Tạo Một ví dụ khác về giải thuật machine learning chính là giải thuật Author Extraction mà Moz đã xây dựng trong công cụ Moz content tool. Nếu có cơ hội, tôi sẽ đem đến cho các bạn bài viết về công cụ một cách cụ thể hơn. Còn công ty tư vấn Stone Temple có đưa ra một công cụ dựa trên mạng nơ-ron là Twitter Engagement Predictor , tức công cụ dự báo tương tác trên Twitter. Chương trình tạo ra một kết quả dự báo về việc bạn sẽ được retweet hay không, sau đó đưa ra phần trăm dự báo đó trở thành hiện thực. Đi sâu vào chi tiết hơn, mạng nơ-ron này được cấu thnahf bởi 6 đại lượng đâì vào, 15 đại lượng ẩn, và 2 đại lượng đầu ra. Giải thuật này cũng cần tới một triệu tập mẫu chuẩn và hai trăm lần chạy thử nghiệm. Quá trình huấn luyện phải trải qua chỉ gần 45 tỉ phép tính. Một điêu khiến chương trình này trở nên thú vị là có nhiều dữ liệu xung đột trong bảng dữ liệu thô. Sau đây là ví dụ. Trong bảng số liệu trên thì giá trị Followerwork Social Authority là từ 0 đến 9, và số tweet không có ảnh, URLs và không nhắc tới người dùng khác, không dùng hastag và số kí tự từ 0 tới 40. Thì ta có 1156 status không được retweet và 17 status được retwweet. Với bộ dữ liệu như trên thì có lẽ kết quả theo giải thuật sẽ là tweet này không được retweet, do đó nó sẽ cho ta biết được khả năng sai của giải thuật này là 1,4% (17 trường hợp status được retweet trên 1173 lần). Lưu ý rằng kết quả được đánh giá bởi mạng nơ -ron đã cho khả năng retweet là 2,1%. Moz đã thực hiện lập bảng kê rằng có bao nhiêu trường hợp sẽ tồn tại. Moz tìm ra rằng chúng ta có 102,045 tập mẫu thử riêng rẽ trong đó có thể có khả năng làm sai lệch dự đoán, tức là khoảng hơn 10% số bộ dữ liệu tập huấn. Điều đó có nghĩa là mạng nơ ron tốt nhất cũng chỉ có thể dự đoán đúng dưới 90% trường hợp. Moz cũng đã chạy thử hai bộ dữ liệu khác nữa ( Một bộ 470K và một bộ 473K phép thử) trên mạng trí tuệ nhân tạo này để xem độ chính xác của TEP. Và Moz nhận ra rằng có độ chính xác chắc chắn là 81% của khả năng một tweet được retweet. Với việc 10 % số tập mẫu cho kết quả sai là điều hiển nhiên, vậy nên kết quả trên là không tồi. Và dĩ nhiên đó là lý do là người ta chỉ đưa ra kết quả của phần trăm khả năng tweet được retweet chứ không chắc chắn rằng kết quả là có hay không. Bạn có thể thử sử dụng công cụ này tại đây và cho biết cảm nghĩ của bạn là gì. Những bằng chúng cho thấy Google đã ứng dụng machine learning Giờ chúng ta đã rõ hơn một chút về machine learning là gì, hãy tiếp tục tìm hiểu kĩ hơn về cách mà Google đã áp dụng Machine learning. Penguin Thuật toán Penguin được đưa ra nhằm giải quyết việc xác định một nhóm các tính chất của backlink có thể giúp xác định được link đó là link xấu, ví dụ như Link ra ngoài được đặt ở footer Link ra ngoài ở sidebar bên phải Nằm gần các text như “sponsored” Nằm gần các ảnh có từ Sponsored Được nhóm với các link với mức độ liên quan thấp Nhiều anchor text không liên quan tới nội dung page Có link ra ngoài trong phần điều hướng Không có dấu hiệu cho người dùng rằng đó là link Link từ các website xấu Và nhiều yếu tố khác Lưu ý rằng tất cả các điều trên không phải là xấu đối với một link đơn lẻ, nhưng thuật tónd có thể bắt đầu cắm cờ website của bạn nếu một phần đáng kể trong các link trỏ tới site của bạn có nhiều trong số các vấn đề kể trên. Những gì được liệt kê ở trên sẽ là cơ sở để quá trình học có giám sát được thực hiện vì khi đó bạn đã huấn luyện giải thuật bằng các link tốt hoặc xấu đã được xác định qua thời gian. Một khi giải thuật được huấn luyện, bạn có thể gửi những link khác đến nó và nó có thể tự xác định xem chúng có phải link xấu không. Dựa vào phần trăm số link (và hoặc tổng pagerank) tới từ các link xấu, Google có quyết định được có nên giảm rank của site của bạn hay không. Một cách tiếp cận khác đối với vấn đề này là việc sau khi đã có cơ sở dữ liệu của các link tốt và link xấu, sau đó trí tuệ nhân tạo sẽ tìm ra giải thuật để tự xác định những đặc tính của các link đó. Giải thuật sẽ tính đến cả những yếu tố mà con người không thể lường trước. Panda Giờ bạn đã thấy ví dụ về việc ứng dụng Machine learning vào Penguin, có lẽ bạn sẽ cảm thấy dễ hiểu hơn với trường hợp của Panda. Sau đây là những vấn đề sẽ xảy ra với các trang có chất lượng nội dung thấp. Số lượng từ thấp hơn so với các page đang cạnh tranh Sử dụng các từ đồng nghĩa ít Sử dụng quá nhiều từ khóa chính Có lượng lớn text tách biệt ở chân trang Có nhiều link tới các trang không liên quan Page với nội dung lấy từ các trang khác Và rất nhiều yếu tố khác Một lần nữa bạn có thể bắt đầu với một bộ các site tốt và site xấu đã biết (về mặt nội dung) và thiết kế một giải thuật để xác định các đặc tính chung của các site này. Như với đã giải thích với thuật toán Penguin ở trên , tuy tôi không thể trình bày hết tất cả mọi phần của Panda nhưng đó là những gì đủ để thấy khái niệm về cách hoạt động của Panda. Machine learning – Trí Tuệ Nhân Tạo ảnh hưởng tới làm SEO như thế nào? Mấu chốt để hiểu sự ảnh hưởng của Machine learning tới làm SEO là hiểu được nguyên nhân Google (và các công cụ tìm kiếm khác) muốn áp dụng nó. Chắc chắn có mối liên hệ mật thiết giữa việc Google cung cấp các kết quả tìm kiếm tốt và lợi nhuận họ thu được từ quảng cáo. Trờ về năm 2009, Bing và Google đã trình diễn một vài bài kiểm nghiệm cho thấy rằng dù chỉ có một chút chậm trễ trong kết quả tìm kiếm cũng có thể gây tác động lớn tới sự hài long của người dùng. Thêm vào đó, kết quả cho thấy sự hài lòng càng thấp thì số lick và lợi nhuận thu về cũng càng thấp hơn.   Nguyên nhân đằng sau việc này thật đơn giản. Google có các đối thủ cạnh tranh, không chỉ có Bing. Một sự loại cạnh tranh mà Google thường gặp phải là các công cụ tìm kiếm thân thiện với ngôn ngữ khác hơn. Tương tự, Facebook, Apple.,/ Siri và Amazon cũng gặp phải sự cạnh tranh tương tự. Các dịch vụ tra cứu thông tin và câu trả lời khác hiện có trên thị trường đều đang được cải thiện rất nhiều vậy nên Goolge cũng phải chạy đua theo. Vậy điều này có nghĩa là gì Như trên đã nói, sự hài lòng của người dùng có ý nghĩa tối quan trọng với Google, có nghĩa là chất lượng nội dung và sự hài lòng của người dùng với nội dung trên page của bạn phải được coi như yếu tố quan trọng hàng đầu. Bạn sẽ cần phải liên tục đo lường và cải thiện nó. Một số vấn đề bạn phải suy nghĩ trước khi thực hiện là. Page của bạn có đáp ứng được phần lớn người truy cập nó không? Nếu người dùng hứng thú với sản phẩm đó, liệu họ có chọn nó ? Học cách sử dụng nó ? Các nhu cầu liên quan khác ? Nếu một người tìm đến với website của bạn để tìm một sản phẩm xác định, đâu là các sản phẩm liên quan có thể họ sẽ tìm kiếm. Có sự mối liên hệ nào giữa các content trên page ? Page của bạn có mang lại trải nghiệm tốt hơn so với đối thủ Chiến lược để đo lường hiệu quả của page và cải tiến nó theo thời gian Có rất nhiều cách để Google đo lường được page của bạn tốt đến đâu và sủ dụng nó để cải thiện thứ hạng. Và sau đây là một vài trong số đó. Khi người dùng click vào trang của bạn sau khi click vào SERP, họ ở lại trang trong bao lâu ? Thời gian đó so với đối thủ như thế nào Tỉ lệ CTR của kết quả tìm kiếm tương ứng so với đối thủ như thế nào? Có bao nhiêu người tìm kiếm brand của bạn Nếu bạn có một page cho một sản phẩm nhất định, bạn có đưa vào đó nhiều nội dung hữu ích hơn so với đối thủ không Khi người dùng quay trở lại trang kết quả tìm kiếm sau khi vào trang của bạn, họ có những hành động cho thấy tìm kiếm của họ về vấn đề đó đã chấm dứt không? Hay họ tiếp tục ấn vào kết quả khác để tiếp tục tìm kiếm. Tổng kết Machine learning đã trở nên phổ biến. Rào cản để học được những giải thuật cơ bản đã không còn. Mọi ông lớn trong giới công nghệ đã tận dụng nó ở nhiều khía cạnh. Đây là một phần rất nhỏ trong những gì Facebook đang làm, và Apple cũng đang tuyển dụng các vị trí liên quan đến machine learning. Các hãng khác đang tạo ra những tiền đề để ứng dụng machine learning được thuận tiện hơn như Microsoft và Amazon. Với những bạn đang làm công việc liên quan tới SEO và digital Marketing, bạn có thể hi vọng rằng thay đổi chiến lược này sẽ khiến công cụ tìm kiếm trở nên tốt hơn và phục vụ tốt hơn cho các bạn. Đó là lý do mà chúng ta cần có chiến lược phù hợp với sự thay đổi này. Đối với SEO, machine learning sẽ tiếp tục nâng tầm quan trọng của chất lượng conent và trải nghiệm người dùng. Tức là đã đến lúc làm cho những yếu tố này trở thành trọng tâm của chiến lược SEO. Cho Đi Để Nhận Lại Facebook 10 Google+ 1 Hỏi Đáp  Bình Luận Related posts: Có nên tối ưu cùng một từ khóa với nhiều trang bài viết trên website?  SEO từ Đồng Nghĩa – tăng 890% lượng truy cập cho Site  Cách Xây Dựng Backlink Để Seo Website Từ Năm 2017 Về Sau  UX/UI Là Gì và Ảnh Hưởng Tới SEO Như Thế Nào? Thế Nào Là Giao Diện Tốt Cho SEO?  Tags: làm SEO Machine learning Moz.com Panda Penguin SEO chuyên nghiệp trí tuệ nhân tạo Linh Vũ Chuyên viên Marketing online. Đam mê về thiết kế đồ họa. You may also like... 1 UX/UI Là Gì và Ảnh Hưởng Tới SEO Như Thế Nào? Thế Nào Là Giao Diện Tốt Cho SEO? April 2, 2017  by     Linh Vũ  · Published  April 2, 2017 \r\n     · Last modified  August 6, 2017 3 Có nên tối ưu cùng một từ khóa với nhiều trang bài viết trên website? September 4, 2015  by     Linh Vũ  · Published  September 4, 2015 \r\n     · Last modified  August 6, 2017 5 Người dùng tương tác với Công Cụ Tìm Kiếm như thế nào? September 27, 2015  by     Chinh Nguyễn  · Published  September 27, 2015 \r\n     · Last modified  August 6, 2017 3 Responses Comments 1 Pingbacks 2 Hoa Do says: \n\t\t\tJune 9, 2017 at 2:47 pm Một bài viết tuyệt vời! khá dài nhưng mình đọc chăm chú từ đầu đến cuối 😀 Reply Cách Google Ứng Dụng Mạng Nơ-ron Vào Dịch Thuật Tự Động - TRUNG VĂN HOÀNG December 3, 2016 […] đã đã đề cập ở bài viết trước (Machine learning – Trí Tuệ Nhân Tạo ảnh hưởng tới làm SEO thế nào) Google Dịch là một sản phẩm trí tuệ nhân tạo nổi bật của Google. Tuy đây […] UI/UX là gì và ảnh hưởng đến SEO thế nào. Thế nào là giao diện tốt cho SEO April 15, 2017 […] loạt các bài viết vừa qua có lẽ các bạn đã biết thêm về sự liên quan của SEO tới trí tuệ nhân tạo. Và hôm nay mình sẽ gửi đến các bạn một góc nhìn nữa về mối liên quan của […] Leave a Reply  Cancel reply Your email address will not be published.  Required fields are marked  * Comment Name  * Email  * Website Follow: DỰ ÁN ICO Đầu Tư Tài Chính  /  Kiếm Tiền Điện Tử  /  Kiếm Tiền Online Các Mô Hình Đầu Tư Ủy Thác Tiền Điện Tử Trên Thế Giới Bạn Cần Biết Trước Khi Đầu Tư Đầu Tư Tài Chính  /  Kiếm Tiền Điện Tử  /  Kiếm Tiền Etherbanking  /  Kiếm Tiền Online Etherbanking là gì? Trở Thành Nhà Đầu Tư Kiếm Tiền Etherbanking 2018 Đầu Tư Tài Chính  /  Kiếm Tiền Bitconnect  /  Kiếm Tiền Online Lộ Trình Phát Triển Bitconnect 2017 – Đồng Tiền Điện Tử Top 15 Trên CoinMarketCap Đầu Tư Tài Chính  /  ICO  /  Kiếm Tiền Điện Tử Các Dự Án ICO Tặng Token ICO Miễn Phí Cho Người Đăng Ký Tài Khoản Mới Đầu Tư Tài Chính  /  Kiếm Tiền Bitconnect  /  Kiếm Tiền Online Tổng Hợp Các Câu Hỏi Liên Quan Tới Dự Án BITCONNECT – Chuyên Mục Hỏi Đáp Đầu Tư Tài Chính  /  Kiếm Tiền Online  /  Mạng Xã Hội FutureNet FutureNet là gì? Mạng Xã hội FutureNet Đầu Tư Trả Tiền Người Dùng Đầu Tư Tài Chính  /  Kiếm Tiền Litecoin  /  Mua Bán Tiền Ảo Hướng Dẫn Mua Litecoin và Bán Litecoin Sàn Buysellltc Uy Tín Đầu Tư Tài Chính  /  Kiếm Tiền Bitcoin Cash Case Study Nhận Tiền Bitcoin Cash (BCH) Miễn Phí Sau Hark Fork Chia Tách Từ Bitcoin Đầu Tư Tài Chính  /  Kiếm Tiền Điện Tử  /  Ví Cứng Trữ Lạnh Hướng Dẫn Ví Cứng Ledger Nano S Trữ Lạnh Bitcoin và Các Altcoins Đầu Tư Tài Chính  /  Kiếm Tiền Bitcoin  /  Kiếm Tiền Online Segwit và Segwit2x là gì? Cuộc chiến BTC, BCH, BTU Mang Tên Bitcoin 01/08/2017 Home Kiếm Tiền Online Kiếm Tiền Điện Tử Kiếm tiền YouTube Kiếm Tiền với ClickBank Kiếm Tiền Bitcoin Kiếm Tiền Bitconnect Kiếm Tiền Ethereum Kiếm Tiền Litecoin Đầu Tư Questra Holdings Giới Thiệu Questra Holdings Hướng Dẫn Tạo Tài Khoản Nạp Tiền Đầu Tư Chọn Gói Đầu Tư Qũy Đầu Tư An Toàn Tái Đầu Tư Và Lãi Kép Bằng Chứng Thanh Toán Mua Bán Tiền Điện Tử Đầu Tư Bitconnect ICO Liên Hệ TRUNG VĂN HOÀNG © 2015. All Rights Reserved 11 shares 10 1"
        },
        {
          "title": "Machine Learning 101 (1): Làm quen",
          "relevance": "0",
          "url": "http://khanhxnguyen.com/toi-da-hoc-machine-learning-nhu-the-nao-phan-1-lam-quen/",
          "content": "Khanh's little things Official homepage of Khanh Xuan Nguyen About Research Blog Tutorials Programming Machine Learning Tips Contact About Khanh Nguyen Ph.D. Student at UMaryland Recent Posts Deep learning (4): deep learning hoạt động ra sao? Deep learning (3): PyTorch và MNIST Deep learning (2): thách thức Deep learning (1): góc nhìn giáo dục Machine learning for non-differentiable loss functions August 09, 2015 Machine Learning 101 (1): Làm quen (Tựa bài cũ là “Tôi đã học Machine Learning như thế nào?” Mình quyết định đổi tên thành “Machine Learning 101” cho ngắn gọn dễ nhớ, và cũng dễ bắt mắt anh Google hơn) Xin chào, mình là khanhptnk. Mình là một người đang học và nghiên cứu về Machine Learning. Lĩnh vực mình chuyên sâu cho đến bây giờ là Natural Language Processing (gọi tắt là NLP), tức là làm cho máy tính có khả năng hiểu được ngôn ngữ của con người. Mình không phải là một chuyên gia, kinh nghiệm còn khá non nếu so với các giáo sư đầu ngành nhưng mình rất muốn đem bộ môn này về giới thiệu với các bạn, nhất là các bạn còn đang loay hoay tìm câu trả lời cho câu hỏi: học Tin học sau này làm được gì? Vào năm 2014, mình có thuyết trình trước lớp một bài giới thiệu về Machine Learning. Nội dung bài viết này phần lớn dựa vào đó. Để giới thiệu về Machine Learning, mình xin dựa vào mối quan hệ của nó với ba khái niệm sau: \n1. Machine Learning và trí tuệ nhân tạo (Artificial Intelligence hay AI) \n2. Machine Learning và Big Data. \n3. Machine Learning và dự đoán tương lai. Trí tuệ nhân tạo,  AI , một cụm từ vừa gần gũi vừa xa lạ đối với chúng ta. Gần gũi bởi vì thế giới đang phát sốt với những công nghệ được dán nhãn AI. Xa lạ bởi vì một AI thực thụ vẫn còn nằm ngoài tầm với của chúng ta. Nói đến AI, hẳn mỗi người sẽ liên tưởng đến một hình ảnh khác nhau. Các bạn có để ý rằng vài thập niên gần đây có một sự thay đổi về diên mạo của AI trong các bộ phim quốc tế. Trước đây, các nhà sản xuất phim thường xuyên đưa hình ảnh robot hoặc terminator vào phim, nhằm gieo vào đầu người xem suy nghĩ rằng trí tuệ nhân tạo như một phương thức nhân bản con người bằng máy móc. Tuy nhiên, trong những bộ phim gần đây nhất về đề tài này, ví dụ như  Transcendence  do Johny Depp vào vai chính, ta không thấy hình ảnh của một con robot nào cả. Thay vào đó là một bộ não điện toán khổng lồ chỉ huy hàng vạn con Nanobot, được gọi là  Singularity . Tất nhiên cả hai hình ảnh đều là hư cấu và giả tưởng, nhưng sự thay đổi như vậy cũng một phần nào phản ánh sự thay đổi ý niệm của con người về AI. AI bây giờ được xem như vô hình vô dạng, hay nói các khác có thể mang bất cứ hình dạng nào. Trong giới hàm lâm, theo hiểu biết chung, AI là một ngành khoa học được sinh ra với mục đích làm cho máy tính có được trí thông minh. Mục tiêu này vẫn khá mơ hồ vì không phải ai cũng đồng ý với một định nghĩa thống nhất về trí thông minh. Thế nên các nhà khoa học phải định nghĩa một số mục tiêu cụ thể hơn, một trong số đó là việc làm cho máy tính lừa được  Turing Test . Turing Test được tạo ra bởi  Alan Turing  (1912-1954), người được xem là cha đẻ của ngành khoa học máy tính hiện đại, nhằm phân biệt xem người đối diện có phải là người hay không (xem phim  The Imitation Game  về nhân vật này, nhưng đừng tin hết những gì trong phim). AI thể hiện một  mục tiêu  của con người.  Machine Learning  là một  phương tiện  được kỳ vọng sẽ giúp con người đạt được mục tiêu đó. Và thực thế thì Machine Learning đã mang nhân loại đi một quãng đường rất xa trên quãng đường chinh phục AI. Nhưng vẫn còn một quãng đường xa hơn cần phải đi. Machine Learning và AI có mối quan hệ chặt chẽ với nhau nhưng không hẳn là trùng khớp vì một bên là mục tiêu (AI), một bên là phương tiện (Machine Learning). Chinh phục AI mặc dù vẫn là mục đích tối thượng của Machine Learning, nhưng hiện tại Machine Learning tập trung vào những mục tiêu ngắn hạn hơn như: 1. Làm cho máy tính có những khả năng nhận thức cơ bản của con người như nghe, nhìn, hiểu được ngôn ngữ, giải toán, lập trình, … và 2. Hỗ trợ con người trong việc xử lý một khối lượng thông tin khổng lồ mà chúng ta phải đối mặt hàng ngày, hay còn gọi là Big Data. Big Data  thực chất không phải là một ngành khoa học chính thống. Đó là một cụm từ dân gian và được giới truyền thông tung hô để ám chỉ thời kì bùng nổ của dữ liệu hiện nay. Nó cũng không khác gì với những cụm từ như “cách mạng công nghiệp”, “kỉ nguyên phần mềm”. Big Data là một hệ quả tất yếu của việc mạng Internet ngày càng có nhiều kết nối. Với sự ra đời của các mạng xã hội nhưng Facebook, Instagram, Twitter, nhu cầu chia sẻ thông của con người tăng trưởng một cách chóng mặt. Youtube cũng có thể được xem là một mạng xã hội, nơi mọi người chia sẻ video và comment về nội dung của video. Để hiểu được quy mô của Big Data, hãy xem qua nhũng con số sau đây: + Khoảng  300 giờ video  được upload trên youtube trong mỗi phút (theo  https://www.youtube.com/yt/press/statistics.html ) \n+ Hơn  900 triệu người  thật sự sử dụng Facebook mỗi ngày, 82.8% trong số đó ở ngoài Mỹ và Canada (theo  http://newsroom.fb.com/company-info/ ) \n+ Nhu cầu chia sẻ tăng đi đôi với việc nhu cầu tìm kiếm thông tin cũng tăng. Google phải xử lý  100 tỉ lượt  tìm kiếm mỗi tháng, tức là 3,3 tỉ lượt mỗi ngày và 38.000 lượt mỗi giây (theo  http://www.internetlivestats.com/google-search-statistics/ ). Và những con số này đang tăng lên theo từng giây! Bùng nổ thông tin không phải là lý do duy nhất dẫn đến sự ra đời của cụm từ Big Data. Nên nhớ rằng Big Data xuất hiện mới từ vài năm gần đây nhưng khối lượng dữ liệu tích tụ kể từ khi mạng Internet xuất hiện vào cuối thế kỉ trước cũng không phải là nhỏ. Thế nhưng, lúc ấy con người ngồi quanh một đống dữ liệu và không biết làm gì với chúng ngoài lưu trữ và sao chép. Cho đến một ngày, các nhà khoa học nhận ra rằng trong đống dữ liệu ấy thực ra chứa một khối lượng tri thức khổng lồ. Những tri thức ấy có thể giúp cho ta hiểu thêm về con người và xã hội. Từ danh sách bộ phim yêu thích của một cá nhân chúng ta có thể rút ra được sở thích của người đó và giới thiệu những bộ phim người ấy chưa từng xem, nhưng phù hợp với sở thích. Từ danh sách tìm kiếm của cộng đồng mạng chúng ta sẽ biết được vấn đề nóng hổi nhất đang được quan tâm và sẽ tập trung đăng tải nhiều tin tức hơn về vấn đề đó. Big Data chỉ thực sự bắt đầu từ khi chúng ta hiểu được gía trị của thông tin ẩn chứa trong dữ liệu, và có đủ tài nguyên cũng như công nghệ để có thể khai thác chúng trên quy mô khổng lồ. Và không có gì ngạc nhiên khi Machine Learning chính là thành phần mấu chốt của công nghệ đó. Ở đây ta có một quan hệ hỗ tương giữa Machine Learning và Big Data: Machine Learning phát triển hơn nhờ sự gia tăng của khối lượng dữ liệu; ngược lại, thành công của Big Data phụ thuộc vào khả năng khai thác tri thức từ dữ liệu. Ngược dòng lịch sử, Machine Learning đã xuất hiện từ rất lâu trước khi mạng Internet ra đời. Một trong những thuật toán Machine Learning đầu tiên là thuật toán  Perceptron  được phát minh ra bởi  Frank Rosenblatt  vào năm 1957. Đây là một thuật toán kinh điển dùng để  phân loại  hai khái niệm. Một ví dụ đơn gỉan là phân loại thư rác (tam gíac) và thư bình thường (vuông). Chắc các bạn sẽ khó hình ra được làm thế nào để làm được điều đó. Đối với Perceptron, điều này không khác gì với việc vẽ một đường thẳng trên mặt phẳng để phân chia hai tập điểm: Sơ lược quy trình phân loại thư được mô tả sau. Trước hết, ta cần một thuật toán để chuyển email thành những điểm dữ liệu. Công đoạn này rất rất quan trọng vì nếu chúng ta chọn được biểu diễn phù hợp, công việc của Perceptron sẽ nhẹ nhàng hơn rất nhiều. Tiếp theo, Perceptron sẽ đọc tọa độ của từng điểm và sử dụng thông tin này để cập nhật tham số của đường thẳng cần tìm. Các bạn có thể xem demo của Perceptron tại đây (điểm xanh lá cây là điểm Perceptron đang xử lý): Những điểm tam giác và vuông đại diện cho những email chúng ta đã biết nhãn trước. Chúng được dùng để “huấn luyện” (train) Perceptron. Sau khi vẽ đường thẳng chia hai tập điểm, ta nhận thêm các điểm chưa được dán nhãn, đại diện cho các email cần được phân loại (điểm tròn). Ta dán nhãn của một điểm theo nhãn của các điểm cùng nửa mặt phẳng với điểm đó. Vì là một thuật toán khá đơn giản, có rất nhiều vấn đề có thể nảy sinh với Perceptron, ví dụ như điểm cần phân loại nằm ngay trên đường thẳng phân chia. Hoặc tệ hơn là với một tập dữ liệu phức tạp hơn, đường thẳng phân chia không tồn tại: Lúc này, ta cần các loại đường phân chia “không thẳng”. Nhưng đó lại là một câu chuyện khác. Perceptron là một thuật toán “học có hướng dẫn” ( supervised learning ): ta đưa cho máy tính hàng loạt các ví dụ cùng câu trả lời mẫu với hy vọng máy tính sẽ tìm được những đặc điểm cần thiết để đưa ra dự đoán cho những ví dụ khác chưa có câu trả lời trong tương lai. Ngoài ra, cũng có những thuật toán Machine Learning “không cần hướng dẫn” ( unsupervised learning ): máy tính cố gắng khai thác ra cấu trúc ẩn của một tập dữ liệu mà không cần câu trả lời mẫu. Một loại Machine Learning khác được gọi là “học củng cố” ( reinforcement learning ). Trong dạng này, cũng không hề có câu trả lời mẫu, nhưng thay vì đó máy tính nhận được phản hồi cho mỗi hành động. Dựa vào phản hồi tích cực hay tiêu cực mà máy tính sẽ điều chỉnh hoạt động cho phù hợp. Sau đây là một ví dụ minh họa: Mục tiêu của chiếc xe là leo lên được đỉnh đồi và lấy được ngôi sao. Chiếc xe có hai chuyển động tới và lui. Bằng cách thử các chuyển động và nhận được phản hồi là độ cao đạt được và thời gian để lấy được ngôi sao, chiếc xe dần trở nên thuần thục hơn trong việc leo đồi lấy sao. Machine Learning có mối quan hệ rất mật thiết đối với thống kê ( statistics ). Machine Learning sử dụng các mô hình thống kê để “ghi nhớ” lại sự phân bố của dữ liệu. Tuy nhiên, không đơn thuần là thống kê dữ liệu, Machine Learning phải có  khả năng tổng quát hóa  những gì đã được nhìn thấy và đưa ra dự đoán cho những trường hợp chưa được nhìn thấy. Bạn có thể hình dung một mô hình Machine Learning không có khả năng tổng quát như một đứa trẻ học vẹt: chỉ trả lời được những câu trả lời mà nó đã học thuộc lòng đáp án. Khả năng tổng quát là một khả năng tự nhiên và kì diệu của con người: bạn không thể nhìn thấy tất cả các khuôn mặt người trên thế giới nhưng bạn có thể nhận biết được một thứ có phải là khuôn mặt người hay không với xác suất đúng gần như tuyệt đối. Đỉnh cao của Machine Learning sẽ là mô phỏng được khả năng tổng quát hóa và suy luận này của con người. Như ta đã thấy, nói đến Machine Learning là nói đến “dự đoán”: từ việc dự đoán nhãn phân loại đến dự đoán hành động cần thực hiện trong bước tiếp theo. Vậy Machine Learning có thể dự đoán tương lai hay không? Có thể có hoặc có thể không: Machine Learning có thể dự đoán được tương lai, nhưng chỉ khi tương lai có mối liên hệ mật thiết với hiện tại. Để kết thúc bài viết, mình muốn cùng các bạn xem xét một ví dụ đơn giản sau. Giả sử bạn được đưa cho một đồng xu, rồi được yêu cầu tung đồng xu một số lần. Vấn đề đặt ra là: dựa vào những lần tung đồng xu đó, bạn hãy tiên đoán ra kết quả lần tung tiếp theo. Chỉ cần dựa vào tỉ lệ xấp/ngửa của những lần tung trước đó, bạn có thể đưa ra một dự đoán khá tốt. Nhưng nếu mỗi lần tung, người ta đưa cho bạn một đồng xu khác nhau thì mọi chuyện sẽ hoàn toàn khác. Các đồng xu khác nhau có xác suất xấp ngửa khác nhau. Lúc này việc dự đoán gần như không thể vì xác suất xấp ngửa của lần tung sau không hề liên quan gì đến lần tung trước. Điều tương tự cũng xảy ra với việc dự đoán tương lai bằng Machine Learning, nếu ta xem như mỗi ngày có một “đồng xu” được tung ra để xem một sự kiện có diễn ra hay không. Nếu “đồng xu” của ngày mai được chọn một cách tùy ý không theo phân bố nào cả thì Machine Learning sẽ thất bại. Rất may là trong nhiều trường hợp điều đó không hoàn toàn đúng, thế giới hoạt động theo những quy luật nhất định và Machine Learning có thể nhận ra những quy luật đó. Nhưng nói cho cùng, Machine Learning hoàn toàn không phải là một bà phủ thủy với quả cầu tiên tri mà cũng giống như chúng ta:  phán đoán bằng cách tổng quát hóa những kinh nghiệm, những gì đã được học từ dữ liệu. Post Views:  3,515 Share this: Click to share on Twitter (Opens in new window) Click to share on Facebook (Opens in new window) Click to share on Google+ (Opens in new window) Related Comments  comments"
        },
        {
          "title": "Trí tuệ nhân tạo, deep learning, machine learning là gì?",
          "relevance": "0",
          "url": "http://khoahoc.tv/tri-tue-nhan-tao-deep-learning-machine-learning-la-gi-70189",
          "content": " f  KhoaHoc.tv  Trang chủ \r\n                            Công nghệ\r\n                         \r\n                            Đời sống\r\n                         \r\n                            Khám phá khoa học\r\n                         \r\n                            Khoa học vũ trụ\r\n                         \r\n                            1001 bí ẩn\r\n                         \r\n                            Y học - Sức khỏe\r\n                         4 4 8.349 Tham khảo thêm \r\n                        Trí tuệ nhân tạo\r\n                             4.507 người xem \r\n                        Trí tuệ nhân tạo viết chữ giống y như con người\r\n                             2.338 người xem \r\n                        Thành phần cấu tạo nên Mặt trời là gì?\r\n                             16.995 người xem \r\n                        Trí tuệ nhân tạo vẫn còn kém xa con người ở một điểm quan trọng\r\n                             6.434 người xem \r\n                        Khi trí tuệ nhân tạo mạnh hơn đầu óc con người, thế giới sẽ ra sao?\r\n                             5.368 người xem \r\n                        Trí thông minh nhân tạo có thể \"vẽ” tranh như Van Gogh và Picasso\r\n                             1.453 người xem \r\n                        Trí thông minh nhân tạo phát triển đặt dấu chấm hết cho loài người\r\n                             5.765 người xem 4 4 8.349 Bài viết mới nhất Đào được viên kim cương vàng cực hiếm ở Nga Thành phố sao Hỏa tương lai của UAE trên Trái Đất Những yếu tố của điện thoại di động có thể gây cháy nổ ở trạm xăng Những căn bệnh kinh dị không dành cho người yếu tim xem Kinh nghiệm phòng tránh và điều trị kiến ba khoang đốt người 6 lời khuyên của các tiếp viên hàng không về chứng say máy bay Xem thêm Công nghệ Bài viết Công nghệ mới khác Thiệp Giáng sinh nhỏ nhất thế giới Google ra mắt Google Home, đối thủ trực tiếp của Amazon Echo Phát hiện sử dụng ma tuý qua mồ hôi Loại gel dẻo bền chắc gấp 5 lần thép Thiết bị chống kính Google Sinh viên Ấn Độ công bố đồng hồ thông minh Androidly Xem thêm Tiêu điểm \r\n                        Đại chiến robot khổng lồ giữa Mỹ và Nhật sắp diễn ra\r\n                     \r\n                        Máy bay siêu thanh vượt Đại Tây Dương trong ba tiếng bay thử\r\n                     \r\n                        Tỷ phú Mỹ tiết lộ về quái vật đào hầm thứ hai\r\n                     \r\n                        Mỹ chiến thắng Nhật Bản trong đại chiến Robot khổng lồ\r\n                     \r\n                        Xanh hoá sa mạc nhờ siêu công nghệ chế tạo nước và canh tác khô\r\n                     \r\n                        Máy bay không động cơ bay bằng sóng núi phá vỡ kỷ lục thế giới\r\n                     \r\n                        Đại học Stanford chế tạo thành công pin siêu rẻ, không cần lithium\r\n                     ☰ Danh mục \r\n                            Công nghệ mới\r\n                         \r\n                            Phần mềm hữu ích\r\n                         \r\n                            Khoa học máy tính\r\n                         \r\n                            Phát minh khoa học\r\n                         \r\n                            Khám phá khoa học\r\n                         \r\n                            Sinh vật học\r\n                         \r\n                            Khảo cổ học\r\n                         \r\n                            Đại dương học\r\n                         \r\n                            Thế giới động vật\r\n                         \r\n                            Khoa học vũ trụ\r\n                         \r\n                            Danh nhân thế giới\r\n                         \r\n                            Ngày tận thế\r\n                         \r\n                            1001 bí ẩn\r\n                         \r\n                            Di sản thế giới\r\n                         \r\n                            Chinh phục sao Hỏa\r\n                         \r\n                            Người ngoài hành tinh\r\n                         \r\n                            Lịch sử\r\n                         \r\n                            Y học - Sức khỏe\r\n                         \r\n                            Môi trường\r\n                         \r\n                            Ứng dụng khoa học\r\n                         \r\n                            Khoa học & Bạn đọc\r\n                         \r\n                            Thư viện ảnh\r\n                         \r\n                            Góc hài hước\r\n                         \r\n                            Video\r\n                         \r\n                            Công trình khoa học\r\n                         \r\n                            Câu chuyện khoa học\r\n                         \r\n                            Sự kiện Khoa học\r\n                         Bài viết liên quan \r\n                        Stephen Hawking lần nữa cảnh báo siêu AI là mốc diệt vong của loài người\r\n                     \r\n                        AI của Google đã vượt qua giới hạn của trí tuệ nhân tạo\r\n                     \r\n                        Bạn có biết Facebook ra mắt chatbot để làm gì không?\r\n                     \r\n                        Kỹ sư gốc Việt muốn thay đổi thế giới bằng trí tuệ nhân tạo\r\n                     \r\n                        Robot giúp việc mới là kẻ đe dọa nhân loại trong 20 năm tới\r\n                     \r\n                        Trí thông minh nhân tạo đánh bại nhà vô địch cờ vây châu Âu\r\n                     \r\n                        Ông hoàng vật lý: \"Con người đủ khả năng di cư tới hành tinh khác\"\r\n                     \r\n                        Trí tuệ nhân tạo vẫn còn kém xa con người ở một điểm quan trọng\r\n                     \r\n                        Khi trí tuệ nhân tạo mạnh hơn đầu óc con người, thế giới sẽ ra sao?\r\n                     Trang chủ  . \r\n         Liên hệ   . \r\n         Facebook  .\r\n        \r\n     \r\n        Copyright © 2017 KhoaHoc.tv\r\n    "
        },
        {
          "title": "Unsupervised Machine Learning 1: Các thuật toán phân cụm",
          "relevance": "0",
          "url": "http://rstudio-pubs-static.s3.amazonaws.com/264432_95f817c998b24d23bba6ed5057257aa5.html",
          "content": "Giới thiệu về các thuật toán phân cụm Các thuật toán phân cụm được xếp vào nhóm phương pháp học không giám sát ( Unsupervided Learning ) vì mục tiêu chủ yếu của nó là tìm ra các thông tin hữu ích tiềm ẩn từ tập dữ liệu hoặc giảm chiều dữ liệu (Dimension Reduction) từ một số rất lớn ban đầu. Các thuật toán phân cụm (Clustering) nói riêng và cách tiếp cận của phương pháp học không giám sát nói chung hiện có vai trò quan trọng trong nhiều lĩnh vực nghiên cứu như Y Học, Marketing cũng như nghiên cứu Kinh Tế, Tài Chính. Trong series này về một số thuật toán phân cụm, những cách tiếp cận (thuật toán sau sẽ được giới thiệu): Phân cụm K Means (K Mean Clustering). Phâm cụm cấp bậc (Hierarchical Clustering). Phân tích thành phần chính PCA (Principal Component Analysis) Trước khi đi chi tiết vào ba thuật toán phân cụm trên chúng ta sẽ nghiên cứu tác động của việc lựa chọn phương pháp đo khoảng cách lên kết quả của thuật toán phân cụm K means clustering (cũng như một số thuật toán phân cụm khác). Các phương pháp đo khoảng cách Việc lựa chọn phương pháp đo khoảng cách là bước quan trọng khi thực hiện thuật toán phân cụm. Khoảng cách được hiểu tổng quát trong trường hợp này là “mức độ tương tự” giữa hai quan sát. Có hai nhóm phương pháp chính đo khoảng cách giữa hai quan sát. Nhóm thứ nhất bao gồm: Khoảng cách Euclidean. Khoảng cách Manhattan. Nhóm thứ hai là đo  khoảng cách dựa trên tương quan (correlation-based distances)  bao gồm: Dựa trên tương quan Pearson (Pearson correlation distance). Dựa trên tương quan cosin Eisen (Eisen cosine correlation distance). Dự trên tương quan Spearman. Dựa trên tương quan Kendall. Chúng ta xét ngay ví dụ sau về khoảng cách Euclidean với bộ số liệu  USArrests  bằng cách lấy ba quan sát đầu tiên: # Load dữ liệu\r\ndata(USArrests)\r\n(df1 <- USArrests[1:3, ]) ##         Murder Assault UrbanPop Rape\r\n## Alabama   13.2     236       58 21.2\r\n## Alaska    10.0     263       48 44.5\r\n## Arizona    8.1     294       80 31.0 # \"Khoảng cách\" giữa Alabama và Alaska: \r\n\r\nalab <- df1[1, ]\r\nalas <- df1[2, ]\r\n(d1 <- sqrt(sum((alab - alas)^2))) ## [1] 37.17701 # \"Khoảng cách\" giữa Alabama và và Arizona: \r\n\r\nariz <- df1[3, ]\r\n(d2 <- sqrt(sum((alab - ariz)^2))) ## [1] 63.00833 # Khoảng cách giữa Alaska và Arizona: \r\n(d3 <- sqrt(sum((alas - ariz)^2))) ## [1] 46.59249 Khoảng cách và vài trò của đồng bộ hóa (Scaling) hay chuẩn hóa (Standalization) thước đo Vì rằng tỉ lệ giết người, tội ác liên quan đến bạo lực, tỉ lệ án hiếp dâm và đô thị hóa trong ví dụ trên được đo bằng các đơn vị (hay thước đo) khác nhau nên việc chuẩn hóa (hay đồng bộ hóa) các thước đo là quan trọng và việc này có ảnh hưởng đến khoảng cách giữa các quan sát. Nói thế này cho dễ hiểu: Cân nặng đo bằng Kg, tuổi đo bằng năm nên ta cần một “đơn vị đo chung” cho chúng. Có nhiều cách thức chuẩn hóa “thước đo” và dưới đây chúng ta xem xét tác động của việc biến các tọa độ (các biến) sao cho chúng có mean là 0 và độ lệch chuẩn là 1 với số liệu ở trên: # Viết hàm chuẩn hóa và chuẩn hóa bộ dữ liệu: \r\n\r\nmy_scale <- function(x) {(x - mean(x))/sd(x)}\r\ndf1 <- sapply(df1, my_scale) \r\nsummary(df1) ##      Murder           Assault            UrbanPop            Rape        \r\n##  Min.   :-0.9053   Min.   :-0.97624   Min.   :-0.8552   Min.   :-0.9431  \r\n##  1st Qu.:-0.5367   1st Qu.:-0.51109   1st Qu.:-0.5498   1st Qu.:-0.5243  \r\n##  Median :-0.1681   Median :-0.04594   Median :-0.2443   Median :-0.1054  \r\n##  Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.0000  \r\n##  3rd Qu.: 0.4526   3rd Qu.: 0.48812   3rd Qu.: 0.4276   3rd Qu.: 0.4716  \r\n##  Max.   : 1.0734   Max.   : 1.02218   Max.   : 1.0995   Max.   : 1.0485 # Lúc này, các khoảng cách sẽ là: \r\n\r\nalab <- df1[1, ]\r\nalas <- df1[2, ]\r\n(d1 <- sqrt(sum((alab - alas)^2))) ## [1] 2.59743 # \"Khoảng cách\" giữa Alabama và và Arizona: \r\n\r\nariz <- df1[3, ]\r\n(d2 <- sqrt(sum((alab - ariz)^2))) ## [1] 3.22747 # Khoảng cách giữa Alaska và Arizona: \r\n(d3 <- sqrt(sum((alas - ariz)^2))) ## [1] 2.614727 Tất nhiên, thay vì tính toán thủ công như trên chúng ta có thể dùng hàm  dist  như sau: library(tidyverse)\r\ndf1 %>% dist(method = \"euclidean\") %>% head() ## [1] 2.597430 3.227470 2.614727 # Các con số 2.597430, 3.227470 và 2.614727 chính  là các kết quả thu được ở trên.  Nhược điểm của hàm dist ở trên là không áp dụng được cho dữ liệu factor, ordinal. Trong những tình huống này chúng ta sử dụng hàm  daisy()  . Trở lại với bộ số liệu USArrests: data(USArrests)\r\n(df1 <- USArrests[1:3, ]) ##         Murder Assault UrbanPop Rape\r\n## Alabama   13.2     236       58 21.2\r\n## Alaska    10.0     263       48 44.5\r\n## Arizona    8.1     294       80 31.0 # Load gói cluster package\r\nlibrary(cluster)\r\n\r\n# euclidean và không chuẩn hóa: \r\ndaisy(df1, metric = \"euclidean\", stand = FALSE) ## Dissimilarities :\r\n##          Alabama   Alaska\r\n## Alaska  37.17701         \r\n## Arizona 63.00833 46.59249\r\n## \r\n## Metric :  euclidean \r\n## Number of objects : 3 # euclidean và chuẩn hóa thước đo nhưng  theo  một kiểu khác là subtracting the variable's mean value and dividing by the variable's mean absolute deviation: \r\ndaisy(df1, metric = \"euclidean\", stand = TRUE) ## Dissimilarities :\r\n##          Alabama   Alaska\r\n## Alaska  3.699459         \r\n## Arizona 4.587244 3.654346\r\n## \r\n## Metric :  euclidean \r\n## Number of objects : 3 # Nếu muốn sử dụng kiểu chuẩn hóa quen thuộc của chúng ta: \r\ndf1 <- sapply(df1, my_scale)\r\ndaisy(df1, metric = \"euclidean\", stand = FALSE) ## Dissimilarities :\r\n##          1        2\r\n## 2 2.597430         \r\n## 3 3.227470 2.614727\r\n## \r\n## Metric :  euclidean \r\n## Number of objects : 3 Thế ý nghĩa của các khoảng cách này là gì? Là nếu, ví dụ, bạn chọn một ngưỡng là “khoảng cách bé hơn 2.6” thì bạn có thể “nhóm” Alabama và Alaska vào một cụm (Cluster). Còn riêng ông Arizona đứng lẻ loi một mình - tức thuộc một cụm khác. Đây là cách giải thích trực quan nhất (tuy chưa hoàn toàn chính xác) cho  k mean clustering  - một thuật toán phân cụm dựa trên khoảng cách. Dưới đây trình bày một ví dụ trực quan khác về phân cụm với bộ số liệu  WordCities  về 23018 thành phố trên thế giới. Tuy nhiên chúng ta chỉ lọc ra 4000 thành phố lớn nhất theo tiêu chí số dân và phân cụm chúng thành 6 nhóm dự trên kinh độ và vĩ độ. Chi tiết kĩ thuật của R codes các bạn chưa cần quan tâm tại thời điểm này. library(mdsr)\r\ndata(\"WorldCities\")\r\nBigCities <- WorldCities %>% \r\n  arrange(desc(population)) %>% \r\n  select(longitude, latitude) %>% \r\n  slice(1:4000)\r\n\r\nhead(BigCities) ##   longitude  latitude\r\n## 1 121.45806  31.22222\r\n## 2 -58.37723 -34.61315\r\n## 3  72.88261  19.07283\r\n## 4 -99.12766  19.42847\r\n## 5  67.08220  24.90560\r\n## 6  28.94966  41.01384 library(mclust)\r\nset.seed(29)\r\n\r\ncity_clusts <- BigCities %>% \r\n  kmeans(centers = 6) %>% \r\n  fitted(\"classes\") %>% \r\n  as.character()\r\n\r\nBigCities <- BigCities %>% \r\n  mutate(cluster = city_clusts) \r\n\r\nBigCities %>% \r\n  ggplot(aes(x = longitude, y = latitude)) + \r\n  geom_point(aes(color = cluster), alpha = 0.5) Nếu dựa trên kinh độ và vĩ độ có thể thây 4000 thành phố này được phân thành các cụm rõ ràng. Ví dụ, các thành phố ở châu Mĩ là tách biệt rõ ràng với phần còn lại. Thuật toán K means Clustering Về cơ bản đây là thuật toán phân chia bộ dữ liệu gồm n quan sát ban đầu thành K cụm (Cluster) sao cho sự đồng nhất (homogeneous) giữa các quan sát trong nhóm là cao nhất có thể. Hay nói cách khác, thuật toán này nhóm các quan sát thành K cụm khác nhau sao cho sự khác biệt giữa các quan sát trong mỗi cụm là thấp nhất. Sự khác biệt ấy có thể là các một đặc tính hay một nhóm đạc tính nào đó (thường gọi là attributes) của các quan sát. Sự đồng nhất (hay khác biệt giữa các quan sát) ấy được lượng hóa bằng tổng các “khoảng cách” giữa các quan sát trong một cụm con (Sub Cluster) mà chúng ta đã đề cập ở trên và sẽ là tối ưu khi tổng này bé nhất có thể được. Để hiểu sâu hơn về những khái niệm rắc rối này, chúng ta sẽ nghiên cứu một ví dụ trực quan lấy từ cuốn  R for Marketing Research and Analytics . Mini Project 1: K Means Clustering trong Marketing cho phân loại khách hàng Giới thiệu bài toán Mục tiêu của chúng ta là tìm cách phân loại những khách hàng tiêu dùng tiềm năng thành các nhóm (trong Marketing, việc này gọi là Segmentation) dựa trên các thông tin như thu nhập, các đặc điểm chủng tộc và cá nhân khác nhau của họ. path <- dir(\"E:/R_projects/Marketing\", full.names = TRUE)\r\nseg.raw <- read.csv(path[[8]])\r\nhead(seg.raw) ##        age gender   income kids ownHome subscribe    Segment\r\n## 1 47.31613   Male 49482.81    2   ownNo     subNo Suburb mix\r\n## 2 31.38684   Male 35546.29    1  ownYes     subNo Suburb mix\r\n## 3 43.20034   Male 44169.19    0  ownYes     subNo Suburb mix\r\n## 4 37.31700 Female 81041.99    1   ownNo     subNo Suburb mix\r\n## 5 40.95439 Female 79353.01    3  ownYes     subNo Suburb mix\r\n## 6 43.03387   Male 58143.36    4  ownYes     subNo Suburb mix dim(seg.raw) ## [1] 300   7 str(seg.raw) ## 'data.frame':    300 obs. of  7 variables:\r\n##  $ age      : num  47.3 31.4 43.2 37.3 41 ...\r\n##  $ gender   : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 1 1 2 2 2 1 1 ...\r\n##  $ income   : num  49483 35546 44169 81042 79353 ...\r\n##  $ kids     : int  2 1 0 1 3 4 3 0 1 0 ...\r\n##  $ ownHome  : Factor w/ 2 levels \"ownNo\",\"ownYes\": 1 2 2 1 2 2 1 1 1 2 ...\r\n##  $ subscribe: Factor w/ 2 levels \"subNo\",\"subYes\": 1 1 1 1 1 1 1 1 1 1 ...\r\n##  $ Segment  : Factor w/ 4 levels \"Moving up\",\"Suburb mix\",..: 2 2 2 2 2 2 2 2 2 2 ... Nếu chúng ta biết trước các nhóm khách hàng, thì hình ảnh sau cung cấp nhiều thông tin ý nghĩa: df1 <- seg.raw %>% select(age, income, kids, Segment)\r\npar(mfrow = c(1, 3)) \r\npar(bg = \"grey98\") \r\nfor (i in 1:3) {\r\n  boxplot(df1[, i] ~ df1$Segment, \r\n          main = names(df1[i]), \r\n          col = rainbow(4)) \r\n}  par(mfrow = c(1, 1))  Thực vậy, có vẻ như là nếu căn cứ theo độ tuổi thì những quan sát này có thể được phân thành 4 nhóm riêng biệt. Tất nhiên trong thực tế chúng ta không biết trước 4 nhóm này và do vậy nhiệm vụ của chúng ta là phân cụm các khách hàng này. Bài toán này, nếu giải quyết được thì VP Bank hay các tổ chức kinh doanh nói chung sẽ có cách thức tiếp cận và có những chiến lược Marketing phù hợp. Đánh giá khả năng sử dụng thuật toán phân cụm Tuy nhiên, trước khi thực hiện bất kì thuật toán phân cụm nào thì chúng ta cần trả lời câu hỏi: liệu bộ dữ liệu của chúng ta có thể áp dụng được các thuật toán phân cụm hay không? Để làm việc này, chúng ta sử dụng một thao tác gọi là đánh giá xu hướng cụm (Clustering tendency assessment) bằng thống kê  Hopkins  bằng hàm  hopkins()  của gói  clustertend  : # Loại ra biến phân loại    khách hàng  vì thực  tế chúng ta chưa biết thông tin này: \r\ndf1_raw <- seg.raw %>% select(-Segment) \r\n# Chuyển hóa dữ liệu: \r\ndf1_raw <- df1_raw %>% mutate_if(is.factor, as.numeric) \r\n\r\nlibrary(clustertend)\r\nset.seed(29)\r\nhopkins(df1_raw, n = nrow(df1_raw) - 1) ## $H\r\n## [1] 0.2049759 Giá trị của thống kê này thấp hơn ngưỡng 0.5 nên có thể sử dụng các thuật toán phân loại cho bộ dữ liệu trên. Lựa chọn k tối ưu Bước kết tiếp là lựa chọn một k tối ưu cho thuật toán phân cụm. Một giá trị k lớn sẽ gia tăng tính đồng nhất của các quan sát thuộc một cụm cụ thể nhưng sẽ dấn đến khả năng overfiting. Do vậy lựa chọn k phù hợp là một bước quan trọng. K tối ưu sẽ là một giá trị nằm đâu đó trong khoảng từ 2 đến căn bậc hai của n/2 với n là số quan sát trong bộ dữ liệu (Lantz, 2015). Có nhiều thuật toán được sử dụng để tìm k tối ưu nhưng trong bài này chúng ta sử dụng phương pháp Elbow (Elbow method): set.seed(123)\r\n\r\nk.max <- round(sqrt(nrow(df1_raw) / 2))\r\ndata <- sapply(df1_raw, my_scale)\r\n\r\nwss <- sapply(1:k.max, \r\n        function(k){kmeans(data, k, nstart = 10 )$tot.withinss})\r\nplot(1:k.max, wss,\r\n       type=\"b\", pch = 19, frame = FALSE, \r\n       xlab=\"Number of clusters K\",\r\n       ylab=\"Total within-clusters sum of squares\")\r\n\r\n# Căn cứ theo  phương pháp này chúng ta có  thể chọn k là 4 hoặc 5. Vẽ thêm cho vui: \r\nabline(v = 4, lty =2) Thực hiện phân cụm và hình ảnh hóa # Nếu chọn k =  4\r\nset.seed(29)\r\nkm.res4 <- kmeans(data, 4, nstart = 25)\r\n\r\nlibrary(factoextra)\r\nfviz_cluster(km.res4, \r\n             data = data, \r\n             geom = \"point\",\r\n             stand = FALSE, \r\n             frame.type = \"norm\") # Nếu chọn k =  5: \r\n\r\nkm.res5 <- kmeans(data, 5, nstart = 25)\r\nfviz_cluster(km.res5,\r\n             data = data, \r\n             geom = \"point\",\r\n             stand = FALSE, \r\n             frame.type = \"norm\") (CÒN NỮA)"
        },
        {
          "title": "Chuyện tình chàng Cuder",
          "relevance": "1",
          "url": "https://career.fpt-software.com/vi/blog/2017/03/31/chuyen-tinh-chang-cuder/",
          "content": "Liên Hệ | Hỏi Đáp Login & Register Username Password Remember Me Lost your password? Register Forgotten Password Cancel Register For This Site A password will be e-mailed to you. Username E-mail Toggle navigation Giới thiệu Chúng tôi là ai Đội ngũ lãnh đạo Thành tựu mục tiêu Chúng tôi làm gì Chiến lược công nghệ Dịch vụ và lĩnh vực Hiện diện toàn cầu Trụ sở FPT Software FPT Software Hà Nội FPT Software Đà Nẵng FPT Software HCM FPT Software Cần Thơ FPT Japan FPT Asia Pacific FPT Software Malaysia FPT Software Myanmar FPT Software Philippines FPT USA FPT Europe FPT Deutschland FPT Slovakia Tại sao Fsoft Lộ trình công danh toàn cầu Công việc đa dạng và hấp dẫn Cơ hội học hỏi và phát triển Văn hóa đặc sắc Thu nhập và phúc lợi cạnh tranh Fsoft qua những câu chuyện Việc làm Danh Sách Việc Làm Kết quả Entry Test Tại sao bạn nên tham gia Bạn có kinh nghiệm Bạn là sinh viên Bạn mới ra trường Góc sinh viên Sự kiện Tin sinh viên Tin tức và blog Thông tin dự án Nhịp đập FSOFT Xu hướng công nghệ Tư vấn nghề nghiệp Tài nguyên FSOFT Career Website Trang tuyển dụng FPT Software Chuyện tình chàng Cuder 2017-03-31 14:43:00 - hienptt5 Ngày xửa ngày xưa, ở một công ty phần mềm nọ, có một chàng thanh niên làm nghề coder. Công việc của chàng là gia công phần mềm theo yêu cầu của khách hàng. Chàng có kĩ năng rất tốt, các ngôn ngữ lập trình từ bậc thấp như Cobol, Assembly đến bậc cao như C/C++, Java, C# đều rất thuần thục. Quản lý đơn vị và Quản lý dự án  đều yêu quý và tạo điều kiện cho chàng được phát triển chuyên môn của bản thân. Vì giỏi như vậy, nên chàng sinh ra kiêu ngạo, coi thường đồng nghiệp, trên chiếc iPhone  của chàng, có một phần mềm trợ lý ảo tên là White Queen. Mỗi sáng khi thức dậy, chàng đều hỏi trợ lý ảo của mình : “ White Queen ngự ở trên giường Thế gian ai code được dường như ta “ White Queen liền nói: Trình độ code của ngài đã đạt đến mức “ Tay nhanh hơn não”. Chàng Cuder nghe xong mừng lắm, từ đó chàng càng không coi ai ra gì. Rồi một ngày, khách hàng yêu cầu chàng phát triển một phần mềm Face Recogintion. Mặc dù đã sử dụng các loại thuật toán khác nhau như Model-based Face Tracking, Edge-Orientation Matching, Weak classifier cascades…. đông tây y kết hợp, nhưng khi đưa vào thực tế, các thuật toán đều gặp nhược điểm. Chẳng hạn như điều kiện ánh sáng thay đổi, mặt người nghiêng, thay đổi cảm xúc thì thuật toán không hoạt động. Sau nhiều ngày tăng ca,nhiều đêm thức trắng mà không thành công, chàng mệt mỏi rã rời, về nhà kể chuyện cho trợ lý ảo của mình: “Ta buồn lắm, không biết ta có còn là coder giỏi nhất nữa không…” White Queen trả lời: “Xưa kia ngài giỏi nhất trần Ngày nay “máy học” muôn phần giỏi hơn”. Chàng Cuder tò mò: “Máy học là gì, có ăn được không?” White Queen đáp: “Máy học  hay còn gọi là  machine learning  là một nhánh nhỏ của trí tuệ nhân tạo (AI) liên quan đến việc nghiên cứu và xây dựng các kĩ thuật cho phép máy tính “học” tự động từ dữ liệu để giải quyết những vấn đề cụ thể. Máy học rất gần với suy diễn thống kê (statistical inference) thưa ngài” “Các ứng dụng điển hình của Machine Learning có thể kể đến như cỗ máy tìm kiếm Google. Hằng ngày ngài vào Google tìm kiếm và để ý rằng, Google có thể hiểu được ngữ nghĩa của từ khóa mà ngài tìm kiếm nhằm gợi ý và đưa ra các kết quả tìm kiếm tốt hơn hay gợi ý các kết quả tìm kiếm cho ngài”. “Hoặc như Facebook, có thể nhận dạng khuôn mặt ở chức năng tag ảnh, có thể gợi ý bài viết liên quan hoặc gợi ý kết bạn”. Ngài có thể dùng Machine Learning để áp dụng cho bài toán nhận dạng của mình. Tuy nhiên để làm Machine Learning, ngoài phải học về đại số tuyến tính, xác suất thống kê, biết lập trình Python, Mathlab… Ngài hãy đi tìm nàng  Bạch Code , cháu 10 đời của Bạch Tuyết, họ hàng xa với Bạch Cốt Tinh, nàng sẽ dạy cho ngài kiến thức về Machine Learning”. Nàng ta làm ở Google Ở bên nước Mỹ muôn trùng xa xôi. Chàng Cuder mừng lắm, liền khăn gói đáp máy bay giá rẻ sang Silicon Valley để gặp nàng Bạch Code. Nàng Bạch Code hiện giờ đang là kĩ sư của Google DeepMind. Nhóm nghiên cứu về trí tuệ nhân tạo của Google, là tác giả của phần mềm Deep Learning Alpha Go nổi tiếng (Phần mềm đã đánh bại cao thủ cờ vây). Đó là một người con gái “xinh đẹp tuyệt trần”, kính dày như “đít chai”, da trắng như “bạch tạng”, tóc xù như “lông nhím”, nàng đúng là một kĩ sư công nghệ thông tin thực thụ. Nàng Bạch Code đã giảng giải cho chàng Cuder những kiến thức cơ bản nhất về Machine Learning. Chàng hãy nhìn Machine Learning theo góc độ của khoa học thống kê. Hãy xem bài toán nhận dạng mặt như Pattern recognition (nhận diện mẫu hay nhận diện quy luật). Nếu chàng gặp một cô gái xinh xinh nào đó (như Ngọc Trinh chẳng hạn), chàng có thể nhớ những đặc điểm trên khuôn mặt của nàng đó như mặt trái xoan, mũi cao, răng khểnh, mắt bồ câu. Nếu gặp càng nhiều, hoặc xem ảnh càng nhiều thì chàng càng nhớ, và lần sau nếu gặp lại cô gái đấy, chàng sẽ nhận ra được cô ấy là Ngọc Trinh. Machine Learning cũng tương tự như vậy, để nhận dạng một khuôn mặt nào đó (Ngọc Trinh), chàng cần làm những bước sau: Chuẩn bị dữ liệu để cho máy tính “học” (Tập ảnh của Ngọc Trinh). Xây dựng mô hình thông qua dữ liệu đầu vào (Algorithm). Đánh giá mô hình vừa mới xây dựng (Model). Phương pháp học đơn giản nhất là học có giám sát (Supervised Learning). Supervised Learning Supervised Learning (SL) là một kĩ thuật học máy để học tập từ tập dữ liệu được gán nhãn cho trước. Tập dữ liệu cho trước sẽ chứa nhiều bộ dữ liệu. Mỗi bộ dữ liệu có cấu trúc theo cặp {x, y} với x được xem là dữ liệu thô (raw data) và y là nhãn của dữ liệu đó. Nhiệm vụ của SL là dự đoán đầu ra mong muốn dựa vào giá trị đầu vào. Ví dụ chàng đưa tập ảnh dữ liệu vào máy tính và gán Label “Ngọc Trinh” cho từng ảnh. Nhiệm vụ của thuật toán là xây dựng một hàm có thể xuất ra giá trị đầu ra tương ứng với tập dữ liệu ảnh trên. Một số framework về Machine Learning như OpenFace sẽ extract một vector features mô tả đặc trưng của khuôn mặt trong ảnh. Vector features là ma trận là các tham số đặc trưng của mặt kiểu như Da, Mũi, Mồm, Mắt và hàm đầu ra chính là khuôn mặt được nhận dạng. Như vậy, khi đưa một ảnh bất kì vào để nhận dạng, vector features sẽ được extract và so sánh với các đặc điểm đã được học để nhận dạng. Da Mũi Răng Mắt Output Trắng Cao Khểnh Bồ Câu Ngọc Trinh Đen Tẹt Vẩu Lé Cuder Dễ nhận ra, học có GIÁM SÁT tức là máy học dựa vào sự trợ giúp của con người, hay nói cách khác con người dạy cho máy học và giá trị đầu ra mong muốn được định trước bởi con người. Tập dữ liệu huấn luyện hoàn toàn được gán nhãn dựa vào con người. Tập càng nhỏ thì máy tính học càng ít. Cụ thể ở đây là chàng đưa tập ảnh đầu vào máy tính và “nói” cho máy tính biết rằng “Đây là Ngọc Trinh” hãy học đi. Máy tính sẽ xây dựng một model với các tham số đầu vào Da, Mũi, Răng, Mắt để mô tả mối quan hệ giữa Ngọc Trinh (hàm đầu ra) với các tham số đầu vào trên. Unsupervised Learning Unsupervised Learning (UL) là một kĩ thuật của máy học nhằm tìm ra một mô hình hay cấu trúc bị ẩn bởi tập dữ liệu KHÔNG được gán nhãn cho trước. UL khác với SL là không thể xác định trước output từ tập dữ liệu huấn luyện được. Tùy thuộc vào tập huấn luyện kết quả output sẽ khác nhau. Trái ngược với SL, tập dữ liệu huấn luyện của UL không do con người gán nhãn, máy tính sẽ phải tự học hoàn toàn. Có thể nói, học KHÔNG GIÁM SÁT thì giá trị đầu ra sẽ phụ thuộc vào thuật toán UL. Một cách toán học, Unsupervised learning là khi chúng ta chỉ có dữ liệu vào X mà không biết nhãn Y tương ứng. Máy tính sẽ tự tìm ra mối quan hệ trong tập dữ liệu đó. Các bài toán Unsupervised learning được tiếp tục chia nhỏ thành hai loại: Clustering (phân nhóm) Một bài toán phân nhóm toàn bộ dữ liệu X thành các nhóm nhỏ dựa trên sự liên quan giữa các dữ liệu trong mỗi nhóm. Ví dụ : Phân nhóm khách hàng dựa trên hành vi mua hàng.: Khách hàng hay mua túi Hermes, Louis Vuitton thuộc nhóm showbiz. Khách hàng hay mua rau dưa, gà vịt là nhóm bà nội trợ. Khách hàng hay chơi game, lướt facebook là nhóm trẻ trâu. Association Là bài toán khi chúng ta muốn khám phá ra một quy luật dựa trên nhiều dữ liệu cho trước. Ví dụ khách hàng mua iPhone thì thường hay mua thêm phụ kiện. Phụ nữ mua quần áo thì thường sắm thêm son phấnà dựa vào đó tạo ra một hệ thống gợi ý khách hàng (Recommendation System), thúc đẩy nhu cầu mua sắm. Và từ đó, mỗi ngày chàng Cuder lại đến phòng làm việc của nàng Bạch Code để nghe nàng giảng dạy về Machine Learning. Chàng đã được học rất nhiều khái niệm mới như Cost Function, Hierarchical learning, Gradient descent,… Cả một chân trời kiến thức mở ra cho chàng. Thế rồi hai người yêu nhau, chàng Cuder quyết định rời bỏ công ty phần mềm nọ, và hai người sống với nhau hạnh phúc ở Sillicon Valley đến trọn đời. Nguồn : Tạp chí công nghệ Potato – FPT SOFTWARE Comments  comments Tin tức và blog Nhịp đập FSOFT Tài nguyên Thông tin dự án Tư vấn nghề nghiệp Xu hướng công nghệ Giới thiệu Chúng tôi là ai Đội ngũ lãnh đạo Thành tựu mục tiêu Chúng tôi làm gì Chiến lược công nghệ Dịch vụ và lĩnh vực Find A Job .NET/C# ABAP Administrator Android Business Analyst Business Development Executive Business Intelligence C/C++ CAD/CAE Cobol Cobol Comtor CSS Design EMBEDDED Fresher IOS Japanese Java Java Web Java Web JavaScript Linux Marketing Microsoft Dynamics CRM NETWORK/SYSTEM Non-IT Objective-C Other Perl PHP Project Quality Python Recruitment Ruby Sales Security SharePoint Solution Architect Technical Consultant Tester Training Executive Visual Basic Visual Studio Popular Tags .NET AMS android BA BI Cad CAE cobol danang DEV Developer devs fresher hanoi intern IOS Japanese java lập trình viên onsite Onsite Singapore oracle phiendichvien qa SA SAP Senior seniordeveloper signing bonus Tester tiengnhat Trainer translator Xamarin Working in Fsoft Danh Sách Việc Làm Kết quả Entry Test Tại sao bạn nên tham gia Góc sinh viên Trang chủ Liên hệ Tìm kiếm việc làm Hỏi đáp Hotline support:  +84 (4) 3768 9048 (ext 689) Hướng dẫn sử dụng Điều khoản sử dụng Copyright @FPT Software"
        },
        {
          "title": "Lính mới & Deep Learning",
          "relevance": "0",
          "url": "https://blog.chappiebot.com/l%C3%ADnh-m%E1%BB%9Bi-deep-learning-62e6abf81739?gi=bca7a93f1bce",
          "content": "Homepage Follow Sign in Get started Homepage Tran Phuong (Ri) Blocked Unblock Follow Following Hope is Everything Oct 12, 2016 Lính mới & Deep Learning Hành trình tiến đến Deep Learning và câu chuyện tìm ra các model ứng dụng tối ưu. Khởi đầu Thật khó có thể hình dung được cách đây 1 vài năm chúng ta có thể nghĩ rằng AI đang trở thành một lĩnh vực nở rộ. Chúng ta đang ở tiệm cận của quy mô dữ liệu và tự động hoá ở mức cực đại và chắc có lẽ AI là thứ mà rất nhiều các nhà lớn, nhà đầu tư, các công ty và nhóm khởi nghiệp đang tìm cách tham gia. Ngày 9 tháng 1, 2007 Gần 10 năm trước, chiến iPhone đầu tiên được giới thiệu bởi Steve Jobs, cuộc cách mạng di dộng diễn ra ngay sau đó với sự bùng nổ đến chóng mặt. Ít ai để ý rằng Steve Jobs đã không muốn cho bất cứ ai phát triển ứng dụng (apps) cho nó, bởi theo ông thì chỉ những gì ông kiểm soát được trong một thể thống nhất mới tạo nên một thiết bị tuyệt vời. Ông đã chần chừ, nhưng không quá lâu và App Store ra đời vào ngày 7 tháng 10, 2008 (một năm rưỡi sau). Và đến giờ App Store đã có hơn 2 triệu apps với tổng hơn 50 tỷ USD doanh thu đo đếm được từ IAP của Apple. Và một câu hỏi lớn đặt ra là đây có phải là đỉnh của sự phát triển mô hình ứng dụng di động hay chưa? Mặc dù có tới 2 triệu apps nhưng thực sự chỉ có một nhóm rất nhỏ các app phổ biến, và ngày càng nhiều nhà lớn chi phối các app quan trọng, hay bản thân Apple cũng như Google đều nắm những app lớn như Video Call, Messaging, Mail, Calendar, Music, Mobile Cloud Storage… Hố đen mới từ Messaging Chúng ta chứng kiến thương vụ đình đám ông chủ Facebook mua lại WhatsApp với giá 19 tỷ USD và biết Facebook trở thành nhà lớn chi phối quá bán thị trường các ứng dụng tin nhắn di động. Bộ đôi WhatsApp Facebook Messenger đã có 1 tỷ người dùng active mỗi tháng, và có hơn 60 tỷ tin nhắn mỗi ngày. Những con số thật đáng kinh ngạc. Hố đen mới được hình thành, nó hút nhiều nguồn lực xoay quanh câu hỏi  “Messaging sẽ biến đổi ra sao và các mô hình kinh doanh trên đó?” Tại hội nghị F8, 4/2016, Facebook công bố Messenger Platform và trình diễn các ứng dụng Chatbots đầu tiên về ecommerce và publishing. Không phải Facebook là nhà đầu tiên giới thiệu nền tảng cho chatbots, trước đó có Telegram, Slack, Wechat…, thế nhưng sự áp đảo có tính chi phối của môi trường Messenger + WhatsApp đã tạo nên một cú hích lớn và thực sự là một hố đen bắt đầu thu hút anh tài vào đây. Khởi đầu cho Chatbots Ý tưởng Chatbots không mới, những năm 80 đã có nhiều ứng dụng mang tính chất trình diễn, một số đề tài khoa học xoay quanh câu hỏi “Liệu chúng ta có thể tạo ra cỗ máy giao tiếp giống người không?” Rất nhiều paper và các nhà khoa học đã cố gắng đi tìm lời giải. Đến nay, việc tạo ra cỗ máy “có khả năng trừu tượng và ý thức được mình đang tồn tại\" dường như bế tắc. Andrew Ng’s view: “Most of the value of deep learning today is in relatively narrow domains where you can get a lot of data. Here’s one example of something it cannot do: have a meaningful conversation. There are demos, and if you cherry-pick the conversation, it looks like it’s having a meaningful conversation, but if you actually try it yourself, it quickly goes off the rails.” Vậy phải chăng chatbots khó chinh phục người dùng? Họ có thể chat với bạn bè mỗi ngày, mọi lúc mọi nơi, còn chatbots có lẽ quá ngô nghê và máy móc để thuyết phục họ quay lại? Machine Learning (ML) vs Deep Learning (DL) Câu trả lời cho thế hệ Chatbots mới nằm ở ML và DL. Một thế hệ mới có khả năng duy trì và phát triển hội thoại (context & developing conversation). Một thế hệ có khả năng được huấn luyện (train) và tìm ra được những giá trị trong khối dữ liệu lớn và mang đến cho từng người dùng. ML được bắt đầu bằng ý tưởng sử dụng các thuật toán thống kê trong đó xác suất tiền nghiệm tác động đến kết quả hậu nghiệm, ví dụ bạn có thể ứng dụng  Suy luận Bayes  để tính xác suất hậu nghiệm. Machine learning is the subfield of computer science that “gives computers the ability to learn without being explicitly programmed” (Arthur Samuel, 1959). Machine Learning có quan hệ rất gần và thường có bao gồm với tính máy dựa trên thống kê ( computational statistics ), vì vậy đại số tuyến tính ( Linear Algebra ) được vận dụng rất nhiều trong ML. Ứng dụng của ML chủ yếu tập trung nhiều vào việc tiên đoán (prediction) dựa vào việc huấn luyện bởi các tập mẫu (datasets), các tập này được các chuyên gia lựa chọn các đặc trưng (features engineering). Nguồn Sklearn Classifier Comparison  http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html Bạn có thể dùng các thuật toán dạng học có giám sát (supervised) cho các bài toán phân loại dữ liệu. Ưu điểm của supervised là độ chính xác cao nếu có tập train tốt, các nhãn được định sẵn. Trong trường hợp tập train chưa gán nhãn trước bạn có thể dùng học không giám sát (unsupervised). Trong ứng dụng thực tế, nếu bạn có một toà soạn hoặc một trang cập nhật nội dung thường xuyên, bạn có thể ứng supervised để train cho tập mẫu theo chủ đề, từ đó có thể tiên đoán cho các bài viết mới, độ chính xác có thể lên tới 97 đến 98%. Độ chính xác của các giải thuật ML này bị chi phối lớn bởi việc định nghĩa các đặc trưng (features). Ví dụ, để đoán xem tên một người là nam hay nữ, bạn có thể dựa vào tên đệm, tên hay thậm chí là chữ cái đầu và cuối. Trong xử lý văn bản thì bước tiền xử lý như tokenization, tính tf-idf, kbest có tác động lớn đến độ chính xác (accuracy). Như vậy rõ ràng ML cổ điển có một giới hạn lớn đó là phụ thuộc rất nhiều features engineering, điều đó cần rất nhiều kinh nghiệm của các chuyên gia ML. Deep Learning và cuộc chơi của dữ liệu siêu lớn Deep Learning là  một thuật toán dựa trên một số ý tưởng từ não bộ tới việc tiếp thu nhiều tầng biểu đạt, cả cụ thể lẫn trừu tượng, qua đó làm rõ nghĩa của các loại dữ liệu.  Deep Learning  được ứng dụng trong nhận diện hình ảnh, nhận diện giọng nói, xử lý ngôn ngữ tự nhiên. (Phát biểu khá trừu tượng, trên Google khi bạn gõ từ khoá “Deep Learning là gì\"). Để diễn giải, bạn có thể xem cách Andrew Ng biểu đạt bằng đồ thị dưới đây: Nguồn  http://cs229.stanford.edu/materials/CS229-DeepLearning.pdf Có thể hiểu Deep Learning là những phương thức có tính trí tuệ nhân tạo mới nhằm đạt được hiệu suất và độ chính xác cao hơn khi tăng khối lượng dữ liệu. Việc xây dựng các mạng neural như  CNN ,  RNN  hay cả các mạng lai ghép nhằm giải quyết bài toán giảm tối đa sự phụ thuộc vào quá trình chọn features nói ở phần trên. Bạn có thể tưởng tượng bây giờ để nhận diện giọng nói hay chữ viết, bạn không cần phải tiền xử lý nhiều và phải định nghĩa các đặc trưng nhận diện, bạn chỉ cần đưa vào gần như là dữ liệu thô (raw input) vào mạng neural để xử lý. Bản thân của các mạng này vẫn sử dụng các thuật toán thống kê, có thể nói đây là một kiểu học vẹt ở quy mô siêu lớn, càng nhiều dữ liệu, độ chính xác càng cao. Trên phương diện kinh doanh, việc hiểu rõ Machine Learning và Deep Learning có ý nghĩa rất lớn trong việc lựa chọn và mức độ ứng dụng thực tế, Chatbots là một ứng dụng điển hình, không phải cứ nói ứng dụng ML/DL/AI là được, bạn cần tỉnh táo và hiểu đầy đủ trước khi dồn nguồn lực cho những thứ có thể vắt kiệt sức bạn trước khi bạn đến đích. Thách thức của Chatbots Chatbots đang là một xu hướng lớn, được các nhà lớn, giới đầu tư và nhiều startups nhảy vào, rất nhiều anh chị em tinh hoa bước vào chung sức giải. Khoảng thời gian gần 1 năm qua, đã có rất nhiều chatbots ra đời, nhưng tựu chung đều ở mức cơ bản, hiểu nôm na là gõ từ khoá a thì làm việc A, bí quá sẽ tìm kiếm theo cụm từ. Một số thì xài thủ thuật để trong hao hao giống người. Tuy nhiên, không thể phủ nhận rằng bot sau ngon hơn bot trước, từng bước nhiều nhà lớn và tổ chức đầu tư nghiêm túc vào lĩnh vực này. Thách thức của Chatbots gồm: Context & Developing Conversation : Được hiểu là làm sao người dùng giao tiếp với máy mà ko máy móc quá, duy trì các cuộc hội thoại đủ nghĩa và hữu ích, không đơn thuần là mệnh lệnh đơn. Ví dụ khi bạn nói 1 câu ngắn “Tớ muốn tìm khách sạn”, bot cần phát triển hội thoại thêm như “Bạn muốn tìm khách sạn ở đâu, hay phòng đơn hay đôi, có wifi hay ăn sáng miễn phí không…. Hội thoại thì đa dạng, không phải là một kịch bản theo thứ tự 123. Hiện nay NLP, NLU là các công nghệ có thể áp dụng nhưng chưa rõ ràng, đòi hỏi việc nghiên cứu thêm. Training:  Nói đến huấn luyện, có lẽ đây là đặc trưng rất quan trọng của loài người, con người nếu được huấn luyện, được dạy và có khả năng học thì sẽ tiến bộ, làm được nhiều việc phức tạp và đạt kết quả cao hơn. Hiện nay với Chatbots có thể được huấn luyện từ dữ liệu doanh nghiệp, từ lịch sử chat của người dùng, và nhiệm vụ tìm cách nối giữa dữ liệu doanh nghiệp với người dùng sao cho đạt hiệu quả kinh doanh tốt nhất: khách hàng quay lại thường xuyên, bán được nhiều hàng hơn… Performance & Scale Out:  Chatbots đòi hỏi xử lý trên từng truy vấn theo ngữ cảnh người dùng nên việc tối ưu truy vấn đòi hỏi một kiến trúc tốt và có khả năng mở rộng được. Hiện nay, chưa có các mô hình, platform, engine đủ rõ ràng để ứng dụng quy mô, các nhà lớn cũng chưa có câu trả lời. Amazon Skills, IBM Watson, Microsoft Bot Framework, Google Actions with API.AI, Facebook Wit.AI và rất nhiều startups khác đưa ra giải pháp, và ngày một tốt hơn. Nếu bạn phát triển bot lúc này, đồng nghĩa bạn sẽ theo hướng nghiên cứu và thí nghiệm hơn là thương mại hoá. Tuy nhiên nếu bạn chờ thì dường như rất khó cho bạn cạnh tranh khi nhiều đơn vị khác đã đi trước bạn. deeplearningbook.org Những gì tớ nói ở trên cũng để đi đến đây, dự án cùng nhau học Deep Learning :) The Deep Learning textbook is a resource intended to help students and practitioners enter the field of machine learning in general and deep learning in particular. The online version of the book is now complete and will remain available online for free. The deep learning textbook can now be pre-ordered on  Amazon . Pre-orders should ship on December 16, 2016. Nội dung Part I: Applied Math and Machine Learning Basics 2 Linear Algebra 3 Probability and Information Theory 4 Numerical Computation 5 Machine Learning Basics Part II: Modern Practical Deep Networks 6 Deep Feedforward Networks 7 Regularization for Deep Learning 8 Optimization for Training Deep Models 9 Convolutional Networks 10 Sequence Modeling: Recurrent and Recursive Nets 11 Practical Methodology 12 Applications Part III: Deep Learning Research 13 Linear Factor Models 14 Autoencoders 15 Representation Learning 16 Structured Probabilistic Models for Deep Learning 17 Monte Carlo Methods 18 Confronting the Partition Function 19 Approximate Inference 20 Deep Generative Models Đây là một cuốn textbook (sách giáo khoa) mà team tớ lựa chọn để anh em cùng học và chia sẻ. Mục tiêu là hiểu rõ về Deep Learning và ứng dụng nó một cách hiệu quả vào thực tế. Mô hình học: Học viên truy cập vào tài liệu deeplearningbook này trên Google Docs, tại đây người học sẽ để lại các note/comment. Nhóm tớ sẽ follow up, đưa ra các điểm chính, các ý tưởng, source code (lưu trên github) cụ thể từ các giải thuật nhắc đến trong textbook này. Học viên có thể mời các chuyên gia về Deep Learning vào thảo luận hoặc trả lời các thắc mắc, nhóm tớ sẽ tích cực mời các anh chị về Deep Learning tham gia. Đây là dự án phi lợi nhuận và tất cả mọi người đều có thể tham gia dựa vào các quy tắc ứng xử do nhóm đặt ra. Khai giảng khoá học vào 23pm 20/10/2016, bây giờ bạn có thể đăng ký tham gia bằng cách request quyền truy cập  tại đây. ❤ Machine Learning Deep Learning AI Deeplearingbook Course A single golf clap? Or a long standing ovation? By clapping more or less, you can signal to us which stories really stand out. 40 16 Blocked Unblock Follow Following Tran Phuong (Ri) Hope is Everything Follow Chappiebot Viết về Machine Learning, Deep Learning và một chút hương vị AI 40 Never miss a story from  Chappiebot , when you sign up for Medium.  Learn more Never miss a story from  Chappiebot Get updates Get updates"
        },
        {
          "title": "Login",
          "relevance": "1",
          "url": "https://ducnt.org/2017/06/23/bai-2-phan-nhom-thuat-toan-machine-learning/",
          "content": " DUCNT   Top Menu  TRANG CHỦ Bảo mật & Tấn công Phân tích mã độc HD Lập trình Lập trình C/C++ Cơ bản Nâng cao Lập trình C# Nâng cao Lập trình JAVA Machine Learning DUCNT trên Tạp chí Phần mềm DUCNT Dịch vụ Lập trình Website theo yêu cầu Lập trình Ứng dụng cho di động theo yêu cầu Lập trình Phần mềm/Công cụ theo yêu cầu Tư vấn, dạy kèm CNTT theo yêu cầu Lost your password? TRANG CHỦ Bảo mật & Tấn công Phân tích mã độc HD Lập trình Lập trình C/C++ Cơ bản Nâng cao Lập trình C# Nâng cao .NET Core căn bản ASP.NET Core căn bản Lập trình JAVA Machine Learning DUCNT trên Tạp chí Phần mềm DUCNT Dịch vụ Lập trình Website theo yêu cầu Lập trình Ứng dụng cho di động theo yêu cầu Lập trình Phần mềm/Công cụ theo yêu cầu Tư vấn, dạy kèm CNTT theo yêu cầu  DUCNT  Hướng dẫn hack Facebook và cách phòng tránh Bài 4: Unsupervised Learning DUCNT trên Tạp chí EVN-ICT Số 3 – Tháng 5/2017 – Những điều cần biết về công nghệ ảo hóa và công nghệ ảo hóa Hyper-V của Microsoft – Phần 2 Bài 3: Supervised Learning Bài 2: Phân nhóm thuật toán Machine Learning Machine Learning Home › Machine Learning › Bài 2: Phân nhóm thuật toán Machine Learning  Bài 2: Phân nhóm thuật toán Machine Learning Theo  DucNT.org 23 Tháng Sáu, 2017 138 0 Chia sẻ: Bài viết trước Bài 1: Giới thiệu Machine Learning, AI ... Bài tiếp theo Bài 3: Supervised Learning 0 Chia sẻ 0 + 0 Related articles More from author Machine Learning Bài 4: Unsupervised Learning 28 Tháng Sáu, 2017 Theo  DucNT.org Machine Learning Series Hướng dẫn Machine Learning cơ bản 17 Tháng Năm, 2017 Theo  DucNT.org Machine Learning Bài 3: Supervised Learning 26 Tháng Sáu, 2017 Theo  DucNT.org Machine Learning Bài 1: Giới thiệu Machine Learning, AI và Big Data 23 Tháng Sáu, 2017 Theo  DucNT.org Để lại phản hồi  Hủy BÀI HAY NÊN ĐỌC C Cơ bản Lập trình Lập trình C/C++ Lập trình C – Bài 4: Cấu trúc rẽ nhánh và Bài tập minh họa – Phần 1 .NET Core C# Nâng cao Lập trình Lập trình C# Bài 2: Thiết lập môi trường phát triển .Net Core trên Ubuntu 17.04 – Series Hướng dẫn lập trình ứng dụng bằng .NET Core 1.1 căn bản .NET Core C# Nâng cao Lập trình Lập trình C# Series Hướng dẫn lập trình ứng dụng bằng .NET Core 1.1 căn bản C Cơ bản Lập trình Lập trình C/C++ Lập trình C – Bài 2: Viết chương trình đầu tiên .NET Core C# Nâng cao Lập trình Lập trình C# .Net Core 1.1 – Kỷ nguyên mới cho .NET – Bài 1: Giới thiệu về .NET Core Theo dõi chúng tôi 2277 Likes Tôi là ai? Nguyễn Thành Đức /Senior .NET Developer Thành phố Huế, Việt Nam Mail:   mail@ducnt.org Website:   https://ducnt.org - Tốt nghiệp chuyên ngành  Công nghệ phần mềm  tại  Đại học Khoa Học Huế  - Tốt nghiệp khóa  Let's Arena  tại  Hue-Arena Multimedia Center  - Tốt nghiệp khóa  Comptia Security+  tại  Robusta Hà Nội  - Tốt nghiệp khóa  Certified Ethical Hacker  tại  Học viện CNTT Bách Khoa Hà Nội Đam mê lập trình, bảo mật, viết lách và khám phá những cái mới! Series nổi bật Hướng dẫn Lập trình ứng dụng bằng .NET Core Hướng dẫn Lập trình Website với ASP.NET Core Hướng dẫn Machine Learning cơ bản Quảng cáo Gần đây Phổ biến Các đánh giá Hướng dẫn hack Facebook và cách phòng tránh Theo  DucNT.org 25 Tháng Bảy, 2017 Bài 4: Unsupervised Learning Theo  DucNT.org 28 Tháng Sáu, 2017 DUCNT trên Tạp chí EVN-ICT Số 3 – Tháng 5/2017 – Những điều cần biết ... Theo  DucNT.org 27 Tháng Sáu, 2017 Bài 3: Supervised Learning Theo  DucNT.org 26 Tháng Sáu, 2017 Bài 2: Phân nhóm thuật toán Machine Learning Theo  DucNT.org 23 Tháng Sáu, 2017 Hack camera trên điện thoại Android và cách phòng chống – Phần 1: Cách tạo ... Theo  DucNT.org 12 Tháng Tư, 2017 Cách tạo, sử dụng Backdoor và cách phòng chống Theo  DucNT.org 4 Tháng Tư, 2017 Bẻ khóa mật khẩu Windows – Cách 1: Kali Linux và chntpw Theo  DucNT.org 7 Tháng Tư, 2017 Cách phòng chống Backdoor Theo  DucNT.org 4 Tháng Tư, 2017 Lập trình C – Bài 2: Viết chương trình đầu tiên Theo  DucNT.org 16 Tháng Tư, 2017  DucNT.org  on  26 Tháng Bảy, 2017  Hướng dẫn hack Facebook và cách phòng tránh em đăng nhập vào ...  hoàng văn thực  on  25 Tháng Bảy, 2017  Hướng dẫn hack Facebook và cách phòng tránh ad cho mình hỏi ...  DucNT.org  on  18 Tháng Sáu, 2017  Top 7 công cụ thường dùng để hack Wifi trong Kali Linux aircrack không thành công ...  Trần Văn Minh  on  18 Tháng Sáu, 2017  Top 7 công cụ thường dùng để hack Wifi trong Kali Linux đã thử aircrack và ...  DucNT.org  on  11 Tháng Năm, 2017  Hack camera trên điện thoại Android và cách phòng chống – Phần 1: Cách tạo Backdoor đúng thế Bản quyền thuộc về  DucNT.org  - Thành lập: 04/2017"
        }
      ]
    },
    {
      "description": "Giới thiệu khái niệm học máy có giám sát, cách thức hoạt động, nền tảng kiến thức, ứng dụng thực tiễn",
      "query": "hoc co giam sat trong hoc may la gi",
      "sites": [
        {
          "title": "Thuật ngữ Machine Learning là gì, tại sao các công ty lớn đều đổ hàng tỷ USD vào đây?",
          "relevance": "0",
          "url": "http://genk.vn/thuat-ngu-machine-learning-la-gi-tai-sao-cac-cong-ty-lon-deu-do-hang-ty-usd-vao-day-20160629105623677.chn",
          "content": "Gamek Kenh14 Cafebiz Mobile Điện thoại Máy tính bảng INTERNET Digital Marketing Media KHÁM PHÁ Lịch sử Tri thức TRÀ ĐÁ CÔNG NGHỆ Tản mạn Ý tưởng sáng tạo TIN ICT THỦ THUẬT Sống APPS - GAMES ĐỒ CHƠI SỐ VIDEO Mobile Tin ICT Internet Khám phá Video Trang chủ › Tri thức \r\nThuật ngữ Machine Learning là gì, tại sao các công ty lớn đều đổ hàng tỷ USD vào đây?\r\n NPQM  ,  Theo  Trí Thức Trẻ Bình luận  0 Chia sẻ  NASA nghiên cứu ứng dụng có thể dự đoán tuổi thọ của máy móc 10 lần chạm trán kinh điển giữa loài người và máy móc: ngoài cờ vây còn có cả StarCraft Có bao giờ bạn đi tìm lời giải đáp cho câu hỏi liệu khả năng thật sự của máy móc có thể đạt tới đâu trong thời đại hiện nay hay không? Một trong những vai trò của công nghệ hiện nay liên quan đến việc phát triển cũng như hoàn thiện những dịch vụ trên nền tảng smartphone và web đó là khả năng học hỏi, nhận thức của hệ thống máy móc (Machine Learning). Đôi khi, khái niệm trên bị hiểu và sử dụng như một cách gọi khác của “trí tuệ nhân tạo” (AI), đặc biệt là khi những tên tuổi lớn trong giới công nghệ muốn quảng bá hình ảnh về phát minh tiên tiến mới nhất của họ. Tuy nhiên, hai phạm trù trên, dù có một mối liên hệ nhất định, nhưng lại thuộc về những khía cạnh hoàn toàn khác biết trong lĩnh vực công nghệ máy tính. Mục đích của những dự án AI là tạo ra một cỗ máy có khả năng mô phỏng lại bộ não của con người, và tất nhiên, để đạt được điều đó, các nhà khoa học phải tìm được ra cách gán cho chúng kỹ năng nhận thức và tiếp thu vốn có của não bộ. Dù vậy, giới hạn của trí thông minh nhân tạo không chỉ dừng lại ở đó, mà còn bao gồm cả lưu trữ và biểu hiện kiến thức, lập luận, hay thậm chí cả tư duy trừu tượng. Mặt khác, Machine Learning chỉ tập trung vào những phần mềm, ứng dụng được viết ra để giúp máy móc “học tập” từ những sự kiện, trải nghiệm và phản hồi nhận được trước đó. Một điều nữa có thể khiến bạn cảm thấy bất ngờ và ngạc nhiên là Machine Learning thực ra lại được áp dụng nhiều hơn trong những phân tích số liệu và thông tin thống kê so với AI. Tại sao lại như vậy? Chẳng phải AI đi kèm với rất nhiều ưu điểm và tiềm năng vượt trội hơn để có thể hỗ trợ con người sao? Để giải đáp cho những thắc mắc, câu hỏi trên, hãy cùng nhìn sâu hơn nữa vào bản chất thật sự của vấn đề này. Một trong số những định nghĩa được lấy làm tiêu chuẩn mẫu mực nhất giải thích cho Machine Learning đã được phát biểu bởi Giáo sư Tom Mitchell tại Đại học Carnegie Mellon (CMU), cho rằng: Một chương trình máy tính được coi là có khả năng học tập từ những “kinh nghiệm” (E) khi thực hiện một số tác vụ (Task) và đối chiếu với thước đo hiệu suất (Performance), KHI hiệu quả công việc tại tác vụ T, được đo bằng thang P đã cải thiện bằng kinh nghiệm E. Nghe có vẻ khá phức tạp và rắc rối phải không? Do đó, giải thích một cách đơn giản, dễ hiểu hơn: Nếu một hệ thống máy tính có thể cải thiện hiệu suất làm việc chỉ bằng cách tận dụng những dữ liệu thu thập trước đó, bạn có thể kết luận rằng nó thực sự đã biết “học hỏi” một cách đúng đắn. Điều này mang những đặc điểm riêng biệt so với một chương trình cũng được lập ra để hỗ trợ hoàn thành những tác vụ tương tự, nhưng đó là vì lập trình viên đã xác định, vạch rõ hoàn toàn từng giới hạn, phạm vi và đường đi nước bước cũng như dữ liệu nó cần để thỏa mãn yêu cầu đặt ra. Chẳng hạn, một ứng dụng máy tính có khả năng chơi cờ caro là do đã được tích hợp những mã code có sẵn liên quan đến việc tìm cách để chiến thắng đối thủ. Nhưng nếu một chương trình khác đơn thuần chỉ được cung cấp và lập trình dữ liệu về luật chơi và cách giành chiến thắng mà không hề có một chiến thuật, nước đi nào được cài sẵn, đó là khi máy móc cần phải tiếp thu và học hỏi qua việc liên tục chơi đi chơi lại ván cờ, tự rút ra những cách thức phù hợp của riêng nó để có thể đạt được mục đích thắng cuộc. Khái niệm này không chỉ được áp dụng trong những trò chơi điện tử, mà còn dính dáng đến cả những phần mềm nắm giữ chức năng phân loại và dự đoán.  Phân loại  là quy trình mà qua đó máy móc có thể nhận biết và phân chia một tập hợp dữ liệu, bao gồm cả những thông tin về hình ảnh và số liệu.  Dự đoán  là phạm vi mà một hệ thống phụ thuộc vào để đưa ra những nhận định về giá trị của một thứ dựa trên những số liệu thống kê thu thập được trước đó. Chẳng hạn như việc nêu lên những đặc điểm lý tưởng mà một ngôi nhà cần có, từ đó đưa ra tiên đoán về số tiền cần bỏ ra để sở hữu ngôi nhà dựa vào những thông tin giao dịch liên quan. Nhìn chung, toàn bộ những dẫn chứng minh họa và cách giải thích như trên đã dẫn đến một định nghĩa khác cho Machine Learning, đó là sự đúc kết kiến thức và kinh nghiệm từ dữ liệu. Tương tự như việc bạn đương đầu với một bài toán hóc búa mà câu trả lời cho nó chắc chắn sẽ nằm trong những giả thiết ban đầu được đưa ra - Đó cũng chính là lý do tại sao công nghệ này lại có một mối liên hệ mật thiết với khía cạnh khai thác thông tin và số liệu thống kê. Các loại hình Machine Learning Ứng dụng công nghệ biết học tập và tiếp thu có thể được chia ra thành ba phạm trù chính: giám sát, không giám sát và củng cố. Loại đầu tiên -  giám sát  - thuộc về những trường hợp mà máy móc sử dụng, truy cập những thông tin có nguồn gốc uy tín, cụ thể. Điều này có nghĩa dữ liệu trên vốn đã được đánh dấu, đính kèm với một kết quả đúng và hợp lý. Chẳng hạn: Đây là một bức ảnh về chữ cái A. Kia là lá cờ của nước Anh, có 3 màu đỏ, xanh trắng .v.v… Càng nhiều dữ liệu, kiến thức và tốc độ máy tính học hỏi càng nhanh. Sau một thời gian trau dồi và “luyện tập”, khi bắt gặp phải một vấn đề chưa từng được thông qua trước đó, máy tính sẽ sử dụng đến những thuật toán đúc rút từ quá trình trước đó để tìm ra câu trả lời chính xác. Kiểu hình học hỏi  không giám sát  liên quan đến những dữ liệu có nguồn gốc không rõ ràng, xác thực so với loại đầu tiên. Thuật toán học hỏi sẽ không thu nhận được bản chất thật sự của thông tin: Đây là một chữ cái, nhưng chấm hết, không có thêm một đầu mối nào cả. Kia là hình ảnh của một lá cờ, nhưng lại không được cung cấp tên gợi ý. Nhìn tổng thể, học hỏi không giám sát cũng giống như nghe một buổi phát sóng radio của nước ngoài mà bạn không hiểu người ta đang nói ngôn ngữ gì. Nực cười là bạn cũng không có bất cứ một quyển từ điển nào trong tay hay một người trợ giúp bên cạnh để giúp bạn hiểu được phần nào những gì mình đang nghe. Ban đầu, nghe một bản radio tất nhiên là không mang lại tác dụng gì cả, thậm chí gần như là vô ích vì không có giá trị gì được ghi lại. Thế nhưng, sau hàng trăm lần như vậy, bộ não của bạn sẽ bắt đầu nhận ra những tín hiệu ban đầu giúp bạn “bắt bài” được một số từ vựng, ngữ pháp nhất định, từ đó dần dần phát triển kỹ năng ngôn ngữ cũng như đọc hiểu. Trong quá trình đó, nếu may mắn nhận được một quyển từ điển hay thuê một gia sư, chắc chắn mức độ tiến triển của bạn sẽ còn tiến xa hơn rất nhiều. Điểm mấu chốt gắn liền với loại hình không giám sát này là một khi kết hợp với các dữ liệu xác thực, chúng sẽ bất ngờ khiến cho toàn bộ những thuật toán trở nên hiệu quả và hoàn thiện. Chẳng hạn như hiện tại đang có hàng ngàn bức ảnh về chữ cái được thu thập, chỉ cần chữ A được xác nhận trong bộ nhớ, nó sẽ tác động đến toàn bộ thông tin còn lại, góp phần phân loại những thông tin được xử lý trước đó. Ưu điểm lớn nhất ở đây là quy trình này chỉ cần đến một tập hợp rất nhỏ những dữ liệu giám sát, dù chúng lại khó thu thập và sáng tạo ra hơn so với nhóm không giám sát. Tóm lại, điều đó cũng giúp biểu hiện tỷ lệ tương quan khá chênh lệch giữa hai loại hình học hỏi đầu tiên. Loại cuối cùng -  học hỏi củng cố  - có phần giống với kiểu thứ hai vì dữ liệu hướng đến cũng không có tính chất xác thực, tuy nhiên, khi xử lý và tìm kiếm lời giải đáp cho một vấn đề, kết quả cho ra sau đó sẽ được đánh giá và xếp hạng. Ví dụ tiêu biểu giải thích cho trường hợp này có thể được gán cho những trò chơi quen thuộc. Nếu máy tính thắng cuộc, kết quả chơi sẽ “chảy” ngược lại dòng xoay chuyển của dữ liệu, trở về với bộ nhớ để học tập, củng cố thêm những nước cờ khôn ngoan, chắc chắn hơn nữa, chuẩn bị cơ sở sẵn sàng cho những lần chơi sau. Khẳng định lại một lần nữa, nỗ lực này sẽ không thực sự tỏ ra hiệu quả nếu quá trình chơi chỉ kéo dài trong vài ván đấu ban đầu, nhưng sẽ là cả một sự tiến bộ đến bất ngờ nếu kết quả của hàng trăm lần chơi được tích lũy và đúc kết dần dần trở thành một “tuyển tập” những chiến thuật chơi đa dạng, phong phú. Cơ chế hoạt động Các kỹ sư và lập trình viên phối hợp với nhau dùng rất nhiều phương pháp để thiết kế nên một hệ thống máy móc có khả năng học hỏi hiệu quả. Như đã đề cập phía trên, hầu hết trong số đó gắn liền mật thiết với khía cạnh khai thác thông tin và số liệu. Chẳng hạn, nếu một tập hợp dữ liệu biểu hiện đặc điểm của nhiều đồng tiền xu, bao gồm trọng lượng và đường kính của chúng, bạn có thể sử dụng những kỹ thuật thống kê như thuật toán “láng giềng gần nhất” (nearest neighbors) để lọc ra thông tin của những đồng xu chưa được xử lý. Về cơ bản, thuật toán trên xem xét và phân tích những cách phân loại được áp dụng dựa vào khoảng cách gần nhất giữa các đối tượng cần xếp lớp. Số lượng “láng giềng” liên quan tới quyết định thực hiện thuật toán được biểu diễn bằng “k”, từ đó tên đầy đủ của phương pháp này là “k-nearest neighbors”. Dù vậy, vẫn còn đó rất nhiều công thức, thuật toán khác cũng đóng vai trò và mục đích tương tự, nhưng cách thức khác nhau. Cùng xem qua biểu đồ dưới đây để hiểu rõ hơn: Bức ảnh trên cùng bên trái là tập hợp dữ liệu ban đầu. Tập hợp đó được phân ra làm hai loại chính, đỏ và xanh. Mặc dù dữ liệu trên chỉ mang tính chất lý thuyết đặt ra, nhưng nó có thể đại diện cho hầu hết mọi thứ: trọng lượng và kích cỡ đồng xu, số cánh hoa của một bông hoa hoặc độ rộng… Bên cạnh đó, cũng có thể thấy dễ dàng thấy được một vài cụm nhóm nhất định, như mọi thứ ở phía trên bên trái đều thuộc nhóm màu đỏ, còn phía dưới bên phải thì ngược lại - màu xanh. Tuy nhiên, ở vị trí giữa lại nổi lên một phần giao thoa. Vậy nếu bạn thu được một mẫu thông tin “mới toanh” thuộc về phần này, làm sao có thể biết được chính xác nó thuộc về màu đỏ hay là xanh? Điều này có thể được giải thích bằng những thuật toán khác được hiển thị ở những biểu đồ còn lại, hoàn thành trọn vẹn vai trò phân loại các dữ liệu mới xuất hiện. Ngoài ra, nếu chẳng may một mẫu nào đó rơi vào vùng có màu trắng, thì xin chia buồn với bạn, phương pháp hiện tại không đủ khả thi để phân loại hết dữ liệu. Con số ở góc dưới bên phải diễn tả xác suất phân loại thành công của thuật toán đó. Mạng lưới thần kinh nhân tạo Một trong những từ ngữ thường thấy ở các công ty như Google và Facebook là “Mạng lưới thần kinh” (Neural Net). Đây là một hệ thống máy tính áp dụng Machine Learning, được mô phỏng theo cách vận hành và hoạt động của các neuron thần kinh trong bộ não con người. Xét về khía cạnh sinh học, các neuron sẽ truyền đi tín hiệu về bộ máy xử lý trung tâm dựa trên cách mà nó xử lý thông tin tiếp nhận ban đầu. Còn về phạm trù máy móc, tác vụ trên được thực hiện phụ thuộc vào những cấp số ma trận đi kèm với hàm số chuyển đổi. Ứng dụng của mạng lưới này trong những năm gần đây đã tăng lên với tốc độ chóng mặt, cụ thể là xu hướng sử dụng những hệ thống chuyên sâu tích hợp nhiều lớp neuron liên kết. Trong sự kiện Google I/O 2015, Phó Chủ tịch Phát triển Sản phẩm Sundar Pichai đã giải thích cơ chế học hỏi của máy móc, đồng thời giới thiệu về mạng lưới chuyên sâu đang làm nhiệm vụ hỗ trợ Google hoàn thành sứ mệnh “tổ chức, điều hành và phổ biến thông tin trên toàn thế giới”. Từ đó, ông lớn công nghệ đã cho ra mắt hàng loạt những phát kiến đáp ứng kỳ vọng của những tín đồ công nghệ như Google Now. Hơn nữa, nhờ có DNN - nền tảng mã nguồn mở cho phép phát triển các phần mềm cổng thông tin điện tử - Google như có thêm cánh tay phải đắc lực trong những dự án xây dựng hệ thống nhận diện giọng nói, tiếp nhận và xử lý ngôn ngữ cũng như phiên dịch. Hiện nay số lượng mạng lưới kết nối trong tay Google đã lên đến 30, một con số vô cùng ấn tượng. Tỉ lệ sai sót và nhầm lẫn trong những tính năng nhận diện giọng nói của công ty cũng giảm đi đáng kể: từ 23% trong năm 2013 xuống còn 8% trong năm 2015. Một vài ví dụ tiêu biểu Đúng vậy, việc các hãng công nghệ hàng đầu như Google và Facebook sử dụng Machine Learning để cải thiện chất lượng dịch vụ đã không còn là điều quá xa lạ. Vậy làm như thế có tác dụng gì, hay giúp họ có được thành quả ra sao? Một trong những lĩnh vực khá thú vị mà hệ thống này đảm nhận là công việc chú thích cho ảnh, hay nói đúng hơn là diễn tả lại nội dung được miêu tả và truyền tải bên trong đó. Dưới đây là một vài minh chứng cụ thể, giúp bạn có một hình dung rõ nét hơn về chức năng này: Hai bức ảnh đâu tiên khá chân thực và dễ nhìn, với chú thích chính xác và không có gì đáng phàn nàn. Nhưng hình ảnh cuối cùng thì lại khá khó hiểu khi máy tính, dù không khó khăn khi nhận ra được hộp bánh donut, nhưng lại nhầm lẫn cụm bánh ngọt kia là một cốc cafe. Không chỉ dừng ở đó, thuật toán nhận biết thậm chí còn nhầm lẫn có phần… quá đà một chút: Một công việc nữa cũng thú vị không kém đó là hệ thống này học cách viết chữ như một cá thể con người thực sự. Cleveland Amory, nhà văn, nhà báo và bình luận viên người Mỹ, từng viết: “In my day the schools taught two things, love of country and penmanship — now they don’t teach either” (tạm dịch: “Trong quá khứ, trường học dạy tôi hai điều: tình yêu nước và nghệ thuật thư pháp. Không còn gì trong số đó tồn tại đến ngày nay”). Không biết ông ấy sẽ nghĩ gì khi thấy áng văn trên của mình giờ có thể được viết lại bởi… một cỗ máy như dưới đây: Đây là sản phẩm của máy móc, tạo ra bởi một mạng lưới thần kinh nhân tạo. Để thực hiện được điều này, cần tới 221 người tham gia vào quá trình sử dụng một tấm bảng thông minh để viết câu văn trên. Trong thời gian đó, dữ liệu vị trí và cách di chuyển của ngòi bút được ghi lại dựa trên cảm biến hồng ngoại, từ đó cho ra kết quả là một tập hợp những tọa độ x và y sẽ được dùng để cung cấp thông tin cho công đoạn 'học hỏi giám sát'. Qua đó, thành quả thu được thật sự không tồi đối với một chiếc máy tính. Thậm chí, nhiều phong cách viết khác nhau cũng có thể được biểu hiện, và cả những nét nguệch ngoạc có phần khá tự nhiên nữa. Mới đây, Google đã đưa ra công bố chính thức về việc áp dụng những hệ thống mạng thần kinh nhân tạo vào lĩnh vực mô phỏng, bắt chước các hình thức giao tiếp giữa con người. Cụ thể, các nhà nghiên cứu và lập trình đã “huấn luyện” cho máy móc của họ biết cách tận dụng 62 triệu mẫu câu khai thác từ phần phụ đề của các bộ phim. Kết quả thu được thật kinh ngạc! Trong một lần thử nghiệm, máy tính đã tự khẳng định mình  “không cảm thấy xấu hổ gì khi phải làm một nhà triết học” . Trong khi đó, khi đề cập đến khía cạnh luân thường đạo lý trong cuộc sống, hệ thống lại nhắc đến cách mà nó  “không thích tham gia vào những cuộc thảo luận mang tính triết học” . Có vẻ như nếu lỡ tay nhồi nhét vào “não” những kịch bản và khuôn mẫu phim của Hollywood, máy móc sẽ có xu hướng trở thành một triết gia tính khí thất thường như vậy đó! Kết luận Khác hẳn so với những lĩnh vực liên quan đến trí tuệ nhân tạo, Machine Learning không phải là một thứ gì đó trừu tượng, mơ hồ, mà thực sự là một công cụ hữu hình đóng vai trò quan trọng trong những dịch vụ mà con người sử dụng hằng ngày. Nói một cách khác, nó như một người hùng thầm lặng, một ngôi sao không được công nhận tài năng thực sự của mình, nỗ lực cố gắng phía sau bức màn sân khấu, kiểm tra và rà soát lại tất cả những công đoạn, thông tin để giúp mang lại kết quả tốt nhất cho màn diễn. Và đúng như những lời tâm sự sâu sắc của tác giả Douglas Adam trong Hitchhiker’s Guide to the Galaxy:  Đôi khi chúng ta cần phải hiểu bản chất thực sự của vấn đề trước khi có thể tiếp tục bước đi trên con đường tìm kiếm lời giải đáp thích đáng nhất! Tham khảo: AndroidAuthority Các nhà nghiên cứu tạo ra bàn tay robot linh hoạt như tay người, có thể học tập để tiến bộ như trẻ em mẫu giáo Tags: lập trình viên Trò chơi điện tử trí thông minh nhân tạo trí tuệ nhân tạo lĩnh vực công nghệ hiệu quả công việc Xem theo ngày Ngày 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 Tháng Tháng 1 Tháng 2 Tháng 3 Tháng 4 Tháng 5 Tháng 6 Tháng 7 Tháng 8 Tháng 9 Tháng 10 Tháng 11 Tháng 12 Năm 2017 2016 2015 2014 2013 Xem Scotland khánh thành trang trại năng lượng gió trên biển đầu tiên trên thế giới Dự án này của Scotland có thể cung cấp năng lượng cho khoảng 20.000 hộ gia đình. Tổ chức chống doping thế giới đề xuất cấm chỉnh sửa gen, bắt đầu vào năm 2018 Cơ quan Phòng chống doping Thế giới (WADA) tin rằng việc chỉnh sửa gen là một hành vi gian lận trong thi đấu thể thao khi nó có thể cải thiện thành tích của vận động viên. Phát minh ra loại vải tự làm sạch có thể dùng may đồ cho phi... 3 tuần trước Satya Nadella và những lần phá vỡ \"luật bất thành văn\" trên con... 3 tuần trước 13 sự thật ít người biết về cà phê 4 tuần trước Dự án mô phỏng môi trường sống trên sao Hỏa sẽ được khởi công... 4 tuần trước TIN NỔI BẬT Thiết bị phần cứng của Google chỉ là \"Con ngựa thành Troy\" Đây là nhà sản xuất smartphone lớn thứ hai thế giới sau Samsung mà có thể bạn chưa từng biết tới Không cần phải tốn thời gian cài đặt lại, Windows 10 Fall Creators sẽ cung cấp cho bạn một giải pháp hay hơn rất nhiều Đè bẹp cả Samsung lẫn Google, Apple đang thống trị \"bãi tha ma\" smartwatch/wearable như thế nào? Hãy biến thanh điều hướng ảo nhàm chán của Android trở nên sinh động hơn với hiệu ứng ảnh động vui mắt mà không cần root Video Mobile Tin ICT Internet Khám phá Trà đá công nghệ Thủ thuật Apps - Game Đồ chơi số \r\nChịu trách nhiệm quản lý nội dung: Bà Nguyễn Bích Minh \r\nHà Nội: Tầng 20, Tòa nhà Center Building - Hapulico Complex, Số 1 Nguyễn Huy Tưởng, Thanh Xuân, Hà Nội. \r\nEmail:  info@genk.vn \r\nĐiện thoại: 024.73095555, máy lẻ 62374 \r\nVPĐD tại TP.HCM: Tầng 4, Tòa nhà 123\r\n \r\nVõ Văn Tần, Phường 6, Quận 3, Tp. Hồ Chí Minh\r\n © Copyright 2010 - 2017 - Công ty Cổ phần VCCorp \r\nTầng 17, 19, 20, 21 Toà nhà Center Building - Hapulico Complex, Số 1 Nguyễn Huy Tưởng, Thanh Xuân, Hà Nội. \r\nTrang tin điện tử trên internet: Giấy phép số 460/GP-TTĐT do Sở Thông tin và Truyền thông Hà Nội cấp ngày 03/02/2016 Liên hệ quảng cáo \r\nHotline hỗ trợ quảng cáo: 0942 86 11 33  \r\nEmail:  giaitrixahoi@admicro.vn \r\nHỗ trợ & CSKH: Admicro\r\n  \r\n \r\nAddress: Tầng 20, Tòa nhà Center Building - Hapulico Complex, Số 1 Nguyễn Huy Tưởng, Thanh Xuân, Hà Nội."
        },
        {
          "title": "Tôi đã học Machine Learning như thế nào? Phần 1: Làm quen",
          "relevance": "0",
          "url": "http://vnoi.info/forum/6/5096/",
          "content": "Toggle navigation VNOI Trang chủ Đề bài Kỳ thi Diễn đàn Thư viện Đăng nhập  Tài khoản Mật khẩu Đăng nhập Chưa có tài khoản?  Đăng ký khanhptnk \n                khanhptnk\n               \n              2 năm, 2 tháng trước\n             Xin chào, mình là khanhptnk. Mình là một người đang học và nghiên cứu về Machine Learning. Lĩnh vực mình chuyên sâu cho đến bây giờ là Natural Language Processing (gọi tắt là NLP), tức là làm cho máy tính có khả năng hiểu được ngôn ngữ của con người. Mình không phải là một chuyên gia, kinh nghiệm còn khá non nếu so với các giáo sư đầu ngành nhưng mình rất muốn đem bộ môn này về giới thiệu với các bạn, nhất là các bạn còn đang loay hoay tìm câu trả lời cho câu hỏi: học Tin học sau này làm được gì? Đọc bài viết ở blog của khanhptnk Đăng nhập để trả lời \n            \n            \n               | \n               ▲ \n                      8\n                     ▼ d_t_nguyen \n                d_t_nguyen\n               \n              2 năm, 2 tháng trước\n             Nếu các bạn muốn một trang web dạng competition như voj cho machine learning (data science, big data, etc) thì các bạn có thể vào kaggle.com để thử sức. Rất nhiều cuộc thi có giải thưởng cũng như rất nhiều tin tuyển dụng liên quan ở đây. Đăng nhập để trả lời ▲ \n                      3\n                     ▼ ntthanhpy \n                ntthanhpy\n               \n              2 năm, 2 tháng trước\n             Bài viết bạn rất hay, mình cũng đang tập tọe học ML, và mình cũng muốn bổ sung thêm một vài  thuật ngữ hay sử dụng (tiếng Việt) phù hợp với các bạn sinh viên ở Việt Nam (mình thấy nhiều sách và nhiều thầy hay dùng:) ), đó là: Supervised learning - Học có giám sát. Unsupervised learning - Học không giám sát. Reinforcement learning - Học tăng cường.   Đăng nhập để trả lời ▲ \n                      0\n                     ▼ tungamg \n                tungamg\n               \n              2 năm, 2 tháng trước\n             \n              Trả lời\n               ntthanhpy   Hiện bài gốc Bài viết bạn rất hay, mình cũng đang tập tọe học ML, và mình cũng muốn bổ sung thêm một vài  thuật ngữ hay sử dụng (tiếng Việt) phù hợp với các bạn sinh viên ở Việt Nam (mình thấy nhiều sách và nhiều thầy hay dùng:) ), đó là: Supervised learning - Học có giám sát. Unsupervised learning - Học không giám sát. Reinforcement learning - Học tăng cường.   Mình thích chủ đề này, mà mình đọc tài liệu để tham khảo thôi vì nó phức tạp quá.  cho thue xe 7 cho theo thang Đăng nhập để trả lời ▲ \n                      -3\n                     ▼ Nội dung   OK © VNOI Team 2015"
        },
        {
          "title": "Các phương pháp học máy (Machine Learning)",
          "relevance": "1",
          "url": "http://bis.net.vn/forums/t/619.aspx",
          "content": "\r\n\t\t\t                            \r\n        Chào mừng đến với BIS\r\n         Đăng nhập  \r\n         |  Đăng ký \r\n        |  Trợ giúp \r\n\t\t\t\t\t                                    \r\n\t\t\t\t\t\t                                         trong \r\n\t\t\t\t\t\t                                         Data Mining and Business Intelligence... Data Mining and Business Intelligence... (Entire Site) Tìm kiếm BIS  »  Data Mining and Business Intelligence  »  Data Mining and Business Intelligence  »  Các phương pháp học máy (Machine Learning)  Các phương pháp học máy (Machine Learning)  Bài cuối 06-25-2013 11:27 AM của  thanhthi . 5 trả lời. Trang 1 trong số 1 (6 nội dung)  \r\n\t\t\t\t        \r\n\t\t\t\t                Sắp xếp bài viết:\r\n\t\t\t\t                 Cũ đến mới Mới đến cũ Trước Tiếp theo \r\n\t\t\t\t\t\t\t\t        04-10-2012 05:51 PM    \r\n\t\t\t\t\t\t\t\t     TienPhong Tham gia 03-04-2012 Điểm 110 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Các phương pháp học máy (Machine Learning)\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Trong lĩnh vực Học máy có các phương\r\npháp học sau: 1) Học có giám sát (supervised\r\nlearning) 2) Học không có giám sát\r\n(unsupervised learning) 3) Học bán giám sát\r\n(semi-supervised learning) 4) Học tăng cường (reinforcement\r\nlearning) Không phải ngẫu nhiên người ta lại\r\nphân chia và đặt tên cho các phương pháp học như vậy. Sau một thời gian tìm đọc tài liệu\r\nvề học máy, cố gắng hiểu được bản chất và phân biệt được sự giống và khác nhau\r\ncủa các phương pháp học này, tôi đưa ra một bài toán ví dụ sau: Cho một công-ten-nơ chứa đầy hoa\r\nquả. Nhiệm vụ phải chia số quả này thành các nhóm đúng với loại quả đó. (xoài\r\nra xoài, cam ra cam, táo ra táo,…) 1) Với Học có giám sát\r\n(supervised learning) Là kỹ thuật học sử dụng cho các bài\r\ntoán phân lớp (Classification) Để thực hiện được bài toán trên,\r\ntrước tiên cần phải có 2 điều kiện: Điều kiện 1: phải biết trước số\r\nnhãn lớp cần phân loại, tức là phải biết trong công-ten-nơ đó có nhưng loại quả\r\ngì. Giả sử trong công-ten-nơ đó có 5 loại quả là xoài, cam, táo, ổi, đào (đây\r\nchính là 5 loại nhãn lớp). Điều kiện 2: phải có tập đặc trưng\r\ncủa mỗi loại quả, ví dụ các đặc trưng là: hình dáng, màu sắc, trọng lượng, độ cứng\r\nmềm, v.v… Tập đặc trưng này có được thông qua học một tập dữ liệu huấn luyện\r\n(chính là các công-ten-nơ của các chuyến hàng trước đó) Khi thực hiện phân loại các loại\r\nquả trong công-ten-nơ đang xét, dựa vào đặc trưng của các loại quả (điều kiện\r\n2), quả sẽ được đưa vào 1 trong 5 nhóm đã biết (điều kiện 1). 2) Học không có giám sát\r\n(unsupervised learning) Là kỹ thuật học sử dụng cho các bài\r\ntoán phân cụm, gom cụm (Clustering)   Để thực hiện được bài toán trên,\r\ncần phải có tập đặc trưng của mỗi loại quả. Tập đặc trưng này có được cũng thông\r\nqua học một tập dữ liệu huấn luyện (như điều kiện 2 của Học có giám sát).   Điểm khác của Học không giám sát\r\nso với Học có giám sát là: trước khi phân cụm, không biết trong công-ten-nơ đang\r\nxét có bao nhiêu loại quả và đó là những loại quả gì. Khi thực hiện phân cụm, dựa vào đặc\r\ntrưng của mỗi loại quả, sẽ đưa quả đang xét vào nhóm (cụm) có đặc trưng tương đồng\r\nvới nó nhất. Khi đó, 2 quả bất kỳ ở cùng cụm sẽ tương đồng nhau, 2 quả khác cụm\r\nsẽ khác biệt nhau. * Nhận xét Giống nhau: Cả\r\nhai phương pháp học 1) và 2) đều cần phải có một tập huấn luyện (training data\r\nset) để hệ thống có thể “học” và rút ra được các đặc trưng dùng cho việc gán nhãn. Khác nhau: Phương\r\npháp 1) cần biết trước đầu ra chính là số nhãn lớp. Phương pháp 2) không cần biết\r\ntrước đầu ra  (là số cụm và nhãn) để phân\r\ncụm.   Trên đây là cách hiểu của tôi về\r\nphương pháp Học có giám sát và Học không giám sát. Nhân đây tôi xin đưa một thắc\r\nmắc để các thành viên và những ai quan tâm đưa ra ý kiến để cùng thảo luận. Thắc mắc: Học bán giám sát là gì?\r\nHọc tăng cường là gì? Phân biệt sự khác nhau và giống nhau giữa các phương pháp\r\nhọc bán giám sát và học tăng cường. Cho ví dụ minh hoạ? Từ khóa đại diện:  Machine Learning ,  Phân cụm dữ liệu. ,  Học máy ,  Phân lớp dữ liệu ,  Khai phá dữ liệu Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        06-18-2012 09:41 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  phamvu Tham gia 06-18-2012 Điểm 35 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Các phương pháp học máy (Machine Learning)\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ \r\n\t\t\t\t\t\t\t\t\t\t    Em thấy k - means là học ko giám sát, trong khi đó xác định k cụm là do người dùng xác định hoặc có thể default, vậy số k đã được biết trước.Vậy thuật toán k - means phải là học nửa giám sát?\r\n\t\t\t\t\t\t\t\t\t\t    \r\n\t\t\t\t\t\t\t\t\t     phamvu Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        09-02-2012 01:19 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  TienPhong Tham gia 03-04-2012 Điểm 110 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Các phương pháp học máy (Machine Learning)\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Đúng k means là thuật toán học không giám sát vì nó được dùng để phân cụm cho các dữ liệu chưa biết nhãn. Để dễ phân biệt bạn chú ý là việc biết (cho) trước k cụm không liên quan gì đến dữ liệu đầu vào (hay dữ liệu huấn luyện - training data set). Với học có giám sát dữ liệu đầu vào đều đã được gán nhãn trước, phương pháp học này dùng giải quyết bài toán phân lớp (classification). Với học không giám sát, không biết trước nhãn của dữ liệu (hay dữ liệu đầu vào đều chưa được gán nhãn), phương pháp này dùng để giải quyết bài toán phân cụm (clustering). Còn với học nửa giám sát, sử dụng tập dữ liệu đầu vào gồm cả dữ liệu đã gán nhãn và dữ liệu chưa gán nhãn, trong đó dữ liệu gán nhãn thường (rất) ít và dữ liệu chưa gán nhãn thường (rất) nhiều. Hai kỹ thuật tiêu biểu cho học nửa giám sát là Self-training và Co-training, bạn có thể tìm kiếm trên internet. Chúc vui vẻ! Điểm chủ đề: 20 \r\n\t\t\t\t\t\t\t\t        05-17-2013 12:49 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  TienPhong Tham gia 03-04-2012 Điểm 110 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Các phương pháp học máy (Machine Learning)\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Có nhiều bạn gửi mail hỏi mình về Machine Learning, các bạn thông cảm vì đi làm bận nên không trả lời riêng các bạn, và cũng để mọi người có thể cùng trao đổi mình sẽ post lên đây để mọi người chia sẻ những kiến thức về ML.   Trước tiên mình nêu một số định nghĩa rất quan trọng trong ML.   Định nghĩa học máy Học máy (hay máy học – Machine learning) là một thành phần quan trọng của trí tuệ nhân tạo nhằm nghiên cứu và phát triển các phương pháp, kỹ thuật giúp cho các hệ thống hay máy tính có khả năng học (Tiến Phong).   Vấn đề quá vừa dữ liệu (Over-fitting) Thuật ngữ over-fitting ra đời dùng để chỉ một hiện tượng xuất hiện trong quá trình khai phá dữ liệu sử dụng phương pháp học máy. Hiện tượng này gây khó khăn đáng kể cho việc thiết kế, xây dựng hệ thống ngay từ bước chuẩn bị dữ liệu cho đến bước kiểm thử hệ thống. Khi hiện tượng over-fitting xảy ra sẽ làm cho hiệu quả của hệ thống giảm xuống và kết quả thu được từ hệ thống không còn độ tin cậy cao. Có thể định nghĩa hiện tượng over-fitting như sau:   Định nghĩa quá vừa dữ liệu Một hàm mục tiêu hay một giả thiết học được  h , sẽ được gọi là over-fitting (quá vừa dữ liệu) với một tập dữ liệu huấn luyện nếu tồn tại một hàm mục tiêu khác là  h’  sao cho: h’  kém phù hợp hơn, đạt độ chính xác kém hơn so với  h  trên tập dữ liệu huấn luyện, nhưng  h’  lại đạt độ chính xác cao hơn  h  đối với toàn bộ tập dữ liệu (bao gồm cả tập dữ liệu liệu huấn luyện và tập dữ liệu kiểm tra)   Ví dụ quá vừa dữ liệu Giả sử gọi D là tập toàn bộ các dữ liệu có thể có, Training_D là tập các dữ liệu huấn luyện Giả sử Err_D(h) là mức lỗi mà giả thiết h sinh ra đối với tập D, và Err_Training_D(h) là mức lỗi mà giả thiết h sinh ra đối với tập Training_D. Nếu tồn tại một giả thiết khác là h’ sao cho: Err_Training_D(h) < Err_Training_D(h’) và Err_D(h) > Err_D(h’) Thì khi đó h được coi là quá vừa dữ liệu trên tập huấn luyện Training_D.   Nguyên nhân quá vừa dữ liệu Vấn đề over-fitting thường do các nguyên nhân: - Lỗi (nhiễu) trong tập huấn luyện phát sinh trong quá trình thu thập, xây dựng tập dữ liệu. - Số lượng dữ liệu của tập huấn luyện quá nhỏ, không đại diện cho toàn bộ tập dữ liệu có thể có hay toàn bộ phân bố dữ liệu của bài toán.   Bản chất của học máy dưới góc nhìn của xác suất thống kê. Có thể mô hình hoá một vấn đề máy học như sau: Cho một dãy  l  quan sát:  (x 1 , y 1 ), (x 2 , y 2 ), … , (x l , y l ) .  Trong đó: - x 1 , x 2 , …, x l  là các mẫu, x i R n . Các mẫu x i  được phát sinh ngẫu nhiên theo một hàm phân bố xác suất F(x) nào đó mà ta không biết. - y i  là các kết quả học tương ứng với mẫu x i , y i R. Thường thì y là kết quả của một hàm f(x) nào đó – đơn trị. Tuy nhiên trong trường hợp tổng quát thì y không đơn trị. Do đó y được xác định theo một hàm phân bố điều kiện F(y|x) mà ta cũng không biết. - Bây giờ cho một mẫu x*, vấn đề của máy học là xác định một hàm f 0 (x) mà có thể ước lượng tốt nhất giá trị y* tương ứng. Như vậy theo lý thuyết tương quan trong thống kê thì f 0 (x) tốt nhất theo lý thuyết phải là kỳ vọng của y theo x theo phân bố F(y|x).f 0 (x) còn được gọi là phương trình hồi quy. Với x tuân theo phân bố F(x), y tuân theo phân bố có điều kiện F(y|x) thì hàm phân bố của cặp (x, y) là F(x, y) = F(x)F(y|x). Có thể thấy xác suất để có dãy  (x 1 , y 1 ), (x 2 , y 2 ), … , (x l , y l )  là tích  F(x 1 , y 1 )F(x 2 , y 2 )…F(x l , y l ). Tuy nhiên, ở đây ta không biết F(x) lẫn F(y|x) nên không thể xác định chính xác kỳ vọng này. Tất cả dữ liệu mà ta biết chỉ là dãy hữu hạn các mẫu quan sát  (x 1 , y 1 ), (x 2 , y 2 ), … , (x l , y l ) . Nhiệm vụ của máy học là xác định chính xác nhất có thể được hàm f 0 (x) dựa trên các dữ liệu hữu hạn này. Trong trường hợp khi y   R, tức đây là vấn đề hồi quy (regression).  Trong trường hợp bài toán phân lớp (classification) thì y  {-1, 1} là trường hợp nhận dạng hai lớp, nếu y i  = -1 thì x i  thuộc lớp thứ nhất (không được quan tâm), còn y i  = 1 thì x i  thuộc lớp thứ 2 (lớp được quan tâm)   Một số phương pháp học máy Trong lĩnh vực học máy có nhiều phương pháp học khác nhau, trong phần này đề cập đến 3 phương pháp học được sử dụng phổ biến nhất, gồm có: học không giám sát, học bán/ nửa giám sát và học có giám sát.   Phương pháp học không giám sát (Unsupervised Learning) *  Khái niệm học không giám sát Học không giám sát là học với tập dữ liệu huấn luyện ban đầu hoàn toàn chưa được gán nhãn. Học không giám sát là phương pháp học sử dụng cho lớp bài toán gom cụm, phân cụm (Clustering).   *  Nội dung phương pháp học không giám sát - Để thực hiện phân cụm, trước tiên cần một tập dữ liệu huấn luyện (training dataset) – là một tập các ví dụ học (training examples/instances). Trong đó, mỗi ví dụ học chỉ chứa thông tin biểu diễn (ví dụ: một vector các giá trị thuộc tính), mà không có bất kỳ thông tin gì về nhãn lớp hoặc giá trị đầu ra mong muốn (expected output). - Áp dụng một thuật toán học không có giám sát (ví dụ k-Means) để học hàm/mô hình mục tiêu (trong trường hợp này là hàm phân cụm ứng với thuật toán được chọn). - Sử dụng một phương pháp thử nghiệm (có thể kết hợp với một tập dữ liệu có gán nhãn) để đánh giá hiệu năng/chất lượng của hàm mục tiêu học được.   *  Một số thuật toán học không giám sát Có rất nhiều thuật toán học không giám sát được ra đời và phát triển nhằm giải quyết bài toán phân cụm phục vụ khai thác hiệu quả nguồn dữ liệu chưa gán nhãn nhiều và rất đa dạng. Việc lựa chọn sử dụng thuật toán nào tuỳ thuộc vào dữ liệu và mục đích của từng bài toán. Trong đó các thuật toán thường được sử dụng như: k-means, HAC  (Hierarchical Agglomerative Clustering) , SOM  (Self-Organizing Map) , DBSCAN, FCM,... (chi tiết các thuật toán này có thể tìm kiếm trên Internet)   Phương pháp học bán giám sát (Semi-Supervised Learning) Trong thực tế, để có được một tập dữ liệu có chất lượng và đã được gán nhãn của một lĩnh vực, thường được thực hiện thủ công bằng tay bởi người có nhiều kinh nghiệm về lĩnh vực đó. Vì vậy, dữ liệu đã được gán nhãn thường ít và đắt. Trong khi đó, dữ liệu chưa được gán nhãn lại rất nhiều và phong phú. Phương pháp học bán giám sát (hay học nửa giám sát) được đặt ra để tận dụng cả hai nguồn dữ liệu này.   *  Khái niệm học bán giám sát Học bán giám sát là học với tập dữ liệu huấn luyện gồm cả dữ liệu đã được gán nhãn và dữ liệu chưa được gán nhãn. Tuỳ vào từng mục đích cụ thể, học bán giám sát có thể được áp dụng cho bài toán phân lớp hoặc phân cụm.   *  Nội dung phương pháp học bán giám sát -  Nội dung chính của học bán giám sát là hệ thống  sử dụng  một tập học (training set) gồm 2 phần: các ví dụ học có nhãn, thường với số lượng (rất) ít, và các ví dụ học không có nhãn, thường với số lượng (rất) nhiều. Thực tế cho thấy khi sử dụng kết hợp dữ liệu không có nhãn với một lượng nhất định dữ liệu có nhãn có thể tăng độ chính xác đáng kể. -  Một thuật toán học bán giám sát được sử dụng (ví dụ Self-training) sẽ học các ví dụ có nhãn, sau đó  tiến hành gán nhãn cho một số (có lựa chọn) các ví dụ không có nhãn - một cách hợp lý, có đánh giá chất lượng công việc  hay  độ chính xác. Tiếp theo, chọn các ví dụ vừa được gán nhãn có độ tin cậy cao (vượt trên một ngưỡng chọn trước) đưa vào kết hợp với tập dữ liệu có nhãn, tạo thành một tập dữ liệu huấn luyện mới. -  Áp dụng một phương pháp kiểm thử (có thể kết hợp với một tập dữ liệu đã biết trước nhãn) để đánh giá hiệu năng/độ chính xác của mô hình.   *  Một số thuật toán học bán giám sát Một số thuật toán thường được sử dụng gồm có: thuật toán Cực đại kỳ vọng (EM - Expectation Maximization), SVM truyền dẫn (TSVM - Transductive Support Vector Machine), Self-training, Co-training và các phương pháp dựa trên đồ thị (graph-based). Việc lựa chọn thuật toán nào dựa trên một số định hướng: nếu các lớp dữ liệu có tính phân cụm cao thì nên dùng EM với mô hình hỗn hợp sinh; nếu đã sử dụng SVM thì mở rộng thành TSVM; khi khó nâng cấp mô hình học có giám sát đã có, thì nên dùng self-training; nếu các đặc trưng của dữ liệu phân chia tự nhiên thành hai phần riêng rẽ thì nên dùng Co-training; còn nếu hai mẫu dữ liệu có đặc trưng tương tự nhau hướng tới một lớp thì sử dụng phương pháp dựa trên đồ thị.   Trong số các thuật toán học bán giám sát thông dụng, có 2 thuật toán tiêu biểu là Self-training và Co-training. - Thuật toán Self-training: Self-training là kỹ thuật học bán giám sát được sử dụng khá phổ biến do tận dụng được nguồn dữ liệu chưa gán nhãn lớn và ban đầu chỉ cần lượng nhỏ dữ liệu đã gán nhãn. Nội dung chính của Self-training là lặp nhiều lần phương pháp học có giám sát. Gọi     D: là tập các dữ liệu đã được gán nhãn.           C : là tập các dữ liệu chưa gán nhãn. Thuật toán Self-training thực hiện như sau: Lặp  (cho đến khi C =  Æ ): i. Huấn luyện bộ phân lớp có giám sát  h  trên tập D ii. Sử dụng  h  để phân lớp dữ liệu trong tập C iii. Tìm tập con C’  Í  C có độ tin cậy cao nhất: D + C’  Þ  D ; C – C’  Þ  C. Ban đầu huấn luyện bộ phân lớp bằng cách cho bộ phân lớp học một tập dữ liệu huấn luyện đã được gán nhãn (tập này thường nhỏ so với tập dữ liệu chưa gán nhãn). Dùng bộ phân lớp đã được huấn luyện, phân lớp cho các dữ liệu chưa được gán nhãn. Trong số dữ liệu mới được gán nhãn, chọn các dữ liệu có độ tin cậy cao (lớn hơn một ngưỡng nào đó) kèm với nhãn vừa gán, đem bổ sung vào tập dữ liệu huấn luyện ban đầu. Sau đó, bộ phân lớp được học lại trên tập huấn luyện mới (gồm dữ liệu đã gán nhãn ban đầu và dữ liệu do bộ phân lớp mới gán nhãn) và thuật toán được lặp lại. Sau mỗi vòng lặp, bộ phân lớp sẽ bổ sung một số mẫu dữ liệu có độ tin cậy cao nhất cùng với dự đoán phân lớp của chúng vào tập dữ liệu huấn luyện. Tên gọi Self-training xuất phát từ việc sử dụng dự đoán của nó để huấn luyện chính nó.   - Thuật toán Co-training: Thuật toán Co-training dựa trên giả thuyết rằng các đặc trưng của tập dữ liệu huấn luyện có thể được phân chia thành 2 tập con (trường hợp lý tưởng là hai tập con này thoả mãn điều kiện độc lập nhau - conditional independent). Nội dung chính của thuật toán như sau: + Dùng 2 bộ phân lớp phù hợp để học 2 tập con tương ứng (mỗi tập con huấn luyện một bộ phân lớp). + Mỗi bộ phân lớp thực hiện phân lớp cho các dữ liệu chưa gán nhãn, thu được kết quả là tập dữ liệu chưa gán nhãn kèm theo nhãn dự đoán của chúng. Trong tập kết quả của bộ phân lớp 1, chọn ra những mẫu dữ liệu (kèm nhãn đã dự đoán) có độ tin cậy cao nhất bổ sung vào tập huấn luyện của bộ phân lớp 2 và ngược lại. + Mỗi bộ phân lớp được học lại tập dữ liệu huấn luyện (gồm dữ liệu gán nhãn ban đầu và dữ liệu gán nhãn mới bổ sung từ kết quả của bộ phân lớp kia). Quá trình được lặp lại cho đến khi tập dữ liệu chưa gán nhãn rỗng hoặc số vòng lặp đạt tới một ngưỡng được xác định trước.   Thuật toán Co-training: (1). Huấn luyện hai bộ phân lớp:   f  (1)  từ (X l  (1) , Y l ), f  (2)  từ (X l  (2) , Y l ).  (2). Phân lớp các mẫu dữ liệu chưa gán nhãn X u  với f  (1)  và f  (2)  tách biệt nhau. (U là tập các mẫu dữ liệu chưa gán nhãn) (3).  Chèn thêm vào f  (1)  k-most-confident (x, f  (1) (x)) tới các dữ liệu đã gán nhãn của f  (2) . (4). Chèn thêm vào f  (2)  k-most-confident (x, f  (2)  (x)) tới các dữ liệu đã gán nhãn của f  (1) . (5). Lặp lại các quá trình trên.   Thuật toán Co-training trên có thể viết như sau: L: là tập các mẫu dữ liệu đã gán nhãn U: là tập các mẫu dữ liệu chưa gán nhãn (1). L có thể phân chia thành hai tập con L 1  và L 2  (trường hợp lý tưởng thì L 1  và L 2  độc lập nhau). (2). Cho bộ phân lớp h 1  học L 1  (hay L 1  huấn luyện bộ phân lớp h 1 ) Cho bộ phân lớp h 2  học L 2  (hay dùng L 2  huấn luyện bộ phân lớp h 2 ) (3). Dùng h 1  phân lớp cho U thu được tập U 1 ’ kèm nhãn dự đoán của chúng. Dùng h 2  phân lớp cho U thu được tập U 2 ’ kèm nhãn dự đoán của chúng. (4). Từ U 1 ’ chọn ra u 1  mẫu dữ liệu kèm theo nhãn của nó, có độ tin cậy cao nhất. Bổ sung u 1  vào L 2 . Khi đó, L 2  + u 1  => L 2 . Từ U 2 ’ chọn ra u 2  mẫu dữ liệu kèm theo nhãn của nó, có độ tin cậy cao nhất. Bổ sung u 2  vào L 1 . Khi đó, L 1  + u 2  => L 1 . (5). Dùng L 1  mới huấn luyện bộ phân lớp h 1  (hay h 1  học L 1 ) Dùng L 2  mới huấn luyện bộ phân lớp h 2  (hay h 2  học L 2 ) (6). Lặp lại từ bước (3). cho đến khi tập U rỗng hoặc số vòng lặp đạt đến ngưỡng xác định trước.   Có thể viết rút gọn bằng cách bỏ bước (5). ở trên. Bước (6). đổi thành bước (5): Lặp lại từ bước (2). cho đến khi tập U rỗng hoặc số vòng lặp đạt đến ngưỡng xác định trước.   Phương pháp học có giám sát (Supervised Learning) *  Khái niệm học có giám sát : Học có giám sát là học với tập dữ liệu huấn luyện ban đầu hoàn toàn được gán nhãn từ trước. Học có giám sát là phương pháp học sử dụng cho lớp bài toán phân lớp, phân loại (Classification).   *  Nội dung phương pháp học có giám sát : -  Để thực hiện phân lớp, trước tiên phải chuẩn bị một tập dữ liệu huấn luyện (trainning data set), để có  tập dữ liệu huấn luyện  phải thực hiện gán nhãn cho dữ liệu ban đầu, đây được gọi là quá trình thu thập tập huấn luyện.  -  Lựa chọn một thuật toán phân lớp (ví dụ SVM) xây dựng bộ phân lớp để  học  tập dữ liệu huấn luyện. Hay nói cách khác, dùng tập dữ liệu huấn luyện để huấn luyện bộ phân lớp. Thuật ngữ  học có giám sát  được hiểu là  học  tập dữ liệu đã được gán nhãn trước (các dữ liệu kèm theo nhãn tương ứng này coi như đã được giám sát bởi người thực hiện gán nhãn). -  Sử dụng một tập dữ liệu kiểm tra (test data set) đã được gán nhãn trước, để kiểm tra tính đúng đắn của bộ phân lớp. Sau đó, có thể dùng bộ phân lớp để phân lớp cho các dữ liệu mới.   *  Một số thuật toán học có giám sát : Một số thuật toán thường được lựa chọn khi xây dựng bộ phân lớp gồm có: máy vector hỗ trợ (Support Vector Machine – SVM); k láng giềng gần nhất (K Nearest Neighbours – KNN); tiếp cận xác suất thống kê (Naïve Bayes – NB); Cây quyết định (Decision Tree – DT); sử dụng mạng nơron (Neural Network – Nnet); dựa trên vector trọng tâm (Centroid–base vector); hay tuyến tính bình phương nhỏ nhất (Linear Least Square Fit – LLSF). (Chi tiết các thuật toán này có thể tham khảo trên Internet). Từ khóa đại diện:  Machine Learning ,  Học máy ,  máy học Điểm chủ đề: 20 \r\n\t\t\t\t\t\t\t\t        05-17-2013 01:16 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  TienPhong Tham gia 03-04-2012 Điểm 110 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Các phương pháp học máy (Machine Learning)\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Mình đính chính một vài lỗi soạn thảo nhỏ: Trong phần “Bản chất của học máy dưới góc nhìn của xác suất thống kê”  - x 1 , x 2 , …, x l  là các mẫu,  x i  thuộc R n .  Các mẫu x i  được phát sinh ngẫu nhiên theo một hàm phân bố xác suất F(x) nào đó mà ta không biết. - y i  là các kết quả học tương ứng với mẫu  x i , y i  thuộc R . Thường thì y là kết quả của một hàm f(x) nào đó – đơn trị. Tuy nhiên trong trường hợp tổng quát thì y không đơn trị. Do đó y được xác định theo một hàm phân bố điều kiện F(y|x) mà ta cũng không biết. … Trong trường hợp khi  y thuộc R , tức đây là vấn đề hồi quy (regression).  Trong trường hợp bài toán phân lớp (classification) thì  y thuộc{-1, 1}  là trường hợp nhận dạng hai lớp, nếu y i  = -1 thì x i  thuộc lớp thứ nhất (không được quan tâm), còn y i  = 1 thì x i  thuộc lớp thứ 2 (lớp được quan tâm).   Phần “ Thuật toán Self-training thực hiện như sau:” Lặp (cho đến khi C =  rỗng ): … iii. Tìm tập con C’  <ký hiệu là con thực sự của>  C có độ tin cậy cao nhất: D + C’  =>  D; C – C’  =>  C. Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        06-25-2013 11:27 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  thanhthi Tham gia 06-25-2013 Điểm 20 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Các phương pháp học máy (Machine Learning)\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ  Em có bài toán thế này mong các anh giúp đỡ - Dữ liệu đầu vào là các chỉ số của 1 mã chứng khoán theo ngày: Giá mở đóng cửa, chỉ số trung bình trượt 5 ngày, 25 ngày ... (đầu vào khoảng 10 chiều ) - và được gán nhãn là Tăng hoặc giảm - Em muốn dùng SVM để dự đoán ngày tiếp theo sẽ tăng hay giảm thì em cần phải làm như nào ạ, có thể dùng Weka với libsvm được không ạ? hoặc tool nào có thể giải quyết được bài toán này ạ ?   Các anh, các chị nào biết vui lòng chỉ giáo cho em với, em đang cần rất gấp  Cảm ơn các anh chị nhiêu ạ ! Nếu có tài liệu mong anh chị gửi cho em vào mail giúp em ạ    Trần Thị Thanh:  thanhtd112@gmail.com Điểm chủ đề: 20 Trang 1 trong số 1 (6 nội dung)  ©2008-2017 Business Intelligence Solution. All rights reserved."
        },
        {
          "title": "Tất tần tật về Machine Learning & ứng dụng trong những ngành công nghiệp lớn",
          "relevance": "0",
          "url": "https://techtalk.vn/tat-tan-tat-moi-kien-thuc-co-ban-ve-machine-learning.html",
          "content": "Công nghệ Lập trình Lập trình ứng dụng Lập trình web Tools & Tips Sự kiện Chuyên gia nói Tâm sự coder Devvui Tìm kiếm Sign in Đăng nhập tài khoản Tài khoản mật khẩu của bạn Forgot your password? Get help Password recovery Khởi tạo mật khẩu email của bạn Mật khẩu đã được gửi vào email của bạn. Tech Talk Điều gì sẽ xảy ra khi cập nhật phần mềm mỗi… Trí tuệ nhân tạo của Google tự đánh cờ vây với… 8 xu hướng công nghệ sẽ thống trị trong giai đoạn… Những kĩ năng cần có của một developer thành công Bản update Windows 10 Fall Creators không tương thích với laptop… Tất cả Lập trình ứng dụng Lập trình web Framework Microsoft .Net 4.7.1 có gì mới? 7 lí do để loại bỏ React’s Functional Components Tại sao C # là một trong những ngôn ngữ lập… Vì sao JavaScript là ngôn ngữ lập trình quyến rũ Getting Started with Entity Framework 6 Code First using MVC 5 Thử một lần code mà không dùng If xem nào? 5 cách giúp lập trình viên tăng năng suất làm việc Lập trình viên, liệu bạn đã đủ “hấp dẫn” trong mắt… [TÀI LIỆU] Beginning Mobile App Development with React Native “Ở Việt Nam, cơ hội để thực sự làm về Trí… [ Học bổng] Học tiếng Nhật miễn phí cùng ” Akira”-… VWS2017 – Xây dựng Decentralized web trên nền tảng Blockchain Họp báo ra mắt sự kiện Vietnam Web Summit 2017 [Miễn phí] Tham gia sự kiện App Analytics Tools: con đường… 4 sai lầm có thể khiến các lập trình viên rời… Lương kỹ sư IT đi Nhật có thể lên đến 165… Phỏng vấn chuyên gia Machine Learning từ AdAsia về ứng dụng… 5 điều phiền toái nhất của CSS Thương hiệu cá nhân Dev quan trọng hơn bạn tưởng Tâm sự một coder: hãy dũng cảm thành thật với con… Web Dev: Cố gắng hoàn hảo sẽ cản trở bạn Thay đổi code, thay đổi thế giới Thương hiệu cá nhân Dev quan trọng hơn bạn tưởng Những tình huống “đứng hình” trong JavaScript Trang Chủ Công nghệ Tất tần tật về Machine Learning & ứng dụng trong những ngành... Tất tần tật về Machine Learning & ứng dụng trong những ngành công nghiệp lớn November 25, 2016 5848 Chia sẻ Facebook Tweet Tham gia ngay sự kiện Machine Learning - Đặt vé ngay! Machine Learning  là gì? Có 2 định nghĩa khá rõ ràng về Machine Learning như sau: Theo Arthur Samuel (1959): Máy học là ngành học cung cấp cho máy tính khả năng học hỏi mà không cần được lập trình một cách rõ ràng Theo Giáo sư Tom Mitchell – Carnegie Mellon University: Machine Learning là 1 chương trình máy tính được nói là học hỏi từ kinh nghiệm E từ các tác vụ T và với độ đo hiệu suất P. Nếu hiệu suất của nó áp dụng trên tác vụ T và được đo lường bởi độ đo P tăng từ kinh nghiệm E Ví dụ cho định nghĩa của Tom Mitchell Ví dụ 1: Giả sử như bạn muốn máy tính xác định một tin nhắn có phải là SPAM hay không Tác vụ T: Xác định 1 tin nhắn có phải SPAM hay không? Kinh nghiệm E: Xem lại những tin nhắn đánh dấu là SPAM xem có những đặc tính gì để có thể xác định nó là SPAM. Độ đo P: Là phần trăm số tin nhắn SPAM được phân loại đúng. Ví dụ 2: Chương trình nhận dạng số (số từ 0 -> 9) T: Là nhận dạng được ảnh chứa ký tự số. E: Đặc trưng để phân loại ký tự số từ tập dữ liệu số cho trước. P: Độ chính xác của quá trình nhận dạng. --------------------  Thông tin cho Dev  --------------------   Mối liên hệ giữa Trí Tuệ Nhân Tạo với các nhánh học như Máy Học, Biểu Diễn Tri Thức và Suy Luận, Xử Lý Ngôn Ngữ Tự Nhiên, Thị Giác Máy Tính… Sự phát triển mạnh mẽ của  Machine Learning Nhờ vào công nghệ điện toán, ngày nay Machine Learning không còn là máy tính “học” những chuyện trong quá khứ nữa. Machine Learning được sinh ra từ khả năng nhận diện pattern và từ lý thuyết các máy tính có thể “học” mà không cần phải lập trình để thực hiện các tasks cụ thể đó. Về phía các nhà nghiên cứu quan tâm đến trí tuệ nhân tạo, họ lại muốn xem thử liệu máy tính có thể học dữ liệu như thế nào. Yếu tố lặp trong Machine Learning rất quan trọng vì khi các models tiếp xúc với dữ liệu mới,  Machine Learning  có thể thích ứng được 1 cách độc lập. Machine Learning sẽ “học” các computations trước để trả về các kết quả, các quyết định đáng tin cậy, lặp lại được.  Từ lâu đã có nhiều thuật toán Machine Learning nổi tiếng nhưng khả năng tự động áp dụng các phép tính phức tạp vào Big Data – lặp đi lặp lại với tốc độ nhanh hơn – chỉ mới phát triển gần đây. TopDev Techtalk #52: Công nghệ của tương lai: Đột phá gì từ Machine Learning? *Hồ Chí Minh:  18h00 – 21h00 ngày 02/12/2016. Các ứng dụng của  Machine Learning  đã trở nên quá quen thuộc như: Xe tự lái, giảm thiểu tai nạn của Google? Chính là bản chất của machine learning Các ưu đãi recommendation online như của Amazong & Netflix? Ứng dụng của Machine Learning trong cuộc sống hằng ngày Muốn biết người dùng nói gì về bạn trên Twitter? Machine Learning kết hợp với sự sáng tạo của quy tắc ngôn ngữ Nhận diện lừa đảo? Một trong những nhu cầu sử dụng hiển nhiên ngày nay Làm thế nào để có những hệ thống machine learrning tốt? Khả năng chuẩn bị dữ liệu Thuật toán – căn bản & nâng cao Quy trình tự động và quy trình lặp lại Khả năng scale Ensemble modeling Những đối tượng nào đang xài  Machine Learning ? Hầu hết mọi ngành công nghiệp đang làm việc với hàm lượng lớn dữ liệu đều nhận ra tầm quan trọng của công nghệ Machine Learning. Những insights từ nguồn dữ liệu này – chủ yếu dạng realtime – sẽ giúp các tổ chức vận hành hiệu quả hơn hoặc tạo được lợi thế cạnh tranh so với các đối thủ. Các dịch vụ tài chính Ngân hàng và những doanh nghiệp hoạt động trong lĩnh vực tài chính sử dụng công nghệ Machine Learning với 2 mục đích chính: xác định insights trong dữ liệu và ngăn chặn lừa đảo. Insights sẽ biết được các cơ hội đầu tư hoặc thông báo đến nhà đầu tư thời điểm giao dịch hợp lý. Data mining cũng có thể tìm được những khách hàng đang có hồ sơ rủi ro cao hoặc sử dụng giám sát mạng để chỉ rõ những tín hiệu lừa đảo. Chính phủ Các tổ chức chính phủ hoạt động về an ninh cộng đồng hoặc tiện ích xã hội sở hữu rất nhiều nguồn dữ liệu có thể khai thác insights. Ví dụ, khi phân tích dữ liệu cảm biến, chính phủ sẽ tăng mức độ hiệu quả của dịch vụ và tiết kiệm chi phí.  Machine Learning  còn hỗ trợ phát hiện gian lận và giảm thiểu khả năng trộm cắp danh tính. Chăm sóc sức khỏe Machine Learning  là 1 xu hướng phát triển nhanh chóng trong ngành chăm sóc sức khỏe, nhờ vào sự ra đời của các thiết bị và máy cảm ứng đeo được sử dụng dữ liệu để đánh giá tình hình sức khỏe của bệnh nhân trong thời gian thực (real-time). Công nghệ Machine Learning còn giúp các chuyên gia y tế xác định những xu hướng hoặc tín hiệu để cải thiện khả năng điều trị, chẩn đoán bệnh. Tham gia ngay sự kiện Machine Learning – Công nghệ của Tương lai! Marketing và sales Dựa trên hành vi mua hàng trước đây, các trang web sử dụng Machine Learning phân tích lịch sử mua hàng, từ đó giới thiệu những vật dụng mà bạn có thể sẽ quan tâm và yêu thích. Khả năng tiếp nhận dữ liệu, phân tích và sử dụng những dữ liệu đó để cá nhân hóa trải nghiệm mua sắm (hoặc thực hiện chiến dịch Marketing) chính là tương tai của ngành bán lẻ. Dầu khí Tìm kiếm những nguồn nguyên liệu mới. Phân tích các mỏ dầu dưới đất. Dự đoán tình trạng thất bại của bộ cảm biến lọc dầu. Sắp xếp các kênh phân phối để đạt hiệu quả và tiết kiệm chi phí. Có thể nói, số lượng các trường hợp sử dụng Machine Learning trong ngành công nghiệp này cực kì lớn và vẫn ngày càng mở rộng. Vận tải Phân tích dữ liệu để xác định patterns & các xu hướng là trọng tâm trong ngành vận tải vì đây là ngành phụ thuộc vào khả năng tận dụng hiệu quả trên mỗi tuyến đường và dự đoán các vấn đề tiềm tàng để gia tăng lợi nhuận. Các chức năng phân tích dữ liệu và modeling của  Machine Learning  đóng vai trò quan trọng với các doanh nghiệp vận chuyện, vận tải công cộng và các tổ chức vận chuyển khác. Một số methods machine learning nổi tiếng Hai methods của Machine Learning được chấp nhận rộng rãi chính là  supervised learning  (học có giám sát) và  unsupervised learning  (học không giám sát) nhưng cũng có những methods khác như s emisupervised learning  (học bán giám sát) , reinforcement learning  (học tăng cường) Dưới đây là khái niệm chung về 2 phương pháp phổ biến nhất: Supervised Learning (SL)  là một kĩ thuật học máy để học tập từ tập dữ liệu được gán nhãn cho trước. Tập dữ liệu cho trước sẽ chứa nhiều bộ dữ liệu. Mỗi bộ dữ liệu có cấu trúc theo cặp {x, y} với x được xem là dữ liệu thô (raw data) và y là nhãn của dữ liệu đó. Nhiệm vụ của SL là dự đoán đầu ra mong muốn dựa vào giá trị đầu vào. Dễ nhận ra, học có GIÁM SÁT tức là máy học dựa vào sự trợ giúp của con người, hay nói cách khác con người dạy cho máy học và giá trị đầu ra mong muốn được định trước bởi con người. Tập dữ liệu huấn luyện hoàn toàn được gán nhãn dựa vào con người. Tập càng nhỏ thì máy tính học càng ít. SL cũng được áp dụng cho 2 nhóm bài toán chính là bài toán dự đoán (regression problem) và bài toán phân lớp (classification problem). Kỹ thuật SL thực chất là để xây dựng một hàm có thể xuất ra giá trị đầu ra tương ứng với tập dữ liệu. Ta gọi hàm này là hàm h(x) và mong muốn hàm này xuất ra đúng giá trị y với một hoặc nhiều tập dữ liệu mới khác với dữ liệu được học. Hàm h(x) cần các loại tham số học khác nhau tùy thuộc với nhiều bài toán khác nhau. Việc học từ tập dữ liệu (training) cũng chính là tìm ra bộ tham số học cho hàm h(x). Tham gia ngay sự kiện Machine Learning – Công nghệ của Tương lai! Unsupervised learning  (UL)  là một kĩ thuật của máy học nhằm tìm ra một mô hình hay cấu trúc bị ẩn bơi tập dữ liệu KHÔNG được gán nhãn cho trước. UL khác với SL là không thể xác định trước output từ tập dữ liệu huấn luyện được. Tùy thuộc vào tập huấn luyện kết quả output sẽ khác nhau. Trái ngược với SL, tập dữ liệu huấn luyện của UL không do con người gán nhãn, máy tính sẽ phải tự học hoàn toàn. Có thể nói, học KHÔNG GIÁM SÁT thì giá trị đầu ra sẽ phụ thuộc vào thuật toán UL. Ứng dụng: Ứng dụng phổ biến nhất của học không giám sát là gom cụm (cluster). Đương nhiên sẽ có nhiều ứng dụng khác, có cơ hội tôi sẽ đề cập thêm. Ứng dụng này dễ nhận ra nhất là Google và Facebook. Google có thể gom nhóm các bài báo có nội dung gần nhau, hoặc Facebook có thể gợi ý kết bạn có nhiều bạn chung cho bạn. Các bài báo có cùng nội dung sẽ được gom lại thành một nhóm (cluster) phân biệt với các nhóm khác. Dữ liệu huấn luyện là các bài báo từ quá khứ tới hiện tại và tăng dần theo thời gian. Dễ nhận ra rằng dữ liệu không thể gán nhãn bởi con người. Khi một bài báo mới được cho vào input, nó sẽ tìm cụm (cluster) gần nhất với bài báo đó và gợi ý những bài liên quan. Nguồn tổng hợp:  SAS  &  CAPHUUQUAN TAGS machinelearning Facebook Twitter Bài viết trước Công cụ vọc Docker dành cho người lười Bài kế tiếp Bí mật: Stack Overflow đã deploy ra sao – phiên bản 2016 Điểm qua các thuật toán Machine Learning hiện đại Háo hức đợi sự kiện Data Science – Mỏ vàng của Kỉ nguyên số tại Tp.HCM Điểm lại những khoảnh khắc đáng nhớ của Thuật toán – Tế bào gốc của AI, Machine Learning, Blockchain 10 bước để viết thuật toán giải quyết các vấn đề trong lập trình October 9, 2017 Cơ hội 1 lần duy nhất hỏi trực tiếp chuyên gia bất kỳ điều gì mà bạn đang thắc mắc September 23, 2016 Hai con đường trở thành lập trình viên: Đại học và học đại May 6, 2017 Tâm sự một coder: hãy dũng cảm thành thật với con người mình October 23, 2017 Lập trình viên và chuyện học ngoại ngữ September 19, 2017 Những dòng code giết người – Đạo đức và cái tâm của developer June 27, 2017 “Ở Việt Nam, cơ hội để thực sự làm về Trí tuệ nhân tạo còn quá ít, trong khi những thứ mang hình thù... October 21, 2017 [ Học bổng] Học tiếng Nhật miễn phí cùng ” Akira”- chắp cánh ước mơ October 19, 2017 VWS2017 – Xây dựng Decentralized web trên nền tảng Blockchain October 19, 2017 VỀ CHÚNG TÔI • Giấy phép thiết lập Mạng xã hội số 569/GP-BTTTT do Bộ Thông Tin và Truyền Thông cấp.\r\n• Cơ quan chủ quản: Công ty Cổ phần Applancer.\r\nTrụ sở: 179 Đường Nguyễn Đình Chính, Phường 11, Quận PN, TP.HCM  Liên hệ:  hieuld@applancer.net - Tel: 028 6264 5022 THEO CHÚNG TÔI"
        },
        {
          "title": "Cục Giám sát quản lý về hải quan",
          "relevance": "0",
          "url": "https://www.customs.gov.vn/Lists/HaiQuanVietNam/Details.aspx?ID=45",
          "content": "You may be trying to access this site from a secured browser on the server. Please enable scripts and reload this page. Trang chủ   |   Sitemap   |   Liên hệ   |   ENGLISH \r\n\t\t\t\t\t\t\tTìm trong:\r\n\t\t\t\t\t\t\t Site List Giới thiệu        Thủ tục hải quan        Dịch vụ công        Hệ thống VNACCS/VCIS Tầm nhìn   |   Lịch sử   |   Lãnh đạo Tổng cục   |   Cơ cấu tổ chức   |   Chiến lược phát triển   |   Hợp tác quốc tế   |   Hải quan địa phương Cục Giám sát quản lý về hải quan Địa chỉ: Tòa nhà Tổng cục Hải quan - Lô E3 - đường Dương Đình Nghệ - Cầu Giấy - Hà Nội Điện thoại: (024) 39 440 833, máy lẻ 8825 FAX: (024) 39 440 620 Thư điện tử: cucgsql@customs.gov.vn Theo Quyết định số 1385/QĐ-BTC ngày 20/6/2016 của Bộ Tài chính, Cục Giám sát quản lý về hải quan có vị trí và chức năng, nhiệm vụ và quyền hạn, cơ cấu tổ chức như sau: Vị trí và chức năng: - Cục Giám sát quản lý về hải quan là đơn vị trực thuộc Tổng cục Hải quan, có chức năng tham mưu, giúp Tổng cục trưởng Tổng cục Hải quan quản lý, chỉ đạo, hướng dẫn các đơn vị trong ngành hải quan thực hiện thủ tục hải quan, kiểm tra, giám sát hải quan đối với hàng hóa xuất khẩu, nhập khẩu, quá cảnh, phương tiện vận tải xuất cảnh, nhập cảnh, quá cảnh; kiểm tra, xác định xuất xứ hàng hóa; hướng dẫn thực thi bảo vệ quyền sở hữu trí tuệ và nhãn mác hàng hóa theo quy định của pháp luật (sau đây gọi tắt là nghiệp vụ giám quản). - Cục Giám sát quản lý về hải quan có tư cách pháp nhân, con dấu riêng, được mở tài khoản tại Kho bạc Nhà nước và ngân hàng theo quy định của pháp luật. Nhiệm vụ và quyền hạn: 1. Trình Tổng cục trưởng Tổng cục Hải quan để trình Bộ trưởng Bộ Tài chính: - Văn bản quy phạm pháp luật; các văn bản quy định về nghiệp vụ giám quản thuộc thẩm quyền ban hành của Bộ trưởng Bộ Tài chính; - Các chương trình, kế hoạch, dự án, đề án về nghiệp vụ giám quản; đề án hiện đại hóa quy trình thủ tục hải quan, trang thiết bị, phương tiện phục vụ kiểm tra, giám sát hải quan; - Chương trình, kế hoạch để kiểm triển khai các giải pháp nâng cao hiệu lực, hiệu quả hoạt động kiểm tra chuyên ngành đối với hàng hóa xuất khẩu, nhập khẩu; xác định, chuẩn hóa mã số hàng hóa thuộc các Danh mục hàng hóa quản lý chuyên ngành phù hợp với Danh mục hàng hóa xuất khẩu, nhập khẩu Việt Nam; - Đề xuất kiến nghị cấp có thẩm quyền xem xét sửa đổi bổ sung các văn bản quy phạm pháp luật và văn bản quy đinh khác liên quan đến nghiệp vụ giám quản; - Quyết định ban hành mẫu ấn chỉ có liên quan đến thủ tục hải quan, kiểm tra, giám sát hải quan; - Quyết định thành lập, chấm dứt, di chuyển, mở rộng, thu hẹp, chuyển quyền sở hữu hoặc đổi tên chủ sở hữu: địa điểm làm thủ tục hải quan ngoài cửa khẩu, địa điểm làm thủ tục hải quan tại cảng xuất khẩu, nhập khẩu hàng hóa được thành lập trong nội địa, kho hàng không kéo dài và địa điểm khác theo quy định pháp luật; - Đề xuất giải quyết các vướng mắc có liên quan đến nghiệp vụ giám quản vượt quá thẩm quyền quyết định của Tổng cục trưởng Tổng cục Hải quan. 2. Trình Tổng cục trưởng Tổng cục Hải quan xem xét, quyết định: - Quy chế, quy trình nghiệp vụ về thủ tục hải quan và kiểm tra, giám sát hải quan; kiểm tra và xác định xuất xứ hàng hóa, thực thi bảo vệ quyền sở hữu trí tuệ và nhãn mác hàng hóa; hướng dẫn trả lời vướng mắc thuộc lĩnh vực nghiệp vụ giám quản thuộc thẩm quyền của Tổng cục trưởng Tổng cục Hải quan; - Kiến nghị cấp có thẩm quyền xem xét, sửa đổi, bổ sung quy định liên quan đến lĩnh vực nghiệp vụ giám quản thuộc thẩm quyền của các Bộ, cơ quan ngang Bộ, Ủy ban nhân dân tỉnh, thành phố liên quan;  - Quyết định thành lập, chấm dứt, di chuyển, mở rộng, thu hẹp, chuyển quyền sở hữu hoặc đổi tên chủ sở hữu: kho ngoại quan; kho bảo thuế; địa điểm kiểm tra hàng hóa tập trung; địa điểm thu gom hàng lẻ (CFS); địa điểm tập kết, kiểm tra hàng hóa xuất khẩu, nhập khẩu ở biên giới; đại lý làm thủ tục hải quan; địa điểm chuyển phát nhanh, địa điểm kiểm tra hải quan khác theo quy định pháp luật. 3. Tổ chức chỉ đạo, hướng dẫn, kiểm tra Cục Hải quan tỉnh, thành phố triển khai thực hiện các văn bản quy phạm pháp luật, quy chế, quy trình thuộc lĩnh vực nghiệp vụ giám quản theo quy định. 4. Giải quyết các vướng mắc của cơ quan hải quan các cấp, doanh nghiệp, tổ chức, cá nhân trong phạm vi lĩnh vực nghiệp vụ giám quản theo quy định của pháp luật. 5. Tham gia đàm phán các Hiệp định thương mại biên giới, Hiệp định quản lý biên giới, Hiệp định thương mại tự do; chủ trì triển khai thực hiện các cam kết quốc tế về thủ tục hải quan theo các Điều ước quốc tế mà Việt Nam là thành viên. 6. Xây dựng yêu cầu nghiệp vụ của hệ thống thông quan điện tử và các hệ thống kiểm tra, giám sát, quản lý về hải quan; nghiệm thu về mặt nghiệp vụ các hệ thống trước khi đưa vào vận hành. 7. Hướng dẫn kiểm tra, giám sát hoạt động của các đại lý làm thủ tục hải quan, các doanh nghiệp kinh doanh địa điểm, kho, bãi, dịch vụ giao nhận có liên quan đến hoạt động xuất nhập khẩu hàng hóa. 8. Phối hợp với các đơn vị liên quan thực hiện đầu tư, mua sắm trang thiết bị, phương tiện phục vụ hoạt động của Cục và nghiệp vụ kiểm tra, giám sát, quản lý của ngành Hải quan. 9. Tuyên truyền, phổ biến các văn bản pháp luật, các quy trình, quy chế nghiệp vụ trong lĩnh vực được phân công quản lý. 10. Tổ chức thực hiện các công việc về cải cách, hiện đại hóa và hội nhập quốc tế trong lĩnh vực nghiệp vụ giám quản theo quy định của pháp luật và phân công của Tổng cục trưởng Tổng cục Hải quan. 11. Tổ chức nghiên cứu khoa học, ứng dụng công nghệ hiện đại trong lĩnh vực được phân công theo quy định của pháp luật.  12. Tham gia xây dựng chương trình, giáo trình và giảng dạy nghiệp vụ thuộc lĩnh vực nghiệp vụ giám quản. 13. Tổng hợp, phân tích, đánh giá kết quả thực hiện chính sách pháp luật hải quan liên quan đến lĩnh vực nghiệp vụ giám quản. 14. Quản lý công chức và tài chính, tài sản được giao theo quy định của pháp luật và phân cấp quản lý của Bộ Tài chính.  15. Thực hiện các nhiệm vụ khác do Tổng cục trưởng Tổng cục Hải quan giao và theo quy định của pháp luật. Cơ cấu tổ chức: Cục Giám sát quản lý về hải quan có các phòng: 1. Phòng Tổng hợp. 2. Phòng Giám sát, quản lý hàng hóa xuất nhập khẩu (Phòng Giám quản 1). 3. Phòng Giám sát, quản lý hàng hóa đầu tư, gia công và sản xuất xuất khẩu (Phòng Giám quản 2). 4. Phòng Giám sát, quản lý hàng hóa khác (Phòng Giám quản 3). 5. Phòng Giám sát, quản lý xuất xứ; hàng hóa và sở hữu trí tuệ (Phòng Giám quản 4). 4. Phòng Giám sát, quản lý địa điểm và phương tiện xuất nhập cảnh (gọi tắt là Phòng Giám quản 5). Nhiệm vụ cụ thể của các phòng do Tổng cục trưởng Tổng cục Hải quan quy định. Biên chế của Cục Giám sát quản lý về hải quan do Tổng cục trưởng Tổng cục Hải quan quyết định trong tổng số biên chế được giao.  Về trang trước\r\n\t\t\t\t\t\t \r\n\t\t\t\t\t\t \r\n\t\t\t\t\t\t  Bản in\r\n\t\t\t\t\t\t \r\n\t\t\t\t\t\t \r\n\t\t\t\t\t\t  Về đầu trang\r\n\t\t\t\t\t\t DỊCH VỤ CÔNG Đăng ký sử dụng Hệ thống VNACCS cho doanh nghiệp Phân loại hàng hóa Tư vấn - Hỗ trợ về CSPL hải quan Tra cứu Biểu thuế - Phân loại - HS In bảng kê mã vạch phương tiện chứa hàng Tra cứu nợ thuế Tra cứu nộp thuế của tờ khai hải quan Tra cứu thông tin tờ khai hải quan Thông tin đại lý hải quan Thư viện văn bản Đăng ký Doanh nghiệp sử dụng chữ ký số Tiếp nhận thông tin doanh nghiệp Tiếp nhận thông tin vi phạm pháp luật hải quan Thống kê Hải quan Thông tin chung và tài khoản kho bạc của các đơn vị hải quan Tra cứu tỷ giá \r\n\t\t\t\t\t\tLIÊN KẾT\r\n\t\t\t\t\t Trang chủ   |   Giới thiệu   |   Trợ giúp   |   Sitemap   |   Liên hệ   |   ENGLISH"
        },
        {
          "title": "Khái quát về máy học",
          "relevance": "0",
          "url": "http://viet.jnlp.org/kien-thuc-co-ban-ve-xu-ly-ngon-ngu-tu-nhien/machine-learning-trong-nlp/khai-quat-ve-may-hoc",
          "content": "Tìm kiếm trang web này Trang chủ LY NAM PHONG Lưu Tuấn Anh Nguyễn Văn Hải Các công cụ xử lý Trích lọc tiếng Việt từ HTML DongDu Download dữ liệu Giới thiệu về các nghiên cứu mới [Máy học]Learning Combination Features with L1 Regularization [phân loại]Text Categorization with All Substring Features  [in Japanese] Automatic Tree and String Based Wrapper Generation for Semi-structured Documents Extracting Structured Data from Web Pages Online Feature Selection using Grafting Kiến thức cơ bản về xử lý ngôn ngữ tự nhiên Khởi đầu NLP với Python Liblinear-thư viện học máy Lựa chọn đặc trưng (Feature selection) Machine Learning trong NLP Mô hình ngôn ngữ NLP là gì ? Phân nhóm dữ liệu (Clustering) Thuật toán tách từ (Tokenizer) Xử lý tiếng Việt bằng Python (1) Ứng dụng Pointwise để tách từ Nghiên cứu của tác giả Bài toán thêm dấu cho tiếng Việt Việt hoá Mecab Nhập môn Linux SHELL là gì SHELL mạnh nhất : zsh Tài nguyên ngôn ngữ tiếng Việt Khái yếu về corpus Khái yếu về từ điển Kế hoạch xây dựng tự động corpus từ nguồn Web Đặc trưng của tiếng Việt Tạp đàm seminar là gì Sơ đồ trang web Hoạt động gần đây của trang web Tác giả trang anh tháng hai 5, 2012 Kiến thức cơ bản về xử lý ngôn ngữ tự nhiên ‎ > ‎ Machine Learning trong NLP ‎ > ‎\n   Khái quát về máy học Máy học  Bách khoa\ntoàn thư mở Wikipedia Học máy , có tài liệu gọi là  Máy học ,\n(tiếng Anh:  machine learning ) là một lĩnh vực của trí tuệ nhân\ntạo liên quan đến việc phát triển các kĩ thuật cho phép các máy tính có thể\n\"học\". Cụ thể hơn, học máy là một phương pháp để tạo ra các chương\ntrình máy tính bằng việc phân tích các tập dữ liệu. Học máy có liên quan lớn đến thống\nkê, vì cả hai lĩnh vực đều nghiên cứu việc phân tích dữ liệu, nhưng khác với thống\nkê, học máy tập trung vào sự phức tạp của các giải thuật trong việc thực thi\ntính toán. Nhiều bài toán suy luận được xếp vào loại bài toán NP-khó, vì\nthế một phần của học máy là nghiên cứu sự phát triển các giải thuật suy luận xấp\nxỉ mà có thể xử lí được. Học máy có tính ứng dụng\nrất cao bao gồm máy truy tìm dữ liệu, chẩn đoán y khoa, phát hiện thẻ\ntín dụng giả, phân tích thị trường chứng khoán, phân loại các chuỗi\nDNA, nhận dạng tiếng nói và chữ viết, dịch tự động, chơi\ntrò chơi và cử động rô-bốt ( robot locomotion ). Tương tác với con người Một số hệ thống học máy nỗ lực loại bỏ nhu cầu trực giác của con người\ntrong việc phân tích dữ liệu, trong khi các hệ thống khác hướng đến việc tăng sự\ncộng tác giữa người và máy.  Không\nthể loại bỏ hoàn toàn tác động của con người vì các nhà thiết kế hệ thống phải\nchỉ định cách biểu diễn của dữ liệu và những cơ chế nào sẽ được dùng để tìm kiếm\ncác đặc tính của dữ liệu. Học máy có thể được xem là một nỗ lực để tự động hóa\nmột số phần của   phương pháp\nkhoa học . Một số nhà nghiên cứu học máy tạo ra các phương\npháp bên trong các khuôn khổ của   thống kê\nBayes .   Các loại giải thuật Các thuật\ntoán học máy được phân loại theo kết quả mong muốn của thuật toán. Các loại\nthuật toán thường dùng bao gồm: §    Học\ncó giám sát -- trong đó, thuật toán tạo ra một hàm ánh xạ dữ liệu vào tới\nkết quả mong muốn. Một phát biểu chuẩn về một việc học có giám sát là bài toán  phân loại : chương trình cần học (cách xấp\nxỉ biểu hiện của) một hàm ánh xạ một vector [X1,X2,..,Xn]  tới\nmột vài lớp bằng cách xem xét một số mẫu dữ_liệu - kết_quả của hàm đó. §    Học\nkhông giám sát -- mô hình hóa một tập dữ liệu, không có sẵn các ví dụ đã\nđược gắn nhãn. §    Học\nnửa giám sát -- kết hợp các ví dụ có gắn nhãn và không gắn nhãn để sinh một\nhàm hoặc một bộ phân loại thích hợp. §    Học\ntăng cường -- trong đó, thuật toán học một chính sách hành động tùy theo\ncác quan sát về thế giới. Mỗi hành động đều có tác động tới môi trường, và môi\ntrường cung cấp thông tin phản hồi để hướng dẫn cho thuật toán của quá trình học. §    Chuyển đổi  -- tương tự\nhọc có giám sát nhưng không xây dựng hàm một cách rõ ràng. Thay vì thế, cố gắng\nđoán kết quả mới dựa vào các dữ liệu huấn luyện, kết quả huấn luyện, và dữ liệu\nthử nghiệm có sẵn trong quá trình huấn luyện. §    Học cách học  -- trong\nđó thuật toán học thiên kiến quy nạp của chính mình, dựa theo các\nkinh nghiệm đã gặp. Phân tích hiệu\nquả các thuật toán học máy là một nhánh của ngành  thống kê ,\nđược biết với tên  lý thuyết học điện toán . Các chủ đề về học máy Danh sách các\nchủ đề của môn học này: §    Mô\nhình hóa các hàm mật độ xác suất điều kiện: hồi quy và phân\nloại §    Mạng\nnơ-ron §    Cây\nquyết định §    Lập\ntrình biểu thức gen §    Lập\ntrình di truyền §    Hồi\nquy quá trình Gauss §    Phân\ntích biệt thức tuyến tính §    k\nláng giềng gần nhất §    Độ\ndài thông điệp tối thiểu §    Cảm\ntri nguyên §    Hàm\ncơ sở xuyên tâm §    Máy\nhỗ trợ vector §    Mô\nhình hóa các hàm mật độ xác suất qua các mô hình phát sinh: §    Thuật\ntoán cực đại kì vọng §    Các mô\nhình đồ họa gồm mạng Bayes và mạng Markov §    Ánh\nxạ topo phát sinh §    Các\nkỹ thuật suy luận xấp xỉ đúng: §    Chuỗi\nMarkov  phương pháp Monte Carlo §    Phương\npháp biến thiên §    Tối\nưu hóa: hầu hết các phương pháp trên đều sử dụng tối ưu hóa hoặc là các thể hiện\ncủa các thuật toán tối ưu hóa.   Comments Sign in | Recent Site Activity | Report Abuse | Print Page | Powered By Google Sites"
        },
        {
          "title": "Bài 2: Phân nhóm các thuật toán Machine Learning",
          "relevance": "1",
          "url": "https://machinelearningcoban.com/2016/12/27/categories/",
          "content": "Latest FundaML.com 33. Đánh giá hệ thống phân lớp (1/2) 32. Naive Bayes Classifier Viết và nhận xét các bài báo khoa học 31. Maximum Likelihood và Maximum A Posteriori Con đường học Toán của tôi 30. Ôn tập Xác Suất Q2. Transfer Learning 29. Linear Discriminant Analysis Q1. Quick Notes 1 28. Principal Component Analysis (2/2) 27. Principal Component Analysis (1/2) 26. Singular Value Decomposition 25. Matrix Factorization Collaborative Filtering 24. Neighborhood-Based Collaborative Filtering 23. Content-based Recommendation Systems 22. Multi-class SVM 21. Kernel SVM 20. Soft Margin SVM 19. Support Vector Machine 18. Duality 17. Convex Optimization Problems 16. Convex sets và convex functions 15. Overfitting 14. Multi-layer Perceptron và Backpropagation 13. Softmax Regression 12. Binary Classifiers 11. Feature Engineering 10. Logistic Regression 9. Perceptron Learning Algorithm 8. Gradient Descent (2/2) 7. Gradient Descent (1/2) 6. K-nearest neighbors 5. K-means Clustering - Applications 4. K-means Clustering 3. Linear Regression 2. Phân nhóm các thuật toán Machine Learning 1. Giới thiệu về Machine Learning Machine Learning cơ bản  About Index Tags Categories Archive Math Copyrights ebook Search Có hai cách phổ biến phân nhóm các thuật toán Machine learning. Một là dựa trên phương thức học (learning style), hai là dựa trên chức năng (function) (của mỗi thuật toán). Trong trang này: 1. Phân nhóm dựa trên phương thức học Supervised Learning (Học có giám sát) Classification (Phân loại) Regression (Hồi quy) Unsupervised Learning (Học không giám sát) Clustering (phân nhóm) Association Semi-Supervised Learning (Học bán giám sát) Reinforcement Learning (Học Củng Cố) 2. Phân nhóm dựa trên chức năng Regression Algorithms Classification Algorithms Instance-based Algorithms Regularization Algorithms Bayesian Algorithms Clustering Algorithms Artificial Neural Network Algorithms Dimensionality Reduction Algorithms Ensemble Algorithms 3. Tài liệu tham khảo 1. Phân nhóm dựa trên phương thức học Theo phương thức học, các thuật toán Machine Learning thường được chia làm 4 nhóm: Supervise learning, Unsupervised learning, Semi-supervised lerning và Reinforcement learning.  Có một số cách phân nhóm không có Semi-supervised learning hoặc Reinforcement learning. Supervised Learning (Học có giám sát) Supervised learning là thuật toán dự đoán đầu ra (outcome) của một dữ liệu mới (new input) dựa trên các cặp ( input, outcome ) đã biết từ trước. Cặp dữ liệu này còn được gọi là ( data, label ), tức ( dữ liệu, nhãn ). Supervised learning là nhóm phổ biến nhất trong các thuật toán Machine Learning. Một cách toán học, Supervised learning là khi chúng ra có một tập hợp biến đầu vào \\( \\mathcal{X} = \\{\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_N\\} \\) và một tập hợp nhãn tương ứng \\( \\mathcal{Y} = \\{\\mathbf{y}_1, \\mathbf{y}_2, \\dots, \\mathbf{y}_N\\} \\), trong đó \\( \\mathbf{x}_i, \\mathbf{y}_i \\) là các vector. \nCác cặp dữ liệu biết trước \\( (\\mathbf{x}_i, \\mathbf{y}_i) \\in \\mathcal{X} \\times \\mathcal{Y} \\) \nđược gọi là tập  training data  (dữ liệu huấn luyện). Từ tập traing data này, chúng ta cần tạo ra một hàm số ánh xạ mỗi phần tử từ tập \\(\\mathcal{X}\\) sang một phần tử (xấp xỉ) tương ứng của tập \\(\\mathcal{Y}\\): \\[ \\mathbf{y}_i \\approx f(\\mathbf{x}_i), ~~ \\forall i = 1, 2, \\dots, N\\] \nMục đích là xấp xỉ hàm số \\(f\\) thật tốt để khi có một dữ liệu \\(\\mathbf{x}\\) mới, chúng ta có thể tính được nhãn tương ứng của nó \\( \\mathbf{y} = f(\\mathbf{x}) \\). Ví dụ 1:  trong nhận dạng chữ viết tay, ta có ảnh của hàng nghìn ví dụ của mỗi chữ số được viết bởi nhiều người khác nhau. Chúng ta đưa các bức ảnh này vào trong một thuật toán và chỉ cho nó biết mỗi bức ảnh tương ứng với chữ số nào. Sau khi thuật toán tạo ra (sau khi  học ) một mô hình, tức một hàm số mà đầu vào là một bức ảnh và đầu ra là một chữ số, khi nhận được một bức ảnh mới mà mô hình  chưa nhìn thấy bao giờ , nó sẽ dự đoán bức ảnh đó chứa chữ số nào. MNIST : bộ cơ sở dữ liệu của chữ số viết tay.   (Nguồn:  Simple Neural Network implementation in Ruby) Ví dụ này khá giống với cách học của con người khi còn nhỏ. Ta đưa bảng chữ cái cho một đứa trẻ và chỉ cho chúng đây là chữ A, đây là chữ B. Sau một vài lần được dạy thì trẻ có thể nhận biết được đâu là chữ A, đâu là chữ B trong một cuốn sách mà chúng chưa nhìn thấy bao giờ. Ví dụ 2:  Thuật toán dò các khuôn mặt trong một bức ảnh đã được phát triển từ rất lâu. Thời gian đầu, facebook sử dụng thuật toán này để chỉ ra các khuôn mặt trong một bức ảnh và yêu cầu người dùng  tag friends  - tức gán nhãn cho mỗi khuôn mặt. Số lượng cặp dữ liệu ( khuôn mặt, tên người ) càng lớn, độ chính xác ở những lần tự động  tag  tiếp theo sẽ càng lớn. Ví dụ 3:  Bản thân thuật toán dò tìm các khuôn mặt trong 1 bức ảnh cũng là một thuật toán Supervised learning với training data (dữ liệu học) là hàng ngàn cặp ( ảnh, mặt người ) và ( ảnh, không phải mặt người ) được đưa vào. Chú ý là dữ liệu này chỉ phân biệt  mặt người  và  không phải mặt người  mà không phân biệt khuôn mặt của những người khác nhau. Thuật toán supervised learning còn được tiếp tục chia nhỏ ra thành hai loại chính: Classification (Phân loại) Một bài toán được gọi là  classification  nếu các  label  của  input data  được chia thành một số hữu hạn nhóm. Ví dụ: Gmail xác định xem một email có phải là spam hay không; các hãng tín dụng xác định xem một khách hàng có khả năng thanh toán nợ hay không. Ba ví dụ phía trên được chia vào loại này. Regression (Hồi quy) (tiếng Việt dịch là  Hồi quy , tôi không thích cách dịch này vì bản thân không hiểu nó nghĩa là gì) Nếu  label  không được chia thành các nhóm mà là một giá trị thực cụ thể. Ví dụ: một căn nhà rộng \\(x ~ \\text{m}^2\\), có \\(y\\) phòng ngủ và cách trung tâm thành phố \\(z~ \\text{km}\\) sẽ có giá là bao nhiêu? Gần đây  Microsoft có một ứng dụng dự đoán giới tính và tuổi dựa trên khuôn mặt . Phần dự đoán giới tính có thể coi là thuật toán  Classification , phần dự đoán tuổi có thể coi là thuật toán  Regression .  Chú ý rằng phần dự đoán tuổi cũng có thể coi là  Classification  nếu ta coi tuổi là một số nguyên dương không lớn hơn 150, chúng ta sẽ có 150 class (lớp) khác nhau. Unsupervised Learning (Học không giám sát) Trong thuật toán này, chúng ta không biết được  outcome  hay  nhãn  mà chỉ có dữ liệu đầu vào. Thuật toán unsupervised learning sẽ dựa vào cấu trúc của dữ liệu để thực hiện một công việc nào đó, ví dụ như phân nhóm (clustering) hoặc giảm số chiều của dữ liệu (dimension reduction) để thuận tiện trong việc lưu trữ và tính toán. Một cách toán học, Unsupervised learning là khi chúng ta chỉ có dữ liệu vào \\(\\mathcal{X} \\) mà không biết  nhãn  \\(\\mathcal{Y}\\) tương ứng. Những thuật toán loại này được gọi là Unsupervised learning vì không giống như Supervised learning, chúng ta không biết câu trả lời chính xác cho mỗi dữ liệu đầu vào. Giống như khi ta học, không có thầy cô giáo nào chỉ cho ta biết đó là chữ A hay chữ B. Cụm  không giám sát  được đặt tên theo nghĩa này. Các bài toán Unsupervised learning được tiếp tục chia nhỏ thành hai loại: Clustering (phân nhóm) Một bài toán phân nhóm toàn bộ dữ liệu \\(\\mathcal{X}\\) thành các nhóm nhỏ dựa trên sự liên quan giữa các dữ liệu trong mỗi nhóm. Ví dụ: phân nhóm khách hàng dựa trên hành vi mua hàng. Điều này cũng giống như việc ta đưa cho một đứa trẻ rất nhiều mảnh ghép với các hình thù và màu sắc khác nhau, ví dụ tam giác, vuông, tròn với màu xanh và đỏ, sau đó yêu cầu trẻ phân chúng thành từng nhóm. Mặc dù không cho trẻ biết mảnh nào tương ứng với hình nào hoặc màu nào, nhiều khả năng chúng vẫn có thể phân loại các mảnh ghép theo màu hoặc hình dạng. Association Là bài toán khi chúng ta muốn khám phá ra một quy luật dựa trên nhiều dữ liệu cho trước. Ví dụ: những khách hàng nam mua quần áo thường có xu hướng mua thêm đồng hồ hoặc thắt lưng; những khán giả xem phim Spider Man thường có xu hướng xem thêm phim Bat Man, dựa vào đó tạo ra một hệ thống gợi ý khách hàng (Recommendation System), thúc đẩy nhu cầu mua sắm. Semi-Supervised Learning (Học bán giám sát) Các bài toán khi chúng ta có một lượng lớn dữ liệu \\(\\mathcal{X}\\) nhưng chỉ một phần trong chúng được gán nhãn được gọi là Semi-Supervised Learning. Những bài toán thuộc nhóm này nằm giữa hai nhóm được nêu bên trên. Một ví dụ điển hình của nhóm này là chỉ có một phần ảnh hoặc văn bản được gán nhãn (ví dụ bức ảnh về người, động vật hoặc các văn bản khoa học, chính trị) và phần lớn các bức ảnh/văn bản khác chưa được gán nhãn được thu thập từ internet. Thực tế cho thấy rất nhiều các bài toán Machine Learning thuộc vào nhóm này vì việc thu thập dữ liệu có nhãn tốn rất nhiều thời gian và có chi phí cao. Rất nhiều loại dữ liệu thậm chí cần phải có chuyên gia mới gán nhãn được (ảnh y học chẳng hạn). Ngược lại, dữ liệu chưa có nhãn có thể được thu thập với chi phí thấp từ internet. Reinforcement Learning (Học Củng Cố) Reinforcement learning là các bài toán giúp cho một hệ thống tự động xác định hành vi dựa trên hoàn cảnh để đạt được lợi ích cao nhất (maximizing the performance). Hiện tại, Reinforcement learning chủ yếu được áp dụng vào Lý Thuyết Trò Chơi (Game Theory), các thuật toán cần xác định nước đi tiếp theo để đạt được điểm số cao nhất. AlphaGo chơi cờ vây với Lee Sedol. AlphaGo là một ví dụ của Reinforcement learning.   (Nguồn:  AlphaGo AI Defeats Sedol Again, With 'Near Perfect Game') Ví dụ 1: AlphaGo gần đây nổi tiếng với việc chơi cờ vây thắng cả con người .  Cờ vây được xem là có độ phức tạp cực kỳ cao  với tổng số nước đi là xấp xỉ \\(10^{761} \\), so với cờ vua là \\(10^{120} \\) và tổng số nguyên tử trong toàn vũ trụ là khoảng \\(10^{80}\\)!! Vì vậy, thuật toán phải chọn ra 1 nước đi tối ưu trong số hàng nhiều tỉ tỉ lựa chọn, và tất nhiên, không thể áp dụng thuật toán tương tự như  IBM Deep Blue  (IBM Deep Blue đã thắng con người trong môn cờ vua 20 năm trước). Về cơ bản, AlphaGo bao gồm các thuật toán thuộc cả Supervised learning và Reinforcement learning. Trong phần Supervised learning, dữ liệu từ các ván cờ do con người chơi với nhau được đưa vào để huấn luyện. Tuy nhiên, mục đích cuối cùng của AlphaGo không phải là chơi như con người mà phải thậm chí thắng cả con người. Vì vậy, sau khi  học  xong các ván cờ của con người, AlphaGo tự chơi với chính nó với hàng triệu ván chơi để tìm ra các nước đi mới tối ưu hơn. Thuật toán trong phần tự chơi này được xếp vào loại Reinforcement learning. (Xem thêm tại  Google DeepMind’s AlphaGo: How it works ). Ví dụ 2: Huấn luyện cho máy tính chơi game Mario . Đây là một chương trình thú vị dạy máy tính chơi game Mario. Game này đơn giản hơn cờ vây vì tại một thời điểm, người chơi chỉ phải bấm một số lượng nhỏ các nút (di chuyển, nhảy, bắn đạn) hoặc không cần bấm nút nào. Đồng thời, phản ứng của máy cũng đơn giản hơn và lặp lại ở mỗi lần chơi (tại thời điểm cụ thể sẽ xuất hiện một chướng ngại vật cố định ở một vị trí cố định). Đầu vào của thuật toán là sơ đồ của màn hình tại thời điểm hiện tại, nhiệm vụ của thuật toán là với đầu vào đó, tổ hợp phím nào nên được bấm. Việc huấn luyện này được dựa trên điểm số cho việc di chuyển được bao xa trong thời gian bao lâu trong game, càng xa và càng nhanh thì được điểm thưởng càng cao (điểm thưởng này không phải là điểm của trò chơi mà là điểm do chính người lập trình tạo ra). Thông qua huấn luyện, thuật toán sẽ tìm ra một cách tối ưu để tối đa số điểm trên, qua đó đạt được mục đích cuối cùng là cứu công chúa. Huấn luyện cho máy tính chơi game Mario 2. Phân nhóm dựa trên chức năng Có một cách phân nhóm thứ hai dựa trên chức năng của các thuật toán. Trong phần này, tôi xin chỉ liệt kê các thuật toán. Thông tin cụ thể sẽ được trình bày trong các bài viết khác tại blog này. Trong quá trình viết, tôi có thể sẽ thêm bớt một số thuật toán. Regression Algorithms Linear Regression Logistic Regression Stepwise Regression Classification Algorithms Linear Classifier Support Vector Machine (SVM) Kernel SVM Sparse Representation-based classification (SRC) Instance-based Algorithms k-Nearest Neighbor (kNN) Learning Vector Quantization (LVQ) Regularization Algorithms Ridge Regression Least Absolute Shrinkage and Selection Operator (LASSO) Least-Angle Regression (LARS) Bayesian Algorithms Naive Bayes Gaussian Naive Bayes Clustering Algorithms k-Means clustering k-Medians Expectation Maximization (EM) Artificial Neural Network Algorithms Perceptron Softmax Regression Multi-layer Perceptron Back-Propagation  Dimensionality Reduction Algorithms Principal Component Analysis (PCA) Linear Discriminant Analysis (LDA) Ensemble Algorithms Boosting AdaBoost Random Forest Và còn rất nhiều các thuật toán khác. 3. Tài liệu tham khảo A Tour of Machine Learning Algorithms Điểm qua các thuật toán Machine Learning hiện đại Share Share Interactive Learning Facebook page Machine Learning cơ bản Forum Recommended books \"Pattern recognition and Machine Learning.\", C. Bishop  \"The Elements of Statistical Learning\", T. Hastie et al.   \"Computer Vision:  Models, Learning, and Inference\", Simon J.D. Prince  \"Convex Optimization\", Boyd and Vandenberghe Recommended courses \"Machine Learning\", Andrew Ng  CS224n: Natural Language Processing with Deep Learning CS231n: Convolutional Neural Networks for Visual Recognition CS246: Mining Massive Data Sets CS20SI: Tensorflow for Deep Learning Research  Introduction to Computer Science and Programming Using Python Others Top-down learning path: Machine Learning for Software Engineers Blog này được tạo như thế nào? Chúng tôi đã apply và học tiến sỹ như thế nào? (1/2) Chúng tôi đã apply và học tiến sỹ như thế nào? (2/2) 8 Inspirational Applications of Deep Learning Matrix calculus TensorFlow-Examples Eight Easy Steps To Get Started Learning Artificial Intelligence The 9 Deep Learning Papers You Need To Know About"
        },
        {
          "title": "Hướng dẫn sử dụng quyền giám sát & mã hóa ứng dụng & bảo vệ quyền riêng tư Color OS 3.0",
          "relevance": "0",
          "url": "http://www.oppomobile.vn/forum/faq/huong-dn-s-dung-quyn-giam-sat-ma-hoa-ng-dung-bao-v-quyn-rieng-tu-color-os-3-0.202/",
          "content": "Tìm kiếm  Chỉ tìm trong tiêu đề Được đăng bởi thành viên: Ngăn cách tên bằng dấu phẩy (,). Mới hơn ngày: Tìm kiếm hữu ích Bài viết mới nhất Thêm... Tìm kiếm  Chỉ tìm trong tiêu đề Được đăng bởi thành viên: Ngăn cách tên bằng dấu phẩy (,). Mới hơn ngày: Tìm kiếm hữu ích Bài viết mới nhất Thêm... Diễn đàn OPPO Việt Nam  Trang tin FAQ > Hỗ trợ ColorOS 3.0 > FAQ #202 > Hướng dẫn sử dụng quyền giám sát & mã hóa ứng dụng & bảo vệ quyền riêng tư Color OS 3.0 Category  Tất cả Câu hỏi Tải Rom & Up Rom Sao lưu và khôi phục... Khôi phục cài đặt gốc Bộ nhớ & RAM Cài đặt chung Tổng hợp ColorOS Hỗ trợ ColorOS 3.0 Hỗ trợ  ColorOS 2.1 Hỗ trợ ColorOS 1.x - 2.0 Pin điện thoại Kết nối Ứng dụng Khắc phục sự cố Khác Phụ kiện chính hãng VIDEOS Hướng dẫn sử dụng Bảo hành Để kiểm tra phiên bản rom Color nào trên máy  :  Cài đặt - giới thiệu điện thoại - phiên bản Color OS.  A. Quyền giám sát :  Khi bật \"Quyền giám sát\" thì các ứng dụng bên thứ 3 khi sữ dụng dữ liệu mạng ,GPS , danh bạ, tin nhắn , lịch... sẽ được hệ thống thông báo như hình sau :  Ví dụ: Zalo cần truy xuất vào danh bạ trên máy các bạn  ​ *** Trên F1 Plus , F1s , A37  :  \nKhi cần sử dụng hoặc tắt quyền giám sát các bạn có thể thao tác như sau :  Cài đặt -> Trung Tâm Bảo Mật -> Quyền giám sát ​ *** Trên A39/ F3/ F3 Plus : Bỏ tùy chọn BẬT/TẮT Quyền giám sát trong \"Trung tâm bảo mật\" ​ *** Lưu ý :  \n- Các cài đặt khác trong Quyền giám sát của các máy Color 3.0 là tương tự nhau. ​ - Cho phép chỉ 1 hay 1 số ứng dụng hoàn toàn có quyền truy cập dữ liệu trên máy còn các ứng dụng bên thứ 3 thì không như sau : ​ B. Mã hóa ứng dụng  :  -  Đường dẫn các máy Color 3.0 chung   : Vào ứng dụng Trung Tâm Bảo Mật -> Quyền Bảo Mật -> Mã Hoá Ứng Dụng -> Chọn ứng dụng cần mã hoá -> Sau đó Bật mã hoá -> đặt mật khẩu hình vẽ và thêm câu hỏi bảo mật để khôi phục khi quên mật khẩu hình vẽ. \n-  Riêng F3/ F3 Plus  : Cài đặt  -> Trung tâm bảo mật  -> Cài đặt mã hóa ứng dụng  Chọn ứng dụng cần mã hoá -> Sau đó Bật mã hoá -> đặt mật khẩu hình vẽ và thêm câu hỏi bảo mật để khôi phục khi quên mật khẩu hình vẽ. Minh họa trên F3/ F3 Plus ​ Lưu ý  : -  Riêng Color 3.0  : Khi sử dụng tính năng mã hóa ứng dụng và  đã BẬT mã hóa Tin nhắn  =>  Có tin nhắn đến  từ 1 số bạn  đã lưu trong danh bạ  => Lúc này bạn  chỉ thấy được số điện thoại gửi tin nhắn và  KHÔNG thấy TÊN ĐÃ LƯU TRONG DANH BẠ VÀ NỘI DUNG TIN NHẮN   ở 3 vị trí:  Màn hình khóa/  Thanh thông báo/Thanh trạng thái Ưu điểm :   Tính năng này sẽ giúp bạn bảo mật luôn cả phần tin nhắn ( gửi từ ai & nội dung tin nhắn là gì ) cộng thêm phần cài mật khẩu riêng cho ứng dụng đó rồi. ​ C. Bảo mật quyền riêng tư :  -  Đường dẫn các máy Color 3.0 chung  :  Trung tâm bảo mật ( trên màn hình chính ) > Quyền bảo mật : KHÔNG còn Bảo vệ riêng tư -  Riêng F3/ F3 Plus đã  lược bỏ   \"Bảo vệ quyền riêng tư\" trong Trung tâm bảo mật - Thêm 1 tính năng mở rộng ở Trung Tâm Bảo Mật trên Color OS 3.0 đó là bảo vệ Quyền Riêng Tư : A. Nội dung bảo vệ : - Liên hệ riêng ( Bao gồm lịch sử cuộc gọi và tin nhắn SMS của liên hệ đó ) - Ứng dụng riêng tư - Tệp riêng tư ( Bao gồm ảnh, video, file ghi âm, các tập tin riêng tư khác ) B. Chức năng : 1. Kích hoạt chức năng bảo vệ  : Khi bật lên  các nội dung bảo vệ ở danh sách liệt kê trên sẽ sẽ bị ẩn hoàn toàn , nghĩa là không còn thấy ứng dụng đó trên màn hình chính mà chỉ khi vào lại Bảo vệ quyền riêng tư - thư mục chứa ứng dụng/ liên hệ / tập tin nào đang bị ẩn mới thấy được. 2. Ẩn nâng cao : Sau khi chức năng bảo vệ và ẩn nâng cao bật lên thì nguyên phần bảo vệ quyền riêng tư sẽ ẩn luôn. Muốn hiện trở lại  thì chạm vào biểu tượng chìa khóa trong Trung tâm bảo mật 3 lần để nó hiện ra màn hình nhập mật khẩu vào chế độ bảo vệ quyền riêng tư là xong. Ẩn nâng cao : ẩn luôn phần bảo vệ quyền riêng tư ​ ​ ​ Chúc các bạn thực hiện thành công !!! ​ Category: Hỗ trợ ColorOS 3.0 Published: 12/5/16 Page Views: 2250 FAQ Manager ©2017  Iversia  from  RPGfix . Log in with Facebook Log in with Google Tên tài khoản hoặc địa chỉ Email: Bạn đã có tài khoản rồi? \n\t\t\t\t\t\tTích vào đây để đăng ký \n\t\t\t\t\t\tVâng, Mật khẩu của tôi là: Bạn đã quên mật khẩu?  Ghi nhớ? Tham gia ngay! FAQ #202 \n                    To give another user a direct link to this FAQ answer, use the following BB Code in your post: \n                     Need More Help? \n                    Did this answer your question? If not, please feel free to request assistance on our forum.\n                 Diễn đàn OPPO Việt Nam  Trang tin FAQ > Hỗ trợ ColorOS 3.0 > FAQ #202 > Trang tin Trang tin Liên kết nhanh Hoạt động gần đây Có gì mới? Trợ giúp Diễn đàn Diễn đàn Liên kết nhanh Tìm kiếm Bài viết mới nhất Popular Content  Sản phẩm  FAQ FAQ Liên kết nhanh Tải Rom & Up Rom Sao lưu và khôi phục dữ liệu Khôi phục cài đặt gốc Bộ nhớ & RAM Cài đặt chung Tổng hợp ColorOS Hỗ trợ ColorOS 3.0 Hỗ trợ  ColorOS 2.1 Hỗ trợ ColorOS 1.x - 2.0 Pin điện thoại Kết nối Ứng dụng Khắc phục sự cố Khác Phụ kiện chính hãng VIDEOS Hướng dẫn sử dụng Bảo hành Thư viện ảnh  Thư viện ảnh  Liên kết nhanh Chuyên mục Khám phá Album Album mới Hình ảnh mới New Videos Tags Cloud Menu Đăng nhập/Đăng ký Ngôn ngữ Tiếng Việt (VN) Trợ giúp FAQ Trang tin Lên đầu trang RSS Diễn đàn OPPO Việt Nam Trang chủ Diễn đàn Nội quy Hỗ trợ và Dịch vụ Trung tâm CSKH Tải Rom Chính sách bảo hành Tổng đài hỗ trợ miễn phí 1800-577-776 Hỗ trợ trực tuyến Giờ làm việc: 8h00 - 17h30\n\t\t\t từ Thứ 2 tới Thứ 7 Công Ty TNHH Một Thành Viên Kỹ Thuật Và Khoa Học OPPO, Tòa Nhà SCB, 242 Cống Quỳnh, Q.1, TP.HCM Giấy ĐKKD: 0312059023 cấp ngày 21/11/2012 ▲"
        },
        {
          "title": "Giám sát tài sản 1.000 cán bộ cấp cao: Cần bàn tay sắt và sạch",
          "relevance": "0",
          "url": "http://tuoitre.vn/giam-sat-tai-san-1000-can-bo-cap-cao-can-ban-tay-sat-va-sach-1321854.htm",
          "content": "iTuyển sinh Cần biết Nhà đất Đặt báo Bạn đọc làm báo 0918.033.133 Thứ 2, ngày 23.10.2017 Media Thời sự Thế giới Pháp luật Kinh doanh Công nghệ Xe Du lịch Nhịp sống trẻ Văn hóa Giải trí Thể thao Giáo dục Khoa học Sức khỏe Giả-thật Thư giãn Thời sự Xã hội Phóng sự Nghĩ RSS \r\n                                        Giám sát tài sản 1.000 cán bộ cấp cao: Cần bàn tay sắt và sạch\r\n                                     28/05/2017 08:59 GMT+7 TTO - Để làm trong sạch bộ máy phải lựa chọn, bổ nhiệm được những con người có tài năng, phẩm hạnh, lương tâm - Đại biểu Quốc hội Lê Thanh Vân ủng hộ kiểm tra, giám sát tài sản của khoảng 1.000 quan chức. Kiểm tra việc kê khai tài sản của cán bộ Vì sao bản kê khai tài sản chủ tịch Đà Nẵng bị tuồn ra ngoài? Hơn 1 triệu bản kê khai tài sản, không phát hiện tham nhũng Biệt thự “khủng” tại TP Bến Tre của ông Trần Văn Truyền - nguyên tổng Thanh tra Chính phủ - đã bị Ủy ban Kiểm tra trung ương kết luận là gây phản cảm và tạo dư luận xấu trong xã hội - Ảnh: Ngọc Tài Ông Lê Thanh Vân là ủy viên thường trực Ủy ban Tài chính - ngân sách của Quốc hội. Trao đổi với phóng viên  Tuổi Trẻ  ngày 27-5, ông nói: “Tôi thấy Phó chủ nhiệm Ủy ban Kiểm tra trung ương Lê Thị Thủy đã nói rõ rồi, Ủy ban Kiểm tra sẽ kiểm tra, giám sát khoảng 1.000 cán bộ thuộc đối tượng Bộ Chính trị, Ban Bí thư quản lý. Đây là đội ngũ cán bộ cấp cao nhất của Đảng, Nhà nước ta đang giữ những vị trí quan trọng nhất trong bộ máy. Theo trả lời của bà Thủy được báo chí đăng tải, Ủy ban Kiểm tra trung ương sẽ kiểm tra, giám sát tài sản của các quan chức này theo kế hoạch, khi có yêu cầu của cơ quan tổ chức có thẩm quyền, khi có kiến nghị, phản ánh, đơn thư tố cáo việc kê khai tài sản không trung thực và khi có dấu hiệu vi phạm các quy định về kê khai tài sản. Tôi cho rằng nếu làm tốt việc này thì nhân dân rất ủng hộ”. * Thưa ông, người dân rất trông đợi vào kết quả của công tác này, nhưng chắc rằng dư luận cũng băn khoăn bởi việc kê khai, xác minh tài sản đã được quy định trong Luật phòng, chống tham nhũng nhưng lâu nay thực hiện không hiệu quả? - Tôi ủng hộ việc kiểm tra, giám sát trên bởi trước hết là thể hiện quyết tâm của Đảng mà người đứng đầu là Tổng bí thư Nguyễn Phú Trọng đã chỉ đạo rất quyết liệt trong thời gian qua. Nhiều vụ việc sau khi có ý kiến của Tổng bí thư đã được khẩn trương làm rõ, xử lý trước công luận. Vấn đề là giữa kỳ vọng và hiện thực, cách làm đem đến hiệu quả như thế nào. Chúng ta thấy rằng Thủ tướng Nguyễn Xuân Phúc cũng thể hiện quyết tâm xây dựng Chính phủ liêm chính, hành động, kiến tạo phát triển, nhưng với một bộ máy như thế thì bao giờ mong muốn sẽ thành hiện thực? Bộ máy mà trước đây chính Thủ tướng Nguyễn Xuân Phúc đã từng nêu đánh giá rằng có đến “30% sáng cắp ô đi tối cắp ô về” thì phải làm thế nào để thực sự liêm chính,  kiến tạo? Tôi cho rằng sự vận hành của bộ máy quyết định đến hiệu quả của các chủ trương, chính sách, kế hoạch của Đảng và Nhà nước. Muốn như vậy, những bức xúc đang xảy ra như tình trạng bổ nhiệm người nhà, cả họ làm quan... mà báo chí nêu cần phải xử lý rất mạnh mẽ. Việc quan trọng nhất để làm trong sạch bộ máy là phải lựa chọn, bổ nhiệm được những con người có tài năng, phẩm hạnh, lương tâm. Nói riêng về chủ trương kiểm tra, giám sát tài sản của 1.000 quan chức, tôi nghĩ nếu muốn làm ra tấm ra món để nhân dân tin tưởng thì Đảng phải chọn được một đội ngũ tham mưu, thực hiện đủ mạnh bao gồm những người thật sự tinh túy, những người có bàn tay sắt và bàn tay ấy phải sạch. Tôi rất mong Tổng bí thư tiến hành việc này mạnh mẽ như Bác Hồ đã từng làm. Tôi nghĩ, việc trước hết là tạo ra không khí dân chủ, thẳng thắn trong nội bộ, từ trên xuống dưới, không để tình trạng thấy đúng không dám bảo vệ, thấy sai không dám phản đối. Ông Lê Thanh Vân - Ảnh: V.H. Tài sản chuyển dịch sang con cái, xử lý thế nào? * Nhiều chuyên gia khẳng định sở dĩ quy định kê khai, công khai tài sản trong Luật phòng, chống tham nhũng không phát huy tác dụng trước hết là do tính “nửa vời” của các quy định này (như công khai phạm vi hẹp, kê khai không gắn với trách nhiệm giải trình), đồng thời với tình trạng không kiểm soát được dịch chuyển của tài sản, trong một nền kinh tế tiền mặt với vàng, đôla dễ dàng cất trữ trong nhà... - Tôi nghĩ rằng kiểm tra thì một phần dựa trên kê khai của đối tượng chịu sự kiểm tra, nhưng quan trọng hơn là phải dựa vào nhân dân, báo chí để làm rõ. Có những khối tài sản chuyển dịch sang con cái, bố mẹ, anh em mà không làm rõ nguồn gốc thì làm sao mà xử lý được. Bố làm quan chức, con mới đi học nước ngoài về thời gian ngắn đã sở hữu biệt thự tiền tỉ, xe sang... thì những tài sản ấy ở đâu ra? Nếu không có các biện pháp buộc giải trình nguồn gốc tài sản và xử lý tài sản không giải trình rõ được nguồn gốc thì đúng là còn băn khoăn về hiệu quả của việc giám sát, kiểm soát tài sản. Chính vì vậy, chúng ta vừa trông đợi Quốc hội sớm sửa đổi, bổ sung các quy định của pháp luật vừa trông đợi vào tính hiệu quả của các biện pháp mạnh của Đảng. Tôi ủng hộ việc Đảng ta áp dụng các biện pháp đặc biệt, mạnh tay trừng trị nghiêm khắc quan tham. Trước hết, cần tập trung làm rõ một số vụ việc, một số nhân vật mà dư luận, nhân dân đặt nghi vấn. * Ví dụ như vụ việc “phố biệt thự” của một số quan chức ở Lào Cai mà báo chí nêu mấy hôm nay cần ưu tiên làm trước, công bố rõ. Nếu như cán bộ chứng minh được thu nhập, giải trình được nguồn gốc tài sản thì cũng tránh để dư luận hiểu nhầm, suy diễn? - Tôi nghĩ cần ưu tiên làm trước những vụ việc như vậy, đúng sai thế nào cần sớm công bố để nhân dân biết. Cán bộ mà giàu chính đáng, tài sản không mờ ám thì phải giải trình, sau đó kiểm tra làm rõ. Như vậy, việc kiểm tra, giám sát tài sản một cách minh bạch cũng là để bảo vệ những cán bộ trong sáng và giàu chính đáng. Chi bộ tham gia giám sát Bộ Chính trị vừa ban hành quy định kiểm tra, giám sát việc kê khai tài sản đối với cán bộ thuộc diện Bộ Chính trị, Ban Bí thư quản lý. Nội dung kiểm tra, giám sát việc kê khai tài sản gồm việc thực hiện các quy định về kê khai tài sản, biến động tài sản hằng năm thuộc quyền sở hữu, quyền sử dụng của cán bộ thuộc đối tượng kiểm tra, giám sát, của vợ hoặc chồng và con chưa thành niên; việc giải trình biến động tài sản và nguồn gốc của tài sản tăng thêm. Đối với việc kiểm tra, Bộ Chính trị, Ban Bí thư, Ủy ban Kiểm tra trung ương được quyền yêu cầu đối tượng kiểm tra cung cấp thông tin, báo cáo, giải trình về việc kê khai tài sản, biến động tài sản và giải trình nguồn gốc tài sản tăng thêm; yêu cầu cơ quan, tổ chức, cá nhân có liên quan cung cấp thông tin, hồ sơ, tài liệu liên quan đến tài sản, thu nhập của cán bộ được kiểm tra. Xác minh, kết luận về sự trung thực, đầy đủ, rõ ràng, kịp thời của việc kê khai tài sản và biến động tài sản phải kê khai, tính xác thực, hợp pháp về nguồn gốc của tài sản tăng thêm; xử lý hoặc kiến nghị cơ quan, tổ chức có thẩm quyền xem xét, xử lý các hành vi vi phạm trong việc kê khai tài sản. Đối với việc giám sát, ngoài Bộ Chính trị, Ban Bí thư, Ủy ban Kiểm tra trung ương, chi bộ nơi cán bộ đang sinh hoạt cũng sẽ tham gia giám sát, trong đó có quyền yêu cầu đối tượng giám sát cung cấp thông tin, báo cáo, giải trình về việc kê khai tài sản, biến động tài sản và nguồn gốc của tài sản tăng thêm. K.HƯNG - Đ.BÌNH TP.HCM: 10 năm có... 1 trường hợp kê khai tài sản không trung thực TTO - Trong thời gian từ ngày 1-1-2007 đến 30-6-2016, đã xảy ra 1 trường hợp xác minh việc kê khai tài sản, thu nhập và đã bị kết luận kê khai không trung thực. LÊ KIÊN thực hiện  quan tâm 0 Chia sẻ facebook Chia sẻ google Thông tin của bạn Email Vui lòng nhập Email Email Không đúng định dạng Họ và tên Vui lòng nhập Họ & Tên. Gửi bình luận Thông báo \r\n                    Bạn vui lòng đợi  0 s để tiếp tục comment\r\n                 Thông báo \r\n                    Bình luận được gửi thành công\r\n                 Tuổi Trẻ News  Tuổi Trẻ Cuối Tuần  Truyền Hình Tuổi Trẻ Gửi bài về tòa soạn Media Thời sự Thế giới Pháp luật Kinh doanh Nhịp sống trẻ Văn hóa Giải trí Thể thao Công nghệ Giáo dục Khoa học Sức khỏe Giả-thật Thư giãn Bạn đọc làm báo Theo dõi Tuổi trẻ \r\n                © Copyright 2017 TUOITRE.VN, All rights reserved\r\n             \r\n                ® Tuổi Trẻ Online giữ bản quyền nội dung trên website này\r\n             Thông tin Tòa soạn  -\r\n\t\t\t Thông tin Thành Đoàn  -\r\n\t\t\t Liên hệ Quảng cáo Đường dây nóng:  0918.033.133  - Liên hệ Quảng cáo:  0283.997.4848"
        },
        {
          "title": "Luật số 08/2012/QH13 của Quốc hội : LUẬT GIÁO DỤC ĐẠI HỌC",
          "relevance": "0",
          "url": "http://www.chinhphu.vn/portal/page/portal/chinhphu/hethongvanban?mode=detail&document_id=163054",
          "content": "QUỐC HỘI"
        },
        {
          "title": "Kỳ thi THPT quốc gia 2017: Một giảng viên “kèm” một giáo viên",
          "relevance": "0",
          "url": "http://tuoitre.vn/ky-thi-thpt-quoc-gia-2017-mot-giang-vien-kem-mot-giao-vien-1275821.htm",
          "content": "iTuyển sinh Cần biết Nhà đất Đặt báo Bạn đọc làm báo 0918.033.133 Thứ 2, ngày 23.10.2017 Media Thời sự Thế giới Pháp luật Kinh doanh Công nghệ Xe Du lịch Nhịp sống trẻ Văn hóa Giải trí Thể thao Giáo dục Khoa học Sức khỏe Giả-thật Thư giãn Giáo dục Học đường Du học Câu chuyện giáo dục Góc học tập RSS \r\n                                        Kỳ thi THPT quốc gia 2017: Một giảng viên “kèm” một giáo viên\r\n                                     07/03/2017 08:22 GMT+7 TTO - Bộ GD-ĐT vừa có công văn đề nghị huy động giảng viên tham gia coi thi THPT quốc gia trên cả nước, đảm bảo đúng tinh thần quy chế mỗi phòng thi có một giảng viên ĐH và một giáo viên THPT coi thi. Giao lưu trực tuyến: Đề thi và cách làm bài thi THPT quốc gia Kỳ thi THPT quốc gia 2017: Thi theo tổ hợp, ôn theo ban Học sinh lớp 12 Hà Nội tập dượt thi THPT quốc gia Giảng viên Trường ĐH Sư phạm TP.HCM coi thi trong kỳ thi THPT quốc gia năm 2016 - Ảnh: NHƯ HÙNG Các trường ĐH đều khẳng định sẵn sàng thực hiện nhiệm vụ nhưng cũng cùng chung nỗi lo lắng: làm sao đảm bảo an toàn, thuận lợi cho các thầy cô tham gia coi thi... Lo điều kiện đi lại, ăn ở Trao đổi với  Tuổi Trẻ , GS.TS Vũ Văn Hóa - phó hiệu trưởng Trường ĐH Kinh doanh và công nghệ Hà Nội - cho biết năm 2016 trường huy động hơn 300 giảng viên ĐH tham gia coi thi. Tuy nhiên năm nay, theo đề nghị của Bộ GD-ĐT, trường sẽ huy động tổng số 700 người tham gia coi thi. “Số lượng giảng viên coi thi năm nay tăng gấp đôi năm ngoái. Nhưng do xác định phải đi các tỉnh xa, nên trường sẽ huy động toàn bộ giảng viên trẻ và cử 5-7 cán bộ nòng cốt tham gia chỉ đạo đoàn. Để công tác coi thi hiệu quả, an toàn, 700 giảng viên này sẽ được tập huấn trước khi phân về các địa phương. Ngoài phổ biến quy chế thi, trường cũng sẽ đặt ra những tiêu chuẩn, quy định cụ thể để đảm bảo an toàn trong đi lại, sinh hoạt cho các thầy cô” - ông Hóa nói. Theo ông Hóa, năm 2016 đoàn giảng viên của trường về một huyện của Hưng Yên coi thi mà cả huyện chỉ có... hai nhà nghỉ. “Trong đó một nhà nghỉ bị mất điện, chúng tôi phải đề xuất điểm trưởng cũng là hiệu trưởng một trường THPT bố trí chỗ nghỉ cho thầy cô ngay trong trường THPT của huyện” - ông Hóa kể. Năm nay, để thuận lợi hơn trong công tác coi thi, Trường ĐH Kinh doanh và công nghệ Hà Nội đã chủ động xin đề xuất được tham gia coi thi ở tỉnh Hải Dương. “Khó huy động 100% giảng viên tham gia coi thi” Theo tính toán của một chuyên gia tuyển sinh, nếu số thí sinh tham dự kỳ thi THPT quốc gia năm 2017 là hơn 1 triệu người thì với quy tắc 24 thí sinh/phòng thi, dự kiến có 40.000 - 45.000 giảng viên ĐH sẽ được huy động tham gia coi thi. PGS.TS Nguyễn Văn Thư - hiệu trưởng Trường ĐH Giao thông vận tải TP.HCM - cho rằng đối với các điểm thi ở vùng sâu vùng xa, việc di chuyển, ăn ở trong các ngày thi là vấn đề đáng lưu tâm. Hơn nữa, mọi năm kỳ thi THPT quốc gia vào tháng 7 là thời điểm các trường đã hoàn tất việc thi học kỳ. Năm nay kỳ thi rơi vào tháng 6, lúc này ở các trường vẫn đang diễn ra thi học kỳ. Như vậy, các trường phải cân đối giảng viên sao cho phù hợp với lịch thi học kỳ của trường (đã thông báo từ đầu học kỳ). “Do đó, đối tượng đi coi thi khó có thể đảm bảo tất cả đều là giảng viên. Trường có thể phải huy động thêm cán bộ, nhân viên làm nhiệm vụ này trong trường hợp cần số lượng cán bộ coi thi nhiều” - ông Thư nói. TS Mỵ Giang Sơn - trưởng phòng đào tạo Trường ĐH Sài Gòn - cho biết rất khó huy động 100% giảng viên tham gia. Năm 2016, Trường ĐH Sài Gòn bố trí xe đưa giảng viên về tỉnh làm nhiệm vụ coi thi. Không phải tất cả giảng viên đều coi đủ 8 buổi nên sau 3, 4 hay 5 buổi thi, trường phải bố trí xe đưa giảng viên trở về vì đường đi rất xa. Ông Sơn đề xuất các tỉnh cũng cần bố trí phương án tổ chức xe đưa đón giảng viên đi coi thi. Bởi lẽ nếu để giảng viên tự đi bằng xe cá nhân thì “thứ nhất nguy hiểm, thứ hai có thể có sự cố không đảm bảo đúng thời gian thực hiện nhiệm vụ”. “Năm ngoái, chúng tôi huy động 100% giảng viên, cán bộ có trình độ ĐH trở lên đi coi thi. Người nào vắng phải có chứng minh cụ thể và được sự đồng ý của hiệu trưởng. Vậy mà cuối cùng số lượng vẫn không đủ, phải huy động thêm sinh viên năm 4. Nếu giảng viên không muốn đi coi thi, họ vẫn có đủ lý do để có thể đối phó” - ông Sơn nói. Giảng viên coi thi sẽ được hỗ trợ kinh phí Ngày 6-3, trao đổi với  Tuổi Trẻ , ông Trần Văn Nghĩa, phó cục trưởng Cục Khảo thí và kiểm định chất lượng giáo dục Bộ GD-ĐT, cho biết trên cơ sở số liệu báo cáo từ các địa phương về số lượng thí sinh tham gia dự thi năm 2017, căn cứ năng lực thực tế của các trường, bộ sẽ điều động cán bộ, giảng viên từ các trường ĐH, CĐ sư phạm về các địa phương với số lượng đáp ứng các quy định của quy chế. Việc tổ chức thi theo hình thức phối hợp giữa địa phương và sự tham gia của các giảng viên từ các trường ĐH đã được thực hiện từ năm 2015, 2016, đặc biệt năm 2016 đã huy động cụm thi do sở GD-ĐT chủ trì, có sự tham gia của các trường ĐH, CĐ trong các khâu coi thi, chấm thi. “Về cơ bản, việc phối hợp này là thuận lợi. Như vậy, hình thức phối hợp giữa địa phương và các trường ĐH, CĐ trong tổ chức thi THPT quốc gia đã có kinh nghiệm, đã có sự chuẩn bị, các địa phương sẵn sàng tạo điều kiện thuận lợi cho các trường phối hợp trong ăn nghỉ, đảm bảo an ninh, an toàn cho cán bộ giảng viên về địa phương trong thời gian công tác. Các cán bộ, giảng viên từ các trường ĐH sẽ được hỗ trợ kinh phí đi lại, ăn ở và kinh phí tham gia tổ chức thi (coi thi, chấm thi...)” - ông Nghĩa nhấn mạnh. MINH GIẢNG - NGỌC HÀ quan tâm 0 Chia sẻ facebook Chia sẻ google Thông tin của bạn Email Vui lòng nhập Email Email Không đúng định dạng Họ và tên Vui lòng nhập Họ & Tên. Gửi bình luận Thông báo \r\n                    Bạn vui lòng đợi  0 s để tiếp tục comment\r\n                 Thông báo \r\n                    Bình luận được gửi thành công\r\n                 Tuổi Trẻ News  Tuổi Trẻ Cuối Tuần  Truyền Hình Tuổi Trẻ Gửi bài về tòa soạn Media Thời sự Thế giới Pháp luật Kinh doanh Nhịp sống trẻ Văn hóa Giải trí Thể thao Công nghệ Giáo dục Khoa học Sức khỏe Giả-thật Thư giãn Bạn đọc làm báo Theo dõi Tuổi trẻ \r\n                © Copyright 2017 TUOITRE.VN, All rights reserved\r\n             \r\n                ® Tuổi Trẻ Online giữ bản quyền nội dung trên website này\r\n             Thông tin Tòa soạn  -\r\n\t\t\t Thông tin Thành Đoàn  -\r\n\t\t\t Liên hệ Quảng cáo Đường dây nóng:  0918.033.133  - Liên hệ Quảng cáo:  0283.997.4848"
        },
        {
          "title": "Học không có giám sát",
          "relevance": "0",
          "url": "https://vi.wikipedia.org/wiki/H%E1%BB%8Dc_kh%C3%B4ng_c%C3%B3_gi%C3%A1m_s%C3%A1t",
          "content": "Bách khoa toàn thư mở Wikipedia \n\t\t\t\t\tBước tới:\t\t\t\t\t menu , \t\t\t\t\t tìm kiếm Bài viết này  cần thêm  chú thích nguồn gốc  để  kiểm chứng thông tin .  Mời bạn giúp  hoàn thiện bài viết này  bằng cách bổ sung chú thích tới  các nguồn đáng tin cậy . Các nội dung không có nguồn có thể bị nghi ngờ và xóa bỏ. Học không có giám sát  (tiếng Anh là  unsupervised learning ) là một phương pháp của ngành  học máy  nhằm tìm ra một mô hình mà phù hợp với các quan sát. Nó khác biệt với  học có giám sát  ở chỗ là đầu ra đúng tương ứng cho mỗi đầu vào là không biết trước. Trong học không có giám sát, một tập dữ liệu đầu vào được thu thập. Học không có giám sát thường đối xử với các đối tượng đầu vào như là một tập các  biến ngẫu nhiên . Sau đó, một  mô hình mật độ  kết hợp sẽ được xây dựng cho tập dữ liệu đó. Học không có giám sát có thể được dùng kết hợp với  suy diễn Bayes  ( Bayesian inference ) để cho ra xác suất có điều kiện (nghĩa là học có giám sát) cho bất kì biến ngẫu nhiên nào khi biết trước các biến khác. Học không có giám sát cũng hữu ích cho việc  nén dữ liệu : về cơ bản, mọi giải thuật nén dữ liệu hoặc là dựa vào một  phân bố xác suất  trên một tập đầu vào một cách tường minh hay không tường minh. Một dạng khác của học không có giám sát là gom nhóm dữ liệu ( data clustering ), nó đôi khi không mang tính  xác suất . Xem thêm  phân tích khái niệm hình thức  ( formal concept analysis ). Tham khảo [ sửa  |  sửa mã nguồn ] Geoffrey Hinton ,  Terrence J. Sejnowski  (editors) (1999) Unsupervised Learning and Map Formation: Foundations of Neural Computation, MIT Press,  ISBN 026258168X  (Cuốn sách này tập trung vào học không có giám sát trong  mạng nơ-ron .) Xem thêm [ sửa  |  sửa mã nguồn ] Phân mảnh dữ liệu  ( Data clustering ), Self-organizing map , Giải thuật EM  (Expectation-maximization algorithm) Chương trình mạng nơ ron đa lớp (Multi Layer Neural Network) và mạng nơ ron tự tổ chức (Self Organizing Maps) có giải thích bằng tiếng Việt. Sử dụng phần mềm mạng nơ ron 3 lớp Spice-MLP Sử dụng phần mềm mạng tự tổ chức Spice-SOM Hướng dẫn sử dụng mạng nơ ron trong các ứng dụng thực tế  trong đó có minh họa phân loại ảnh khuôn mặt, ảnh người đi bộ, ảnh xe hơi, dự báo chứng khoán và một số ví dụ khác Bài viết này vẫn còn  sơ khai . Bạn có thể giúp Wikipedia bằng cách  mở rộng nội dung  để bài được hoàn chỉnh hơn. x t s \n\t\t\t\t\t\tLấy từ “ https://vi.wikipedia.org/w/index.php?title=Học_không_có_giám_sát&oldid=25402283 ”\t\t\t\t\t Thể loại :  Sơ khai Học máy Thể loại ẩn:  Trang thiếu chú thích trong bài Trang sử dụng liên kết tự động ISBN Trình đơn chuyển hướng Công cụ cá nhân Chưa đăng nhập Thảo luận cho địa chỉ IP này Đóng góp Mở tài khoản Đăng nhập Không gian tên Bài viết Thảo luận Biến thể Các hiển thị Đọc Sửa đổi Sửa mã nguồn Xem lịch sử Khác Tìm kiếm Xem nhanh Trang Chính Bài viết chọn lọc Tin tức Bài viết ngẫu nhiên Thay đổi gần đây Phản hồi lỗi Quyên góp Tương tác Hướng dẫn Giới thiệu Wikipedia Cộng đồng Thảo luận chung Giúp sử dụng Liên lạc Công cụ Các liên kết đến đây Thay đổi liên quan Các trang đặc biệt Liên kết thường trực Thông tin trang Khoản mục Wikidata Trích dẫn trang này In/xuất ra Tạo một quyển sách Tải về dưới dạng PDF Bản để in ra Ngôn ngữ khác العربية Deutsch Ελληνικά English Español فارسی Français 한국어 Italiano 日本語 Polski Русский Suomi ไทย Türkçe Українська 中文 Sửa liên kết  Trang này được sửa đổi lần cuối lúc 13:36 ngày 26 tháng 10 năm 2016. Văn bản được phát hành theo  Giấy phép Creative Commons Ghi công–Chia sẻ tương tự ; có thể áp dụng điều khoản bổ sung. Với việc sử dụng trang web này, bạn chấp nhận  Điều khoản Sử dụng  và  Quy định quyền riêng tư . \nWikipedia® là thương hiệu đã đăng ký của  Wikimedia Foundation, Inc. , một tổ chức phi lợi nhuận. Quy định quyền riêng tư Giới thiệu Wikipedia Lời phủ nhận Nhà phát triển Tuyên bố về cookie Phiên bản di động"
        },
        {
          "title": "Hướng dẫn sử dụng Quyền bảo mật (Quyền giám sát và Mã hóa ứng dụng) Color OS 2.1",
          "relevance": "0",
          "url": "http://www.oppomobile.vn/forum/faq/huong-dn-s-dung-quyn-bao-mt-quyn-giam-sat-va-ma-hoa-ng-dung-color-os-2-1.205/",
          "content": "Tìm kiếm  Chỉ tìm trong tiêu đề Được đăng bởi thành viên: Ngăn cách tên bằng dấu phẩy (,). Mới hơn ngày: Tìm kiếm hữu ích Bài viết mới nhất Thêm... Tìm kiếm  Chỉ tìm trong tiêu đề Được đăng bởi thành viên: Ngăn cách tên bằng dấu phẩy (,). Mới hơn ngày: Tìm kiếm hữu ích Bài viết mới nhất Thêm... Diễn đàn OPPO Việt Nam  Trang tin FAQ > Hỗ trợ  ColorOS 2.1 > FAQ #205 > Hướng dẫn sử dụng Quyền bảo mật (Quyền giám sát và Mã hóa ứng dụng) Color OS 2.1 Category  Tất cả Câu hỏi Tải Rom & Up Rom Sao lưu và khôi phục... Khôi phục cài đặt gốc Bộ nhớ & RAM Cài đặt chung Tổng hợp ColorOS Hỗ trợ ColorOS 3.0 Hỗ trợ  ColorOS 2.1 Hỗ trợ ColorOS 1.x - 2.0 Pin điện thoại Kết nối Ứng dụng Khắc phục sự cố Khác Phụ kiện chính hãng VIDEOS Hướng dẫn sử dụng Bảo hành Color OS 2.1 có sự thay đổi so với Color OS 1.x và 2.0 khi kết hợp cả 2 tính năng Quyền giám sát và Mã hóa ứng dụng vào chung trong Quyền bảo mật \n1.  Quyền giám sát Giúp chúng ta quản lý hiệu quả các ứng dụng cài đặt bên thứ 3. \"Quyền giám sát\" cho chúng ta giám sát ứng dụng về: Gửi tin nhắn, MMS, đọc danh bạ, chụp ảnh, micro, loa,...trên máy. \nĐiều này hữu ích khi nhiều game và ứng dụng hiện nay có tình trạng lừa đảo người dùng bằng cách gửi tin nhắn trừ tiền tài khoản, hay 1 số ứng dụng có tính năng nâng cấp bản Pro tính phí mà không may chúng ta bấm vào kích hoạt, hoặc dùng để bảo mật thông tin trước các phần mềm gián điệp... Cách kích hoạt: Vào Trung tâm bảo mật - Quyền bảo mật -  Bật  Quyền giám sát. Để có thể quản lý sâu hơn về Quyền giám sát bạn vào Quản lý quyền ứng dụng. ​ Lưu ý: Đối với máy OPPO F1 cần vào Trung tâm bảo mật - Chọn biểu tượng bánh răng - Quyền bảo mật - Bật Quyền giám sát ​ Có 2 hình thức giám sát: Theo ứng dụng cho phép và quản lý theo ứng dụng - Theo ứng dụng cho phép:  Sẽ được liệt kê theo các mục như cuộc gọi, đọc danh bạ, đọc SMS, đọc nhật ký cuộc gọi,.... Bạn sẽ chọn ứng dụng để có thể cho phép hay từ chồi hay luôn hỏi sử dụng các quyền này ​ - Quản lý theo ứng dụng:  Sẽ liệt kê theo các ứng dụng đã tải về. Trong từng ứng dụng bạn có thể bật các quyền mà bạn cho phép được dùng. Xuất hiện hộp thoại với 3 tính năng là: Cho phép, từ chối và luôn hỏi để bạn lựa chọn. ​ Lưu ý: Khi bật Quyền giám sát nếu những tính năng chọn luôn hỏi sẽ hiện thông báo khi vào ứng dụng hoặc những tính năng bị chặn sẽ không được phép truy cập ​ 2. Mã hóa ứng dụng ( Nghĩa là Khoá mật khẩu ứng dụng ) \nCác ứng dụng mà bạn lựa chọn sẽ được đặt mật khẩu. Sau này ai muốn truy cập vào thì đều cần phải có mật khẩu, khá tiện cho ai cho bạn bè mượn điện thoại mà không muốn họ đọc trộm tin nhắn hay xem hình ảnh của mình. Vào Trung tâm bảo mật - Quyền bảo mật - Mã hóa ứng dụng - Bật mã hóa ứng dụng (góc bên phải phía trên) - Nhập mật khẩu mà bạn muốn thiết lập ​ Lưu ý với máy OPPO F1 vào  Vào Trung tâm bảo mật - Quyền bảo mật - Mã hóa ứng dụng - Bật mã hóa (ô phía dưới) -  Nhập mật khẩu mà bạn muốn thiết lập. Mặc định OPPO F1 sẽ có 4 ứng dụng được khuyến nghị mã hóa: Thư viện, Tin nhắn, Trình quản lý tệp và Trình duyệt sẽ được bật sẳn ​ Để đặt mật khẩu cho các ứng dụng bạn chọn Mã hóa ứng dụng - Nhập mật khẩu đã cài trước đó - Bật ứng dụng muốn được bảo vệ ​ Lưu ý  : - Riêng máy  F1 tính năng  Mã hóa ứng dụng  trong Trung tâm bảo mật > Quyền bảo mật =>  Khi đã kích hoạt thì không tắt được. \n- Khi không muốn sử dụng được nữa - Vào  Trung tâm bảo mật > Quyền bảo mật > Mã hóa ứng dụng > Tắt các ứng dụng đang Bật mã hóa nhé. Cám ơn các ban đã theo dõi ! ​ By Service Club ​ Category: Hỗ trợ  ColorOS 2.1 Published: 15/5/16 Page Views: 4918 FAQ Manager ©2017  Iversia  from  RPGfix . Log in with Facebook Log in with Google Tên tài khoản hoặc địa chỉ Email: Bạn đã có tài khoản rồi? \n\t\t\t\t\t\tTích vào đây để đăng ký \n\t\t\t\t\t\tVâng, Mật khẩu của tôi là: Bạn đã quên mật khẩu?  Ghi nhớ? Tham gia ngay! FAQ #205 \n                    To give another user a direct link to this FAQ answer, use the following BB Code in your post: \n                     Need More Help? \n                    Did this answer your question? If not, please feel free to request assistance on our forum.\n                 Diễn đàn OPPO Việt Nam  Trang tin FAQ > Hỗ trợ  ColorOS 2.1 > FAQ #205 > Trang tin Trang tin Liên kết nhanh Hoạt động gần đây Có gì mới? Trợ giúp Diễn đàn Diễn đàn Liên kết nhanh Tìm kiếm Bài viết mới nhất Popular Content  Sản phẩm  FAQ FAQ Liên kết nhanh Tải Rom & Up Rom Sao lưu và khôi phục dữ liệu Khôi phục cài đặt gốc Bộ nhớ & RAM Cài đặt chung Tổng hợp ColorOS Hỗ trợ ColorOS 3.0 Hỗ trợ  ColorOS 2.1 Hỗ trợ ColorOS 1.x - 2.0 Pin điện thoại Kết nối Ứng dụng Khắc phục sự cố Khác Phụ kiện chính hãng VIDEOS Hướng dẫn sử dụng Bảo hành Thư viện ảnh  Thư viện ảnh  Liên kết nhanh Chuyên mục Khám phá Album Album mới Hình ảnh mới New Videos Tags Cloud Menu Đăng nhập/Đăng ký Ngôn ngữ Tiếng Việt (VN) Trợ giúp FAQ Trang tin Lên đầu trang RSS Diễn đàn OPPO Việt Nam Trang chủ Diễn đàn Nội quy Hỗ trợ và Dịch vụ Trung tâm CSKH Tải Rom Chính sách bảo hành Tổng đài hỗ trợ miễn phí 1800-577-776 Hỗ trợ trực tuyến Giờ làm việc: 8h00 - 17h30\n\t\t\t từ Thứ 2 tới Thứ 7 Công Ty TNHH Một Thành Viên Kỹ Thuật Và Khoa Học OPPO, Tòa Nhà SCB, 242 Cống Quỳnh, Q.1, TP.HCM Giấy ĐKKD: 0312059023 cấp ngày 21/11/2012 ▲"
        },
        {
          "title": "Nghi phạm vẫn giám sát học sinh đến trường sau vụ sát hại Nhật Linh",
          "relevance": "0",
          "url": "https://vnexpress.net/tin-tuc/the-gioi/nguoi-viet-5-chau/nghi-pham-van-giam-sat-hoc-sinh-den-truong-sau-vu-sat-hai-nhat-linh-3570484.html",
          "content": "Rao vặt VnExpress International – Vietnam and ASEAN news 24h qua  RSS   Thế giới Người Việt 5 châu Thứ sáu, 14/4/2017  |  16:10 GMT+7 Cảnh sát tịch thu xe của nghi phạm  \n\tNgười dân sống tại khu phố thành phố Matsudo cho biết tối 24/3, ngày mà bé Linh mất tích, xe của nghi phạm Shibuya Yasumasa không có ở bãi đỗ như thường lệ. Ông ta sống tại căn hộ ở tầng 4 của chung cư đối diện nhà ga, cách nhà Linh chỉ 300 mét, theo  NNN. \n\t\"Khi chúng tôi cùng tìm kiếm bé Linh tối 24/3, tôi thấy lạ vì xe của Shibuya không đỗ dưới bãi như mọi ngày\", một người đàn ông 44 tuổi có quầy hàng kinh doanh gần ga cho biết. \n\tNhững ngày hôm sau, Shibuya vẫn tham gia giám sát trẻ em đi học như thường lệ trên vai trò trưởng hội phụ huynh. Ông ta có hai con nhỏ cũng đang học tiểu học. \"Trước cửa ga Rokumi khoảng một tuần trước, tôi thấy một người đeo băng xanh giống như người giám hộ đang giám sát khoảng 5,6 đứa trẻ. Tôi bắt chuyện, hỏi ông ta 'Anh vất vả nhỉ?' thì ông ta trả lời 'Vâng'. Tôi không thấy ông ta có gì bất thường cả\", một người đàn ông khoảng 60 tuổi sống gần nhà  Shibuya  hôm nay cho biết. \n\t  Nghi phạm giám sát trẻ đi học hôm 5/4 10 ngày sau khi thi thể bé Linh được tìm thấy ở một bãi cỏ gần cống thoát nước tại thành phố Abiko, cách nhà khoảng 10 km,  Shibuya còn kêu gọi người dân quyên tiền để trao tặng gia đình bé Linh, sau khi họ đưa thi thể em về nước và dự định quay lại Nhật hôm 15/4. Shibuya bị bắt sáng nay với kết quả giám định cho thấy ADN của ông ta trùng khớp với ADN trên các đồ vật của Linh được tìm thấy tại hiện trường. Xe của nghi phạm cũng trùng với hình ảnh phương tiện khả nghi thu thập được qua camera an ninh. Nghi phạm bình thản và giữ im lặng sau khi bị bắt. Cảnh sát xác định Linh thiệt mạng do bị siết cổ đến ngạt thở và còn có nhiều vết thương nhỏ ở hai bên cánh tay và chân.  Linh Phương Xem thêm:  \n                            Nghi phạm từng kêu gọi quyên tiền cho bé gái Việt sau vụ sát hại \n                                                        Bé gái Việt bị sát hại ở Nhật Bản  \n                          Cha bé Nhật Linh: 'Tôi muốn bé đầu thai làm con tôi'                           (26/9) \n                          Người vợ Nhật ba lần tự thú giết chồng nhưng cảnh sát không tin                           (10/7) \n                          Nhật Bản khởi tố nghi phạm sát hại bé Linh                           (26/5)                           \n             \n                          Phát hiện dấu vết nước tiểu có thể của bé Nhật Linh trên xe nghi phạm                           (7/5)                           \n             \n                          Nghi phạm trong vụ án Nhật Linh bị cáo buộc giết người                           (5/5)                           \n              Xem thêm  Xem nhiều nhất \n\t\t\t\t\t\t\t\tCampuchia dọa trục xuất người Việt nhập cư sau 2002\t\t\t\t\t\t\t  \n\t\t\t\t\t\t\t\tTriệu chứng 'núi mỏi' tại nơi Triều Tiên thử bom hạt nhân\t\t\t\t\t\t\t  \n\t\t\t\t\t\t\t\tTriều Tiên đặt điều kiện với Mỹ để quay lại đàm phán hạt nhân\t\t\t\t\t\t\t  \n\t\t\t\t\t\t\t\tCái chết không tránh khỏi của các tay súng IS ngoại quốc ở Syria\t\t\t\t\t\t\t  \n\t\t\t\t\t\t\t\tQuân nổi dậy Syria giải phóng hoàn toàn Raqqa từ tay IS\t\t\t\t\t\t\t  Ý kiến bạn đọc  ( 0 ) Mới nhất  |\r\n           Quan tâm nhất Xem thêm Ý kiến của bạn Gửi 20 /1000 Ý kiến của bạn Gửi 20 /1000    Tags \n                        nghi phạm                      \n                        sát hại                      \n                        bé Nhật Linh                      \n                        xe                      \n                        mất tích                      \n                        giám sát trẻ đi học                      \n                        hàng xóm                      \n                        Shibuya Yasumasa                      Tin khác \n                    Campuchia dọa trục xuất người Việt nhập cư sau 2002                   \n                    Nhân viên an ninh Mỹ kéo bác sĩ gốc Việt khỏi máy bay bị đuổi việc                                      \n             \n                    Mỹ triệt phá mạng lưới trồng cần sa 7 triệu USD của người gốc Việt                   \n              Chú chó cào cửa cứu chủ Việt khỏi chết cháy ở California                          \n             \n              Hàng chục trẻ em Việt ở Anh mất tích sau khi được cứu khỏi nạn buôn người             \n              Người thân cô dâu Việt bị từ chối visa đến New Zealand dự lễ cưới                          \n             \n               Đêm chạy cháy rừng của cô gái Việt ở California                            \n             \n               Người Việt ở Mỹ đòi công lý cho nam sinh bị cảnh sát bắn chết              \n               Người Việt lo lắng khi Campuchia sắp tịch thu giấy tờ              Phân tích \n                Tham vọng chấm dứt chính sách 'náu mình chờ thời' của ông Tập             Khác với triết lý \"ẩn mình\" của Đặng Tiểu Bình, ông Tập muốn Trung Quốc có vị thế lớn trên trường quốc tế. \n                Tình thế không bạn trong nỗ lực ly khai của Catalonia             \n                Sự thay đổi của Trung Quốc dưới thời Tập Cận Bình             \n                Thăm chính thức Việt Nam, ông Trump thể hiện mối quan tâm lớn tới châu Á             Đội tình nguyện viên đeo băng đỏ bảo vệ Bắc Kinh trong Đại hội đảng Putin trổ tài nói tiếng Anh trước thanh niên thế giới Hong Kong bắt nghi phạm đẩy nữ lao công xuống đường ray Đệ nhất phu nhân Melania sử dụng ít trợ lý hơn người tiền nhiệm Tư liệu \n                5 thế hệ lãnh đạo của đảng Cộng sản Trung Quốc             Từ thời Mao Trạch Đông đến Tập Cận Bình, mỗi thế hệ lãnh đạo Trung Quốc đều có dấu ấn tư tưởng chính trị của riêng mình. \n                Những ứng viên sáng giá cho ủy ban lãnh đạo cao nhất của Trung Quốc             \n                Cỗ máy chế tạo tên lửa được Triều Tiên ví như 'anh hùng quốc gia'                          \n             \n                Thành bại của những phong trào ly khai trên thế giới             Mỹ - Hàn phô trương sức mạnh tại triển lãm hàng không Seoul Các hình thức chào mừng Đại hội đảng của người Trung Quốc Những bóng hồng bên súng ống tại các triển lãm quân sự thế giới Phi đội tiêm kích biểu diễn Nga bay chỉ cách nhau vài mét \n                    Thế giới                 \n                    Tàu sân bay Mỹ cập cảng Hàn Quốc                                      \n             Tàu sân bay hạt nhân USS Ronald Reagan cuối tuần trước cập cảng Busan, Hàn Quốc sau cuộc diễn tập chung, khiến một số người dân lo ngại.   Giây phút oanh tạc cơ B-2 thả bom 14 tấn chuyên xuyên phá hầm ngầm                \n             Chuyên gia Nga nói Triều Tiên đủ sức hủy diệt tàu sân bay Mỹ Bộ trưởng Quốc phòng Mỹ thăm châu Á, bàn về Triều Tiên Chia sẻ bài viết qua email Bài viết Nghi phạm vẫn giám sát học sinh đến trường sau vụ sát hại Nhật Linh Thế giới  \r\n                     >                                     \r\n                     Người Việt 5 châu Nhập mã xác nhận: Hoàn tất Trang chủ  Tìm kiếm\r\n         Video Thời sự Góc nhìn Thế giới Kinh doanh Giải trí Thể thao Pháp luật Giáo dục Sức khỏe Gia đình Du lịch Khoa học Số hóa Xe Cộng đồng Tâm sự Cười Ảnh Infographics Rao vặt Shop VnExpress Pay VnExpress Quà tặng Tải VnExpress App\r\n             Trang chủ Tải VnExpress App Video Thời sự Góc nhìn Thế giới Kinh doanh Giải trí Thể thao Pháp luật Giáo dục Sức khỏe Gia đình Du lịch Khoa học Số hóa Xe Cộng đồng Tâm sự Cười Ảnh Infographics Rao vặt Shop VnExpress Pay VnExpress Quà tặng Liên hệ quảng cáo Đường dây nóng Liên hệ tòa soạn 0123.888.0123  (HN) -  0129.233.3555  (TP HCM) Thuộc Bộ Khoa học Công nghệ. © Copyright 1997 VnExpress, All rights reserved. ® VnExpress giữ bản quyền nội dung trên website này. VnExpress tuyển dụng  Liên hệ:  Quảng cáo  /  Tòa soạn Đường dây nóng:  0123.888.0123  (HN) -  0129.233.3555  (TP HCM) \n        ×\n     \n           Phiên bản:  VnExpress International – Vietnam and ASEAN news Trang chủ APEC VIETNAM 2017 Video Thời sự Góc nhìn Thế giới Kinh doanh VEPF 2017 Giải trí Thể thao Pháp luật Giáo dục Sức khỏe Gia đình Du lịch Khoa học Số hóa Xe Cộng đồng Tâm sự Ảnh  Infographics  Cười Rao vặt 24h qua Liên hệ Tòa soạn Thông tin Tòa soạn \n            Thuộc Bộ Khoa học Công nghệ.\n             © Copyright 1997 VnExpress, All rights reserved ® VnExpress giữ bản quyền nội dung trên website này. \n            Hotline: 0123.888.0123  (Hà Nội) 0129.233.3555  (TP HCM) "
        },
        {
          "title": "Phương pháp luận sáng tạo và đổi mới là gì?",
          "relevance": "0",
          "url": "http://cstc.vn/index.php/vi/phuong-phap-luan-sang-tao-va-doi-moi-la-gi.html",
          "content": "Trung tâm Sáng tạo Khoa học–kỹ thuật (TSK) Center for Scientific and Technical Creativity (CSTC) Phòng 31, Lầu 3, Nhà B, 227 Nguyễn Văn Cừ, P.4, Q.5, TP. Hồ Chí Minh ĐT: (84.8) 38 301 743  |  Email: tsk@hcmus.edu.vn  |  Fax: (84.8) 38 350 096 Khóa cơ bản 497 Khai giảng:  thứ năm, 9/11/2017 Giờ học:  17g45 - 20g45 Ngày học:  thứ ba và thứ năm Thời gian học:  15 buổi (khoảng 2 tháng) Học phí hiện nay:  990.000 đồng (bao gồm cả tài liệu học tập) Chiêu sinh tất cả mọi người có trình độ văn hóa lớp 12 trở lên, không phân biệt tuổi, nghề nghiệp chuyên môn, chức vụ... Ghi danh tại địa chỉ của Trung tâm. Điện thoại: (08) 38301743 Để biết các ích lợi của môn học, bấm vào đây Chương trình học bấm vào đây Lưu ý: Lớp học không nhận quá 30 người . Về TSK Các thành viên của TSK Phương pháp luận sáng tạo và đổi mới là gì? Hoạt động của TSK Làm gì và làm thế nào để Việt Nam đạt được mục tiêu nước mạnh Ý kiến của người học Báo chí Việt Nam viết về TSK Các ý kiến và đề nghị của Bộ giáo dục-đào tạo về việc phát triển Phương pháp luận sáng tạo ở Việt Nam Các sách, bài báo khoa học đã công bố Danh sách các đơn vị mời TSK nói chuyện hoặc dạy tại chỗ Về thầy Genrikh Saulovich Altshuller Một số thông tin về Phương pháp luận sáng tạo và đổi mới (TRIZ) trên thế giới Video về phương pháp luận sáng tạo Bản tin TSK Bài nhiều người đọc Liên hệ Báo tường mới nhất BTSK số 1/2017 (73) ra tháng 3 năm 2017 Toàn bộ + Tin TSK - Tin thế giới + Thế giới từ góc nhìn sáng tạo: 10 phát minh \"không tưởng\" của Nicola Tesla + Đa dạng: Bên trong trung tâm dữ liệu của Facebook + Sản phẩm sáng tạo \n\tPhương pháp luận sáng tạo và đổi mới là gì? Lời nói đầu Người viết: Phan Dũng Phương pháp luận sáng tạo và đổi mới (viết tắt là PPLSTVĐM, tiếng Anh là Creativity and Innovation Methodologies) là phần ứng dụng của Khoa học về sáng tạo (Sáng tạo học, tên cổ điển – Heuristics, tên hiện đại – Creatology), gồm hệ thống các phương pháp và các kỹ năng cụ thể giúp nâng cao năng suất và hiệu quả, về lâu dài tiến tới điều khiển tư duy sáng tạo (quá trình suy nghĩ giải quyết vấn đề và ra quyết định) của người sử dụng. Suốt cuộc đời, mỗi người chúng ta dùng suy nghĩ rất nhiều, nếu không nói là hàng ngày. Từ việc trả lời những câu hỏi bình thường như  \"Hôm nay ăn gì? mặc gì? làm gì? mua gì? xem gì? đi đâu?...\"  đến làm các bài tập thầy, cô cho khi đi học; chọn ngành nghề đào tạo; lo sức khỏe, việc làm, thu nhập, hôn nhân, nhà ở; giải quyết các vấn đề nảy sinh trong công việc, trong quan hệ xã hội, gia đình, nuôi dạy con cái..., tất tần tật đều đòi hỏi phải suy nghĩ và chắc rằng ai cũng muốn mình suy nghĩ tốt, ra những quyết định đúng để  \"đời là bể khổ\"  trở thành  \"bể sướng\" . Chúng ta tuy được đào tạo và làm những nghề khác nhau nhưng có lẽ có một nghề chung, giữ nguyên suốt cuộc đời, cần cho tất cả mọi người. Đó là \"nghề\" suy nghĩ và hành động giải quyết các vấn đề gặp phải trong suốt cuộc đời nhằm thỏa mãn các nhu cầu chính đáng của cá nhân mình, đồng thời thỏa mãn các nhu cầu để xã hội tồn tại và phát triển. Nhìn dưới góc độ này, PPLSTVĐM giúp trang bị loại nghề chung nói trên, bổ sung cho giáo dục, đào tạo hiện nay, chủ yếu, chỉ đào tạo các nhà chuyên môn. Nhà chuyên môn có thể giải quyết tốt các vấn đề chuyên môn nhưng nhiều khi không giải quyết tốt các vấn đề ngoài chuyên môn, do vậy, không thực sự hạnh phúc như ý. Các nghiên cứu cho thấy, phần lớn mọi người thường suy nghĩ một cách tự nhiên như đi lại, ăn uống, hít thở mà ít khi suy nghĩ về chính suy nghĩ của mình, xem nó hoạt động ra sao để cải tiến, làm suy nghĩ của mình trở nên tốt hơn, như người ta thường chú ý cải tiến các dụng cụ, máy móc dùng trong sinh hoạt và công việc. Cách suy nghĩ tự nhiên nói trên có năng suất, hiệu quả rất thấp và nhiều khi trả giá đắt cho các quyết định sai. Nói một cách nôm na, cách suy nghĩ tự nhiên ứng với việc lao động bằng xẻng thì PPLSTVĐM là máy xúc với năng suất và hiệu quả cao hơn nhiều. Nếu xem bộ não của mỗi người là máy tính tinh xảo – đỉnh cao tiến hóa và phát triển của tự nhiên thì phần mềm (cách suy nghĩ) tự nhiên đi kèm với nó chỉ khai thác một phần rất nhỏ tiềm năng của bộ não. PPLSTVĐM là phần mềm tiên tiến giúp máy tính – bộ não hoạt động tốt hơn nhiều. Nếu như cần  \"học ăn, học nói, học gói, học mở\"  thì  \"học suy nghĩ\"  cũng cần thiết cho tất cả mọi người. PPLSTVĐM dạy và học được như các môn học truyền thống: Toán, lý, hóa, sinh, tin học, quản trị kinh doanh,... Trên thế giới, nhiều trường và công ty đã bắt đầu từ lâu và đang làm điều đó một cách bình thường. Dưới đây là vài thông tin về PPLSTVĐM trên thế giới và ở nước ta. Từ những năm 1950, ở Mỹ và Liên Xô đã có những lớp học dạy thử nghiệm PPLSTVĐM. Dưới ảnh hưởng của A.F. Osborn, phó chủ tịch công ty quảng cáo BBD & O và là tác giả của phương pháp não công (Brainstorming) nổi tiếng, Trung tâm nghiên cứu sáng tạo (Center for Studies in Creativity) được thành lập năm 1967 tại Đại học Buffalo, bang New York. Năm 1974, Trung tâm nói trên bắt đầu đào tạo cử nhân khoa học và năm 1975 – thạc sỹ khoa học về sáng tạo và đổi mới (BS, MS in Creativity and Innovation). Ở Liên Xô, G.S. Altshuller, nhà sáng chế, nhà văn viết truyện khoa học viễn tưởng và là tác giả của Lý thuyết giải các bài toán sáng chế (viết tắt theo tiếng Nga và chuyển sang ký tự Latinh – TRIZ) cộng tác với \"Hiệp hội toàn liên bang các nhà sáng chế và hợp lý hóa\" (VOIR) thành lập Phòng thí nghiệm các phương pháp sáng chế năm 1968 và Học viện công cộng về sáng tạo sáng chế (Public Institute of Inventive Creativity) năm 1971. Người viết, lúc đó đang học ngành vật lý bán dẫn thực nghiệm tại Liên Xô, có may mắn học thêm được khóa đầu tiên của Học viện sáng tạo nói trên, dưới sự hướng dẫn trực tiếp của thầy G.S. Altshuller. Chịu ấn tượng rất sâu sắc do những ích lợi PPLSTVĐM đem lại cho cá nhân mình, bản thân lại mong muốn chia sẻ những gì học được với mọi người, cùng với sự khuyến khích của thầy G.S. Altshuller, năm 1977 người viết đã tổ chức dạy dưới dạng ngoại khóa cho sinh viên các khoa tự nhiên thuộc Đại học tổng hợp TpHCM (nay là Trường đại học khoa học tự nhiên, Đại học quốc gia TpHCM). Những khóa PPLSTVĐM tiếp theo là kết quả của sự cộng tác giữa người viết và Câu lạc bộ thanh niên (nay là Nhà văn hóa thanh niên TpHCM), Ủy ban khoa học và kỹ thuật TpHCM (nay là Sở khoa học và công nghệ TpHCM). Năm 1991, được sự chấp thuận của lãnh đạo Đại học tổng hợp TpHCM, Trung tâm Sáng tạo Khoa học – kỹ thuật (TSK) hoạt động theo nguyên tắc tự trang trải ra đời và trở thành cơ sở chính thức đầu tiên ở nước ta giảng dạy, đào tạo và nghiên cứu PPLSTVĐM. Đến nay đã có vài chục ngàn người với nghề nghiệp khác nhau thuộc mọi thành phần kinh tế, xã hội, từ Hà Nội đến Cà Mau tham dự các khóa học từng phần hoặc đầy đủ chương trình 120 tiết của TSK dành đào tạo những người sử dụng PPLSTVĐM. TSK cũng tích cực tham gia các hoạt động quốc tế như công bố các công trình nghiên cứu khoa học dưới dạng các báo cáo, báo cáo chính (keynotes) tại các hội nghị, các bài báo đăng trong các tạp chí chuyên ngành và giảng dạy PPLSTVĐM cho các cán bộ quản lý, giảng dạy, nghiên cứu ở nước ngoài theo lời mời. Năm 2000, tại Mỹ, nhà xuất bản Kendall/Hunt Publishing Company xuất bản quyển sách  \"Facilitative Leadership: Making a Difference with Creative Problem Solving\"  (Tạm dịch là  \"Lãnh đạo hỗ trợ: Tạo sự khác biệt nhờ giải quyết vấn đề một cách sáng tạo\" ) do tiến sỹ Scott G. Isaksen làm chủ biên. Ở các trang 219, 220, dưới tiêu đề  Các tổ chức sáng tạo (Creativity Organizations)  có đăng danh sách đại biểu các tổ chức hoạt động trong lĩnh vực sáng tạo và đổi mới trên thế giới. Trong 17 tổ chức được nêu tên, TSK là tổ chức duy nhất ở châu Á. Nhiều nhà nghiên cứu cho rằng, xã hội loài người trong quá trình phát triển trải qua bốn thời đại hay nền văn minh (làn sóng phát triển): Nông nghiệp, công nghiệp, thông tin và tri thức. Nền văn minh nông nghiệp chấm dứt thời kỳ săn bắn, hái lượm, du cư bằng việc định cư, trồng trọt và chăn nuôi, sử dụng các công cụ lao động còn thủ công. Nền văn minh công nghiệp cho thấy, mọi người lao động bằng các máy móc hoạt động bằng năng lượng ngoài cơ bắp, giúp tăng sức mạnh và nối dài đôi tay của con người. Ở thời đại thông tin, máy tính, các mạng lưới thông tin giúp tăng sức mạnh, nối dài các bộ phận thu, phát thông tin trên cơ thể người như các giác quan, tiếng nói, chữ viết và một số hoạt động lôgích của bộ não. Nhờ công nghệ thông tin, thông tin trở nên truyền, biến đổi nhanh, nhiều, lưu trữ gọn, truy cập dễ dàng. Tuy nhiên, trừ loại thông tin có ích lợi thấy ngay đối với người nhận tin, các loại thông tin khác vẫn phải cần bộ não của người nhận tin xử lý, biến đổi để trở thành thông tin có ý nghĩa và ích lợi (tri thức) cho người có thông tin. Nếu người có thông tin không làm được điều này trong thời đại bùng nổ thông tin thì có thể trở thành bội thực thông tin nhưng đói tri thức, thậm chí ngộ độc vì nhiễu thông tin và chết đuối trong đại dương thông tin mà không khai thác được gì từ đại dương giàu có đó. Thời đại tri thức mà thực chất là thời đại sáng tạo và đổi mới, ở đó đông đảo quần chúng sử dụng PPLSTVĐM được dạy và học đại trà để biến thông tin thành tri thức với các ích lợi toàn diện, không chỉ riêng về mặt kinh tế. Nói cách khác, PPLSTVĐM là hệ thống các công cụ dùng để biến đổi thông tin thành tri thức, tri thức đã biết thành tri thức mới. Rất tiếc, ở nước ta hiện nay chưa chính thức đào tạo các cán bộ giảng dạy, nghiên cứu Sáng tạo học và PPLSTVĐM với các bằng cấp tương ứng: Cử nhân, thạc sỹ và tiến sỹ như một số nước tiên tiến trên thế giới. Người viết tin rằng sớm hay muộn, những người có trách nhiệm quyết định sẽ phải để tâm đến vấn đề này và \"sớm\" chắc chắn tốt hơn \"muộn\". Hy vọng rằng, PPLSTVĐM nói riêng, Sáng tạo học nói chung sẽ có chỗ đứng xứng đáng, trước hết, trong chương trình giáo dục và đào tạo của nước ta trong tương lai không xa.   Tư duy sáng tạo - Tiềm năng sáng tạo của mỗi người Người viết: Phan Dũng Mỗi người làm việc, không thể không suy nghĩ và đòi hỏi cải tiến công việc phải là cơ sở cho mọi suy nghĩ của chúng ta. Nói cách khác, mỗi người chúng ta đều cần suy nghĩ để sáng tạo. Tư duy sáng tạo là tài nguyên cơ bản nhất của mỗi con người. Chúng ta cần sáng tạo vì chúng ta cảm thấy rằng, mọi việc cần được thực hiện theo cách đơn giản hơn và tốt hơn. Dù chúng ta tài giỏi như thế nào, chúng ta vẫn luôn mong muốn tốt hơn nữa. Sáng tạo gắn liền với sự thay đổi, đưa ra cái mới (đổi mới), sáng chế, các ý tưởng mới, các phương án lựa chọn mới. Sự sáng tạo thuộc về năng lực ra quyết định, thuộc về sự kết hợp độc đáo hoặc liên tưởng, phát ra các ý tưởng đạt được kết quả mới và ích lợi. Mọi người có thể dùng tính sáng tạo của mình để đặt vấn đề một cách bao quát, phát triển các phương án lựa chọn, làm phong phú các khả năng và tưởng tượng các hậu quả có thể nảy sinh. Tóm lại,  bạn làm được gì mới, khác và có ích lợi, đấy là sáng tạo. Sự sáng tạo nảy sinh ở mọi tầng lớp và mọi giai đoạn trong cuộc sống của chúng ta. Ðối với một công ty hay tổ chức, tài nguyên quan trọng nhất chính là nguồn nhân lực, tức là những người làm việc cho công ty, tổ chức. Họ gồm các thợ bảo trì, những người bán hàng, các công nhân trong dây chuyền sản xuất, những người đánh máy… và các cán bộ quản lý mọi cấp bậc. Nguồn nhân lực của công ty làm cho các tài nguyên khác hoạt động, mang lại hiệu quả cao. Thiếu nhân sự tốt, một công ty, tổ chức, dù được trang bị máy móc hoàn hảo nhất, được tài trợ tốt nhất, sẽ hoạt động kém hiệu quả. Vì vậy, mỗi người trong mỗi cơ cấu tổ chức cần học phương pháp luận (các thủ thuật cơ bản, các phương pháp, lý thuyết) về tư duy sáng tạo. Ðiều này làm cho cơ cấu tổ chức của bạn mạnh lên rất nhiều.  Trong mỗi cơ cấu tổ chức, càng nhiều người học phương pháp luận về tư duy sáng tạo, tổ chức hoạt động càng có hiệu quả   Phương pháp luận sáng tạo là gì? Người viết: Phan Dũng LTS: Ở thành phố ta, từ năm 1977 đã có những khóa học dạy về \"phương pháp luận sáng tạo\". Để giúp hiểu rõ hơn đối tượng và mục đích của môn học mới mẻ này, chúng tôi giới thiệu cùng bạn đọc bài viết sau đây của GS, tiến sĩ khoa học PHAN DŨNG, Giám đốc Trung tâm Sáng tạo Khoa học-kỹ thuật (TSK) thuộc Đại học Tổng hợp TP.HCM. Nói một cách ngắn gọn,  \"Phương pháp luận sáng tạo\"  là bộ môn khoa học có mục đích trang bị cho người học hệ thống các phương pháp, các kỹ năng thực hành về suy nghĩ để giải quyết các vấn đề và ra quyết định một cách sáng tạo, về lâu dài, tiến tới điều khiển được tư duy. Có người đưa ra định nghĩa về đời người như sau:  Cuộc đời là chuỗi các vấn đề cần phải giải quyết và chuỗi các quyết định cần phải ra . Quả thật, mỗi người chúng ta trong cuộc đời của mình gặp biết bao vấn đề, từ chuyện mua sắm, học hành, quan hệ giao tiếp đến chọn ngành nghề, nơi ở, thu nhập, xã hội… phải suy nghĩ để giải quyết và ra quyết định xem phải làm gì và làm như thế nào. Nói như vậy để thấy tuy cái tên  \"Phương pháp luận sáng tạo\"  còn \"dội\" đối với nhiều người nhưng đối tượng và mục đích của bộ môn khoa học này lại hết sức gần gũi với mỗi người. Nếu như trước đây, ngay cả đối với các nhà nghiên cứu, sáng tạo được coi là huyền bí, mang tính thiên phú, may mắn, ngẫu hứng… thì ngày nay với những phát hiện mới, người ta cho rằng có thể khoa học hóa được lĩnh vực sáng tạo và sáng tạo có thể dạy và học được. Không những thế, còn cần phải quản lý sự sáng tạo như là lâu nay người ta vẫn quản lý một cách có kết quả nhiều lĩnh vực khác. Ví dụ, hiện nay, một tạp chí khoa học quốc tế, trụ sở đặt tại Manchester, nước Anh, có tên gọi rất rõ ràng về mục đích ấy  \"Quản lý sự sáng tạo và đổi mới\"  (Creativity and Innovation Management) mà số đầu tiên của nó mới ra đời năm 1992. Trên thế giới, các trung tâm, trường học, công ty chuyên về sáng tạo được thành lập cách đây chưa lâu. Ở Mỹ, lâu đời nhất là Trung tâm nghiên cứu sáng tạo (Center for Studies in Creativity) thuộc đại học Buffalo, New York ra đời năm 1967. Đại học sáng tạo sáng chế đầu tiên của Liên Xô cũ hoạt động từ năm 1971. Ở Anh, khi người ta bắt đầu chương trình dạy sáng tạo tại Trường kinh doanh Manchester năm 1972 thì chưa một trường đại học Tây Âu nào làm việc này. Ngày nay, ít nhất đã có 12 nước Tây Âu triển khai các chương trình tương tự. Các hiệp hội, mạng lưới về sáng tạo được thành lập ở nhiều nước và nhiều khu vực trên thế giới. Chỉ riêng Mạng lưới sáng tạo quốc tế (International Creativity Network), trụ sở liên lạc ở Mỹ, tuy mới thành lập ba năm nay, đã có hơn 300 hội viên ở hơn 25 nước. Các hội nghị khoa học về sáng tạo cũng được tổ chức thường xuyên. Riêng năm 1990 đã có 7 hội nghị như vậy. Năm 1994 từ ngày 10 đến 13 tháng 8 đã có một hội nghị quốc tế tại Québec, Canađa, sắp tới đây có một hội nghị về sáng tạo tại London (Anh Quốc). Ở nước ta, lớp học đầu tiên về tư duy sáng tạo được tổ chức vào năm 1977. Hiện nay, Trung tâm Sáng tạo KHKT (TSK) thuộc Đại học Tổng hợp TPHCM thường xuyên mở các lớp, chiêu sinh theo cách ghi danh tự do cho những người nào quan tâm đến việc nâng cao chất lượng suy nghĩ. Gần 60 khóa học đã mở với hơn 2.300 người tham dự. Qua các ý kiến của các học viên có thể thấy được những ích lợi cụ thể do môn học mang lại. Một số học viên đã có những thành công đáng kể trong công việc và trong cuộc sống của chính mình mà báo chí thành phố ta đã có dịp nói tới. Thế kỷ 21, theo các dự báo là thế kỷ trí tuệ. Sự cạnh tranh trên thế giới, càng ngày càng sẽ là cạnh tranh chất xám sáng tạo chứ không phải theo lối chụp giựt, trả lương rẻ hay do có được nhiều tài nguyên thiên nhiên, có được vị trí địa lý thuận tiện… Dưới cách nhìn hiện đại, sáng tạo là nguồn tài nguyên cơ bản của con người (a fundamental human resource), nguồn tài nguyên đặc biệt mà theo như nhà khoa học Mỹ George Koznetsky: bạn càng sử dụng nó nhiều thì bạn càng có nó nhiều hơn. Từ đây, chúng ta thấy, giáo dục và rèn luyện tính sáng tạo sẽ càng ngày càng đóng vai trò quan trọng như John Dewey nhận xét:  \"Mục đích giáo dục trẻ em không phải là thông tin về những giá trị của quá khứ, mà là sáng tạo những giá trị mới của tương lai\"  (chắc là, không chỉ đối với trẻ em). Người viết tin rằng, trên con đường phát triển, đất nước chúng ta sẽ không tránh khỏi bộ môn khoa học mới mẻ này. Do vậy, chúng ta cần có những nỗ lực cần thiết để đưa nó vào cuộc sống xã hội, giúp nâng cao khả năng sáng tạo của mỗi người, của toàn dân tộc. (Báo \"Sài Gòn Giải Phóng\", ra ngày 28/1/1995)   Các kỹ năng của tương lai Người viết: Phan Dũng Các kỹ năng cơ bản thường được coi là các kỹ năng đọc, viết và làm các phép tính số học. Những người tìm việc trong tương lai sẽ cần những kỹ năng làm việc mới để đáp ứng các công việc của ngày mai. Hiệp hội Mỹ về huấn luyện và phát triển, một tổ chức nghề nghiệp của những người chuyên làm công tác huấn luyện cho các công ty, gần đây đã nghiên cứu “những kỹ năng cơ bản” mới cho Bộ Lao động Mỹ. Dưới đây là danh sách những kỹ năng ấy : Tư duy sáng tạo:  do công việc ngày càng trở nên linh động, các giải pháp của những người làm việc cần phải trở nên sáng tạo hơn. Ðặt mục đích / Ðộng cơ:  những người làm việc cần có khả năng đặt mục đích và kiên trì theo đuổi để đạt chúng. Các kỹ năng quan hệ giữa người với người:  có khả năng làm việc tương hợp với những người cung cấp, các đồng nghiệp và các khách hàng sẽ là điều cần thiết trong công việc tương lai. Lãnh đạo:  những người làm việc sẽ được yêu cầu nhận càng ngày càng nhiều trách nhiệm và hướng dẫn các đồng nghiệp của họ khi cần thiết. Học cách học tập:  những người làm việc sẽ cần biết cách học để thu nhận những thông tin, các kỹ năng mới và có khả năng vận dụng chúng vào công việc. Lắng nghe:  các kỹ năng biết lắng nghe sẽ giúp những người làm việc hiểu những nỗi bận tâm của các đồng nghiệp, các người cung cấp và các khách hàng. Thương lượng – đàm phán:  những người làm việc cần có khả năng xây dựng được sự thỏa thuận thông qua việc cho và nhận. Giao tiếp bằng lời nói:  những người làm việc phải có khả năng đáp lại một cách rõ ràng đối với những nỗi bận tâm của các đổng nghiệp, các người cung cấp và các khách hàng. Tính hiệu quả của tổ chức:  những người làm việc phải hiểu làm thế nào để đáp ứng được các mục đích công việc của công ty và làm sao cho công việc của họ đóng góp vào việc thực hiện các mục đích này. Các kỹ năng phát triển cá nhân / nghề nghiệp:  những người làm việc được đánh giá cao nhất là những người hiểu rằng họ cần phải phát triển liên tục trong công việc. Giải quyết vấn đề:  những tổ chức làm việc theo lối mới có nghĩa là sẽ đòi hỏi những người làm việc giải quyết các vấn đề và tìm ra các giải pháp. Tự trọng:  những người giám sát nói rằng họ muốn những người làm việc là những người có lòng tự hào về bản thân và về những khả năng của mình. Làm việc tập thể:  làm việc tập thể có nghĩa là những người làm việc cần phải biết cách phân công công việc một cách công bằng, có hiệu quả và làm việc cùng nhau để đạt được mục đích của tập thể.   Có một khoa học như thế Người viết: Phan Dũng LTS: Khoa học tư duy sáng tạo trên thế giới đã hình thành từ lâu. Sau một thời gian dài bị lãng quên, gần đây, do yêu cầu của thực tiễn nó được nhìn nhận lại, phát triển và đem lại hiệu quả đáng kể. Ở nước ta lĩnh vực khoa học này mới trong giai đoạn gây dựng. Để phát triển nó, cần có sự quan tâm của Nhà nước mà trước hết là Bộ giáo dục và đào tạo và Ủy ban khoa học Nhà nước. Ba lĩnh vực loài người cần nhận thức và tiến tới làm chủ là tự nhiên, xã hội và tư duy. Tư duy – sản phẩm bộ não, chỉ riêng con người mới có. Con người không ngừng sáng tạo ra những giá trị vật chất, tinh thần nhằm thỏa mãn nhu cầu ngày càng tăng theo nguyên tắc: Đạt hiệu quả cao nhất với những chi phí ít nhất. Có thể nói, những thành tựu vĩ đại đạt được trong hai lĩnh vực tự nhiên và xã hội là kết quả \"vật chất hóa\" quá trình tư duy sáng tạo. Từ đây dễ dàng nhận thấy: Tư duy sáng tạo là công nghệ của mọi công nghệ và nếu nâng cao được hiệu quả tư duy sáng tạo thì những thành tựu của hai lĩnh vực kia chắc chắn sẽ nhân lên gấp bội. Ý định \"khoa học hóa tư duy sáng tạo\" có từ lâu. Nhà toán học Hy Lạp Papp ở Alexanđri, sống vào thế kỷ thứ ba, gọi khoa học này là Ơrixtic (Heuristics) có gốc là từ Ơrica (Eureka). Theo quan niệm lúc bấy giờ, Ơrixtic là khoa học về các phương pháp và quy tắc làm sáng chế, phát minh trong mọi lĩnh vực khoa học, kỹ thuật, văn học, nghệ thuật, chính trị, triết học, toán học, quân sự... Do cách tiếp cận quá chung và chủ yếu do không có nhu cầu xã hội, Ơrixtic bị quên lãng cho đến thời gian gần đây. Cùng với cuộc cách mạng khoa học – kỹ thuật, số lượng bài toán phức tạp mà loài người cần giải quyết tăng nhanh, đồng thời yêu cầu thời gian giải chúng phải rút ngắn lại. Trong khi đó không thể tăng mãi phương tiện và số lượng người làm công tác khoa học và kỹ thuật. Thêm nữa, cho đến nay và tương lai khá xa sẽ không có công cụ nào thay thế được bộ óc tư duy sáng tạo. Người ta đã nhớ lại Ơrixtic và đặt vào nó nhiều hy vọng tìm ra cách tổ chức hợp lý, nâng cao hiệu quả quá trình tư duy sáng tạo – công nghệ làm sáng chế, phát minh. Các nhà tâm lý, qua các nghiên cứu của mình cho thấy: Người ta thường giải bài toán (hiểu theo nghĩa rộng) bằng cách lựa chọn phương án – phương pháp thử và sai. Mỗi một phương án giúp người giải hiểu bài toán đúng hơn để cuối cùng đưa ra phương án may mắn là lời giải chính xác bài toán. Các nhà tâm lý cũng phát hiện ra vai trò quan trọng của liên tưởng, hình tượng, linh tính, ngữ nghĩa, ngữ cảnh, các gợi ý trong các tình huống có vấn đề... Quá trình giải bài toán phụ thuộc rất nhiều vào kinh nghiệm có trước đó của người giải. Cho nên tính ì tâm lý cản trở sự sáng tạo trong phần lớn các trường hợp. Phương pháp thử và sai có nhược điểm chính là tốn thời gian, sức lực và phương tiện vật chất do phải làm rất nhiều phép thử. Để cải tiến phương pháp thử và sai, người ta đưa ra các thủ thuật, gần 30 phương pháp tích cực hóa tư duy như: Não công (brainstorming), phương pháp đối tượng tiêu điểm (method of focal objects), phương pháp các câu hỏi kiểm tra, phương pháp phân tích hình thái (morphological method), Synectics... Các phương pháp này có tác dụng nhất định khi giải các bài toán sáng tạo. Tuy nhiên, chúng bộc lộ nhiều hạn chế, đặc biệt đối với những bài toán có số các phép thử lớn. Một hướng khác trong Ơrixtic nghiên cứu các quy luật phát triển, tiến hóa của các hệ thống kỹ thuật nhằm đưa ra phương pháp luận mới, thay thế phương pháp thử và sai. Đó là lý thuyết giải các bài toán sáng chế (viết tắt và đọc theo tiếng Nga là TRIZ) với hạt nhân của nó là Algôrit giải các bài toán sáng chế (ARIZ). Tác giả của TRIZ là Genrikh Saulovich Altshuller (có thể đọc tiểu sử của ông trong tạp chí Liên Xô \"Nhà sáng chế và hợp lý hóa\", trang 9, số 2/1990). Ông bắt đầu nghiên cứu, xây dựng lý thuyết này từ năm 1946. Tiền đề cơ bản của TRIZ là: Các hệ kỹ thuật phát triển tuân theo các quy luật khách quan, nhận thức được. Chúng cần được phát hiện và sử dụng để giải một cách có ý thức những bài toán sáng chế. TRIZ được xây dựng như một khoa học chính xác, có lĩnh vực nghiên cứu riêng, ngôn ngữ riêng, các công cụ riêng. Ý nghĩa của TRIZ là ở chỗ xây dựng tư duy định hướng nhằm đi đến lời giải bằng con đường ngắn nhất dựa trên các quy luật phát triển các hệ kỹ thuật và sử dụng chương trình tuần tự các bước, có kết hợp một cách hợp lý bốn yếu tố: Tâm lý, lôgích, kiến thức và trí tưởng tượng. Ở Liên Xô có khoảng 300 trường dạy TRIZ. Mới đây Hội TRIZ được thành lập và dự định sẽ ra tờ tạp chí riêng về hướng khoa học này. Theo thống kê chưa đầy đủ, trong vòng mười năm (1972-1981) đã có 7000 người tốt nghiệp các trường sáng tạo sáng chế. Họ đã gửi được 11000 đơn xin công nhận sáng chế và đã nhận được hơn 4000 bằng tác giả. Thành phần tham dự các trường, lớp này gồm từ sinh viên đến tiến sĩ khoa học, đôi khi cả học sinh trung học. Người ta cũng bắt đầu dạy thử cho các cháu mẫu giáo dưới hình thức các trò chơi, các bài toán đố. Thực tế cho thấy một rúp đầu tư vào các lớp học thu được 16,1 rúp tiền lãi do các sáng chế được áp dụng mang lại. Ở Ba Lan, Bungari... cũng mở các lớp tương tự. Tài liệu về TRIZ được dịch ở các nước tư bản như: Nhật, Mỹ, Phần Lan, Tây Đức... Ngoài ra TRIZ còn được dùng kết hợp với các phương pháp kinh tế – tổ chức (như phương pháp phân tích giá thành – chức năng, gọi tắt là FSA) tạo nên công cụ tổng hợp và có hiệu quả mạnh mẽ, tác động tốt đến sự phát triển công nghệ. Ở nước ta, những hoạt động liên quan đến khoa học về tư duy sáng tạo mới thực sự bắt đầu vào cuối những năm 70 và thể hiện trên ba hình thức: Giới thiệu bằng các bài báo ngắn trên các báo Trung ương như \"Nhân Dân\", \"Khoa học và đời sống\", trên các báo của thành phố Hồ Chí Minh, bằng các buổi nói chuyện tại cơ quan, xí nghiệp, trường học, trên \"màn ảnh nhỏ\". Hình thức này mới mang tính chất \"đánh động\" đông đảo quần chúng về môn khoa học còn ít người biết đến nhưng khá gần gũi, thiết thực với mọi người. Xuất bản những tài liệu chi tiết hơn về môn khoa học tư duy sáng tạo.Ví dụ cuốn sách \"Algôrit sáng chế\" (Nhà xuất bản Khoa học và kỹ thuật, Hà Nội, 1983) hoặc đăng thường kỳ trong tạp chí \"Sáng tạo\" của Ủy ban khoa học và kỹ thuật thành phố Hồ Chí Minh. Hình thức này đã có bề sâu hơn và được những người quan tâm hưởng ứng. Dạy và học những phương pháp tư duy sáng tạo. Cho đến nay đã mở được một số lớp tại thành phố Hồ Chí Minh và Hà Nội. Qua kinh nghiệm của các nước tiên tiến và kinh nghiệm thực tế ở nước ta thì hình thức này là hình thức tốt nhất để lĩnh hội và áp dụng vào cuộc sống, công tác. Khoa học về tư duy sáng tạo mới du nhập vào nước ta, mới làm được một số việc như vừa nêu, còn trong giai đoạn gây dựng. Để khoa học này thực sự phát huy tác dụng (mà tác dụng chắc chắn là to lớn) cần phát triển nó thành hệ thống với ba chức năng: Đào tạo, áp dụng và nghiên cứu. Ở đây, Nhà nước, trước hết là Bộ giáo dục và đào tạo, Ủy ban khoa học Nhà nước cần có sự đầu tư cần thiết, nhất là giai đoạn đầu. Về lâu dài, ngành này có thể tiến tới chỗ tự trang trải và tự phát triển nhờ hiệu quả kinh tế do các ý tưởng mới, các sáng kiến cải tiến, các sáng chế, các hàng hóa mới mang lại. Khoa học về tư duy sáng tạo sẽ giúp ích thiết thực việc phát huy tiềm năng sáng tạo của mỗi người, do đó, của toàn dân tộc. (Tạp chí \"Hoạt động khoa học\" số 10/1990, Ủy ban khoa học Nhà nước)   Một ngành khoa học mới mẻ Người viết: Tấn Phong Làm việc cho một cơ sở sản xuất của tư nhân ở quận 5, kỹ sư Lê Văn K. là người có nhiều sáng kiến, cải tiến kỹ thuật, làm lợi cho cơ sở hàng triệu đồng. Có lần, anh được giao đúc một chi tiết cao su rỗng, bên trong có áp suất và mặt ngoài phải bóng láng. Anh hiểu trong kỹ thuật đúc cao su, muốn chi tiết đúc sao chép chính xác bề mặt khuôn, người ta phải khoan thủng nhiều lỗ nhỏ xuyên qua vỏ khuôn để \"đuổi\" lớp không khí giữa chi tiết đúc và bề mặt khuôn ra ngoài. Những lỗ thủng này chắc chắn sẽ để lại trên bề mặt chi tiết những sợi \"râu\" cao su. Làm thế nào để khuôn đúc của anh vừa có lỗ cho không khí thoát ra, vừa không có lỗ để chi tiết khỏi \"mọc râu\" khi định hình? Suy nghĩ kỹ, anh thấy cách tốt nhất là phải tạo ra vô số những lỗ nhỏ li ti trên bề mặt khuôn. Điều này có thể thực hiện dễ dàng bằng cách dùng vật liệu xốp làm vỏ khuôn, nhưng ở xưởng không có loại vật liệu này. Cái khó ló cái khôn, anh đã nghĩ ra cách quét một lớp mỏng bột lưu huỳnh lên bề mặt khuôn đúc bằng thép. Quả thật, không khí dễ dàng thoát ra ngoài qua lớp bột này. Và, khi chi tiết sắp định hình, bột lưu huỳnh tác dụng với cao su, biến bề mặt chi tiết thành một lớp bóng láng. Đó chỉ là một việc nhỏ trong rất nhiều việc kỹ sư Lê Văn K. đã làm được trong quá trình sản xuất. Anh tiết lộ:  \"Nhờ nắm vững Algôrit giải các bài toán sáng chế (ARIZ) nên khi gặp bài toán kỹ thuật, tôi đã nhanh chóng xác định được mâu thuẫn kỹ thuật, mâu thuẫn lý học, cương quyết đẩy các mâu thuẫn đó đến tột cùng để có được lời giải gần với kết quả lý tưởng cuối cùng (KLC)\" . ARIZ là một nội dung quan trọng của môn phương pháp luận sáng tạo KHKT mà mấy năm trước, anh Lê Văn K. được học tại Trung tâm Sáng tạo KHKT (Trường đại học Tổng hợp). Ghé Trung tâm vào một buổi sáng đầu Xuân, chúng tôi gặp anh Hà Văn Luân, giáo viên của một trường cấp 2 ở Thủ Đức, đến ghi danh theo học lớp đêm. Anh thành thật kể:  \"Khi tôi giảng bài, học sinh thường kêu khó hiểu. Nghe đồn ở trung tâm này có lớp dạy cách suy nghĩ để đạt hiệu quả cao trong công việc nên tôi theo học. Chưa biết hiệu quả ra sao nhưng cái tên môn học nghe dội quá\" . Khoa học về tư duy sáng tạo đúng là ngành khoa học còn quá mới mẻ ở nước ta. Tiến sĩ Phan Dũng, giám đốc Trung tâm, là một trong số rất ít nhà khoa học Việt Nam được tiếp cận với khoa học này ở một trường đại học của Liên Xô (cũ). Trung tâm Sáng tạo KHKT chỉ mới có tên gọi chính thức từ cuối tháng 4/1991, nhưng môn khoa học về tư duy sáng tạo được tiến sĩ Phan Dũng tổ chức giảng dạy ở thành phố suốt từ năm 1977 đến nay. Gần 900 học viên của 22 khóa học bước đầu được trang bị hệ thống những thủ thuật và phương pháp tích cực hóa tư duy. Nghề nghiệp và trình độ khác nhau, nhưng mỗi người học đều tìm thấy từ môn học này những cách thức tốt nhất để khắc phục tính ì tâm lý, đánh thức tiềm năng sáng tạo của mình trong từng công việc cụ thể. Phương pháp phân tích hình thái đã giúp anh Đặng Quốc Trí, huấn luyện viên võ thuật, thành lập được nhiều đòn thế tấn công có thể có trong môn Việt Võ Đạo; chị thợ may Nguyễn Thị Mai Diễm thiết kế được nhiều kiểu áo mới lạ, ưng ý. Phương pháp đối tượng tiêu điểm được anh Phạm Văn Thu, cán bộ giảng dạy Đại học Y-Dược, vận dụng vào việc liên kết những kiến thức rời rạc, hình thành nên một hệ thống kiến thức hoàn chỉnh trong bài giảng, giúp sinh viên dễ hiểu bài hơn. Anh Hồng Tuấn Minh, sinh viên Đại học Tổng hợp, thấy rõ việc học tập môn tiếng Anh của mình có kết quả hơn khi học theo phương pháp các câu hỏi kiểm tra. Tư duy sáng tạo, nếu được hiểu như là những suy tưởng khoa học nhằm đạt tới cái mới, cái có ích cho cuộc sống thì không chỉ nhờ những tư chất trời phú, mà mọi người đều có khả năng sáng tạo. Nhưng việc biến những khả năng ấy thành hiện thực lại phụ thuộc nhiều vào phương pháp tư duy của mỗi người. Kiên trì \"thử và sai\" đến một lúc nào đó, có thể đạt được kết quả gần với KLC, song phải tốn rất nhiều thời gian, công sức, vật tư… Khoa học về tư duy sáng tạo chỉ ra cho người học con đường ngắn nhất đi đến kết quả công việc. Cuối năm 1990, một nhóm học viên dưới sự hướng dẫn của tiến sĩ Phan Dũng, đã nghiên cứu thành công giải pháp  \"Cơ cấu kẹp các tờ giấy rời\" , được Cục sáng chế cấp bằng độc quyền giải pháp hữu ích số HI-0049. Anh Phùng Hữu Hạt, người đại diện nhóm đứng tên chủ bằng, cho biết toàn bộ chi phí cho việc nghiên cứu thành công giải pháp này chưa đến 2 triệu đồng. Không thể đòi hỏi kết quả nhiều hơn ở một lớp sơ cấp với 60 tiết học. Thu hoạch lớn nhất, phổ biến nhất của những người theo học lớp này là thái độ tự tin, chủ động và cách giải quyết khá hợp lý những bài toán trong cuộc sống. Tiến sĩ Phan Dũng cho biết: \"Từ những kết quả ban đầu, tôi dự định sẽ phổ biến môn học này cho nhiều người hơn, ở trình độ cao hơn, để từng bước đưa môn học này vào nhà trường\". Thiết nghĩ, Nhà nước cần tạo điều kiện cho ý tưởng khoa học và tâm huyết này sớm biến thành hiện thực. (Báo \"Sài Gòn Giải Phóng\", ra ngày 21/02/1992)   Một khoa học dành cho sự cất cánh Người viết: Lê Vinh Quốc \"Thật là một điều kỳ diệu!\"  – anh Quách Thụy Môn, cử nhân hóa học 42 tuổi thốt lên –  \"Chỉ sáu chục giờ học mà tôi thấy quý vô cùng, vì từ nay cách nhìn của tôi, suy nghĩ của tôi, tư duy của tôi mới thật sự trả về đúng tên gọi của nó… Tôi có thể chắc chắn nói rằng: Tôi sẽ sáng tạo được!\" . Đó là cảm tưởng của anh sau khi học hết chương trình sơ cấp của bộ môn \"Phương pháp luận sáng tạo khoa học kỹ thuật\". Ở Việt Nam, môn khoa học mới mẻ này lần đầu tiên xuất hiện tại thành phố Hồ Chí Minh chỉ 2 năm sau đại thắng mùa xuân 1975. Nhờ miền Nam được giải phóng, nhà vật lý trẻ tuổi Phan Dũng tốt nghiệp ở Liên Xô được điều động vào công tác tại thành phố Hồ Chí Minh. Trong khi còn theo học ngành vật lý, chàng sinh viên Phan Dũng đã không bỏ lỡ cơ hội theo học khóa đầu tiên của trường đại học đầu tiên ở Liên Xô giảng dạy bộ môn \"Phương pháp luận sáng tạo\" vào năm 1971, do chính người đề xướng khoa học đó ở Liên Xô là Genrikh Saulovich Altshuller hướng dẫn. Hiểu rõ tầm quan trọng của khoa học này đối với tương lai phát triển của đất nước, anh luôn mơ ước đến ngày đem các hạt giống của nó gieo trồng trên đất nước quê hương. Chính tiềm năng, phong cách và điều kiện của thành phố mang tên Bác đã giúp anh biến ước mơ thành hiện thực. Kể từ khi lớp học đầu tiên về \"Phương pháp luận sáng tạo\" được mở vào năm 1977 cho đến nay, 26 khóa với gần 1.000 học viên đã được học khoa học này (đến năm 1987, ở Hà Nội mới khai giảng khóa đầu tiên của khoa học về sáng tạo). Nhưng mãi đến tháng 4/1991, Trung tâm Sáng tạo Khoa học-kỹ thuật do giáo sư tiến sĩ Phan Dũng làm giám đốc mới chính thức được thành lập tại Trường đại học tổng hợp thành phố Hồ Chí Minh để nghiên cứu, giảng dạy và phổ biến sâu rộng khoa học này. Khoa học về tư duy sáng tạo đã được đề cập đến từ đầu Công nguyên, và người Hy Lạp cổ gọi nó là Ơ-ri-xtic (Heuristics) có gốc là từ Ơ-ri-ca (Eureka). Nhưng do cách tiếp cận quá chung và chủ yếu do không có nhu cầu xã hội nên Ơ-ri-xtic bị nhân loại lãng quên. Mãi đến gần đây, nhất là từ sau chiến tranh thế giới thứ hai, do nhu cầu bức bách của cuộc cách mạng khoa học – kỹ thuật đặt ra, người ta phải nhớ lại Ơ-ri-xtic và đặt vào nó những hy vọng tìm ra cách tổ chức hợp lý, nâng cao hiệu quả quá trình tư duy sáng tạo – công nghệ làm sáng chế, phát minh. Nhằm mục đích đó, khoa học về tư duy sáng tạo được nghiên cứu ở cả Liên Xô (trước đây) và phương Tây theo những phương hướng không hoàn toàn giống nhau. Ở Liên Xô (trước đây), khoa học này được coi là \"Lý thuyết giải các bài toán sáng chế\" (viết tắt theo tiếng Nga là TRIZ) với hạt nhân của nó là \"Algorit giải các bài toán sáng chế\" (ARIZ). Phần lớn học viên các lớp Phương pháp luận sáng tạo ở thành phố Hồ Chí Minh đã chọn môn học này chỉ vì tò mò trước tên gọi của một khoa học mới lạ. Họ đã không thất vọng khi bước vào học tập, vì bị cuốn hút mạnh mẽ bởi những tri thức hết sức mới mẻ có tính thực tiễn rất cao, và bởi năng lực cũng như phương pháp truyền thụ của người thầy. Bởi thế, đã có những học viên sống và công tác ở Thủ Đức vẫn đều đặn đạp xe về thành phố theo học lớp này trong những buổi tối khuya. Đã có những trường hợp hai cha con học cùng khóa, hoặc kẻ trước người sau. Nhiều học viên từ các tỉnh Sông Bé, Hậu Giang, Đắc Lắc… về thành phố học nghiệp vụ, đã tranh thủ học thêm môn khoa học sáng tạo. Trong lớp học, có thể thấy các thanh niên nam nữ vừa tốt nghiệp lớp 12 ngồi bên các bậc cha chú họ. Có những người thợ may, thợ cơ khí cùng các kỹ sư, bác sĩ, dược sĩ, nhà giáo các cấp; các nhà khoa học và giảng viên đại học; có cán bộ tuyên huấn, đoàn thể, có sĩ quan quân đội… cùng học với nhau. Đã có người trước ngày lên máy bay xuất cảnh còn ráng theo học buổi cuối cùng, và khi đến nước Mỹ đã viết thư về xin tài liệu để tiếp tục học thêm. Ở nước ngoài cũng có dạy môn này nhưng học phí quá cao ít ai theo nổi. Ở Mỹ, khoa học này gọi là Synectics. Mỗi nhóm theo học môn này tại Cambridge (Massachusetts) phải trả hàng trăm ngàn đô-la. Tại một trung tâm giảng dạy Phương pháp luận sáng tạo ở Singapore do người Mỹ tổ chức, học viên phải trả 2.000 đô-la Mỹ cho 3 ngày học. Tại Trung tâm Sáng tạo ở thành phố Hồ Chí Minh, học phí cho 1 khóa 60 tiết kéo dài trong 2 tháng là một số tiền Việt Nam nhỉnh hơn… 5 đô-la một chút! Kết thúc mỗi khóa học, hầu hết học viên đều thu hoạch được nhiều điều như một nhà hóa học đã phát biểu ở đầu bài viết này. Học viên Ngô Thị Thu Tâm, 23 tuổi với trình độ văn hóa lớp 12, đã bộc bạch:  \"Trước kia tôi thiếu tự tin, lười suy nghĩ. Giờ đây tôi lạc quan hơn và thích quan sát, ham tìm tòi, suy nghĩ, đầu óc trở nên sắc sảo, nhạy bén hơn, giải quyết vấn đề nhanh hơn, hiệu quả hơn và ít tốn kém hơn\" . Các học viên thường lấy làm tiếc rằng họ được học môn này quá muộn; nếu sớm hơn thì sự nghiệp mà họ đang phục vụ sẽ có thể thành đạt sớm hơn và to lớn hơn nhiều. Dĩ nhiên ý kiến của các học viên vẫn chỉ là cảm tưởng. Nhưng cảm tưởng của những người trong cuộc ấy chứa đựng hạt nhân chân lý. Ngày nay ai cũng biết rằng, mặc dù tài nguyên và nguồn vốn đầu tư là rất quan trọng, yếu tố quyết định sự phát triển của đất nước vẫn là khả năng sáng tạo của con người. Lịch sử đã khẳng định rằng tầm cao của tiến bộ và thành đạt của một dân tộc bao giờ cũng quan hệ mật thiết với sức sáng tạo và sức sản xuất của dân tộc đó. Chính vì vậy mà khoa học về sáng tạo ngày càng trở nên đắt giá trên thế giới. Cũng như đa số các dân tộc châu Á, người Việt Nam không thua sút ai về sự cần cù, thông minh, nhưng lại yếu kém về hơn về khả năng sáng tạo. Bởi thế, phương pháp luận sáng tạo lại càng quan trọng đối với nước ta trên con đường cất cánh. Lẽ dĩ nhiên, như tiến sĩ Phan Dũng thường nói, chỉ một ngành khoa học không thôi, không làm gì được. Vấn đề là ở sự đồng bộ và tính hệ thống của mọi lĩnh vực. Tuy vậy, vai trò thúc đẩy và tác dụng thiết thực của phương pháp luận sáng tạo là hết sức rõ ràng. Giúp con người khắc phục \"sức ì tâm lý\", gạt bỏ những nhầm lẫn quanh co của \"phương pháp thử và sai\", nó góp phần làm cho tốc độ phát triển trong sự nghiệp của mỗi người và của toàn dân tộc tăng lên. Trong chuyến đến thăm Trung tâm Sáng tạo Khoa học – kỹ thuật của Trường đại học Tổng hợp thành phố Hồ Chí Minh ngày 19/12/1992, các chuyên viên giám định sáng chế thuộc Cục sáng chế Nhật Bản là các ông Mitsuharu Oda và Yoshiaki Kawasaki đã phát biểu:  \"Chúng tôi tin tưởng rằng sự phát triển sau này của Việt Nam rất nhanh, nhờ vào Trung tâm đáng chú ý như thế này\" . Nảy mầm và phát triển tại thành phố Hồ Chí Minh, trung tâm \"đáng chú ý\" này đã được Ủy ban khoa học kỹ thuật thành phố, Trường đại học Tổng hợp, Nhà văn hóa thanh niên, các cơ quan truyền thông báo chí cùng một số cơ quan khác giúp đỡ và ủng hộ. Tuy vậy, phương thức hoạt động chủ yếu của trung tâm vẫn là tự trang trải kinh phí theo cơ chế thị trường. Với phương thức này, trung tâm đã và sẽ có khả năng phát triển. Nhưng nếu không được Nhà nước đầu tư một cách thích đáng, thì dù là một Trung tâm sáng tạo khoa học-kỹ thuật đi nữa, cũng khó làm nên những điều kỳ diệu. (Báo \"Sài Gòn Giải Phóng\", ra ngày 06/05/1992)   Có gì thú vị như phương pháp luận sáng tạo? Người viết: Hữu Thiện Tiếng sét ái tình với anh giáo viên trẻ Từ Tân Phú (An Giang), anh giáo viên trẻ Trịnh Xuân Khanh quyết định kết thúc chặng đường tám năm làm thầy giáo của mình để về Sài Gòn học đại học và luyện thi lấy bằng C Anh ngữ. Một trong những địa chỉ đầu tiên anh tìm tới để ghi danh: Trung tâm Sáng tạo Khoa học-kỹ thuật (viết tắt là TSK) thuộc Đại học tổng hợp TPHCM. Môn học: Phương pháp luận sáng tạo (PPLST). Buổi học đầu tiên, Khanh gặp một cú sét ái tình! Qua phần giới thiệu và phân tích của tiến sĩ Phan Dũng – giám đốc kiêm giảng viên... duy nhất của TSK, anh chợt phát hiện được sự tồn tại của tính ì tâm lý trong con người và ngay trong chính mình, lực cản trong mọi hoạt động sáng tạo. Sự cuốn hút của môn PPLST từ đó cứ tăng dần lên. Khanh về nhà nhất định kéo chị, em và các cháu phải đi học môn này bằng được. Hiện, nhà anh đã có tới sáu người gồm ba sinh viên, hai bác sĩ và một kỹ sư địa chất đang học các lớp về tư duy sáng tạo. Học về các thủ thuật (nguyên tắc sáng tạo), về các phương pháp tích cực hóa tư duy... Không chỉ theo học, họ còn tham dự thường xuyên vào nhóm Chủ nhật, một loại Câu lạc bộ tự nguyện gồm hơn 30 cựu học viên về PPLST. Nhóm tự thành lập từ tháng 10/1992, sinh hoạt thường kỳ vào mỗi chiều chủ nhật để giúp nhau tìm hiểu thêm về lịch sử môn học. Để làm giàu quỹ bài tập của nhóm bằng cách hàng tuần, mỗi người nộp một bài tập chọn từ cuộc sống, từ những tình huống có vấn đề trong học tập, trong lao động sản xuất, kinh doanh... và cả trong... chuyện tình yêu của mình (!) để cùng nhau phân tích, rèn luyện các thao tác sáng tạo. Hướng xa hơn: Nhóm sẽ tìm nhận hợp đồng sáng tạo các mẫu mã mới, đưa ra các giải pháp mới, ý tưởng mới... theo đơn đặt hàng của các công ty, xí nghiệp. Tất cả đều mong ước tự hình thành nên một nhóm Synectics chuyên nghiệp, đầu tiên của thành phố – bao gồm những nhà sáng tạo thuộc những ngành nghề khác nhau, tập hợp lại để cố gắng giải một cách sáng tạo các bài toán thiết kế kỹ thuật và quản lý hành chính, xã hội... Ở đâu lại chẳng cần sáng tạo Trong 1.294 học viên của 31 khóa PPLST tại TSK, Dương Ngọc Thạch là một học viên khá... đặc biệt: Anh đã vận dụng thành công môn học này từ trước khi là học viên của TSK. Năm 1987, tình cờ Thạch mua được cuốn sách Algôrit sáng chế. Lúc đó, anh vẫn còn là một thanh niên đang vất vả kiếm sống bằng đủ thứ việc linh tinh: Sửa ống nước, sửa điện nhà, vẽ chân dung, vẽ trang trí bảng hiệu... Quan sát cuộc sống của trẻ trong các trường mẫu giáo, anh phát hiện một điều: Các cháu quá thiếu đồ chơi, trong khi những mẫu mã do Bộ giáo dục-đào tạo hướng dẫn thực hiện lại quá đơn điệu và thô sơ. Thiên hướng yêu trẻ kết hợp với việc ngẫm nghĩ và vận dụng triệt để các thủ thuật sáng tạo cơ bản từ cuốn sách gối đầu giường vừa nêu đã đưa anh vào một bước ngoặt mới trong cuộc đời: Trở thành nhà thiết kế mẫu đồ chơi trẻ em. Vận dụng nguyên tắc đổi chiều, Thạch đã sáng tạo từ chiếc xích đu bình thường theo kiểu ngang thành xích đu chiều dọc, từ đó cải tiến thành xích đu xe buýt với nhiều chỗ ngồi hơn, thú vị hơn với trẻ. Cuộc sáng tạo vẫn chưa chịu ngừng lại: Vận dụng thêm nguyên tắc cầu hóa (làm tròn), Thạch làm tiếp kiểu xích đu tự xoay theo đủ mọi chiều. Rồi lại cải tiến loại đu quay bình thường thành đu quay xe đạp (lắp bánh xe) nhờ áp dụng nguyên tắc chuyển sang chiều khác và nguyên tắc kết hợp... Từ năm 1987 tới năm 1992, Thạch đã có hơn 40 mẫu đồ chơi sáng tạo như thế, giúp các cô mẫu giáo-nhà trẻ thực hiện được rất nhiều yêu cầu giáo dục, rèn luyện trẻ em theo yêu cầu của Bộ. Từ một thanh niên nghèo và nặng nợ vợ con, nay Thạch đã là ông chủ trẻ mới 30 tuổi của một cơ sở sản xuất đồ chơi nổi tiếng khắp từ Bắc vào Nam. Nhớ lại những buổi đầu theo học TSK, Thạch cảm thấy như tìm được một kho báu quý giá hơn mọi của cải vật chất, cảm thấy từ nay mình có thể bay bổng với nhiều suy nghĩ mới lạ trên một hướng đi mới mà biết chắc sẽ thành công lớn hơn. Được thầy giúp đỡ, Thạch phát hiện được ngay tính ì tâm lý của chính mình ngay trong những hoạt động sáng tạo đúng bài bản sách vở! Từ nay, anh đã biết nhìn nhiều chiều hơn, nhìn rộng hơn và rộng lượng hơn, biết biến hại thành lợi ngay cả trong hoạt động quản lý sản xuất, kinh doanh... Phương pháp luận sáng tạo cho trẻ em – sao lại không? Trong 1.294 học viên của TSK, hơn 40% là học sinh, sinh viên đã đến với PPLST ở các lớp đêm. Số học viên còn lại thì đa dạng hơn: Giáo viên, giáo sư đại học, bác sĩ, dược sĩ, tiến sĩ, kỹ sư... bên cạnh những cán bộ Đảng, công nhân, thợ may, huấn luyện viên thể thao, tiểu thương... Có dịp đọc gần 1.000 bài thu hoạch cuối khóa của 32 khóa PPLST, phóng viên ghi nhận: Tất cả học viên đều khẳng định họ đã biết lắng nghe và tôn trọng ý kiến người khác để chắt lọc những yếu tố có giá trị, biết nhìn một vấn đề theo nhiều chiều và lại quen tìm ra cái mới trong mỗi chiều nhìn, biết tự tin hơn... để vươn tới một nhân cách sáng tạo! Đúng như cô bạn Diễm Linh – giáo viên Trường thực nghiệm quận 1, học viên khóa 28 PPLST nói:  \"Khát vọng sáng tạo là nhân bản!\" . Trong khi đó, cô thợ may Mai Diễm – một cựu học viên đã biết vận dụng các thủ thuật sáng tạo để thiết kế nhiều kiểu áo mới, lạ thì tâm sự:  \"Giá như tôi biết tới môn PPLST từ khi mới thôi học phổ thông thì có lẽ nghề nghiệp của mình đã khác nhiều! Vì vậy, tôi quyết định khi con mình đủ tuổi sẽ cho tới học môn PPLST...\" Tại sao lại không? Tại sao lại không nghĩ tới chuyện dạy trẻ em VN về tư duy sáng tạo, theo những hình thức phù hợp với tâm sinh lý của các em? Liệu có thể đưa môn PPLST vào các trường phổ thông, xem như một môn học chính khóa? Tại sao trong lĩnh vực mới mẻ và rất quan trọng này, dường như chỉ mới có bàn tay của tiến sĩ Phan Dũng? (Báo “Tuổi Trẻ”, ra ngày 3/12/1992)  "
        },
        {
          "title": "Học có giám sát",
          "relevance": "1",
          "url": "https://vi.wikipedia.org/wiki/H%E1%BB%8Dc_c%C3%B3_gi%C3%A1m_s%C3%A1t",
          "content": "Bách khoa toàn thư mở Wikipedia \n\t\t\t\t\tBước tới:\t\t\t\t\t menu , \t\t\t\t\t tìm kiếm Bài viết này  cần thêm  chú thích nguồn gốc  để  kiểm chứng thông tin .  Mời bạn giúp  hoàn thiện bài viết này  bằng cách bổ sung chú thích tới  các nguồn đáng tin cậy . Các nội dung không có nguồn có thể bị nghi ngờ và xóa bỏ. Học có giám sát  là một kĩ thuật của ngành  học máy  để xây dựng một hàm ( function ) từ dữ liệu huấn luyện.  Dữ liệu huấn luyện  bao gồm các cặp gồm đối tượng đầu vào (thường dạng vec-tơ), và đầu ra mong muốn. Đầu ra của một hàm có thể là một giá trị liên tục (gọi là  hồi qui ), hay có thể là dự đoán một nhãn phân loại cho một đối tượng đầu vào (gọi là  phân loại ). Nhiệm vụ của chương trình học có giám sát là dự đoán giá trị của hàm cho một đối tượng bất kì là đầu vào hợp lệ, sau khi đã xem xét một số ví dụ huấn luyện (nghĩa là, các cặp đầu vào và đầu ra tương ứng). Để đạt được điều này, chương trình học phải tổng quát hóa từ các dữ liệu sẵn có để dự đoán được những tình huống chưa gặp phải theo một cách \"hợp lý\" (xem  thiên kiến qui nạp  -  inductive bias ). (So sánh với  học không có giám sát .) Học có giám sát có thể tạo ra hai loại mô hình. Phổ biến nhất, học có giám sát tạo ra một mô hình toàn cục ( global model ) để ánh xạ đối tượng đầu vào đến đầu ra mong muốn. Tuy nhiên, trong một số trường hợp, việc ánh xạ được thực hiện dưới dạng một tập các mô hình cục bộ (như trong phương pháp  lập luận theo tình huống  ( case-based reasoning ) hay  giải thuật láng giềng gần nhất ). Để có thể giải quyết một bài toán nào đó của học có giám sát (ví dụ: học để  nhận dạng chữ viết tay ) người ta phải xem xét nhiều bước khác nhau: Xác định loại của các ví dụ huấn luyện. Trước khi làm bất cứ điều gì, người kĩ sư nên quyết định loại dữ liệu nào sẽ được sử dụng làm ví dụ. Chẳng hạn, đó có thể là một ký tự viết tay đơn lẻ, toàn bộ một từ viết tay, hay toàn bộ một dòng chữ viết tay. Thu thập tập huấn luyện. Tập huấn luyện cần đặc trưng cho thực tế sử dụng của hàm chức năng. Vì thế, một tập các đối tượng đầu vào được thu thập và đầu ra tương ứng được thu thập, hoặc từ các chuyên gia hoặc từ việc đo đạc tính toán. Xác định việc biểu diễn các đặc trưng đầu vào cho hàm chức năng cần tìm. Sự chính xác của hàm chức năng phụ thuộc lớn vào cách các đối tượng đầu vào được biểu diễn. Thông thường, đối tượng đầu vào được chuyển đổi thành một vec-tơ đặc trưng, chứa một số các đặc trưng nhằm mô tả cho đối tượng đó. Số lượng các đặc trưng không nên quá lớn, do  sự bùng nổ tổ hợp  ( curse of dimensionality ); nhưng phải đủ lớn để dự đoán chính xác đầu ra. Xác định cấu trúc của hàm chức năng cần tìm và giải thuật học tương ứng. Ví dụ, người kĩ sư có thể lựa chọn việc sử dụng  mạng nơ-ron nhân tạo  hay  cây quyết định . Hoàn thiện thiết kế. Người kĩ sư sẽ chạy giải thuật học từ tập huấn luyện thu thập được. Các tham số của giải thuật học có thể được điều chỉnh bằng cách tối ưu hóa hiệu năng trên một tập con (gọi là tập  kiểm chứng  - validation  set) của tập huấn luyện, hay thông qua  kiểm chứng chéo  ( cross-validation ). Sau khi học và điều chỉnh tham số, hiệu năng của giải thuật có thể được đo đạc trên một tập kiểm tra độc lập với tập huấn luyện. Mục lục 1 Cực tiểu hóa rủi ro kinh nghiệm 2 Hướng tiếp cận và giải thuật 3 Ứng dụng 4 Vấn đề chung 5 Tham khảo 6 Liên kết ngoài Cực tiểu hóa rủi ro kinh nghiệm [ sửa  |  sửa mã nguồn ] Mục tiêu của việc học có giám sát một mô hình toàn cục là tìm ra một hàm  g , khi cho sẵn một tập các điểm có dạng ( x ,  g ( x )). Giả thiết rằng đã biết trước đặc điểm của hàm  g  đối với một tập điểm. Tập điểm đó đã được lấy mẫu  độc lập và có cùng phân bố  ( independent and identically distributed (i.i.d.) ) theo một xác suất phân bố  p  chưa biết từ một tập lớn hơn và có thể vô hạn. Ngoài ra, giả sử tồn tại một hàm  hàm tổn thất  ( loss function ) theo tác vụ  L  có dạng: L : Y × Y → R + {\\displaystyle L:Y\\times Y\\to {\\mathbb {R}}^{+}} trong đó  Y  là trùng với  miền xác định  của  g  và  L  ánh xạ tới các  số thực  không âm (có thể đặt thêm hạn chế cho  L ). Giá trị  L ( z ,  y ) là tổn thất nảy sinh khi đoán giá trị của  g  tại một điểm cho trước là  z  trong khi giá trị thực của nó là  y . Hàm  rủi ro f  được định nghĩa là  giá trị kỳ vọng  của hàm tổn thất và có công thức như sau: R ( f ) = ∑ i L ( f ( x i ) , g ( x i ) ) p ( x i ) {\\displaystyle R(f)=\\sum _{i}L(f(x_{i}),g(x_{i}))\\;p(x_{i})} nếu xác suất phân bố  p  là rời rạc (trường hợp xác suất phân bố liên tục cần một  tích phân xác định  ( definite integral ) và một  hàm mật độ xác suất . Mục tiêu là tìm một hàm  f *  trong số một lớp con cố định các hàm để cho rủi ro  R ( f * ) là  cực tiểu . Tuy nhiên, do thường chỉ biết được đặc điểm của hàm  g  cho một tập hữu hạn điểm ( x 1 ,  y 1 ),..., ( x n ,  y n ), người ta chỉ có thể xác định gần đúng rủi ro thực sự, ví dụ, với  rủi ro kinh nghiệm  ( empirical risk ): R ~ n ( f ) = 1 n ∑ i = 1 n L ( f ( x i ) , y i ) {\\displaystyle {\\tilde {R}}_{n}(f)={\\frac {1}{n}}\\sum _{i=1}^{n}L(f(x_{i}),y_{i})} Nguyên lý của  cực tiểu hóa rủi ro kinh nghiệm  là chọn hàm  f *  sao cho rủi ro kinh nghiệm là nhỏ nhất. Lý thuyết học bằng thống kê tìm hiểu xem việc cực tiểu hóa rủi ro kinh nghiệm có thể đạt được trong những điều kiện nào và có thể trông đợi các tính toán xấp xỉ tốt đến đâu. Hướng tiếp cận và giải thuật [ sửa  |  sửa mã nguồn ] học bằng phân tích  ( analytical learning ) mạng nơ-ron  nhân tạo\n Instantaneously trained neural networks kỹ thuật lan truyền ngược  ( backpropagation ) boosting thống kê Bayes lập luận theo tình huống  ( case-based reasoning ) học  cây quyết định inductive logic programming hồi quy Gauss  ( Gaussian process regression ) learning automata theory Minimum message length  ( cây quyết định , đồ thị quyết định, v.v.) naive Bayes classifier thuật toán láng giềng gần nhất probably approximately correct learning  (PAC) learning symbolic machine learning  algorithms subsymbolic machine learning  algorithms support vector machines Random Forests Ứng dụng [ sửa  |  sửa mã nguồn ] Tin sinh học Nhận dạng chữ viết tay Thu thập thông tin  ( information retrieval ) Nhận dạng đối tượng trong  computer vision Nhận dạng ký tự quang học Phát hiện spam Nhận dạng mẫu Nhận dạng tiếng nói Vấn đề chung [ sửa  |  sửa mã nguồn ] computational learning theory  (ngành toán học liên quan đến việc phân tích các thuật toán học máy) thiên kiến qui nạp  ( inductive bias ) overfitting  (hàm học được quá thích nghi với tập huấn luyện) version space Tham khảo [ sửa  |  sửa mã nguồn ] Liên kết ngoài [ sửa  |  sửa mã nguồn ] Chương trình mạng nơ ron đa lớp (Multi Layer Neural Network) và mạng nơ ron tự tổ chức (Self Organizing Maps) có giải thích bằng tiếng Việt. Sử dụng phần mềm mạng nơ ron 3 lớp Spice-MLP Sử dụng phần mềm mạng tự tổ chức Spice-SOM Hướng dẫn sử dụng mạng nơ ron trong các ứng dụng thực tế  trong đó có minh họa phân loại ảnh khuôn mặt, ảnh người đi bộ, ảnh xe hơi, dự báo chứng khoán và một số ví dụ khác \n\t\t\t\t\t\tLấy từ “ https://vi.wikipedia.org/w/index.php?title=Học_có_giám_sát&oldid=26626395 ”\t\t\t\t\t Thể loại :  Học máy Thể loại ẩn:  Trang thiếu chú thích trong bài Trình đơn chuyển hướng Công cụ cá nhân Chưa đăng nhập Thảo luận cho địa chỉ IP này Đóng góp Mở tài khoản Đăng nhập Không gian tên Bài viết Thảo luận Biến thể Các hiển thị Đọc Sửa đổi Sửa mã nguồn Xem lịch sử Khác Tìm kiếm Xem nhanh Trang Chính Bài viết chọn lọc Tin tức Bài viết ngẫu nhiên Thay đổi gần đây Phản hồi lỗi Quyên góp Tương tác Hướng dẫn Giới thiệu Wikipedia Cộng đồng Thảo luận chung Giúp sử dụng Liên lạc Công cụ Các liên kết đến đây Thay đổi liên quan Các trang đặc biệt Liên kết thường trực Thông tin trang Khoản mục Wikidata Trích dẫn trang này In/xuất ra Tạo một quyển sách Tải về dưới dạng PDF Bản để in ra Ngôn ngữ khác العربية Català Čeština Deutsch Ελληνικά English Español فارسی Français 한국어 Italiano עברית 日本語 Polski Русский Simple English Suomi ไทย Türkçe Українська 中文 Sửa liên kết  Trang này được sửa đổi lần cuối lúc 05:27 ngày 31 tháng 5 năm 2017. Văn bản được phát hành theo  Giấy phép Creative Commons Ghi công–Chia sẻ tương tự ; có thể áp dụng điều khoản bổ sung. Với việc sử dụng trang web này, bạn chấp nhận  Điều khoản Sử dụng  và  Quy định quyền riêng tư . \nWikipedia® là thương hiệu đã đăng ký của  Wikimedia Foundation, Inc. , một tổ chức phi lợi nhuận. Quy định quyền riêng tư Giới thiệu Wikipedia Lời phủ nhận Nhà phát triển Tuyên bố về cookie Phiên bản di động"
        },
        {
          "title": "Máy học là gì?(Marchine Learning- Week1)",
          "relevance": "1",
          "url": "https://tranvan.wordpress.com/2013/04/28/may-hoc-la-gimarchine-learning-week1/",
          "content": "Vant on software Search: Trang chủ About Bài viết Bình luận Khác Python Django webserver Nginx social network CS Java Oracle NoSQL ←  Tiếng Việt trong Django model phiên bản 1.4 Mật khẩu cho màn hình quản trị Tomcat 7 trong Debian  → Tháng Tư 28, 2013 %(count) bình luận Like this: Số lượt thích Đang tải... Liên quan Filed under  CS Tagged with  machine learning mataza says: Tháng Năm 29, 2014 lúc 5:39 chiều tài liệu thô sơ quá Phản hồi Nhập bình luận của bạn tại đây... Mời bạn điền thông tin vào ô dưới đây hoặc kích vào một biểu tượng để đăng nhập: Thư điện tử  (bắt buộc) (Địa chỉ của bạn được giấu kín) Tên  (bắt buộc) Trang web  Bạn đang bình luận bằng tài khoản WordPress.com  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Twitter  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Facebook  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Google+  (  Đăng xuất  /  Thay đổi  ) Hủy bỏ Connecting to %s Notify me of new comments via email. Tài liệu Các bước bảo mật hệ điều hành RedHat 5 Bài viết mới Năm mức ngu dốt Đổi NLS_CHARACTERSET trong Oracle Import export dữ liệu trên Oracle sử dụng exp, imp Giới thiệu về NoSQL Giới thiệu về mô hình MVC, ví dụ sử dụng ngôn ngữ Python Tháng Tư 2013 H B T N S B C « Th1   Th7 » 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30   Meta Đăng ký Đăng nhập RSS  cho bài viết Dòng thông tin  các phản hồi. WordPress.com Tag aptana studio 3 ccna command copytables date time debian decorators django i18n java6 ldf machine learning mạng xã hội networking Nginx ngôn ngữ lập trình pip psycopg2 python python egg Shell social network Solaris sql sql 2008 ssl certificate Thích Nhất Hạnh webserver zen python Tìm kiếm cho: RSS RSS - Bài viết Thống kê Blog 19,210 hits Trang About Tạo một website miễn phí hoặc 1 blog với WordPress.com. %d  bloggers like this:"
        },
        {
          "title": "Chứng chỉ kỹ sư giám sát thi công xây dựng công trình",
          "relevance": "0",
          "url": "http://www.baoxaydung.com.vn/news/vn/phap-luat/tu-van-phap-luat/chung-chi-ky-su-giam-sat-thi-cong-xay-dung-cong-trinh.html",
          "content": "Liên hệ Ban biên tập \r\n                                    Báo điện tử Xây dựng: baoxaydung.com.vn - Email: baoxaydung@baoxaydung.com.vn \r\n                                    Cơ quan chủ quản: Báo điện tử Xây dựng - Bộ Xây dựng;  \r\n                                    Giấy phép số: 2033/GP-BTTTT Bộ Thông tin và Truyền thông Cấp ngày: 30/11/2011. \r\n                                    Tòa soạn: 37 Lê Đại Hành - Q.Hai Bà Trưng - Hà Nội, điện thoại: 04.39740487 Fax: 04.38216555 Tổng Biên tập Nguyễn Anh Dũng Phó Tổng Biên tập Tào Khánh Hưng Phó Tổng Biên tập Nguyễn Thái Bình Phó Tổng Biên tập Trần Thị Thu Hà Phó Tổng Biên tập Vũ Văn Tiến Trưởng ban điện tử Báo Xây dựng Nguyễn Sơn Tùng Trưởng phòng Hành chính - Trị sự Nguyễn Văn Khang P.Trưởng phòng Hành chính - Trị sự Nguyễn T Quỳnh Hoa Trưởng Ban Kinh tế La Đức Hùng Trưởng Ban Xây dựng & Pháp luật  Hà Thu Hiền Trưởng  Phòng Biên tập - Phóng viên Dương Thị Hồng Diên Trưởng phòng Thư ký Tòa soạn Nguyễn Ngọc Quyết Phó Trưởng ban Điện tử Báo Xây dựng  Nguyễn Thị Bắc Thái Phó Trưởng phòng Thư ký Tòa soạn  Nguyễn Thị Hạnh Phó Trưởng  phòng Biên tập - Phóng viên  Vũ Thị Thanh Tâm Phó Trưởng phòng Biên tập - Phóng viên  Dương Văn Bắc Trưởng Đại diện VP 2 TPHCM Lê Xuân Viên Phó Trưởng Đại diện VP 2 TPHCM Đào Mạnh Cường Trưởng Đại diện VP tại Thanh Hóa Đào Nguyên Xim Trưởng đại diện tại Đà Nẵng Vũ Thị Ngọc Long Liên hệ quảng cáo Đọc báo giấy Báo Xây dựng 094.540.6866 Thứ hai 23/10/2017 11:28 Trang chủ Đảng ủy Bộ Thời sự Quy hoạch - Kiến trúc Bất động sản Vật liệu Kinh tế Xã hội Pháp luật Bạn đọc Thế giới Văn hóa - Du lịch Clip - Giải trí Sức khỏe Pháp luật Tư vấn pháp luật Video RSS 06:04  |  25/08/2014 Chứng chỉ kỹ sư giám sát thi công xây dựng công trình (Xây dựng) - Tôi là cựu sinh viên Đại học xây dựng Hà Nội, tôi tốt nghiệp năm 2006. Vốn là một người đam mê và yêu nghề, kể từ ngày ra trường tôi đã đi làm nhiều nơi và tham gia thực hiện rất nhiều dự án lớn về xây dựng. Nay cảm thấy bản thân đủ lớn để có thể thành lập công ty kinh doanh độc lập nhưng tôi lại chưa có chứng chỉ kỹ sư giám sát thi công xây dựng công trình để đăng ký kinh doanh. \r\n\tVậy cho tôi hỏi điều kiện để được cấp chứng chỉ trên là gì? Tôi đã có kinh nghiệm làm việc 7 năm thì có đủ điều kiện được cấp chứng chỉ không? \r\n\t(Hoàng Đức Việt, quận Đống Đa, Hà Nội) Ảnh minh họa. Trả lời: \r\n\tCăn cứ vào Điều 6 Thông tư 12/2009/TT-BXD hướng dẫn chi tiết về cấp chứng chỉ hành nghề hoat động xây dựng thì điều kiện để được cấp chứng chỉ hành nghề giám sát thi công xây dựng công trình bao gồm: \r\n\t- Có quyền công dân và có đủ năng lực hành vi dân sự theo quy định của pháp luật. \r\n\t- Có đạo đức nghề nghiệp và có hồ sơ xin cấp chứng chỉ, đã nộp lệ phí theo quy định. \r\n\t- Có bằng tốt nghiệp đại học trở lên (đối với chứng chỉ hành nghề loại màu đỏ) hoặc bằng tốt nghiệp cao đẳng, trung cấp (đối với chứng chỉ hành nghề loại màu hồng) thuộc chuyên ngành đào tạo phù hợp với nội dung xin đăng ký hành nghề, do cơ sở đào tạo hợp pháp của Việt Nam hoặc nước ngoài cấp. \r\n\t- Đã trực tiếp tham gia thực hiện thiết kế hoặc thi công xây dựng từ 3 năm trở lên hoặc đã tham gia thực hiện thiết kế, thi công xây dựng ít nhất 5 công trình được nghiệm thu bàn giao. \r\n\t-  Đã thực hiện giám sát thi công xây dựng công trình từ 3 năm trở lên (áp dụng đối với trường hợp trước khi Luật Xây dựng có hiệu lực thi hành). \r\n\t- Có chứng nhận bồi dưỡng nghiệp vụ giám sát thi công xây dựng phù hợp với loại công trình xin cấp chứng chỉ do cơ sở đào tạo được Bộ Xây dựng công nhận cấp. \r\n\tNhư vậy, nếu ông đã có kinh nghiệm làm việc 7 năm trong lĩnh vực thiết kế hoặc thi công xây dựng thì về mặt kinh nghiệm chuyên môn ông đã đủ điều kiện để được cấp chứng chỉ. Tuy nhiên, ngoài điều kiện về kinh nghiệm làm việc, ông còn phải đáp ứng đầy đủ các điều kiện khác nêu trên thì sẽ được Giám đốc Sở Xây dựng (nơi xin cấp chứng chỉ hành nghề) quyết định cấp chứng chỉ. Luật sư Đào Thị Dung \r\n        Các bài mới:\r\n     Chương trình họp đại hội đồng cổ đông                                  (26/09)\r\n             Việt kiều - người nước ngoài mở nhà hàng ăn tại Việt Nam                                  (25/09)\r\n             Sự kiện bất khả kháng trong hợp đồng giao dịch                                  (24/09)\r\n             Việt kiều sở hữu, sử dụng nhà, đất ở Việt Nam                                  (23/09)\r\n             Người nước ngoài làm việc tại Việt Nam                                  (22/09)\r\n             Xử lý tiền đền bù, hỗ trợ khi gia đình không nhận                                  (18/09)\r\n             Quy định về việc giao đất, cho thuê đất đối với đất đang có người sử dụng cho người khác                                  (18/09)\r\n             Quy định về chuyển giao công nghệ                                  (17/09)\r\n             Con riêng có quyền hưởng di sản thừa kế?                                  (16/09)\r\n             Hợp đồng góp vốn dự án nhà chung cư                                  (15/09)\r\n             Các bài đã đăng: Chương trình họp đại hội đồng cổ đông            \t              (26/09) Việt kiều - người nước ngoài mở nhà hàng ăn tại Việt Nam            \t              (25/09) Sự kiện bất khả kháng trong hợp đồng giao dịch            \t              (24/09) Việt kiều sở hữu, sử dụng nhà, đất ở Việt Nam            \t              (23/09) Người nước ngoài làm việc tại Việt Nam            \t              (22/09) Xử lý tiền đền bù, hỗ trợ khi gia đình không nhận            \t              (18/09) Quy định về việc giao đất, cho thuê đất đối với đất đang có người sử dụng cho người khác            \t              (18/09) Quy định về chuyển giao công nghệ            \t              (17/09) Con riêng có quyền hưởng di sản thừa kế?            \t              (16/09) Hợp đồng góp vốn dự án nhà chung cư            \t              (15/09) Trang chủ Đảng ủy Bộ Thời sự Quy hoạch - Kiến trúc Bất động sản Vật liệu Kinh tế Xã hội Pháp luật Bạn đọc Thế giới Văn hóa - Du lịch Clip - Giải trí Sức khỏe \r\n        \t© Báo điện tử Xây dựng: baoxaydung.com.vn - Email: baoxaydung@baoxaydung.com.vn \r\n            Cơ quan chủ quản: Bộ Xây dựng; Giấy phép (cấp lại) số: 559/GP-BTTTT Bộ Thông tin và Truyền thông; Cấp ngày: 12/12/2016. \r\n            Tòa soạn: 37 Lê Đại Hành - Quận Hai Bà Trưng - Hà Nội. Fax: 04.38216555.\r\n         \r\n            Tổng biên tập: NGUYỄN ANH DŨNG - Phó Tổng biên tập: TÀO KHÁNH HƯNG, NGUYỄN THÁI BÌNH, TRẦN THỊ THU HÀ \r\n            Phó Tổng biên tập phụ trách Báo điện tử: NGUYỄN SƠN TÙNG \r\n            Cơ quan đại diện: 14 Kỳ Đồng, Phường 9, Quận 3, TP. Hồ Chí Minh; ĐT: 0839312246 – Fax: 08.39312245; Email: cqhcm@baoxaydung.com.vn\r\n         \r\n            ® Nghiêm cấm sao chép dưới mọi hình thức nếu không được Báo điện tử Xây dựng chấp thuận bằng văn bản. Based on MasterCMS 2013 Ultimate edition Ver 2.4"
        },
        {
          "title": "1. Bảo đảm các điều kiện về cấp thoát nước và vệ sinh môi trường trong trường học",
          "relevance": "0",
          "url": "https://thuvienphapluat.vn/van-ban/Giao-duc/Thong-tu-13-2016-TTLT-BYT-BGDDT-huong-dan-thuc-hien-cong-tac-y-te-truong-hoc-295062.aspx",
          "content": "...loại rủi ro pháp lý, nắm cơ hội làm giàu... Trang chủ Các\r\n                    gói dịch vụ Tin tức Pháp Luật Cộng đồng DânLuật Liên hệ Sơ đồ WebSite    \r\n         Giới thiệu     Hướng dẫn sử dụng      Rss      Homepage     \r\n         Widget Phần mềm  THƯ VIỆN PHÁP LUẬT Giới thiệu phần\r\n                        mềm Cập nhật thư viện Lấy mã sử dụng Đại lý phân phối Hướng dẫn sử dụng Trang chủ Văn bản mới Công điện 1632/CĐ-TTg năm 2015 về tăng cường công tác phòng chống sốt xuất huyết do ... \r\n            VĂN BẢN GỐC ...Văn bản đăng trên  Công báo \r\n            là văn bản chính thức và có giá trị như bản gốc... \r\n              \r\n                  Điều 87 Nghị định 34/2016/NĐ-CP X \r\n            CÁC NỘI DUNG ĐƯỢC SỬA ĐỔI, HƯỚNG DẪN\r\n           \r\n        Các nội dung của VB này được VB khác thay đổi, hướng dẫn sẽ được làm nổi bật bằng\r\n        các màu sắc:\r\n      \r\n               : Sửa đổi, thay thế,\r\n        hủy bỏ\r\n               : Bổ sung\r\n               : Đính chính\r\n               : Hướng dẫn\r\n         \r\n        Click vào nội dung được bôi màu       \r\n        để xem chi tiết.\r\n          Từ khoá:  Số Hiệu, Tiêu đề  hoặc  Nội dung ngắn gọn  của Văn Bản... Văn bản PL Dự thảo Công văn TCVN \r\n            Tra cứu nhanh :    \r\n             Thời điểm Áp dụng Tình trạng Hiệu lực Lĩnh vực, Ngành Thời gian Ban hành Thành Viên Quên mật khẩu?   \r\n                         Đăng ký mới Đăng\r\n                                                        nhập bằng Google  \r\n                            ĐĂNG KÝ THÀNH VIÊN MIỄN PHÍ ĐỂ Khai thác hơn\r\n                                278.000\r\n                                văn bản Pháp Luật Nhận Email văn bản mới hàng tuần Được hỗ trợ tra cứu trực tuyến Tra cứu Mẫu hợp đồng, Bảng giá\r\n                                đất ... và nhiều  Tiện ích quan trọng khác Hỗ\r\n                                            trợ Dịch Vụ (028) 3930 3279 Hỗ trợ trực tuyến 0906 22 99 66 01238 22 99 66 \r\n                                        Họ và tên:\r\n                                     Ông Bà Anh Chị \r\n                                        Tên Thành Viên:\r\n                                     \r\n                                        Mật khẩu:\r\n                                     \r\n                                        E-mail:\r\n                                     \r\n                                        Điện thoại di động:\r\n                                     Vui lòng nhập thêm số điện thoại để chúng tôi hỗ trợ bạn tốt hơn \r\n                                        Thỏa Ước Dịch Vụ:\r\n                                     Tôi đã đọc và đồng ý Bạn đã là thành viên thì đăng nhập để sử dụng tiện ích \r\n                                        Tên Thành Viên:\r\n                                     \r\n                                        Mật khẩu:\r\n                                     Đăng\r\n                                                        nhập bằng Google  \r\n                                        Thông tư liên tịch 13/2016/TTLT-BYT-BGDĐT quy định về công tác y tế trường học do Bộ Y tế, Bộ Giáo dục và Đào tạo ban hành Số hiệu: \r\n                                            13/2016/TTLT-BYT-BGDĐT\r\n                                         Loại văn bản: \r\n                                            Thông tư liên tịch\r\n                                         Nơi ban hành: \r\n                                            Bộ Giáo dục và Đào tạo, Bộ Y tế\r\n                                         Người ký: \r\n                                            Nguyễn Thị Nghĩa, Nguyễn Thanh Long\r\n                                         Ngày ban hành: \r\n                                            12/05/2016\r\n                                         \r\n                                                Ngày hiệu lực: Đã biết Ngày công báo: \r\n                                                Đã biết\r\n                                             Số công báo: \r\n                                                Đã biết\r\n                                             Tình trạng: Đã biết Thông tư liên tịch 13/2016/TTLT-BYT-BGDĐT về công tác y tế trường học, bao gồm: quy định về cơ sở vật chất, trang thiết bị, môi trường học tập, chăm sóc y tế có liên quan tới sức khỏe của học sinh trong trường học. \r\n\t  \r\n\t1. Bảo đảm các điều kiện về cấp thoát nước và vệ sinh môi trường trong trường học \r\n\tĐối với công trình vệ sinh, Thông tư liên tịch 13 quy định như sau: \r\n\t- Về thiết kế: \r\n\t+ Đối với cơ sở giáo dục mầm non: áp dụng tiêu chuẩn tại mục 5.2.7 và mục 5.5.8 của TCVN 3907:2011 kèm theo Quyết định 2585/QĐ-BKHCN; \r\n\t+ Đối với trường tiểu học; lớp tiểu học trong trường phổ thông có nhiều cấp học và trong trường chuyên biệt: áp dụng tiêu chuẩn tại mục 5.6.1, mục 5.6.2 và mục 5.6.3 TCVN 8793:2011 kèm theo Quyết định 2585/QĐ-BKHCN; \r\n\t+ Đối với trường THCS; trường THPT; lớp THCS, lớp THPT trong trường phổ thông có nhiều cấp học và trong trường chuyên biệt: áp dụng theo mục 5.6 của TCVN 8794:2011 kèm theo Quyết định 2585/QĐ-BKHCN. \r\n\t- Về điều kiện bảo đảm hợp vệ sinh nhà tiêu: áp dụng theo QCVN 01:2011/BYT theo Thông tư 27/2011/TT-BYT; \r\n\t- Ngoài ra, Thông tư liên tịch số 13/2016 của Bộ Y tế và Bộ Giáo dục và Đào tạo quy định trường học phải có chỗ rửa tay với nước sạch, xà phòng hoặc dung dịch sát khuẩn khác. \r\n\t2. Bảo đảm các điều kiện về phòng y tế, nhân viên y tế trường học \r\n\t- Thông tư liên tịch 13/2016/BYT-BGDĐT quy định trường học phải có phòng y tế riêng, bảo đảm diện tích, ở vị trí thuận tiện cho công tác sơ cứu, cấp cứu và chăm sóc sức khỏe học sinh; \r\n\t- Phòng y tế của các trường tiểu học, THCS, THPT, trường phổ thông có nhiều cấp học, trường chuyên biệt được trang bị tối thiểu 01 giường khám bệnh và lưu bệnh nhân, bàn làm việc, ghế, tủ đựng dụng cụ, thiết bị làm việc thông thường, cân, thước đo, huyết áp kế, nhiệt kế, bảng kiểm tra thị lực, bộ nẹp chân, tay và một số thuốc thiết yếu phục vụ cho công tác sơ cấp cứu và chăm sóc sức khỏe học sinh theo Quyết định 1221/QĐ-BYT. \r\n\tMặt khác, theo Thông tư liên tịch số 13 năm 2016, đối với các cơ sở giáo dục mầm non cần có các trang bị, dụng cụ chuyên môn và thuốc thiết yếu phù hợp với lứa tuổi; \r\n\t3. Tổ chức các hoạt động quản lý, bảo vệ và chăm sóc sức khỏe học sinh \r\n\t- Theo đó, Thông tư liên tịch 13/2016/TTLT-BYT-BGDĐT quy định thực hiện kiểm tra sức khỏe vào đầu năm học để đánh giá tình trạng dinh dưỡng và sức khỏe: đo chiều cao, cân nặng đối với trẻ dưới 36 tháng tuổi; đo chiều cao, cân nặng, huyết áp, nhịp tim, thị lực đối với học sinh từ 36 tháng tuổi trở lên. \r\n\t- Đo chiều cao, cân nặng, ghi biểu đồ tăng trưởng, theo dõi sự phát triển thể lực cho trẻ dưới 24 tháng tuổi mỗi tháng một lần và cho trẻ em từ 24 tháng tuổi đến 6 tuổi mỗi quý một lần; theo dõi chỉ số khối cơ thể (BMI) ít nhất 02 lần/năm học để tư vấn về dinh dưỡng hợp lý và hoạt động thể lực đối với học sinh phổ thông. \r\n\tNgoài ra, còn tổ chức các hoạt động khác cụ thể tại Thông tư liên tịch số 13/2016 của BYT-BGDĐT. \r\n\t  \r\n\tThông tư liên tịch 13 có hiệu lực từ ngày 30/06/2016. MỤC LỤC VĂN BẢN\r\n                                 B Ộ  Y T Ế  - B Ộ  GIÁO\r\n  DỤC VÀ ĐÀO TẠO \r\n  ------- CỘNG\r\n  HÒA XÃ HỘI CHỦ NGHĨA VIỆT NAM \r\n  Độc lập - Tự do - Hạnh phúc  \r\n  --------------- Số: \r\n  13 /2016/TTLT-BYT-BGDĐT Hà\r\n  Nội, ngày  12  tháng  5 năm 20 1 6   THÔNG TƯ LIÊN TỊCH QUY ĐỊNH VỀ CÔNG TÁC Y TẾ TRƯỜNG HỌC Căn cứ  Nghị định số 63/2012/NĐ-CP  ngày 31 tháng 8 năm 2012 của Chính phủ\r\nquy định chức năng, nhiệm vụ, quyền hạn và cơ cấu tổ chức của Bộ Y tế; Căn cứ  Nghị định số 32/2008/NĐ-CP  ngày 19 tháng 3 năm 2008  của  Chính phủ quy định chức năng, nhiệm vụ, quyền\r\nhạn và cơ cấu  tổ chức  của Bộ Giáo dục và\r\nĐào tạo; Căn cứ Chỉ thị số  23/2006/CT-TTg \r\nngày 12 tháng 7 năm 2006 của Thủ tướng Chính phủ về việc tăng cường công tác y\r\ntế trong các trường học; Bộ trưởng Bộ Y tế và Bộ tr ưở ng Bộ Giáo dục và Đào tạo ban hành Thông tư liên tịch quy định  về  công tác  y  tế trường học. Chương I QUY ĐỊNH CHUNG Điều 1. Phạm vi\r\nđiều chỉnh và đối tượng áp dụng 1. Thông tư liên tịch này quy định về\r\ncông tác y tế trường học, bao gồm: quy định về cơ sở vật chất, trang thiết bị,\r\nmôi trường học tập, chăm sóc y tế có liên quan tới sức khỏe của học sinh trong\r\ntrường học. 2. Thông tư liên tịch này áp dụng đ ố i với  cơ sở  giáo dục mầm non; trường\r\ntiểu học; tr ườ n g\r\ntrung học cơ sở; trường trung học phổ thông và trường phổ thông có nhiều cấp học;\r\ntrường chuyên biệt không bao gồm trường dành cho người tàn tật, khuyết tật và\r\ntrường giáo dưỡng (sau đây gọi tắt là trường học); cơ quan, tổ chức và cá nhân\r\ncó liên quan. Điều 2. Giải\r\nthích từ ngữ Trong Thông tư liên tịch này các từ\r\nngữ dưới đây được hiểu như sau: 1. Vệ sinh trường học là các điều kiện\r\nbảo đảm về môi trường, cơ sở vật chất trường, l ớ p, trang\r\nthiết bị, chế độ vệ sinh dạy học, học tập, tập luyện thể dục, thể thao và chăm\r\nsóc sức khỏe trong các trường học. 2. Bệnh, tật học đường là những bệnh,\r\ntật học sinh mắc phải có liên quan đến điều kiện vệ sinh trường học không bảo đảm. Điều 3. Kinh phí\r\nthực hiện 1. Nguồn kinh phí thực hiện công tác\r\ny tế trường học bao gồm: a) Nguồn kinh phí sự nghiệp y tế,\r\ngiáo dục và đào tạo hằng năm theo phân cấp ngân sách hiện hành của các đơn vị; b) Nguồn bảo hiểm y tế học sinh theo\r\nquy định hiện hành; c) Nguồn tài trợ,  hỗ trợ  của các tổ chức, cá nhân trong nước và\r\nnước ngoài theo quy định của pháp luật và các nguồn thu hợp pháp khác. 2. Kinh phí thực hiện cho công tác y\r\ntế trường học phải được sử dụng đúng mục đích, đúng chế độ theo các quy định hiện\r\nhành của Nhà nước. 3. Việc lập dự toán, chấp hành dự\r\ntoán và quyết toán kinh phí thực hiện công tác y tế trường học được áp dụng\r\ntheo các quy định hiện hành. Chương II NỘI DUNG CỦA\r\nCÔNG TÁC Y TẾ TRƯỜNG HỌC Điều 4. Bảo đảm\r\ncác điều kiện về phòng học, bàn ghế, bảng viết, chiếu sáng, đồ chơi trong tr ườ ng học 1. Phòng học a) Đối với cơ sở giáo dục mầm non:\r\nđáp ứng yêu cầu thiết kế áp dụng theo tiêu chuẩn quy định tại mục 5.2 Tiêu chuẩn\r\nViệt Nam ( TCVN 3907: 2011 ) ban hành kèm theo Quyết định số  2585/QĐ-BKHCN  ngày\r\n23 tháng 8 năm 2011 của Bộ trưởng Bộ Khoa học và Công nghệ về việc công bố tiêu\r\nchuẩn quốc gia (sau đây gọi tắt là Quyết định số 2585/QĐ-BKHCN); b) Đối với trường tiểu học; lớp tiểu\r\nhọc trong trường phổ thông có nhiều cấp học và trong trường chuyên biệt: đáp ứng\r\nyêu cầu thiết kế áp dụng theo tiêu chuẩn quy định tại mục 5.2 Tiêu chuẩn Việt\r\nNam ( TCVN 8793: 2011 ) ban hành kèm theo Quyết định số 2585/QĐ-BKHCN; c) Đối với trường trung học cơ sở;\r\ntrường trung học phổ thông; lớp trung học cơ sở, lớp trung học phổ thông trong\r\ntrường ph ổ  thông có nhiều cấp học và trong trường chuyên\r\nbiệt: đáp ứng yêu cầu thiết kế quy định tại mục 5.2 Tiêu chu ẩ n Việt Nam ( TCVN 8794: 2011 ) ban hành kèm theo Quyết định số\r\n2585/QĐ-BKHCN. 2. Bàn ghế a) Đối với cơ sở giáo dục mầm non:\r\nkích thước bàn ghế áp dụng theo Tiêu chuẩn Việt Nam ( TCVN 1993 ) Bàn ghế học\r\nsinh mẫu giáo - Yêu cầu chung; b) Đối với trường tiểu học; trường\r\ntrung học cơ sở; trường trung học phổ thông; trường phổ thông có nhiều cấp học;\r\ntrường chuyên biệt: kích thước bàn ghế áp dụng theo hướng dẫn tại Thông tư liên\r\ntịch số  26/2011/TTLT-BGDĐT-BKHCN-BYT  ngày 16 tháng 6 năm 2011 của Bộ Giáo dục\r\nvà Đào tạo - Bộ Khoa học và Công nghệ - Bộ Y tế  hướ ng dẫn\r\ntiêu chuẩn bàn ghế học sinh trường tiểu học, trường trung học  cơ sở , trường trung học phổ thông. 3. Bảng phòng học đối với cấp học phổ\r\nthông a) Các phòng học phải trang bị bảng\r\nchống lóa; có màu xanh lá cây hoặc màu đen (nếu viết bằng phấn trắng), màu trắng\r\n(nếu viết bằng bút dạ); b) Chiều rộng của bảng từ  1 ,2m -  1 ,5m, chiều dài bảng từ 2, 0 m - 3,2m; c) Bảng treo ở giữa tường, mép dưới bảng\r\ncách nền phòng học từ 0,65m - 0,80m đối với trường tiểu học và từ 0,8m -  1 ,0m đối với trường trung học cơ sở và trung học phổ thông, khoảng cách\r\ntới mép bàn học sinh đầu tiên không nhỏ hơn 1,8m. 4. Chiếu sáng a) Đối với cơ sở giáo dục mầm mon:\r\nyêu cầu thiết kế áp dụng theo quy định tại mục 6.2 của Tiêu chuẩn Việt Nam\r\n( TCVN 3907:2011 ) ban hành kèm theo Quyết định số 2585/QĐ-BKHCN; b) Đối với trường tiểu học; lớp tiểu\r\nhọc trong trường phổ thông có nhiều cấp học và trong trường chuyên biệt: yêu cầu\r\nthiết kế áp dụng theo quy định tại mục 6.2 của Tiêu chuẩn Việt Nam ( TCVN\r\n8793:2011 ) ban hành kèm theo  Quyết định số \r\n2585/QĐ-BKHCN; c) Đối với trường trung học cơ sở; tr ườ ng trung học phổ thông; lớp trung học cơ sở, lớp trung học phổ thông\r\ntrong trường phổ thông có nhiều cấp học và trong trường chuyên biệt: yêu cầu\r\nthiết kế áp dụng theo quy định tại mục 6.2 của Tiêu chuẩn Việt Nam ( TCVN\r\n8794:2011 ) ban hành kèm theo  Quyết định số \r\n2585/QĐ - BKHCN. 5. Đ ồ  chơi cho\r\ntrẻ em trong trường học Đồ chơi cho trẻ em ở các trường học\r\nphải bảo đảm theo quy định tại Thông tư s ố 16/2011/TT-BGDĐT  ngày 13  tháng  4 năm 2011\r\ncủa Bộ trưởng Bộ Giáo dục và Đào tạo về việc trang bị, quản lý, sử dụng đồ chơi\r\ntrẻ em trong nhà trường. Điều 5. Bảo đảm\r\ncác điều kiện về cấp  thoát  nước và vệ\r\nsinh môi trường trong tr ườ ng học 1. Bảo đảm nước uống, nước sinh hoạt a) Trường học cung cấp đủ nước uống\r\ncho học sinh, tối thiểu 0,5 lít về mùa hè, 0,3 lít về mùa đông cho một học sinh\r\ntrong một buổi học; b) Trường học cung cấp đủ nước sinh\r\nhoạt cho học sinh, tối thiểu 4 lít cho một học sinh trong một buổi học; nếu\r\ndùng hệ thống cấp nước bằng đường ống thì mỗi vòi sử dụng tối đa cho 200 học\r\nsinh trong một buổi học; c) Trường học có học sinh nội trú\r\ncung cấp đủ nước ăn uống và sinh hoạt, tối thiểu 100 lít cho một học sinh trong\r\n24 giờ; d) Các trường học sử dụng nguồn nước\r\ntừ các cơ sở đủ điều kiện cung cấp nước ăn uống và nước sinh hoạt. Trường hợp\r\ntrường học tự cung cấp nguồn nước thì  chất \r\nlượng phải bảo đảm tiêu chuẩn về nước ăn uống theo Quy chuẩn kỹ thuật quốc gia\r\n(QCVN 01:2009/BYT) ban hành kèm theo Thông tư số  04/2009/TT-BYT  ngày 17 tháng 6\r\nnăm 2009 của Bộ trưởng Bộ Y tế ban hành quy chuẩn kỹ thuật quốc gia về chất lượng\r\nnước ăn uống; về nước k hoán g thiên nhiên\r\nvà nước uống đóng chai theo Quy chuẩn kỹ thuật quốc gia (QCVN 6 -1:2010/BYT)\r\nban hành kèm theo Thông tư số  34/2010/TT-BYT  ngày 02 tháng 6 năm 2010 của Bộ\r\ntrưởng Bộ Y tế ban hành quy chuẩn kỹ thuật quốc gia đ ố i với\r\nnước k hoán g thiên nhiên và nước uống đóng\r\nchai; về nước sinh hoạt theo Quy chuẩn kỹ thuật quốc gia (QCVN 02:2009/BYT) ban\r\nhành kèm theo Thông tư số  05/2009/TT-BYT  ngày 17 tháng 6 năm 2009 của Bộ trưởng\r\nBộ Y tế ban hành quy chuẩn kỹ thuật quốc gia về chất lượng nước sinh hoạt. 2. Công trình vệ sinh a)  V ề thiết kế: - Đ ố i với cơ sở\r\ngiáo dục mầm non: yêu cầu thiết kế áp dụng theo tiêu chuẩn quy định tại mục\r\n5.2.7 và mục 5.5.8 của Tiêu chuẩn  V iệt Nam ( TCVN\r\n3907:2011 ) ban hành kèm theo Quyết định số 2585/QĐ-BKHCN; - Đối với trường tiểu học; lớp tiểu học\r\ntrong trường phổ thông có nhiều cấp học và trong trường chuyên biệt: yêu cầu\r\nthiết kế áp dụng theo tiêu chuẩn quy định tại mục 5.6.1, mục 5.6.2 và mục 5.6.3\r\ncủa Tiêu chuẩn Việt Nam ( TCVN 8793:2011 ) ban hành kèm theo Quyết định số\r\n2585/QĐ-BKHCN; - Đối với trường trung học cơ sở; trường\r\ntrung học phổ thông; lớp trung học  cơ sở ,\r\nlớp trung học phổ thông trong trường phổ thông có nhiều cấp học và trong trường\r\nchuyên biệt: yêu cầu thiết kế áp dụng theo quy định tại mục 5.6 của Tiêu chuẩn\r\nViệt Nam ( TCVN 8794:2011 ) ban hành kèm theo Quyết định số 2585/ Q Đ-BKHCN. b)  V ề điều kiện\r\nbảo đảm hợp vệ sinh nhà tiêu: áp dụng theo Quy chuẩn k ỹ  thuật\r\nquốc gia (QCVN 01:2011/BYT) theo Thông tư số  27/2011/TT-BYT  ngày 24 tháng 6 năm\r\n2011 của Bộ trưởng Bộ Y tế ban hành quy chuẩn kỹ thuật quốc gia về nhà tiêu -\r\nĐiều kiện bảo đảm hợp vệ sinh; c) Trường học phải có ch ỗ  rửa tay với nước sạch, xà phòng hoặc dung dịch sát khuẩn khác. 3. Thu gom và xử lý chất thải a) Trường học phải có hệ thống cống\r\nrãnh  thoát  nước mưa, nước thải sinh hoạt,\r\nkhông đ ể  nước ứ đọng xung quanh trường lớp; có hệ thống  thoát  nước riêng cho khu vực phòng thí nghiệm,\r\ncơ sở thực hành, phòng y tế, nhà bếp, khu vệ sinh, khu nuôi động vật thí nghiệm; b) Các trường học hợp đồng với các cơ\r\nsở đủ điều kiện thu gom, xử lý chất thải, rác thải sinh hoạt. Trường hợp trường\r\nhọc tự thu gom, xử lý thì phải bảo đảm theo quy định tại khoản 4, mục VII, phần\r\nII của Quy chuẩn kỹ thuật quốc gia (QCVN 07:2010/BYT) vệ sinh phòng bệnh truyền\r\nnhiễm trong các cơ sở giáo dục thuộc hệ thống giáo dục quốc dân ban hành kèm\r\ntheo Thông tư s ố 46/2010/TT-BYT  ngày 29 tháng 12 năm 2010\r\ncủa Bộ trưởng Bộ Y tế ban hành “Quy chuẩn k ỹ  thuật quốc\r\ngia về vệ sinh phòng bệnh truyền nhiễm trong các cơ sở giáo dục thuộc hệ thống\r\ngiáo dục quốc dân” (sau đây gọi tắt là Thông tư số 46/2010/TT-BYT). Điều 6. Bảo đảm\r\ncác điều kiện về an toàn thực phẩm 1. Trường học có bếp ăn nội trú, bán\r\ntrú a) Bảo đảm các điều kiện cơ s ở\r\n vật chất về an toàn vệ sinh thực phẩm theo khoản 1, khoản 2, khoản\r\n3, mục VI và yêu cầu vệ sinh đối với hoạt động bảo quản, chế biến thực phẩm\r\ntheo khoản 5 của Quy chuẩn kỹ thuật quốc gia (QCVN 07:2010/BYT) phòng chống bệnh\r\ntruyền nhiễm trong các cơ sở giáo dục thuộc hệ thống giáo dục quốc dân ban hành\r\nkèm theo Thông tư số 46/2010/TT-BYT; b) Bếp ăn, nhà ăn (khu vực ăn uống),\r\ncăng tin trong trường học bảo đảm theo quy định tại Điều 4 Thông tư số\r\n 30/2012/TT-BYT  ngày 05 tháng 12 năm 2012 của Bộ trưởng Bộ Y tế quy định về điều\r\nkiện vệ sinh an toàn thực phẩm đ ố i với cơ sở kinh doanh dịch\r\nvụ ăn uống, kinh  d oanh thức ăn đường phố; c) Đối với người làm việc tại nhà ăn,\r\nbếp ăn trong trường học phải bảo đảm các yêu cầu về sức khỏe theo quy định tại\r\nThông tư số  15/2012/TT-BYT  ngày 12 tháng 9 năm 2012 của Bộ trưởng Bộ Y tế quy định\r\nvề điều kiện chung bảo đảm an toàn thực phẩm đối với cơ sở sản xuất, kinh doanh\r\nthực phẩm. 2. Đối với các trường học không có bếp\r\năn nội trú, bán trú ký hợp đồng với các cơ sở có giấy chứng nhận cơ sở đủ điều\r\nkiện an toàn thực  phẩm  đ ể  cung  cấp  thức ăn cho học sinh;\r\ncăng tin của nhà trường phải bảo  đ ảm yêu cầu tại điểm b\r\nkhoản 1 Điều này. Điều 7. Bảo đảm\r\nmôi trường thực thi chính sách và xây dựng các mối quan hệ xã hội trong trường\r\nhọc, liên kết cộng đồng 1. Ban chăm sóc sức khỏe học sinh có\r\nphân công trách nhiệm cụ thể cho các thành viên và tổ chức họp tối thiểu 01 lần/học\r\nkỳ. 2. Thực hiện các chính sách, quy định\r\nvà chế độ chăm sóc sức khỏe học sinh trong trường học. 3. Xây dựng m ố i\r\nquan hệ tốt giữa thầy cô giáo với học sinh và học sinh  với  học sinh; xây dựng môi trường trường học lành mạnh, không phân biệt đối\r\nxử, không bạo lực. 4. Xây dựng mối liên hệ giữa trường học\r\nvới gia đình và cộng đồng để giúp đỡ, hỗ trợ chăm sóc sức khỏe học sinh. Điều 8. Bảo đảm\r\ncác điều kiện về phòng y tế, nhân viên y tế trường học 1. Phòng y tế trường học a) Trường học phải có phòng y tế\r\nriêng, bảo đảm diện tích, ở vị trí thuận tiện cho công tác sơ cứu, cấp cứu và\r\nchăm sóc sức khỏe học sinh; b) Phòng y tế của các trường tiểu học,\r\ntrung học cơ sở, trung học phổ thông, trường phổ thông có nhiều cấp học, trường\r\nchuyên biệt được trang bị  tối  thiểu 01\r\ngiường khám bệnh và lưu bệnh nhân, bàn làm việc, ghế, tủ đựng dụng cụ, thiết bị\r\nlàm việc thông thường, cân, thước đo, huyết áp kế, nhiệt kế, bảng kiểm tra thị\r\nlực, bộ nẹp chân, tay và một s ố  thuốc thiết yếu phục vụ\r\ncho công tác sơ cấp cứu và chăm sóc sức khỏe học sinh theo quy định tại Quyết định\r\nsố  1221/QĐ-BYT  ngày 07 tháng 4 năm 2008 của Bộ trưởng Bộ Y tế ban hành danh mục\r\ntrang thiết bị, thuốc thiết yếu dùng trong phòng y tế học đường của các trường\r\ntiểu học, trung học cơ sở, trung học phổ thông, trường phổ thông có nhiều cấp học.\r\nĐối với các cơ sở giáo dục mầm non cần có các trang bị, dụng cụ chuyên môn và\r\nthuốc thi ế t yếu phù hợp với lứa tuổi; c) Có sổ khám bệnh theo mẫu A 1 /YTCS quy định tại Thông tư  27/2014/TT-BYT  ngày 14 tháng 8 năm 2014 của\r\nBộ trưởng Bộ Y tế quy định hệ thống biểu mẫu thống kê y tế áp dụng đ ố i với các cơ sở y tế tuyến tỉnh, huyện, xã; sổ theo dõi sức khỏe học\r\nsinh theo mẫu số 01 và sổ theo dõi tổng hợp tình trạng sức khỏe học sinh theo mẫu\r\nsố 02 quy định tại Phụ lục số 01 ban hành kèm theo Thông tư liên tịch này. 2. Nhân viên y tế trường học a) Nhân viên y tế trường học phải có\r\ntrình độ chuyên môn từ y sĩ trung cấp trở lên. Căn cứ điều kiện thực tiễn tại địa\r\nphương, các trường học bố trí nhân viên y tế trường học đáp ứng quy định tại Điểm\r\nnày hoặc ký hợp đồng với Trạm Y tế xã, phường, thị trấn (sau đây gọi t ắ t là Trạm Y tế xã) hoặc cơ sở khám bệnh, ch ữ a bệnh\r\ntừ hình thức phòng khám đa khoa trở lên để chăm sóc sức khỏe học sinh; b) Nhân viên y tế trường học phải được\r\nthường xuyên cập nhật kiến thức chuyên môn y tế thông qua các hình thức hội thảo,\r\ntập huấn, đào tạo, bồi dưỡng nghiệp vụ chuyên môn do ngành Y tế, ngành Giáo dục\r\ntổ chức để triển khai được các nhiệm vụ quy định; c) Nhân viên y tế trường học có nhiệm\r\nvụ tham mưu, tổ chức thực hiện theo quy định tại  Điều  9,  Điều  10 và các nhiệm vụ khác do Lãnh đạo trường học phân công. Điều 9. Tổ chức\r\ncác hoạt động quản lý, bảo vệ và chăm sóc sức khỏe học sinh 1. Thực hiện  kiểm tra  sức khỏe vào đầu năm học để đánh giá tình trạng dinh dưỡng\r\nvà sức khỏe: đo chiều cao, cân nặng đối với trẻ dưới 36 tháng tuổi; đo chiều\r\ncao, cân nặng, huyết áp, nhịp tim, thị lực đối với học sinh từ 36 tháng tuổi tr ở  lên. 2. Đo chiều cao, cân nặng, ghi biểu đồ\r\ntăng trưởng, theo dõi sự phát triển thể lực cho trẻ dưới 24 tháng tuổi mỗi tháng\r\nmột lần và cho trẻ em từ 24 tháng tuổi đến 6 tuổi m ỗ i quý\r\nmột lần; theo dõi chỉ số khối cơ thể (BMI) ít nhất 02 lần/năm học để tư vấn về\r\ndinh dưỡng hợp lý và hoạt động thể lực đ ố i với học sinh phổ\r\nthông. 3. Thường xuyên theo dõi sức khỏe học\r\nsinh, phát hiện giảm thị lực, cong vẹo cột sống, bệnh răng miệng, rối loạn sức\r\nkhỏe tâm thần và các bệnh tật khác để xử trí, chuy ể n đến\r\ncơ sở khám bệnh, chữa bệnh theo quy định và áp dụng chế độ học tập, rèn luyện\r\nphù hợp với tình trạng sức khỏe. 4. Phối hợp với các cơ sở y tế có đủ\r\nđiều kiện để  tổ chức  khám, điều trị theo\r\ncác chuyên khoa cho học sinh. 5. Sơ cứu, cấp cứu theo quy định hiện\r\nhành của Bộ Y tế. 6. Tư vấn cho học sinh, giáo viên,\r\ncha mẹ hoặc người giám hộ của học sinh về các vấn đề liên quan đến bệnh tật,\r\nphát triển thể chất và tinh thần của học sinh; hướng dẫn cho học sinh biết tự\r\nchăm sóc sức khỏe; trường hợp trong trường học có học sinh khuyết tật thì tư vấn,\r\nhỗ trợ cho học sinh khuyết tật h òa  nhập. 7. Hướng d ẫ n tổ\r\nchức bữa ăn học đường bảo đảm dinh dưỡng hợp lý, đa dạng thực phẩm, phù hợp với\r\nđ ố i tượng và l ứ a tuổi đối với các trường\r\ncó học sinh nội trú, bán trú. 8. Phối hợp với cơ sở y tế địa phương\r\ntrong việc  tổ chức  các chiến dịch tiêm chủng,\r\nuống vắc xin phòng bệnh cho học sinh. 9. Thông báo định kỳ tối thiểu 01 lần/năm\r\nhọc và khi cần thiết về tình hình sức  khỏe \r\ncủa học sinh cho cha mẹ hoặc người giám hộ của học sinh. Nhân viên y tế trường\r\nhọc đánh giá tình trạng sức khỏe học sinh vào cuối m ỗ i cấp\r\nhọc đ ể  làm căn cứ theo dõi sức khỏe ở cấp học tiếp theo. 10. Lập và ghi chép vào sổ khám bệnh,\r\nsổ theo dõi sức khỏe học sinh, sổ theo dõi tổng hợp tình trạng sức khỏe học\r\nsinh. 11. Thường xuyên kiểm tra, giám sát\r\ncác điều kiện học tập, vệ sinh trường lớp, an toàn thực phẩm, cung cấp nước uống,\r\nxà phòng rửa tay. Chủ động triển khai các biện pháp và chế độ vệ sinh phòng, chống\r\ndịch theo quy định tại Thông tư số  46/2010/TT-BYT  và các hướng dẫn khác của cơ\r\nquan y tế. 12. Tổ chức triển khai các chương\r\ntrình y tế, phong trào vệ sinh phòng bệnh, tăng cường hoạt động thể lực, dinh d ưỡ ng hợp lý, xây dựng môi trường không khói thuốc lá, không sử dụng đồ uống\r\ncó cồn và các chất gây nghiện. Điều 10. Tổ chức\r\ncác hoạt động truyền thông, giáo dục sức khỏe 1. Biên soạn, sử dụng các tài liệu\r\ntruyền thông giáo dục sức khỏe với nội dung phù hợp với từng nhóm đối tượng và\r\nđiều kiện cụ thể của từng địa phương. 2. Tổ chức truyền thông, giáo dục sức\r\nkhỏe cho học si nh  và cha mẹ hoặc người giám hộ về các biện\r\npháp phòng chống dịch, bệnh truyền nhiễm; phòng chống ngộ độc thực phẩm; dinh\r\ndưỡng hợp lý; hoạt động thể lực; phòng chống tác hại của thuốc lá; phòng ch ố ng tác hại của rượu, bia; phòng chống bệnh, tật học đường; chăm sóc\r\nrăng miệng; phòng chống các bệnh về mắt; phòng chống tai nạn thương tích và các\r\nchiến dịch truyền thông, giáo dục khác liên quan đến công tác y tế trường học\r\ndo Bộ Y tế, Bộ Giáo dục và Đào tạo phát động. 3. Lồng ghép các nội dung giáo dục sức\r\nkhỏe, phòng chống bệnh tật trong các giờ giảng. 4. Tổ chức cho học sinh thực hành các\r\nhành vi vệ sinh cá nhân, vệ sinh môi trường, phòng chống dịch, bệnh truyền nhiễm;\r\nphòng chống ngộ độc thực  phẩm ; dinh dưỡng\r\nhợp lý; hoạt động thể lực; phòng chống tác hại  của \r\nthuốc lá; phòng chống tác hại của rượu, bia; phòng chống bệnh, tật học đường;\r\nchăm sóc răng miệng; phòng chống các bệnh về mắt; phòng chống tai nạn thương\r\ntích thông qua các hình thức, mô hình phù hợp. Điều 11. Thống\r\nkê báo cáo và đánh giá về công tác y tế trường học 1. Báo cáo định kỳ, báo cáo đột xuất a) Thực hiện báo cáo định kỳ hoạt động\r\ny tế trong năm học chậm nhất vào ngày 30 tháng 5 theo m ẫ u\r\nbáo cáo quy định tại Phụ lục số 02 ban hành kèm theo Thông tư liên tịch này về\r\nTrạm Y tế xã trên địa bàn, Phòng Giáo dục và Đào tạo, Sở Giáo dục và Đào tạo\r\ntheo phân cấp quản lý; b) Thực hiện báo cáo đột xuất theo yêu\r\ncầu của cơ quan quản lý cấp trên. 2. Đánh giá công tác y tế trường học Các trường học tự tổ chức đánh giá kết\r\nquả thực hiện công tác y tế trường học vào cu ố i mỗi năm học:\r\nCơ sở giáo dục mầm non đánh giá theo mẫu quy định tại Phụ lục s ố  03 ban hành kèm theo Thông tư liên tịch này; trường tiểu học, trung học\r\ncơ sở, trung học phổ thông, trường phổ thông có nhiều cấp học, trường chuyên biệt\r\nđánh giá theo m ẫ u quy định tại Phụ lục số 04 ban hành kèm\r\ntheo Thông tư liên tịch này. Chương III TỔ CHỨC THỰC HIỆN Điều 12. Trách\r\nnhiệm của trường học 1. Tổ chức triển khai thực hiện đầy đủ\r\ncác nội dung về y tế trường học được quy định tại Thông tư liên tịch này. 2. Chỉ đạo, kiểm tra, giám sát việc\r\nthực hiện các nhiệm vụ y tế trường học. 3. Bảo đảm về cơ sở vật chất, trang\r\nthiết bị, thuốc cho nhân viên y t ế  trường học thực hiện\r\nnhiệm vụ. 4. Đề xuất với cơ quan có thẩm quyền\r\nđể bảo đảm nhân lực thực hiện công tác y tế trường học. 5. Kiện toàn Ban chăm sóc sức khỏe học\r\nsinh, Trưởng ban là đại diện Ban giám hiệu, Phó trưởng ban là Trạm trưởng Trạm\r\nY tế xã, ủy viên thường trực là nhân viên y tế trường học, các ủy viên khác là\r\ngiáo viên giáo dục thể chất, Tổng phụ trách Đội (đối với  cơ sở  giáo dục tiểu học và trung học cơ sở), đại\r\ndiện Đoàn Thanh niên Cộng sản Hồ Chí Minh, Hội Chữ thập đỏ trường học, Ban đại\r\ndiện cha mẹ học sinh. 6. Trong trường hợp có quy hoạch, xây\r\ndựng mới, cải tạo, sửa chữa trường học, mua sắm trang thiết bị, đồ dùng học tập,\r\nđồ chơi tr ẻ  em, thuốc, trang thiết bị y tế phải thực hiện\r\nhoặc tham mưu với cơ quan có thẩm quyền thực hiện theo các quy chuẩn, tiêu chuẩn\r\nhiện hành. Điều 13. Trách\r\nnhiệm của Trạm Y tế xã 1.  Xây dựng kế hoạch hoạt động y tế\r\ntrường học trong kế hoạch hoạt động chung của Trạm Y tế xã hàng năm. 2.  Phân công cán bộ theo dõi công tác\r\ny tế trường học; hỗ trợ chuyên môn kỹ thuật để thực hiện quy định tại Thông tư\r\nliên tịch này. 3.  Thực hiện việc thống kê, báo cáo kết\r\nquả hoạt động y tế trường học theo quy định. Điều 14. Trách\r\nnhiệm của Phòng Giáo dục và Đào tạo, Sở Giáo dục và Đào tạo 1.  Phối hợp với ngành Y tế địa phương\r\ntham mưu với  Ủy ban  nhân dân các cấp  tro ng việc lập kế hoạch, chỉ đạo tổ chức thực hiện công tác y tế trường học\r\ntrên địa bàn. 2.  Đôn đốc, giám sát và thanh tra, kiểm\r\ntra các trường học trong việc thực hiện các nội dung về công tác y tế trường học\r\ntheo quy định tại Thông tư liên tịch này. 3.  Phối hợp với ngành Y tế trong công\r\ntác đào tạo, tập huấn chuyên môn, nghiệp vụ cho nhân viên y tế trường học. 4.  Phối hợp với ngành Y tế địa phương\r\nhằng năm tiến hành tổng kết, đánh giá công tác y tế trường học trên địa bàn. 5.  Thực hiện việc thống kê, báo cáo kết\r\nquả hoạt động y tế trường học theo quy định. 6.  Việc tuyển dụng nhân viên y tế trường\r\nhọc phải thực hiện theo các quy định của các cơ quan có thẩm quyền. 7.  Trong trường hợp có quy hoạch, xây\r\ndựng mới, cải tạo, sửa chữa trường học, mua sắm trang thiết bị, đồ dùng học tập,\r\nđồ chơi trẻ em, thuốc, trang thiết bị y tế phải thực hiện hoặc tham mưu với cơ\r\nquan có thẩm quyền thực hiện theo các quy chuẩn, tiêu chuẩn hiện hành Điều 15. Trách\r\nnhiệm của Trung tâm Y tế huyện, Trung tâm Y tế dự phòng tỉnh và Sở Y tế 1.  Chủ trì và  phối hợp  với cơ quan quản lý giáo dục tham mưu cho  Ủy ban  nhân dân các cấp trong việc lập kế hoạch,\r\ntổ chức chỉ đạo thực hiện công tác y tế trường học trên địa bàn theo phân cấp. 2.  Phối hợp với cơ quan quản lý giáo\r\ndục tổ chức đào tạo, tập huấn, bồi d ưỡ ng chuyên môn nghiệp\r\nvụ về công tác y tế trường học;  hỗ trợ \r\nchuyên môn nghiệp vụ cho nhân viên y tế trường học; hướng dẫn triển khai quản\r\nlý, chăm sóc, bảo vệ sức khỏe học sinh, truyền thông giáo dục sức khỏe. 3.  Tổ chức thanh tra, kiểm tra, giám\r\nsát các điều kiện vệ sinh trường học, vệ sinh môi trường, phòng chống dịch bệnh,\r\nchăm sóc, quản lý sức khỏe học sinh và các nội dung công tác y tế trường học\r\nkhác theo phân cấp. 4.  Thực hiện việc thống kê, báo cáo kết\r\nquả hoạt động y tế trường học theo quy định. Điều 16. Trách\r\nnhiệm của các đơn vị trực thuộc Bộ Y tế và Bộ Giáo dục và Đào tạo 1.  Cục Y tế dự phòng là cơ quan đầu mối\r\ncủa Bộ Y tế; Vụ Công tác học sinh, sinh viên là cơ quan đầu mối của Bộ Giáo dục\r\nvà Đào tạo trong việc triển khai các nội dung của Thông tư liên tịch này. 2.  Căn cứ chức năng nhiệm vụ của  đơn vị , chủ động xây dựng kế hoạch, tổ chức thực\r\nhiện và báo cáo công tác y tế trường học theo chức năng nhiệm vụ được giao. Điều 17. Trách\r\nnhiệm của  Ủy  ban nhân dân các cấp 1.  Hằng năm phê duyệt kế hoạch về hoạt\r\nđộng y tế trường học của địa phương; chủ động đầu tư kinh phí, nguồn nhân lực,\r\ncơ sở vật chất bảo đảm tổ chức thực hiện tốt công tác y tế trường học trên địa\r\nbàn. 2.  Kiện toàn Ban chỉ đạo công tác y tế\r\ntrường học các cấp hoặc bổ sung nhiệm vụ về y tế trường học cho Ban chăm sóc sức\r\nkhỏe nhân dân cùng cấp. Trưởng ban là lãnh đạo  Ủy\r\nban  nhân dân, Phó trưởng ban thường trực là lãnh đạo ngành Giáo dục, Phó\r\ntrưởng ban chuyên môn là lãnh đạo ngành Y tế, các ủy viên là lãnh đạo ngành Tài\r\nchính, Nội vụ,  Kế hoạch  và đầu tư, Bảo hi ể m xã hội, Trung tâm Y tế dự phòng tỉnh, huyện và các ban ngành, đoàn thể\r\nliên quan. Phân công nhiệm vụ cụ thể cho các thành viên Ban chỉ đạo theo chức\r\nnăng nhiệm vụ. 3.  Huy động các nguồn lực, nâng cấp\r\ncơ sở vật chất, cải thiện môi trường, điều kiện học tập, điều kiện chăm sóc sức\r\nkhỏe trong các trường học trên địa bàn theo quy định. 4.  Chỉ đạo các ngành phối hợp, tham\r\ngia thực hiện các nội dung về công tác y tế trường học trên địa bàn. 5.  Trong quy hoạch, xây dựng, cải tạo,\r\nsửa chữa trường học, mua sắm trang thiết bị, đồ dùng học tập, trang thiết bị y\r\ntế, căn cứ các quy chuẩn, tiêu chuẩn hiện hành để phê duyệt và chỉ đạo thực hiện. 6.  Có chế độ đãi ngộ đặc thù của địa\r\nphương để thu hút đội ngũ cán bộ làm công tác y tế trường học. Chương IV ĐIỀU KHOẢN THI\r\nHÀNH Điều 18. Điều\r\nkhoản tham chiếu Trong trường hợp các văn bản tham chiếu\r\ntrong văn bản này có sự sửa đ ổ i, bổ sung hoặc thay thế thì\r\nthực hiện theo quy định tại văn bản mới. Điều 19. Hiệu lực\r\nthi hành 1.  Thông tư liên tịch này có hiệu lực\r\ntừ ngày 30 tháng 6 năm 2016. 2.  Điều 4 của Quy định về hoạt động\r\ny tế trong các cơ sở giáo dục mầm non ban hành kèm theo Quyết định số\r\n58/2008/QĐ-BGDĐT  ngày 17 tháng 10 năm 2008 của Bộ trưởng Bộ\r\nGiáo dục và Đào tạo;  Điều 4 của Quy định về hoạt động y tế\r\ntrong các trường tiểu học, trung học cơ sở, trường trung học phổ thông và trường\r\nphổ thông có nhiều cấp học ban hành kèm theo Quyết định số 73/2007/QĐ-BGDĐT  ngày 04 tháng 12  năm  2007 của Bộ\r\ntrưởng Bộ Giáo dục và Đào tạo;  Quyết định số 1221/2000/QĐ-BYT  ngày 18 tháng 4 năm 2000 quy định về vệ sinh trường học của Bộ\r\ntrưởng Bộ Y tế; Thông tư liên tịch s ố  18/2011/TTLT-BGDĐT-BYT \r\nngày 28 tháng 4 năm 2011 quy định các nội dung đánh giá công tác y tế tại các\r\ntrường ti ể u học, trường trung học cơ sở, trường trung học\r\nphổ thông và trường phổ thông có nhiều cấp học của Bộ Giáo dục và Đào tạo và Bộ\r\nY tế; Thông tư liên tịch số  22/2013/TTLT-BGDĐT-BYT  ngày 18 tháng 6 năm 2013 quy\r\nđịnh nội dung đánh giá công tác y tế tại các cơ sở giáo dục mầm non của Bộ trưởng\r\nBộ Giáo dục và Đào tạo và Bộ trưởng Bộ Y tế; các nội dung quy định liên quan đến\r\nhướng dẫn hoạt động công tác y tế trường học đối với các trường mầm non và phổ\r\nthông tại Thông tư liên tịch số  03/2000/TTLT-BYT-BGDĐT  ngày 01 tháng 3 năm 2000\r\nhướng dẫn thực hiện công tác y tế trường học của Bộ Y tế và Bộ Giáo dục và Đào\r\ntạo bị bãi bỏ kể từ ngày Thông tư liên tịch này có hiệu lực. Điều 20. Điều\r\nkhoản chuyển tiếp Đối với các trường học hiện nay đang\r\nsử dụng và vận hành, cần phải xây dựng lộ trình để bảo đảm các yêu cầu về quy\r\nhoạch, thiết kế, xây dựng theo các tiêu chuẩn, quy chuẩn hiện hành trước ngày\r\n01 tháng 01 năm 2020. Trong quá trình thực hiện, nếu có vấn\r\nđề phát sinh hoặc khó khăn, vướng mắc, đề nghị phản ánh về Bộ Giáo dục và  Đ ào tạo (Vụ Công tác học sinh, sinh viên) và Bộ Y tế (Cục Y tế dự phòng)\r\nđể liên Bộ xem xét, giải quyết./.   KT. BỘ TRƯỞNG \r\n  BỘ GIÁO DỤC VÀ ĐÀO TẠO \r\n  THỨ TRƯỞNG \r\n  Nguyễn Thị Nghĩa KT. BỘ TRƯỞNG \r\n  BỘ Y TẾ \r\n  THỨ TRƯỞNG \r\n  Nguyễn Thanh Long   Nơi nhận: - Văn phòng Chủ tịch nước; \r\n-  Văn  phòng Quốc hội và các UB của Quốc hội; \r\n- Ban Tuyên giáo Trung ương; \r\n-  Văn  phòng  Chính\r\nphủ :  C ổng thông tin  đ iện tử  Chính phủ ,\r\nCông báo; \r\n- Bộ tr ưở ng Bộ Y tế (để báo cáo); \r\n- Bộ trưởng Bộ GD&ĐT(để báo cáo); \r\n- Các Bộ, cơ quan ngang bộ, cơ quan thuộc Chính phủ; \r\n- Kiểm toán nhà nước; \r\n- UBTW Mặt trận tổ quốc Việt Nam; \r\n- Cục KTVBQPPL (Bộ Tư pháp); \r\n- HĐND, UBND các tỉnh, thành phố trực thuộc Trung ương; \r\n- Các Vụ, Cục, Thanh tra, Văn phòng thuộc Bộ GD&ĐT, Bộ YT; \r\n- Các sở GD&ĐT, s ở  Y tế; \r\n- C ổ ng TTĐT: Bộ GD&ĐT, Bộ YT; \r\n- Lưu: VT, PC, CTHSSV (BGDĐT) ,  VT, PC, YTDP (BYT).   DANH MỤC CÁC PHỤ LỤC (Ban hành kèm theo Thông tư liên tịch s ố 13 /2016/TTLT-BYT-BGDĐT ng ày 22 th áng 5 năm\r\n2016 quy định công tác y tế trường học của Bộ trưởng Bộ Y tế v à  Bộ t rưở ng Bộ Gi á o dục và Đào tạo) Phụ lục 01. M ẫ u\r\nsổ theo dõi sức khỏe học sinh và mẫu sổ theo dõi  tổng\r\nhợp  tình trạng sức khỏe học sinh M ẫ u 01. Sổ theo\r\ndõi sức khỏe học sinh Sổ theo dõi sức khỏe học sinh được in\r\ntrên khổ giấy A5 (14,8cm  x  21cm), trường học căn cứ vào tu ổ i học sinh để lựa chọn một trong các mẫu sổ dưới đây: - Sổ theo dõi sức khỏe học sinh dành\r\ncho trẻ từ 3 tháng tuổi đến < 6 tuổi) - Sổ theo dõi sức khỏe học sinh dành cho\r\nhọc sinh từ lớp 1 đến lớp 5 - Sổ theo dõi sức khỏe học sinh dành\r\ncho học sinh từ lớp 6 đến lớp 9 - Sổ theo dõi sức khỏe học sinh dành\r\ncho học sinh từ lớp 10 đến lớp 12 M ẫ u 02. Sổ theo\r\ndõi tổng hợp tình trạng sức khỏe học sinh Phụ lục 02. M ẫ u\r\nbáo cáo công tác y tế trường học Phụ lục 03. M ẫ u\r\nđánh giá công tác y tế trường học áp dụng cho cơ sở giáo dục mầm non Phụ lục 04. M ẫ u\r\nđánh giá công tác y tế trường học áp dụng cho các cơ sở giáo dục phổ thông     Khổ\r\n  giấy A5 (14,8cm  x  21 c m) Phụ lục 01 MẪU 01. SỔ THEO DÕI SỨC KHỎE HỌC SINH (Ban\r\nhành kèm theo Thông tư liên tịch  số 13 /2016/TTLT-BYT-BGDĐT\r\nngày  12 t háng 5 năm 2016 quy định công tác y t ế\r\n trường học của Bộ trưởng Bộ Y t ế  và Bộ tr ưở ng Bộ Giáo dục và Đào tạo)   Bìa\r\nsổ CỘNG\r\n  HÒA XÃ HỘI CHỦ NGHĨA VIỆT NAM \r\n  Độc lập - Tự do - Hạnh phúc  \r\n  ---------------   S Ổ THEO DÕI SỨC KHỎE HỌC SINH   Họ\r\n  và tên  (chữ in hoa) …………..………...  Nam □ Nữ □ Ngày\r\n  tháng năm sinh:  ……/………/……………………… Trường \r\n  ……………………………………...………………. Xã/phường/huyện/quận \r\n  ………………….………………. Tỉnh/thành\r\n  phố  ……………………………………………..   Dành\r\n  cho học sinh c ơ  sở giáo dục mầm non (3 tháng tuổi đến < 6 tuổi)     (Sổ\r\n  này được sử dụng trong suốt cấp học, khi học sinh chuyển tr ườ ng phải mang theo để tiếp tục được theo dõi\r\n  sức khỏe)   (Trang\r\nnày sẽ được in vào mặt sau trang bìa) PH Ầ N I - THÔNG TIN CHUNG (Phần\r\nnày do cha, mẹ học sinh tự điền) 1.  Họ và tên học sinh  (chữ in hoa) …………………………….………………………..  Nam □ Nữ □ 2.  Ngày tháng năm sinh:  ............/………./………….. 3.  Họ và tên b ố  hoặc\r\nngười giám hộ:  .......................................................................................\r\n Nghề nghiệp  ……………………………..  Số điện thoại liên lạc \r\n.....................................................  Chỗ ở hiện tại: \r\n........................................................................................................................\r\n 4.  Họ và tên mẹ hoặc người giám hộ: \r\n......................................................................................\r\n Nghề nghiệp  ………………………………  Số điện thoại liên lạc  ...................................................\r\n Chỗ ở hiện tại: \r\n........................................................................................................................\r\n 5.  Con thứ mấy:  ……………………………..  Tổng số con trong gia đình:  .....................................\r\n 6.  Ti ề n sử sức  khỏe  bản thân: \r\n..................................................................................................\r\n a)  Sản khoa: - Bình th ường         \r\n □ - Không bình thường: Đẻ thiếu tháng □\r\n    Đẻ thừa tháng  □   Đẻ\r\ncó can thiệp □     Đẻ ngạt □ - Mẹ bị bệnh trong thời kỳ mang thai\r\n(nếu có cần ghi rõ tên bệnh: \r\n..............................................  ...............................................................................................................................................\r\n b)  Tiền sử bệnh/tật: Hen □       Động kinh □      Dị ứng □      Tim bẩm sinh □ c) Tiêm chủng: STT Loại\r\n  vắc xin Tì nh\r\n  trạng tiêm/u ố ng v ắ c xin Có Không Không\r\n  nhớ rõ 1 BCG       2 Bạch h ầ u, ho\r\n  gà, u ố n ván   Mũi 1         Mũi 2         Mũi 3       3 B ạ i li ệ t   Mũi 1         Mũi 2         Mũi 3       4 Viêm gan B   Sơ sinh         Mũi 1         Mũi 2         Mũi 3       5 Sởi       6 Viêm não Nh ậ t\r\n  Bản B   Mũi 1         Mũi 2         Mũi 3       7 …..       d) Hiện tại có đang điều trị bện h  gì không? N ế u có, ghi rõ tên bệnh và liệt kê các\r\nthuốc đang dùng:           ...............................................................................................................................................\r\n 7.  Thay đổi địa chỉ chỗ ở hoặc số điện\r\nthoại (nếu có)  ................................................................\r\n ...............................................................................................................................................\r\n   TRƯỜNG:  ...............................................................................................................................\r\n HỌ TÊN HỌC SINH  .................................................................................................................\r\n (Phần\r\nnày dành cho học sinh < 24 tháng tuổi) PHẦN\r\n2- THEO DÕI SỨC KHỎE (Do\r\nnhân viên y tế trường học thực hiện) 1. Theo dõi về thể lực (mỗi tháng/lần) LỚP  ……………………………………..  NĂM HỌC  ………………………………. Tháng  …./…… Nhân\r\n  viên y t ế  trường học (NVYTTH) ký, gh i  rõ h ọ  tên Chiều cao : ……… m; Cân nặng:  ……… kg; Tháng  …./…… NVYTTH ký, gh i  rõ h ọ  tên Chiều cao : ……… m; Cân nặng:  ……… kg; Tháng  …./…… NVYTTH ký, gh i  rõ h ọ  tên Chiều cao : ……… m; Cân nặng:  ……… kg; Tháng  …./…… NVYTTH ký, gh i  rõ h ọ  tên Chiều cao : ……… m; Cân nặng:  ……… kg; Tháng  …./…… NVYTTH ký, gh i  rõ h ọ  tên Chiều cao : ……… m; Cân nặng:  ……… kg; Tháng  …./…… NVYTTH ký, gh i  rõ h ọ  tên Chiều cao : ……… m; Cân nặng:  ……… kg; Tháng  …./…… NVYTTH ký, gh i  rõ h ọ  tên Chiều cao : ……… m; Cân nặng:  ……… kg; Tháng  …./…… NVYTTH ký, gh i  rõ h ọ  tên Chiều cao : ……… m; Cân nặng:  ……… kg; Tháng  …./…… NVYTTH ký, gh i  rõ h ọ  tên Chiều cao : ……… m; Cân nặng:  ……… kg; Tháng  …./…… NVYTTH ký, gh i  rõ h ọ  tên Chiều cao : ……… m; Cân nặng:  ……… kg; Tháng  …./…… NVYTTH ký, gh i  rõ h ọ  tên Chiều cao : ……… m; Cân nặng:  ……… kg; Tháng  …./…… NVYTTH ký, gh i  rõ h ọ  tên Chiều cao : ……… m; Cân nặng:  ……… kg; Tháng  …./…… NVYTTH ký, gh i  rõ h ọ  tên Chiều cao : ……… m; Cân nặng:  ……… kg; Tháng  …./…… NVYTTH ký, gh i  rõ h ọ  tên Chiều cao : ……… m; Cân nặng:  ……… kg; Tháng  …./…… NVYTTH ký, gh i  rõ h ọ  tên Chiều cao : ……… m; Cân nặng:  ……… kg; Tháng  …./…… NVYTTH ký, gh i  rõ h ọ  tên Chiều cao : ……… m; Cân nặng:  ……… kg; Tháng  …./…… NVYTTH ký, gh i  rõ h ọ  tên Chiều cao : ……… m; Cân nặng:  ……… kg; Tháng  …./…… NVYTTH ký, gh i  rõ h ọ  tên Chiều cao : ……… m; Cân nặng:  ……… kg; Đánh giá tình trạng DD: - Bình thường             \r\n  □ - Suy\r\n  DD                     \r\n  □ - Thừa cân béo phì     \r\n  □ Đánh giá tình trạng DD: - Bình thường             \r\n  □ - Suy\r\n  DD                     \r\n  □ - Thừa cân béo phì     \r\n  □   TRƯỜNG:  ...............................................................................................................................\r\n HỌ TÊN HỌC SINH  .................................................................................................................\r\n (Phần\r\nnày dành cho học sinh  ≥  24 tháng tuổi  đến\r\n<36 tháng tuổi) PHẦN\r\n2- THEO DÕI SỨC KHỎE (Do\r\nnhân viên y tế trường học thực hiện) 1. Theo dõi về thể lực (Lần  I  - đầu năm học, Lần  II\r\n - giữa năm học, Lần  III  - cuối\r\nnăm học) LỚP  ……………………………………..  NĂM HỌC  ………………………………. Lần\r\n  I Nhân\r\n  viên y tế trường học (NVYTTH)\r\n  k ý , ghi rõ họ tên Thể lực: -  Chiều cao:  ………………. m; -  Cân n ặ ng:  ……………….. kg; Tình trạ n g dinh dưỡng: -  Bình thường            □ -  Suy DD                    □ -  Thừa cân b é o\r\n  phì    □ Lần\r\n  II NVYTTH\r\n  k ý , ghi rõ họ tên Thể lực: -  Chiều cao:  ………………. m; -  Cân n ặ ng:  ……………….. kg; Tình trạ n g dinh dưỡng: -  Bình thường            □ -  Suy DD                    □ -  Thừa cân b é o\r\n  phì    □ Lần\r\n  III NVYTTH\r\n  k ý , ghi rõ họ tên Thể lực: -  Chiều cao:  ………………. m; -  Cân n ặ ng:  ……………….. kg; Tình trạ n g dinh dưỡng: -  Bình thường            □ -  Suy DD                    □ -  Thừa cân b é o\r\n  phì    □   TRƯỜNG:  ...............................................................................................................................\r\n HỌ TÊN HỌC SINH  .................................................................................................................\r\n (Phần\r\nnày dành cho học sinh  ≥ 36  tháng tuổi  đến\r\n<6 tuổi) PHẦN\r\n2- THEO DÕI SỨC KHỎE (Do\r\nnhân viên y tế trường học thực hiện) 1. Theo dõi về thể lực (Lần  I  - đầu năm học, Lần  II\r\n - giữa năm học, Lần  III  - cuối\r\nnăm học) LỚP  ……………………………………..  NĂM HỌC  ………………………………. Lần\r\n  I NVYTTH\r\n  k ý , ghi rõ họ tên Thể lực: -  Chiều cao:  ………………. m; -  Cân n ặ ng:  ……………….. kg; Tình trạ n g dinh dưỡng: -  Bình thường            □ -  Suy DD                    □ -  Thừa cân b é o\r\n  phì    □ Huyết áp:  Tâm trương  ……. /mgHg     Tâm thu  …… /mgHg Nh ị p\r\n  tim: ………. l ầ n/phút Th ị  lực:  Không kính: Mắt phải:  ……. /10  Mắt trái:  …… /10              \r\n   Có kính:         Mắt phải:  ……. /10  Mắt trái:  …… /10 Lần\r\n  II NVYTTH\r\n  k ý , ghi rõ họ tên Thể lực: -  Chiều cao:  ………………. m; -  Cân n ặ ng:  ……………….. kg; Tình trạ n g dinh dưỡng: -  Bình thường            □ -  Suy DD                    □ -  Thừa cân b é o\r\n  phì    □ Lần\r\n  III NVYTTH\r\n  k ý , ghi rõ họ tên Thể lực: -  Chiều cao:  ………………. m; -  Cân n ặ ng:  ……………….. kg; Tình trạ n g dinh dưỡng: -  Bình thường            □ -  Suy DD                    □ -  Thừa cân b é o\r\n  phì    □ LỚP  ……………………………………..  NĂM HỌC  ………………………………. Lần\r\n  I NVYTTH\r\n  k ý , ghi rõ họ tên Thể lực: -  Chiều cao:  ………………. m; -  Cân n ặ ng:  ……………….. kg; Tình trạ n g dinh dưỡng: -  Bình thường            □ -  Suy DD                    □ -  Thừa cân b é o\r\n  phì    □ Huyết áp:  Tâm trương  ……. /mgHg     Tâm thu  …… /mgHg Nh ị p\r\n  tim: ………. l ầ n/phút Th ị  lực:  Không kính: Mắt phải:  ……. /10  Mắt trái:  …… /10              \r\n   Có kính:         Mắt phải:  ……. /10  Mắt trái:  …… /10 Lần\r\n  II NVYTTH\r\n  k ý , ghi rõ họ tên Thể lực: -  Chiều cao:  ………………. m; -  Cân n ặ ng:  ……………….. kg; Tình trạ n g dinh dưỡng: -  Bình thường            □ -  Suy DD                    □ -  Thừa cân b é o\r\n  phì    □ Lần\r\n  III NVYTTH\r\n  k ý , ghi rõ họ tên Thể lực: -  Chiều cao:  ………………. m; -  Cân n ặ ng:  ……………….. kg; Tình trạ n g dinh dưỡng: -  Bình thường            □ -  Suy DD                    □ -  Thừa cân b é o\r\n  phì    □ LỚP  ……………………………………..  NĂM HỌC  ………………………………. Lần\r\n  I NVYTTH\r\n  k ý , ghi rõ họ tên Thể lực: -  Chiều cao:  ………………. m; -  Cân n ặ ng:  ……………….. kg; Tình trạ n g dinh dưỡng: -  Bình thường            □ -  Suy DD                    □ -  Thừa cân b é o\r\n  phì    □ Huyết áp:  Tâm trương  ……. /mgHg     Tâm thu  …… /mgHg Nh ị p\r\n  tim: ………. l ầ n/phút Th ị  lực:  Không kính: Mắt phải:  ……. /10  Mắt trái:  …… /10              \r\n   Có kính:         Mắt phải:  ……. /10  Mắt trái:  …… /10 Lần\r\n  II NVYTTH\r\n  k ý , ghi rõ họ tên Thể lực: -  Chiều cao:  ………………. m; -  Cân n ặ ng:  ……………….. kg; Tình trạ n g dinh dưỡng: -  Bình thường            □ -  Suy DD                    □ -  Thừa cân b é o\r\n  phì    □ Lần\r\n  III NVYTTH\r\n  k ý , ghi rõ họ tên Thể lực: -  Chiều cao:  ………………. m; -  Cân n ặ ng:  ……………….. kg; Tình trạ n g dinh dưỡng: -  Bình thường            □ -  Suy DD                    □ -  Thừa cân b é o\r\n  phì    □ 2.  Theo dõi diễn biến bất\r\nthường về sức khỏe Thời\r\n  gian Ch ẩ n đoán ban đầu Xử\r\n  trí Ghi\r\n  chú Xử\r\n  trí tại trường  (ghi nội dung xử trí) Chuyển\r\n  đến (ghi nơi chuyển đến) ……/…../………         ……/…../………         ……/…../………                   ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         PHẦN\r\n3 - KHÁM SỨC KHỎE THEO CHUYÊN KHOA (Do\r\ny, bác sĩ ghi chép khi khám chuyên khoa) Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) Nhi khoa a) Tuần hoàn \r\n  …………………………………………………………………. ………………………………………………………………………………….. b) Hô h ấ p: \r\n  ………………………………………………….…………………. ………………………………………………………………………………….. c) Tiêu hóa \r\n  …………………………………………………..………………. ………………………………………………………………………………….. d) Thận-Tiết ni ệ u  ……………………………………………………………. ………………………………………………………………………………….. đ) Th ầ n\r\n  kinh-Tâm th ầ n  ………………………………………………………. ………………………………………………………………………………….. e) Khám lâm sàng khác \r\n  ……………………………………………………. ………………………………………………………………………………….. Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) M ắ t a)  Kết quả  khám thị lực: -  Không kính: Mắt phải:  …….. /10     Mắt trái:  ………. /10 -  Có kính:         Mắt phải:  ……. /10      Mắt trái:  ………. /10 b)  Các b ệ nh về\r\n  mắt (nếu có)  ……………………………………………….. ………………………………………………………………………………….. ………………………………………………………………………………….. Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) Tai-Mũi- Họng a)  Kết quả khám thính lực: -  Tai trái:     Nói thường:  ……..  m;     Nói thầm:  ……. m -  Tai phải: Nói thường:  ……...\r\n   m;     Nói thầm:  …… m b)  Các b ệ nh về\r\n  Tai-Mũi-H ọ ng (nếu có)  ………………………………….. ………………………………………………………………………………….. ………………………………………………………………………………….. Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) Răn g  - Hàm - Mặt a)  Kết\r\n  quả  khám: - Hàm trên  …………………………………………………………………….. ………………………………………………………………………………….. - Hàm dưới \r\n  ……………………………………………………………………. ………………………………………………………………………………….. b) Các b ệ nh về\r\n  Răng-Hàm-M ặ t (nếu có)  …………………………………. ………………………………………………………………………………….. ………………………………………………………………………………….. Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) Cơ xương khớp a)  Kết quả khám -  Bình thường            □ -  Cong cột sống: Gù □     ưỡn □ -  Vẹo cột sống: Hình chữ  S\r\n     □ Hình chữ  C  □ b)  Các b ệ nh cơ\r\n  xương khớp khác (nếu có)  ………………………………… ………………………………………………………………………………….. …………………………………………………………………………………..     Khổ\r\n  giấy A5 (14,8cm  x  21 c m) Ph ụ\r\n l ụ c  01 MẪU 01. SỔ THEO DÕI SỨC KHỎE HỌC SINH (Ban\r\nhành kèm theo Thông tư liên tịch  số 13 /2016/TTLT-BYT-BGDĐT\r\nngày  12 t háng 5 năm 2016 quy định công tác y t ế\r\n trường học của Bộ trưởng Bộ Y t ế  và Bộ tr ưở ng Bộ Giáo dục và Đào tạo)   Bìa\r\nsổ CỘNG\r\n  HÒA XÃ HỘI CHỦ NGHĨA VIỆT NAM \r\n  Độc lập - Tự do - Hạnh phúc  \r\n  ---------------       S Ổ THEO DÕI SỨC KHỎE HỌC SINH   Họ\r\n  và tên  (chữ in hoa) ………..…………..  Nam □ Nữ □ Ngày\r\n  tháng năm sinh:  ……/………/…………………..… Trường \r\n  …………………………………………………..…. Xã/phường/huyện/quận \r\n  …………………………………. Tỉnh/thành\r\n  phố  ……………………………………………..   Dành\r\n  cho học sinh  từ lớp 1 đến lớp 5     (Sổ\r\n  này được sử dụng trong suốt cấp học, khi học sinh chuyển tr ườ ng phải mang theo để tiếp tục được theo dõi\r\n  sức khỏe)   (Trang\r\nnày sẽ được in vào mặt sau trang bìa) PH Ầ N I - THÔNG TIN CHUNG (Phần\r\nnày do cha, mẹ học sinh tự điền) 1.  Họ và tên học sinh  (chữ in hoa) \r\n…………………………….………………………..  Nam □ Nữ □ 2.  Ngày tháng năm sinh:  ............/………./………….. 3.  Họ và tên b ố  hoặc\r\nngười giám hộ: \r\n.......................................................................................\r\n Nghề nghiệp  ……………………………..  Số điện thoại liên lạc  .....................................................\r\n Chỗ ở hiện tại: \r\n........................................................................................................................\r\n 4.  Họ và tên mẹ hoặc người giám hộ: \r\n......................................................................................\r\n Nghề nghiệp  ………………………………  Số điện thoại liên lạc \r\n...................................................  Chỗ ở hiện tại: \r\n........................................................................................................................\r\n 5.  Con thứ mấy:  ……………………………..  Tổng số con trong gia đình: \r\n.....................................  6.  Ti ề n sử sức  khỏe  bản thân: \r\n..................................................................................................\r\n a)  Sản khoa: - Bình th ường         \r\n □ - Không bình thường: Đẻ thiếu tháng □\r\n    Đẻ thừa tháng  □   Đẻ\r\ncó can thiệp □     Đẻ ngạt □ - Mẹ bị bệnh trong thời kỳ mang thai\r\n(nếu có cần ghi rõ tên bệnh: \r\n..............................................  ...............................................................................................................................................\r\n b)  Tiền sử bệnh/tật: Hen □       Động kinh □      Dị ứng □      Tim bẩm sinh □ c) Tiêm chủng: STT Loại\r\n  vắc xin Tì nh\r\n  trạng tiêm/u ố ng v ắ c xin Có Không Không\r\n  nhớ rõ 1 BCG       2 Bạch h ầ u, ho\r\n  gà, u ố n ván   Mũi 1         Mũi 2         Mũi 3       3 B ạ i li ệ t   Mũi 1         Mũi 2         Mũi 3       4 Viêm gan B   Sơ sinh         Mũi 1         Mũi 2         Mũi 3       5 Sởi       6 Viêm não Nh ậ t\r\n  Bản B   Mũi 1         Mũi 2         Mũi 3       7 …..       d) Hiện tại có đang điều trị bện h  gì không? N ế u có, ghi rõ tên bệnh và liệt kê các\r\nthuốc đang dùng:           ...............................................................................................................................................\r\n 7.  Thay đổi địa chỉ chỗ ở hoặc số điện\r\nthoại (nếu có)  ................................................................\r\n ...............................................................................................................................................\r\n   TRƯỜNG:  ...............................................................................................................................\r\n HỌ TÊN HỌC SINH  .................................................................................................................\r\n PHẦN\r\n2- THEO DÕI SỨC KHỎE (Do\r\nnhân viên y tế trường học thực hiện) 1. Theo dõi về thể lực , huyết áp, nhịp tim, thị lực (Lần  1\r\n - đầu năm học, Lần  2  -  đầu\r\nhọc kỳ II ) LỚP 1 …………………..  NĂM HỌC  ………………………………. Lần\r\n  I Nhân\r\n  viên y tế trường học (NVYTTH)\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) Huy ế t\r\n  áp:  Tâm trương  …….. /mgHg     Tâm thu  ………. /mgHg Nh ị p\r\n  tim: ……… l ầ n/phút Th ị \r\n  I ự c:  Không kính: M ắ t phải:  …….. /10     M ắ t trái:  …….. /10              \r\n   Có kính:         M ắ t phải:  …….. /10     M ắ t trái:  …….. /10 Lần\r\n  II NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) LỚP 2 ……………………..  NĂM HỌC  ………………………………. Lần\r\n  I NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) Huy ế t\r\n  áp:  Tâm trương  …….. /mgHg     Tâm thu  ………. /mgHg Nh ị p\r\n  tim: ……… l ầ n/phút Th ị \r\n  I ự c:  Không kính: M ắ t phải:  …….. /10     M ắ t trái:  …….. /10              \r\n   Có kính:         M ắ t phải:  …….. /10     M ắ t trái:  …….. /10 Lần\r\n  II NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) LỚP 3 ……………………..  NĂM HỌC  ………………………………. Lần\r\n  I NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) Huy ế t\r\n  áp:  Tâm trương  …….. /mgHg     Tâm thu  ………. /mgHg Nh ị p\r\n  tim: ……… l ầ n/phút Th ị \r\n  I ự c:  Không kính: M ắ t phải:  …….. /10     M ắ t trái:  …….. /10              \r\n   Có kính:         M ắ t phải:  …….. /10     M ắ t trái:  …….. /10 Lần\r\n  II NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;    \r\n               Chỉ số BMI:  …………………..  (kg/ m 2 ) LỚP 4 ……………………..  NĂM HỌC  ………………………………. Lần\r\n  I NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) Huy ế t\r\n  áp:  Tâm trương  …….. /mgHg     Tâm thu  ………. /mgHg Nh ị p\r\n  tim: ……… l ầ n/phút Th ị \r\n  I ự c:  Không kính: M ắ t phải:  …….. /10     M ắ t trái:  …….. /10              \r\n   Có kính:         M ắ t phải:  …….. /10     M ắ t trái:  …….. /10 Lần\r\n  II NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) LỚP 5 ……………………..  NĂM HỌC  ………………………………. Lần\r\n  I NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) Huy ế t\r\n  áp:  Tâm trương  …….. /mgHg     Tâm thu  ………. /mgHg Nh ị p\r\n  tim: ……… l ầ n/phút Th ị \r\n  I ự c:  Không kính: M ắ t phải:  …….. /10     M ắ t trái:  …….. /10              \r\n   Có kính:         M ắ t phải:  …….. /10     M ắ t trái:  …….. /10 Lần\r\n  II NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) 2.  Theo dõi diễn biến bất\r\nthường về sức khỏe Thời\r\n  gian Ch ẩ n đoán ban đầu Xử\r\n  trí Ghi\r\n  chú Xử\r\n  trí tại trường  (ghi nội dung xử trí) Chuyển\r\n  đến (ghi nơi chuyển đến) ……/…../………         ……/…../………         ……/…../………                   ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         PHẦN\r\n3 - KHÁM SỨC KHỎE THEO CHUYÊN KHOA (Do\r\ny, bác sĩ ghi chép khi khám chuyên khoa) Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) Nhi khoa a) Tuần hoàn \r\n  …………………………………………………………………. ………………………………………………………………………………….. b) Hô h ấ p: \r\n  ………………………………………………….…………………. ………………………………………………………………………………….. c) Tiêu hóa \r\n  …………………………………………………..………………. ………………………………………………………………………………….. d) Thận-Tiết ni ệ u  ……………………………………………………………. ………………………………………………………………………………….. đ) Th ầ n\r\n  kinh-Tâm th ầ n  ………………………………………………………. ………………………………………………………………………………….. e) Khám lâm sàng khác \r\n  ……………………………………………………. ………………………………………………………………………………….. Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) M ắ t a)  Kết quả  khám thị lực: -  Không kính: Mắt phải:  …….. /10     Mắt trái:  ………. /10 -  Có kính:         Mắt phải:  ……. /10      Mắt trái:  ………. /10 b)  Các b ệ nh về\r\n  mắt (nếu có)  ……………………………………………….. ………………………………………………………………………………….. ………………………………………………………………………………….. Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) Tai-Mũi- Họng a)  Kết quả khám thính lực: -  Tai trái:     Nói thường:  ……..  m;     Nói thầm:  ……. m -  Tai phải: Nói thường:  ……...\r\n   m;     Nói thầm:  …… m b)  Các b ệ nh về\r\n  Tai-Mũi-H ọ ng (nếu có)  ………………………………….. ………………………………………………………………………………….. ………………………………………………………………………………….. Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) Răn g  - Hàm - Mặt a)  Kết\r\n  quả  khám: - Hàm trên  …………………………………………………………………….. ………………………………………………………………………………….. - Hàm dưới \r\n  ……………………………………………………………………. ………………………………………………………………………………….. b) Các b ệ nh về\r\n  Răng-Hàm-M ặ t (nếu có)  …………………………………. ………………………………………………………………………………….. ………………………………………………………………………………….. Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) Cơ xương khớp a)  Kết quả khám -  Bình thường            □ -  Cong cột sống: Gù □     ưỡn □ -  Vẹo cột sống: Hình chữ  S\r\n     □ Hình chữ  C  □ b)  Các b ệ nh cơ\r\n  xương khớp khác (nếu có)  ………………………………… ………………………………………………………………………………….. …………………………………………………………………………………..     Khổ\r\n  giấy A5 (14,8cm  x  21 c m) Ph ụ\r\n l ụ c  01 MẪU 01. SỔ THEO DÕI SỨC KHỎE HỌC SINH (Ban\r\nhành kèm theo Thông tư liên tịch  số 13 /2016/TTLT-BYT-BGDĐT\r\nngày  12 t háng 5 năm 2016 quy định công tác y t ế\r\n trường học của Bộ trưởng Bộ Y t ế  và Bộ tr ưở ng Bộ Giáo dục và Đào tạo)   Bìa\r\nsổ CỘNG\r\n  HÒA XÃ HỘI CHỦ NGHĨA VIỆT NAM \r\n  Độc lập - Tự do - Hạnh phúc  \r\n  ---------------       S Ổ THEO DÕI SỨC KHỎE HỌC SINH   Họ\r\n  và tên  (chữ in hoa) ………..…………..  Nam □ Nữ □ Ngày\r\n  tháng năm sinh:  ……/………/…………………..… Trường \r\n  …………………………………………………..…. Xã/phường/huyện/quận \r\n  …………………………………. Tỉnh/thành\r\n  phố  ……………………………………………..   Dành\r\n  cho học sinh  từ lớp 6 đến lớp 9     (Sổ\r\n  này được sử dụng trong suốt cấp học, khi học sinh chuyển tr ườ ng phải mang theo để tiếp tục được theo dõi\r\n  sức khỏe)   (Trang\r\nnày sẽ được in vào mặt sau trang bìa) PH Ầ N I - THÔNG TIN CHUNG (Phần\r\nnày do cha, mẹ học sinh tự điền) 1.  Họ và tên học sinh  (chữ in hoa) \r\n…………………………….………………………..  Nam □ Nữ □ 2.  Ngày tháng năm sinh:  ............/………./………….. 3.  Họ và tên b ố  hoặc\r\nngười giám hộ: \r\n.......................................................................................\r\n Nghề nghiệp  ……………………………..  Số điện thoại liên lạc \r\n.....................................................  Chỗ ở hiện tại: \r\n........................................................................................................................\r\n 4.  Họ và tên mẹ hoặc người giám hộ: \r\n......................................................................................\r\n Nghề nghiệp  ………………………………  Số điện thoại liên lạc \r\n...................................................  Chỗ ở hiện tại: \r\n........................................................................................................................\r\n 5.  Con thứ mấy:  ……………………………..  Tổng số con trong gia đình: \r\n.....................................  6.  Ti ề n sử sức  khỏe  bản thân: \r\n..................................................................................................\r\n a)  Sản khoa: - Bình th ường         \r\n □ - Không bình thường: Đẻ thiếu tháng □\r\n    Đẻ thừa tháng  □   Đẻ\r\ncó can thiệp □     Đẻ ngạt □ - Mẹ bị bệnh trong thời kỳ mang thai\r\n(nếu có cần ghi rõ tên bệnh: \r\n..............................................  ...............................................................................................................................................\r\n b)  Tiền sử bệnh/tật: Hen □       Động kinh □      Dị ứng □      Tim bẩm sinh □ c) Tiêm chủng: STT Loại\r\n  vắc xin Tì nh\r\n  trạng tiêm/u ố ng v ắ c xin Có Không Không\r\n  nhớ rõ 1 BCG       2 Bạch h ầ u, ho\r\n  gà, u ố n ván   Mũi 1         Mũi 2         Mũi 3       3 B ạ i li ệ t   Mũi 1         Mũi 2         Mũi 3       4 Viêm gan B   Sơ sinh         Mũi 1         Mũi 2         Mũi 3       5 Sởi       6 Viêm não Nh ậ t\r\n  Bản B   Mũi 1         Mũi 2         Mũi 3       7 …..       d) Hiện tại có đang điều trị bện h  gì không? N ế u có, ghi rõ tên bệnh và liệt kê các\r\nthuốc đang dùng:           ...............................................................................................................................................\r\n 7.  Thay đổi địa chỉ chỗ ở hoặc số điện\r\nthoại (nếu có) \r\n................................................................  ...............................................................................................................................................\r\n   TRƯỜNG:  ...............................................................................................................................\r\n HỌ TÊN HỌC SINH  .................................................................................................................\r\n PHẦN\r\n2 - THEO DÕI SỨC KHỎE (Do\r\nnhân viên y tế trường học thực hiện) 1. Theo dõi về thể lực , huyết áp, nhịp tim, thị lực (Lần  1\r\n - đầu năm học, Lần  2  -  đầu\r\nhọc kỳ II ) LỚP 6 ……………………..  NĂM HỌC  ………………………………. Lần\r\n  I Nhân\r\n  viên y tế trường học (NVYTTH)\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) Huy ế t\r\n  áp:  Tâm trương  …….. /mgHg     Tâm thu  ………. /mgHg Nh ị p\r\n  tim: ……… l ầ n/phút Th ị \r\n  I ự c:  Không kính: M ắ t phải:  …….. /10     M ắ t trái:  …….. /10              \r\n   Có kính:         M ắ t phải:  …….. /10     M ắ t trái:  …….. /10 Lần\r\n  II NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) LỚP 7 ……………………..  NĂM HỌC  ………………………………. Lần\r\n  I NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) Huy ế t\r\n  áp:  Tâm trương  …….. /mgHg     Tâm thu  ………. /mgHg Nh ị p\r\n  tim: ……… l ầ n/phút Th ị \r\n  I ự c:  Không kính: M ắ t phải:  …….. /10     M ắ t trái:  …….. /10              \r\n   Có kính:         M ắ t phải:  …….. /10     M ắ t trái:  …….. /10 Lần\r\n  II NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) LỚP 8 ……………………..  NĂM HỌC  ………………………………. Lần\r\n  I NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) Huy ế t\r\n  áp:  Tâm trương  …….. /mgHg     Tâm thu  ………. /mgHg Nh ị p\r\n  tim: ……… l ầ n/phút Th ị \r\n  I ự c:  Không kính: M ắ t phải:  …….. /10     M ắ t trái:  …….. /10              \r\n   Có kính:         M ắ t phải:  …….. /10     M ắ t trái:  …….. /10 Lần\r\n  II NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;  \r\n                 Chỉ số BMI:  …………………..  (kg/ m 2 ) LỚP 9 ……………………..  NĂM HỌC  ………………………………. Lần\r\n  I NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) Huy ế t\r\n  áp:  Tâm trương  …….. /mgHg     Tâm thu  ………. /mgHg Nh ị p\r\n  tim: ……… l ầ n/phút Th ị \r\n  I ự c:  Không kính: M ắ t phải:  …….. /10     M ắ t trái:  …….. /10              \r\n   Có kính:         M ắ t phải:  …….. /10     M ắ t trái:  …….. /10 Lần\r\n  II NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) 2.  Theo dõi diễn biến bất\r\nthường về sức khỏe Thời\r\n  gian Ch ẩ n đoán ban đầu Xử\r\n  trí Ghi\r\n  chú Xử\r\n  trí tại trường  (ghi nội dung xử trí) Chuyển\r\n  đến (ghi nơi chuyển đến) ……/…../………         ……/…../………         ……/…../………                   ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         PHẦN\r\n3 - KHÁM SỨC KHỎE THEO CHUYÊN KHOA (Do\r\ny, bác sĩ ghi chép khi khám chuyên khoa) Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) Nhi khoa a) Tuần hoàn \r\n  …………………………………………………………………. ………………………………………………………………………………….. b) Hô h ấ p: \r\n  ………………………………………………….…………………. ………………………………………………………………………………….. c) Tiêu hóa \r\n  …………………………………………………..………………. ………………………………………………………………………………….. d) Thận-Tiết ni ệ u  ……………………………………………………………. ………………………………………………………………………………….. đ) Th ầ n\r\n  kinh-Tâm th ầ n  ………………………………………………………. ………………………………………………………………………………….. e) Khám lâm sàng khác \r\n  ……………………………………………………. ………………………………………………………………………………….. Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) M ắ t a)  Kết quả  khám thị lực: -  Không kính: Mắt phải:  …….. /10     Mắt trái:  ………. /10 -  Có kính:         Mắt phải:  ……. /10      Mắt trái:  ………. /10 b)  Các b ệ nh về\r\n  mắt (nếu có)  ……………………………………………….. ………………………………………………………………………………….. ………………………………………………………………………………….. Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) Tai-Mũi- Họng a)  Kết quả khám thính lực: -  Tai trái:     Nói thường:  ……..  m;     Nói thầm:  ……. m -  Tai phải: Nói thường:  ……...\r\n   m;     Nói thầm:  …… m b)  Các b ệ nh về\r\n  Tai-Mũi-H ọ ng (nếu có)  ………………………………….. ………………………………………………………………………………….. ………………………………………………………………………………….. Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) Răn g  - Hàm - Mặt a)  Kết\r\n  quả  khám: - Hàm trên \r\n  …………………………………………………………………….. ………………………………………………………………………………….. - Hàm dưới  ……………………………………………………………………. ………………………………………………………………………………….. b) Các b ệ nh về\r\n  Răng-Hàm-M ặ t (nếu có)  …………………………………. ………………………………………………………………………………….. ………………………………………………………………………………….. Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) Cơ xương khớp a)  Kết quả khám -  Bình thường            □ -  Cong cột sống: Gù □     ưỡn □ -  Vẹo cột sống: Hình chữ  S\r\n     □ Hình chữ  C  □ b)  Các b ệ nh cơ\r\n  xương khớp khác (nếu có)  ………………………………… ………………………………………………………………………………….. …………………………………………………………………………………..     Khổ\r\n  giấy A5 (14,8cm  x  21 c m) Ph ụ\r\n l ụ c  01 MẪU 01. SỔ THEO DÕI SỨC KHỎE HỌC SINH (Ban\r\nhành kèm theo Thông tư liên tịch  số 13 /2016/TTLT-BYT-BGDĐT\r\nngày  12 t háng 5 năm 2016 quy định công tác y t ế\r\n trường học của Bộ trưởng Bộ Y t ế  và Bộ tr ưở ng Bộ Giáo dục và Đào tạo)   Bìa\r\nsổ CỘNG\r\n  HÒA XÃ HỘI CHỦ NGHĨA VIỆT NAM \r\n  Độc lập - Tự do - Hạnh phúc  \r\n  ---------------       S Ổ THEO DÕI SỨC KHỎE HỌC SINH   Họ\r\n  và tên  (chữ in hoa) ………..…………..  Nam □ Nữ □ Ngày\r\n  tháng năm sinh:  ……/………/…………………..… Trường \r\n  …………………………………………………..…. Xã/phường/huyện/quận \r\n  …………………………………. Tỉnh/thành\r\n  phố  ……………………………………………..   Dành\r\n  cho học sinh  từ lớp 10 đến lớp 12     (Sổ\r\n  này được sử dụng trong suốt cấp học, khi học sinh chuyển tr ườ ng phải mang theo để tiếp tục được theo dõi\r\n  sức khỏe)   (Trang\r\nnày sẽ được in vào mặt sau trang bìa) PH Ầ N I - THÔNG TIN CHUNG (Phần\r\nnày do cha, mẹ học sinh tự điền) 1.  Họ và tên học sinh  (chữ in hoa) \r\n…………………………….………………………..  Nam □ Nữ □ 2.  Ngày tháng năm sinh:  ............/………./………….. 3.  Họ và tên b ố  hoặc\r\nngười giám hộ:  .......................................................................................\r\n Nghề nghiệp  ……………………………..  Số điện thoại liên lạc \r\n.....................................................  Chỗ ở hiện tại: \r\n........................................................................................................................\r\n 4.  Họ và tên mẹ hoặc người giám hộ: \r\n......................................................................................\r\n Nghề nghiệp  ………………………………  Số điện thoại liên lạc  ...................................................\r\n Chỗ ở hiện tại: \r\n........................................................................................................................\r\n 5.  Con thứ mấy:  ……………………………..  Tổng số con trong gia đình:  .....................................\r\n 6.  Ti ề n sử sức  khỏe  bản thân: \r\n..................................................................................................\r\n a)  Sản khoa: - Bình th ường         \r\n □ - Không bình thường: Đẻ thiếu tháng □\r\n    Đẻ thừa tháng  □   Đẻ\r\ncó can thiệp □     Đẻ ngạt □ - Mẹ bị bệnh trong thời kỳ mang thai\r\n(nếu có cần ghi rõ tên bệnh: \r\n..............................................  ...............................................................................................................................................\r\n b)  Tiền sử bệnh/tật: Hen □       Động kinh □      Dị ứng □      Tim bẩm sinh □ c) Tiêm chủng: STT Loại\r\n  vắc xin Tì nh\r\n  trạng tiêm/u ố ng v ắ c xin Có Không Không\r\n  nhớ rõ 1 BCG       2 Bạch h ầ u, ho\r\n  gà, u ố n ván   Mũi 1         Mũi 2         Mũi 3       3 B ạ i li ệ t   Mũi 1         Mũi 2         Mũi 3       4 Viêm gan B   Sơ sinh         Mũi 1         Mũi 2         Mũi 3       5 Sởi       6 Viêm não Nh ậ t\r\n  Bản B   Mũi 1         Mũi 2         Mũi 3       7 …..       d) Hiện tại có đang điều trị bện h  gì không? N ế u có, ghi rõ tên bệnh và liệt kê các\r\nthuốc đang dùng:           ...............................................................................................................................................\r\n 7.  Thay đổi địa chỉ chỗ ở hoặc số điện\r\nthoại (nếu có) \r\n................................................................  ...............................................................................................................................................\r\n   TRƯỜNG:  ...............................................................................................................................\r\n HỌ TÊN HỌC SINH  .................................................................................................................\r\n PHẦN\r\n2 - THEO DÕI SỨC KHỎE (Do\r\nnhân viên y tế trường học thực hiện) 1. Theo dõi về thể lực , huyết áp, nhịp tim, thị lực (Lần  1\r\n - đầu năm học, Lần  2  -  đầu\r\nhọc kỳ II ) LỚP 10 ……………………..  NĂM HỌC  ………………………………. Lần\r\n  I Nhân\r\n  viên y tế trường học (NVYTTH)\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) Huy ế t\r\n  áp:  Tâm trương  …….. /mgHg     Tâm thu  ………. /mgHg Nh ị p\r\n  tim: ……… l ầ n/phút Th ị \r\n  I ự c:  Không kính: M ắ t phải:  …….. /10     M ắ t trái:  …….. /10              \r\n   Có kính:         M ắ t phải:  …….. /10     M ắ t trái:  …….. /10 Lần\r\n  II NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) LỚP 11 ……………………..  NĂM HỌC \r\n………………………………. Lần\r\n  I NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) Huy ế t\r\n  áp:  Tâm trương  …….. /mgHg     Tâm thu  ………. /mgHg Nh ị p\r\n  tim: ……… l ầ n/phút Th ị \r\n  I ự c:  Không kính: M ắ t phải:  …….. /10     M ắ t trái:  …….. /10              \r\n   Có kính:         M ắ t phải:  …….. /10     M ắ t trái:  …….. /10 Lần\r\n  II NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) LỚP 12 ……………………..  NĂM HỌC  ………………………………. Lần\r\n  I NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) Huy ế t\r\n  áp:  Tâm trương  …….. /mgHg     Tâm thu  ………. /mgHg Nh ị p\r\n  tim: ……… l ầ n/phút Th ị \r\n  I ự c:  Không kính: M ắ t phải:  …….. /10     M ắ t trái:  …….. /10              \r\n   Có kính:         M ắ t phải:  …….. /10     M ắ t trái:  …….. /10 Lần\r\n  II NVYTTH\r\n  k ý , ghi rõ họ tên Th ể  lực:  Chi ề u cao:  ……………  m; Cân n ặ n g :  ………………. kg ;                \r\n   Chỉ số BMI:  …………………..  (kg/ m 2 ) 2.  Theo dõi diễn biến bất\r\nthường về sức khỏe Thời\r\n  gian Ch ẩ n đoán ban đầu Xử\r\n  trí Ghi\r\n  chú Xử\r\n  trí tại trường  (ghi nội dung xử trí) Chuyển\r\n  đến (ghi nơi chuyển đến) ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         ……/…../………         PHẦN\r\n3 - KHÁM SỨC KHỎE THEO CHUYÊN KHOA (Do\r\ny, bác sĩ ghi chép khi khám chuyên khoa) Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) Nhi khoa a) Tuần hoàn \r\n  …………………………………………………………………. ………………………………………………………………………………….. b) Hô h ấ p: \r\n  ………………………………………………….…………………. ………………………………………………………………………………….. c) Tiêu hóa \r\n  …………………………………………………..………………. ………………………………………………………………………………….. d) Thận-Tiết ni ệ u  ……………………………………………………………. ………………………………………………………………………………….. đ) Th ầ n\r\n  kinh-Tâm th ầ n  ………………………………………………………. ………………………………………………………………………………….. e) Khám lâm sàng khác \r\n  ……………………………………………………. ………………………………………………………………………………….. Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) M ắ t a)  Kết quả  khám thị lực: -  Không kính: Mắt phải:  …….. /10     Mắt trái:  ………. /10 -  Có kính:         Mắt phải:  ……. /10      Mắt trái:  ………. /10 b)  Các b ệ nh về\r\n  mắt (nếu có)  ……………………………………………….. ………………………………………………………………………………….. ………………………………………………………………………………….. Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) Tai-Mũi- Họng a)  Kết quả khám thính lực: -  Tai trái:     Nói thường:  ……..  m;     Nói thầm:  ……. m -  Tai phải: Nói thường:  ……...\r\n   m;     Nói thầm:  …… m b)  Các b ệ nh về\r\n  Tai-Mũi-H ọ ng (nếu có)  ………………………………….. ………………………………………………………………………………….. ………………………………………………………………………………….. Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) Răn g  - Hàm - Mặt a)  Kết\r\n  quả  khám: - Hàm trên \r\n  …………………………………………………………………….. ………………………………………………………………………………….. - Hàm dưới \r\n  ……………………………………………………………………. ………………………………………………………………………………….. b) Các b ệ nh về\r\n  Răng-Hàm-M ặ t (nếu có)  …………………………………. ………………………………………………………………………………….. ………………………………………………………………………………….. Thời\r\n   g ian khám:  …. / …. / ……. Y, bác sĩ khám (ký và ghi rõ họ t ê n) Cơ xương khớp a)  Kết quả khám -  Bình thường            □ -  Cong cột sống: Gù □     ưỡn □ -  Vẹo cột sống: Hình chữ  S\r\n     □ Hình chữ  C  □ b)  Các b ệ nh cơ\r\n  xương khớp khác (nếu có)  ………………………………… ………………………………………………………………………………….. …………………………………………………………………………………..     Khổ\r\n  giấy A4 (21cm  x  29,7cm) Ph ụ  l ụ c 01 MẪU 02. SỔ THEO DÕI TỔNG HỢP TÌNH TRẠNG SỨC KHỎE HỌC SINH (Ban\r\nhành kèm theo Thông tư liên tịch s ố 13 /2016/TTLT-BYT-BGDĐT\r\nngày  12  tháng  5 năm 2016 quy định công tác tế trường học của Bộ trưởng Bộ Y tế v à\r\n Bộ trưởng Bộ Giáo dục và Đào tạo) Bìa\r\nsổ CỘNG\r\n  HÒA XÃ HỘI CHỦ NGHĨA VIỆT NAM \r\n  Độc lập - Tự do - Hạnh phúc  \r\n  ---------------       SỔ THEO DÕI TỔNG\r\n  HỢP \r\n  TÌNH TRẠNG SỨC KHỎE HỌC SINH             Trường: \r\n  ……………………………….. Xã/phường/huyện/quận …………….. Tỉnh/thành\r\n  phố ………………………..     DANH\r\nSÁCH HỌC SINH SUY DINH DƯỠNG NĂM\r\nHỌC:  ……………………………….. TT Họ\r\n  và tên học sinh Gi ớ i tính Lớp Ngày,\r\n  tháng, năm phát hiện Chẩn\r\n  đoán Xử\r\n  trí Ghi\r\n  chú Nam Nữ Tại\r\n  trường (ghi nội dung xử trí) Chuy ể n đ ế n (ghi nơi chuy ể n đến)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               DANH\r\nSÁCH HỌC SINH THỪA CÂN, BÉO PHÌ NĂM\r\nHỌC:  …………………………………… TT Họ\r\n  và tên học sinh Gi ớ i tính Lớp Ngày,\r\n  tháng, năm phát hiện Chẩn\r\n  đoán Xử\r\n  trí Ghi\r\n  chú Nam Nữ Tại\r\n  trường (ghi nội dung xử trí) Chuy ể n đ ế n (ghi nơi chuy ể n đến)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               DANH\r\nSÁCH HỌC SINH MẮC BỆNH TIM MẠCH N Ă M HỌC: ……………………………. TT Họ\r\n  và tên học sinh Giới\r\n  tính Lớp Ngày,\r\n  tháng, năm phát hiện Chẩn\r\n  đoán Xử\r\n  trí Ghi\r\n  chú Nam Nữ Tại\r\n  trường (ghi nội dung xử trí) Chuyển\r\n  đến (ghi nơi chuy ể n đến)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           DANH\r\nSÁCH HỌC SINH MẮC BỆNH V Ề  MẮT NĂM  HỌC: \r\n…………………………… TT Họ\r\n  và tên học sinh Giới\r\n  tính Lớp Ngày,\r\n  tháng, năm phát hiện Chẩn\r\n  đoán Xử\r\n  trí Ghi\r\n  chú Nam Nữ Tại\r\n  trường (ghi nội dung xử trí) Chuy ể n đ ế n (ghi nơi chuy ể n đến)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             DANH\r\nSÁCH HỌC SINH MẮC BỆNH VỀ CƠ XƯƠNG KHỚP NĂM\r\nHỌC:  ………………………………………… TT Họ\r\n  và tên học sinh Giới\r\n  tính Lớp Ngày,\r\n  tháng, năm phát hiện Chẩn\r\n  đoán Xử\r\n  trí Ghi\r\n  chú Nam Nữ Tại\r\n  trường (ghi nội dung xử trí) Chuy ể n đ ế n (ghi n ơ i chuy ể n đến)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           DANH\r\nSÁCH HỌC SINH M Ắ C BỆNH RĂNG MIỆNG N Ă M HỌC:  ………………………………. TT H ọ  và tên h ọ c sinh Giới\r\n  tính Lớp Ngày,\r\n  tháng, năm phát hiện Chẩn\r\n  đoán Xử\r\n  trí Ghi\r\n  chú Nam Nữ Tại\r\n  trường (ghi nội dung xử trí) Chuy ể n đ ế n (ghi nơi chuy ể n đến)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           DANH\r\nSÁCH HỌC SINH RỐI LOẠN SỨC KHỎE TÂM THẦN NĂM\r\nHỌC:  ………………..…………………. TT Họ\r\n  và tên học sinh Giới\r\n  tính Lớp Ngày,\r\n  tháng, năm phát hiện Chẩn\r\n  đoán Xử\r\n  trí Ghi\r\n  chú Nam Nữ Tại\r\n  trường (ghi nội dung xử trí) Chuy ể n đ ế n (ghi nơi chuy ể n đến)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Khổ\r\n  giấy A4 (21cm  x  29,7cm) Phụ lục 02 MẪU BÁO CÁO CÔNG TÁC Y TẾ TRƯỜNG HỌC (Ban\r\nhành kèm theo Thông t ư  liên tịch s ố 13 /2016/TTLT-BYT-BGDĐT ngày  12 t háng 5 năm\r\n2016 quy định công t á c y tế trường học của Bộ trưởng\r\nBộ Y tế v à  Bộ tr ưở ng Bộ\r\nGi á o dục và Đào tạo) Tên trường ………………….. CỘNG\r\n  HÒA XÃ HỘI CHỦ NGHĨA VIỆT NAM \r\n  Độc lập - Tự do - Hạnh phúc  \r\n  --------------- ………..,  ngày  …….  tháng  ……  năm 20 ……   BÁO\r\nCÁO CÔNG TÁC Y TẾ TRƯỜNG HỌC 1.  Thông tin chung 1.  Tổng số học sinh:  …………………………..            \r\n Tổng số giáo viên ……………………… 2.  Tổng số lớp học ……………………………. 3.  Ban chăm sóc sức khỏe học sinh:                                     Có □               Không  □ 4.  Kế hoạch YTTH được phê duyệt:                                      Có □               Không  □ 5.  Kinh phí thực hiện:  ……………………….. đồng II.  Hoạt động quản lý, bảo\r\nvệ và chăm sóc sức khỏe học sinh 2.1. Phát hiện các dấu hiệu bất thường và yếu tố\r\nnguy cơ về sức khỏe TT Nguy\r\n  cơ sức khỏe Tổng\r\n  số phát hiện Xử\r\n  trí, chuy ể n tuyến Tỷ\r\n  lệ  % 1. Suy dinh dưỡng       2. Thừa cân, béo phì       3. Bệnh răng miệng       4. Bệnh  về \r\n  m ắ t       5. Tim m ạ ch       6. Hô  h ấ p       7. Tâm th ầ n - th ầ n kinh       8. Bệnh cơ xương khớp       9. Khác  (ghi rõ)       10. ………….       Cộng       * Tỷ lệ % - Số lượng xử trí, chuyển tuyến x\r\n100/Tổng số phát hiện Nhận xét: ……………………………………………………………………………………………… 2.2. Khám, điều trị các bệnh theo chuyên khoa TT Tên\r\n  chuyên khoa T ổ ng s ố  khám T ổ ng s ố  mắc T ổ ng s ố  được điều trị Tỷ lệ % 1. Nhi khoa/nội khoa         2. M ắ t         3. Tai-Mũi-Họng         4. Răng- Hàm- Mặt         5. Cơ xương khớp         6. Tâm th ầ n         7. Khác ( ghi rõ)         8. ……………..         Cộng         * Tỷ lệ % = Tổng số được điều trị x 100/Tổng số\r\nmắc Nhận xét: ……………………………………………………………………………………………… 2.3. Tình hình dịch, bệnh truyền\r\nnhiễm TT Tên\r\n  d ị ch b ệ nh T ổ ng s ố  m ắ c S ố\r\n   tử vong Ghi\r\n  ch ú 1. Tiêu chảy       2. Tay chân miệng       3. Sởi       4. Quai bị       5. Khác  (ghi rõ)       6. ………..       Cộng       Nhận xét: ………………………………………………………………………………………………………….. ………………………………………………………………………………………………………… 2.4. S ơ  cứu, cấp cứu tai nạn thương tích TT Loại\r\n  tai nạn thương tích T ổ ng số mắc Xử\r\n  trí, chuyển tuyến Tỷ\r\n  lệ  % 1. Trượt, ngã       2. Bỏng       3. Đu ố i nước       4. Điện giật       5. Súc vật c ắ n       6. Ngộ độc       7. Hóc d ị  v ậ t       8. C ắ t vào tay\r\n  chân       9. Bị đánh       10. Tai nạn giao thông       11. Khác  ( ghi rõ)       12. ………….         Cộng       * Tỷ lệ %= Số lượng xử trí, chuyển tuyến x 100/\r\nTổng số mắc Nhận xét:  ………………………………………………………………………………………………………………. ……………………………………………………………………………………………………………… 2.5. Hoạt động tư vấn sức kh ỏ e TT Nội\r\n  dung tư vấn T ổ ng s ố  đối tượng nguy  cơ S ố\r\n   học sinh được tư vấn Tỷ\r\n  lệ % 1. Dinh dưỡng hợp lý       2. Hoạt động th ể  lực       3. Tâm sinh lý       4. Phòng ch ố ng bệnh\r\n  tật       5. Phòng ch ố ng bệnh\r\n  tật học đường       6. Sức khỏe tâm th ầ n       7. Khác  (ghi rõ)       8. ……………….       * Tỷ lệ % = Số người được tư vấn x 100/ Tổng số\r\nđối tượng nguy cơ Nhận xét: …………………………………………………………………………………………………………….. 2.5. Tổ chức bữa ăn học đ ườ ng -  Trường có tổ chức ăn bán trú/nội\r\ntrú:                                   Có □        không □ -  Xây dựng thực đơn bảo đảm dinh dưỡng\r\nhợp lý:                Có □        không □ Nhận x é t: …………………………………………………………………………………………………………….. …………………………………………………………………………………………………………….. 2.6. Tiêm chủng phòng bệnh trong\r\ncác chiến dịch tại tr ườ ng TT Loại\r\n  v ắ c xin T ổ ng s ố  h ọ c\r\n  sinh S ố\r\n   học sinh đ ượ c tiêm Tỷ\r\n  lệ % 1.         2.         3. …………       * Tỷ lệ %= Số học sinh được tiêm\r\nchủng đầy đủ  x  100/ Tổng số học sinh Nhận xét: …………………………………………………………………………………………………………….. …………………………………………………………………………………………………………….. 2.7. Quản lý số theo dõi sức khỏe\r\nhọc sinh -  Tổng số h ọ c\r\nsinh có sổ theo dõi sức khỏe:  ……………………… -  Số sổ theo dõi sức khỏe học sinh và\r\nsổ theo dõi tổng hợp tình trạng sức khỏe họ c sinh  đ ượ c  c ậ p  nh ậ t thông tin thường xuyên về sức khỏe:  ……………………… tỷ\r\nl ệ ……………………… % -  Tổng số HS đ ượ c\r\nthông báo về tình tr ạ ng SK cho gia đình/người giám h ộ…………………… Nhận xét: …………………………………………………………………………………………………………….. …………………………………………………………………………………………………………….. 2.8. Kết quả chủ động triển khai\r\ncác biện pháp vệ sinh phòng bệnh TT Nội\r\n  dung Số\r\n  lượt Ghi\r\n  chú 1 T ổ ng vệ sinh\r\n  trường lớp     2 Phun hóa  chất  diệt côn trùng     3 Vệ sinh khử trùng đ ồ  chơi, đ ồ  dùng học tập     4 Vệ sinh nhà ăn, nhà b ế p     5 Vệ sinh ngu ồ n\r\n  nước     6 Thu gom, xử lý rác thải     7 Khác  ( ghi rõ)     Nhận xét: …………………………………………………………………………………………………………….. 2.9. Triển khai các chương trình y tế và phong\r\ntrào vệ sinh phòng bệnh TT Nội\r\n  dung Có Không Ghi chú 1 Phòng ch ố ng\r\n  HIV/AIDS       2 Phòng ch ố ng\r\n  tai nạn thương tích       3 Phòng ch ố ng dịch\r\n  bệnh truy ề n nhi ễ m       4 Phòng ch ố ng\r\n  suy dinh dưỡng       5 An toàn thực  phẩm       6 Phòng ch ố ng\r\n  thu ố c lá       7 Phòng ch ố ng rượu\r\n  bia       8 Xây dựng Trường học nâng cao sức khỏe       9 Khác  (ghi rõ)       Nhận xét: ……………………………………………………………………………………………….. 2.10. Báo cáo kết quả kinh phí\r\ndành cho công tác y tế tr ườ ng học TT Nội\r\n  dung S ố  ti ề n Ghi\r\n  chú 1 T ổ ng s ố  kinh phí     2 Ngu ồ n NSNN       3 Ngu ồ n bảo hi ể m y t ế  học sinh       4 Nguồn kinh phí khác       Nhận xét: ………………………………………………………………………………………………………………. ………………………………………………………………………………………………………………. ………………………………………………………………………………………………………………. ………………………………………………………………………………………………………………. ………………………………………………………………………………………………………………. III.  Hoạt động truyền thông\r\ngiáo dục sức khỏe 3.1.  Biên soạn tài liệu, nội\r\ndung truyền thông  phù hợp  với tình hình dịch\r\nbệnh của địa phương:       \r\n                                                          Có □ Không □ 3.2.  Có góc truyền thông\r\ngiáo dục sức khỏe:     Có □ Không □ 3.3.  T ổ  ch ứ c truyền thông, giáo dục sức khỏe TT Nội\r\n  dung S ố  l ượ t S ố  ng ườ i Ghi\r\n  chú 1 Phòng chống dịch, bệnh truyền nhiễm       2 Phòng ch ố ng ngộ\r\n  độc thực  phẩm       3 Dinh dưỡng hợp lý       4 Hoạt động th ể  lực\r\n  nâng cao sức khỏe       5 Phòng ch ố ng\r\n  tác hại thu ố c lá       6 Phòng ch ố ng\r\n  tác hại rượu bia       7 Phòng ch ố ng bệnh,\r\n  tật học đường       8 Chăm sóc răng miệng       9 Phòng chống các bệnh về mắt       10 Phòng ch ố ng\r\n  tai nạn thương tích       11 Khác  ( ghi rõ)       Nhận xét: ………………………………………………………………………………………………………………. ………………………………………………………………………………………………………………. ………………………………………………………………………………………………………………. ………………………………………………………………………………………………………………. ………………………………………………………………………………………………………………. IV.  B ảo\r\nđảm điều ki ệ n chăm sóc sức kh ỏ e TT Nội\r\n  dung Có Không Gh i\r\n  chú 1 Phòng y t ế  trường\r\n  học       2 Phòng y t ế  có\r\n  đủ  điều kiện  chăm sóc SK học sinh       3 Có s ổ  khám b ệ nh       4 Có s ổ  theo dõi\r\n  sức khỏe h ọ c sinh       5 Có sổ theo dõi tổng hợp tình trạng\r\n  sức khỏe học sinh       6 Nhân viên y t ế  trường học       Nhận xét: ………………………………………………………………………………………………………………. ………………………………………………………………………………………………………………. ………………………………………………………………………………………………………………. ………………………………………………………………………………………………………………. ………………………………………………………………………………………………………………. V. Bảo đảm các điều kiện về  cơ  s ở  vật chất, cấp  thoát  nước, vệ sinh môi trường, an toàn thực phẩm theo quy định TT Nội\r\n  dung Đ ạ t Không\r\n  đạt Ghi\r\n  chú 1 Điều kiện về  phòng học       2 Điều kiện về  bàn gh ế       3 Điều kiện về  bảng phòng học       4 Điều kiện về  chi ế u\r\n  sáng       5 Điều kiện về thiết bị, đồ chơi trẻ\r\n  em       6 Điều kiện về  nước ăn u ố ng       7 Điều ki ệ n  về  nước sinh ho ạ t       8 Điều kiện về  công trình vệ sinh       9 Điều kiện về  thu gom, xử lý  chất  thải       10 Điều kiện về  an toàn thực  phẩm       Nhận xét: \r\n………………………………………………………………………………………….. VI. Bảo đảm môi trường thực thi\r\nchính sách và xây dựng các mối quan hệ xã hội trong trường học, liên  kết  cộng đồng TT Nội\r\n  dung Có Không Ghi \r\n  chú 1 Ban chăm sóc sức khỏe học sinh có\r\n  phân công trách nhiệm cụ thể cho các thành viên và tổ chức họp tối thiểu 01 lần/học\r\n  kỳ       2 Có các quy định đ ể  thực hiện các chính sách, chế độ chăm sóc sức khỏe học sinh       3 Xây dựng m ố i\r\n  quan hệ t ố t giữa th ầ y cô giáo với h ọ c sinh và học sinh với học sinh       4 Xây dựng m ố i\r\n  liên hệ giữa nhà trường với gia đình và cộng đồng trong chăm sóc sức khỏe học\r\n  sinh để giúp đỡ, hỗ trợ       Nhận xét: ………………………………………………………………………………………………………………. VII. Đánh giá công tác y tế trường\r\nhọc -Tự đánh giá kết quả thực hiện công\r\ntác y tế trường học theo mẫu quy định tại Thông tư liên tịch số      \r\n  /TTLT-BYT-BGDĐT ngày       tháng 5 năm 2016:     Có □   \r\n  Không □ Tổng điểm:        điểm X ếp loại: Tốt □  \r\n     Khá □       Trung bình □       Không đạt □ - Đánh giá của cơ quan quản lý:\r\n Có □     Không □ Tổng điểm:        điểm X ếp loại: Tốt □        Khá □       Trung bình □   \r\n  Không đạt □ Nhận xét chung: …………………………………………………………………………………………………………….. …………………………………………………………………………………………………………….. Kiến nghị: …………………………………………………………………………………………………………….. ……………………………………………………………………………………………………………..   Lãnh\r\n  đạo nhà trường (K ý  lên đ ó ng dấu) Ngày\r\n   ……  tháng  …..  năm 201… .. Ng ườ i báo cáo (K ý  ghi rõ họ tên)     Khổ\r\n  giấy A4 (21cm  x  29,7cm) Phụ lục 03 MẪU ĐÁNH GIÁ CÔNG TÁC Y TẾ TRƯỜNG\r\nHỌC (Áp dụng cho cơ s ở  giáo dục mầm\r\nnon) (Ban hành kèm theo Thông t ư  liên\r\ntịch s ố 13 /2016/TTLT-BYT-BGDĐT ngày  12 t háng 5 năm 2016 quy định công t á c y tế trường\r\nhọc của Bộ trưởng Bộ Y tế v à  Bộ tr ưở ng Bộ Gi á o dục và Đào tạo) Tên trường ………………….. CỘNG\r\n  HÒA XÃ HỘI CHỦ NGHĨA VIỆT NAM \r\n  Độc lập - Tự do - Hạnh phúc  \r\n  --------------- ………..,  ngày  …….  tháng  ……  năm 20 ……   ĐÁNH\r\nGIÁ CÔNG TÁC Y T Ế  TRƯỜNG HỌC (Áp dụng cho cơ sở giáo dục mầm non) I. Thông tin chung 1.  Tên cơ sở được đánh giá: Trường mầm\r\nnon  ……………………………………………….. xã/phường  ………………………………………  quận/huyện  …………………….. Tỉnh/ thành\r\nphố  ……………………………………………………………………….. 2.  T ổ ng s ố\r\n lớp:  ………………………….. Tổng số học sinh  ……………. Tổng số giáo viên: ……….. 3.  Hình thức đánh giá:  \r\n  Tự đánh giá □           Đánh giá của cơ quan quản lý □ 4.  Thời gian:      Ngày       tháng    \r\n  năm 200 II.  Kết qu ả  đánh giá TT Nội\r\n  dung đánh giá Điểm\r\n  chuẩn Điểm  đ ạ t I Công tác tổ chức và kế hoạch 5.0   1.1. Ban Chăm sóc sức khỏe học sinh 2.0     Có  Quyết \r\n  định thành lập, phân công trách nhiệm các thành viên 1.0     Định kỳ  tổ chức  họp Ban Chăm sóc sức khỏe và đ ề  ra\r\n  nhiệm vụ cụ thể cho từng học kỳ (tối thiểu 1 lần/học kỳ) 1.0   1.2. Kế hoạch  hoạt động YTTH hàng năm 3.0     Có bản  kế hoạch  hoạt động YTTH theo năm học được phê duyệt 1.0     Nội dung b ả n  kế hoạch  được xây dựng đủ các nội dung  về  YTTH theo quy định 1.0     Có b ố  trí kinh\r\n  phí đ ể  thực hiện nhiệm vụ YTTH hàng năm 1.0   II Bảo đảm các điều ki ệ n về cơ sở v ậ t chất 10   2.1. Phòng sinh hoạt chung, phòng ngủ 2.0   2.1.1 Phòng sinh hoạt chung 1.0     Diện tích từ  1 ,5- 1 ,8m 2 /1 trẻ nhưng không được nhỏ hơn\r\n  24 m2/phòng đ ố i với nhóm trẻ và 36m 2 / 1\r\n   phòng đối với lớp mẫu giáo 0.5     Bảo đảm chi ế u\r\n  sáng và thông gió tự nhiên 0.3     Được trang bị đ ầ y đủ thi ế t bị học tập, đ ồ \r\n  chơi 0.2   2.1.2 Phòng ngủ 1.0     Diện tích từ 1,2 m 2 /trẻ - 1,5 m 2 / tr ẻ nhưng không được nh ỏ  hơn 18 m 2 /phòng đối với nhóm trẻ và 30m 2 /phòng\r\n  đối với lớp mẫu giáo 0.5     Yên tĩnh, t hoán g  mát  về \r\n  mùa hè,  ấ m áp  về \r\n  mùa đông 0.3     Được trang bị đ ầ y đủ đệm, chi ế u, tủ, kệ, giá đựng các đ ồ  dùng 0.2   2.2 Bàn gh ế 3.0     Sử dụng bàn ghế 2 chỗ ngồi (hoặc 4\r\n  chỗ ngồi), mặt bàn phẳng ,  nằm ngang, ghế rời có tựa lưng 1.0     Có đủ 3 cỡ bàn ghế A, B,  C  tương ứng với nhóm tuổi của trẻ theo quy định 1.0     Chi ề u cao bàn,\r\n  gh ế  phù hợp với chiều cao của trẻ, khoảng cách giữa chiều\r\n  cao bàn và mặt ghế ngồi không thấp hơn 220mm và không cao hơn 270mm 1.0   2.3 Bảng dạy học (n ế u có) 1.0     S ố  lượng đáp ứng\r\n  theo quy định tại Thông tư số  02/2010/TT-BGDĐT  của Bộ trưởng Bộ Giáo dục và\r\n  Đào tạo ngày 11/02/2010 về việc ban hành danh mục Đồ dùng - Đồ chơi - Thiết bị\r\n  dạy học tối thiểu dùng cho Giáo dục mầm non 0.5     Bảo đảm an toàn, có giá trị sử dụng\r\n  cao, phù hợp với nội dung giáo dục 0.5   2.4 Chi ế u\r\n  s á ng 2.0     Phòng sinh hoạt chung, phòng t ắ m rửa, phòng vệ sinh, hiên chơi, nhà bếp được chiếu sáng tự nhiên, trực\r\n  tiếp 0.5     Tỷ lệ diện tích cửa s ổ  với diện tích sàn của các phòng không nhỏ hơn 1/5 0.5     Chi ế u sáng\r\n  nhân tạo ở hành lang, c ầ u thang bảo đảm không nhỏ hơn\r\n  100 Lux, các phòng khác bảo đảm không nh ỏ  hơn 300 Lux 1.0   2.5 Đ ồ \r\n  chơi 2.0     Đ ồ  chơi bảo đảm\r\n  an toàn theo quy định tại Thông tư s ố  16/2011/TT-BGDĐT \r\n  ngày 13 tháng 4 năm 2011 của Bộ Giáo dục và Đào tạo 1.0     Đ ồ  chơi có\r\n  tính giáo dục và tính th ẩ m mỹ, giúp trẻ  phát triển  khả năng vận động, ngôn ngữ, cảm\r\n  xúc, thẩm mỹ và quan hệ xã hội 0.5     Phù hợp với thu ầ n phong mỹ tục, tâm sinh lý lứa tu ổ i 0.3     Có tủ, giá đựng đ ồ  chơi ngăn n ắ p, gọn gàng 0.2   III Bảo đảm các điều kiện về cấp  thoát  n ướ c và vệ\r\n  sinh môi trường 10   3.1 Cấp nước ăn uống và sinh hoạt 3.0     N ướ c u ố ng bảo đảm t ố i thi ể u bình\r\n  quân m ỗ i học sinh trong một ca học có 0,5 lít về mùa hè\r\n  và 0,3 lít về mùa đông 1.0     Nước sinh hoạt bảo đảm t ố i thi ể u 4 lít cho một học sinh trong một ca học;\r\n  nếu dùng hệ thống cấp nước b ằ ng đường ống thì mỗi vòi sử\r\n  dụng tối đa cho 200 học sinh trong một ca học 0.5     Khu nội trú của trường học có đủ nước\r\n  sạch đ ể  học sinh sử dụng trong ăn uống và sinh hoạt hằng\r\n  ngày, bảo đảm tối thiểu 100 lít cho một học sinh trong 24 giờ 0.5     Chất  lượng nước bảo đảm theo các quy định của Bộ Y t ế 0.5     Giếng nước, b ể  nước, chum, vại nước (nếu có) có nắp đậy, độ cao bảo đảm an toàn cho\r\n  trẻ khi sử dụng theo quy định 0.5   3.2 Công trình vệ sinh 4.0     Phòng vệ sinh khép kín với phòng\r\n  sinh hoạt chung và phòng ngủ hoặc liền kề với nhóm lớp; riêng cho trẻ và giáo\r\n  viên, riêng nam và nữ 0.5     Bảo đảm diện tích từ 0,4 m 2 /trẻ\r\n  - 0,6 m 2 /trẻ nhưng không nhỏ hơn 12 m 2 /phòng 0.5     Có vách ngăn cao 1,2m giữa chỗ đi\r\n  tiểu và bồn cầu 0.5     Kích thước m ỗ i\r\n  ô đặt bệ xí 0,8m  x  0,7m 0.5     B ố  trí từ 2 -\r\n  3 ti ể u treo dùng cho trẻ em nam và từ 2 - 3 xí bệt dùng\r\n  cho trẻ em nữ 0.5     Khu vực rửa tay của trẻ được b ố\r\n   trí riêng, bảo đảm 8-10 trẻ/ chậu rửa, có xà phòng hoặc dun g\r\n   dịch sát khuẩn khác 1.0     Trang thi ế t bị\r\n  vệ sinh được l ắ p đặt  phù\r\n  hợp  với độ tu ổ i 0.5   3.3 Thu gom và xử lý ch ấ t thải 3.0     Có  hệ\r\n  thống  c ố ng rãnh  thoát \r\n  nước mưa, nước thải sinh hoạt, không có nước ứ đọng xung quanh trường lớp 1.0     Có thùng chứa rác và phân loại rác\r\n  thải 1.0     Có hợp đ ồ ng với\r\n  các cơ sở đủ  điều kiện  thu gom, xử lý  chất  thải, rác thải sinh hoạt hoặc tự thu gom,\r\n  xử lý chất thải, rác thải theo quy định 1.0   IV Bảo đảm các điều kiện về an toàn\r\n  thực phẩm 10   4.1 Nhà ăn, căng tin 4.0     Thông t hoán g, đủ ánh sáng, cửa s ổ  có lưới ch ố ng chuột, ru ồ i nhặng, côn trùng 0.5     Tường, tr ầ n\r\n  nhà b ằ ng ph ẳ ng, nh ẵ n, thuận tiện làm vệ sinh 0.5     Bàn, gh ế , dụng\r\n  cụ, phương tiện làm b ằ ng vật liệu dễ cọ rửa 0.5     Khu vực ăn u ố ng\r\n  t hoán g mát, đủ bàn gh ế  và các trang thi ế t bị để ngăn côn trùng 0.5     Dụng cụ chứa thức ăn và sử dụng đ ể  ăn u ố ng được làm b ằ ng vật\r\n  liệu dễ làm vệ sinh và không thôi nhiễm yếu tố độc hại 0.5     Có phương tiện bảo quản, lưu giữ thực\r\n   phẩm 0.5     Có đủ phương tiện, trang thiết bị phục\r\n  vụ làm vệ sinh, khử trùng 0.5     Có nguồn nước sạch và chỗ rửa tay với\r\n  xà phòng hoặc dung d ị ch sát khuẩn 0.5   4.2 Nhà b ế p 2.0     Có khu sơ chế nguyên liệu, khu chế\r\n  biến nấu nướng, khu bảo quản thức ăn, khu ăn uống, kho nguyên liệu, bảo quản\r\n  thực  phẩm 0.5     Nơi chế biến thức ăn được thiết kế\r\n  theo nguyên tắc 1 chiều, có  đ ủ dụng cụ chế biến, bảo quản,\r\n  sử dụng riêng đối với thực phẩm sống và thức ăn chín 1.0     Có lưu m ẫ u thức\r\n  ăn theo quy định 0.5     Đối với các trường không tự cung cấp\r\n  thức ăn: Có ký h ợ p đ ồ ng với các cơ\r\n  sở có giấy chứng nhận đủ điều kiện an toàn thực phẩm để cung cấp suất ăn cho\r\n  học sinh 2.0   4.3 Kho chứa thực  phẩm 1.0     Bảo đảm lưu thông không khí, đủ ánh\r\n  sáng, cửa s ổ  có lưới chống chuột và côn trùng 0.3     Tường, trần nhà, sàn nhà nhẵn, bằng\r\n  phẳng, thuận tiện cho việc làm  v ệ\r\n   sinh và khử trùng 0.2     Có phương ti ệ n,\r\n  d ụ ng cụ để phân loại, bảo quản và l ưu  giữ thực  phẩm 0.5   4.4 Người làm việc tại nhà ăn, căng\r\n  tin 3.0     Có giấy chứng nhận tập huấn về an\r\n  toàn thực phẩm 1.0     Có giấy ch ứ ng\r\n  nhận sức khỏe theo quy định của Bộ Y tế 1.0     Người trực tiếp làm việc tại nhà\r\n  ăn, nhà bếp mặc tran g  phục bảo hộ riêng, đội mũ, đi găng\r\n  tay chuyên dụng, đeo khẩu trang 1.0   V Bảo đảm môi tr ườ n g thực thị\r\n  chính sách và xây dựng các môi quan hệ xã hội trong trường học, liên kết cộng\r\n  đồng 10   5.1 Thực hiện các ch í nh sách, quy định và chế độ chăm sóc sức khỏe học sinh trong trường học 4.0     Có quy định và thực hiện vệ sinh\r\n  môi trường, vệ sinh cá nhân 0.5     Có quy định và thực hiện phòng ch ố ng tai nạn thương tích 0.5     Có quy định và thực hiện bảo đảm an\r\n  toàn thực  phẩm 0.5     Có quy định và thực hiện dinh dưỡng\r\n  hợp lý 0.5     Có quy định và thực hiện tăng cường\r\n  hoạt động th ể  lực 0.5     Có quy định  cụ thể  trách nhiệm của giáo viên và người chăm sóc 0.5     Có quy ch ế  ph ố i hợp giữa nhà trường, gia đình và cộng đ ồ ng  về  chăm sóc và bảo vệ sức khỏe học sinh 0.5     Có tổ chức chương trình dạy học phù\r\n  hợp lứa tuổi, bảo đảm thời gian nghỉ ngơi, vui chơi, tạo môi trường thuận lợi\r\n  cho học sinh cùng tham gia 0.5   5.2 Xây dựng m ố i quan hệ giữa th ầ y cô giáo với học sinh\r\n  và học sinh với học sinh 3.0     Th ầ y cô giáo\r\n  và người chăm sóc học sinh không vi phạm các nội quy ứng xử, tôn trọng và\r\n  không đối xử thô bạo với học sinh; thực hiện bình đẳng giới, dân tộc, tôn\r\n  giáo, không phân biệt đối xử 2.0     Học sinh có hoàn cảnh khó khăn và học\r\n  sinh khó hòa nhập được phát hiện và giúp đỡ 1.0   5.3 Xây dựng m ố i liên hệ giữa nhà tr ườ n g với gia đình và cộng đồng trong chăm sóc sức khỏe học sinh 3.0     Trườn g  học có\r\n  hướng d ẫ n cha mẹ học sinh bảo đảm các  điều kiện  học tập, rèn luyện cho con em mình\r\n  tại nhà 0.5     Trường học vận động sự ủng hộ của\r\n  chính q uyề n, ban ngành, đoàn thể tại địa phương hỗ trợ\r\n  nguồn lực tạo điều kiện cho hoạt động y tế trường học 1.0     Giáo viên và học sinh tích cực tham\r\n  gia các phong trào, hoạt động thể thao văn hóa của địa phương, tạo sự gắn kết\r\n  giữa trường học và chính quyền, đoàn thể địa phương 0.5     Trường học ph ố i\r\n  hợp với cơ quan y t ế  địa phương  tổ chức  các hoạt động chăm sóc sức khỏe cho học sinh 1.0   VI Bảo đảm các điều kiện về chăm\r\n  sóc sức khỏe cho học sinh 10   6.1 Phòng y t ế  trường học 5.0     Có phòng y tế riêng, bảo đảm diện\r\n  tích để triển khai các hoạt động chuyên môn 1.0     Có vị trí thuận tiện cho công tác\r\n  sơ cứu,  cấp  cứu 0.5     Có ít nhất 01 giường khám bệnh và\r\n  lưu bệnh nhân 0.5     Có bàn,  g h ế , tủ dụng cụ và thi ế t bị làm việc thông thường 1.0     Có thu ố c thi ế t y ế u phù hợp đ ể  phục vụ\r\n  cho việc chăm sóc sức khỏe học sinh trong thời  g ian học\r\n  tập và sinh hoạt tại trường 1.0     Có s ổ  khám bệnh,\r\n  s ổ  theo dõi t ổ ng hợp tình trạng sức\r\n  khỏe học sinh, sổ theo dõi sức khỏe học sinh theo quy định 1.0   6.2. Nhân viên YTTH 5.0     Nhân viên YTTH có trình độ chuyên môn\r\n  từ y sĩ trung  cấp  tr ở  lên 2.0     Trường hợp trường học chưa có nhân\r\n  viên y t ế  hoặc nhân viên y tế chưa đáp ứng trình độ\r\n  chuyên môn theo quy định, các trường học ký hợp đồng với Trạm Y tế xã hoặc cơ\r\n  sở khám bệnh, ch ữ a bệnh từ hình thức phòng khám đa khoa\r\n  trở lên để chăm sóc sức khỏe học sinh 2.0     Nhân viên y t ế  trường học phải được thường xuyên cập nhật kiến thức chuyên môn y t ế\r\n   thông qua các hình thức hội thảo, tập huấn, đào tạo, bồi d ưỡ ng nghiệp vụ chuyên môn do ngành y tế, ngành giáo dục  tổ chức  để triển khai được các nhiệm vụ theo\r\n  quy định 3.0   VII Quản lý, bảo vệ, chăm sóc sức khỏe\r\n  học sinh 20     Thực hiện  kiểm tra  sức khỏe cho học sinh vào đ ầ u năm\r\n  học (đo chiều cao và cân nặng đối với trẻ dưới 36 tháng tuổi;  đ o chiều cao, cân nặng, huyết áp, nhịp tim, thị lực đối với trẻ từ 36\r\n  tháng tuổi trở lên) 2.0     Có đo chi ề u\r\n  cao, cân nặng, ghi bi ể u đ ồ  tăng tr ưở ng, theo dõi sự phát triển thể lực cho trẻ em dưới 24 tháng tuổi mỗi\r\n  tháng một lần, trẻ em trên 24 tháng tuổi mỗi quý một lần 2.0     Có theo dõi sức khỏe học sinh, suy\r\n  dinh dưỡng, thừa cân, béo phì, bệnh răng miệng, dấu hiệu bất thường và các bệnh\r\n  tật khác để xử trí, chuyển đến  cơ sở \r\n  khám bệnh, chữa bệnh theo quy định và áp dụng chế độ học tập, r è n luyện phù hợp với tình trạng sức khỏe. 2.0     Có ph ố i hợp với\r\n  các cơ sở y t ế  có đủ  điều\r\n  kiện  đ ể  tổ chức  khám, điều trị theo các chuyên khoa cho học sinh 1.0     Thực hiện sơ cứu,  cấp  cứu (n ế u có) theo\r\n  quy định  của  Bộ Y t ế 1.0     Có tư v ấ n cho\r\n  giáo viên, cha mẹ hoặc người giám hộ của học sinh về các vấn đề liên quan đến\r\n  bệnh tật, phát triển thể chất và tinh thần của học sinh; hướng dẫn cho học\r\n  sinh tự chăm sóc sức khỏe; trường hợp trong trường học có học sinh khuyết tật\r\n  thì tư vấn, hỗ trợ cho học sinh khuyết tật h òa  nhập 1.0     Có hướng dẫn tổ chức bữa ăn học đường\r\n  bảo đảm dinh dưỡng hợp lý, đa dạng thực phẩm, phù hợp với đ ố i tượng và lứa tu ổ i 1.0     Có phối hợp với cơ sở y tế địa\r\n  phương trong việc  tổ chức  các chi ế n dịch tiêm chủng, uống vắc xin phòng bệnh cho học sinh 1.0     Có thông báo định kỳ tối thiểu 01 lần/năm\r\n  học và khi c ầ n thiết về tình hình sức  khỏe của \r\n  học sinh cho cha mẹ hoặc người giám hộ của học sinh 1.0     Có ghi chép đầy đủ vào sổ khám bệnh,\r\n  s ổ  theo dõi sức khỏe học sinh, sổ theo dõi tổn g  hợp tình trạng sức khỏe học sinh 2.0     Thường xuyên kiểm tra, giám sát các\r\n  điều kiện học tập, vệ sinh trường lớp, an toàn thực phẩm, cung cấp nước uống,\r\n  xà phòng rửa tay 2.0     Chủ động triển khai các biện pháp\r\n  và chế  đ ộ vệ sinh phòng, chống dịch theo quy định tại\r\n  Thông tư số  46/2010/TT-BYT  và các hướng dẫn khác của cơ quan y tế 2.0     Có  tổ\r\n  chức  triển khai các chương trình y tế, phong trào vệ sinh phòng bệnh,\r\n  tăng cường hoạt động th ể  lực, dinh dưỡng  hợp lý 2.0   VIII Hoạt động truy ề n thông, giáo dục sức khỏe 15     Có biên soạn, sử dụng các tài liệu\r\n  truy ề n thông giáo dục sức khỏe với nội dung phù hợp cho từng\r\n  nhóm đối tượng và điều kiện cụ thể của từng địa phương 1.0     Có nội dung truyền thông, giáo dục\r\n  sức khỏe về các biện pháp (1) phòng chống dịch, bệnh truyền nhiễm; (2) phòng chống\r\n  ngộ độc thực phẩm; (3) phòng chống tai nạn thương tích; (4) dinh dưỡng và hoạt\r\n  động thể lực; (5) phòng chống bệnh tật học đường; (6) chăm sóc răng miệng;\r\n  (7) ch ă m sóc mắ t  cho học sinh  (m ỗ i nội dung 1,0 điểm) 7.0     Có l ồ ng ghép\r\n  các nội dung giáo dục sức khỏe, phòng ch ố ng bệnh tật\r\n  trong các giờ giảng 1.0     Có  tổ\r\n  chức  cho học sinh thực hành các hành vi (1) vệ sinh cá nhân; (2) vệ\r\n  sinh môi trường; (3) dinh dưỡng hợp lý; (4) rèn luyện thể lực; (5) chăm sóc răng\r\n  miệng; (6) chăm sóc mắt thông qua các hình thức, mô hình phù hợp  (mỗi nội\r\n  dung 1,0 điểm) 6.0   IX Th ố ng\r\n  kê báo cáo và đánh giá 10     H ằ ng năm có\r\n  báo cáo thực hiện công tác y t ế  trường học khi kết thúc\r\n  năm học theo quy định 3.0     H ằ ng năm có tự\r\n   tổ chức  đánh giá công tác y t ế  trường học theo quy định 5.0     Có sử dụng  kết quả  đánh giá đ ể  xây dựng  kế hoạch 2.0     Tổng điểm 100   Kết  q uả\r\nđánh giá và xếp loại 1.  T ổ ng điểm  đ ạt: ………………điểm 2.  Các tiêu chí bắt buộc:     Đạt □            \r\n Không đạt □ 3. X ếp loại:                          Tốt □     \r\n         Khá             □      Trung bình □       Không đạt □   Đại\r\n  diện đoàn kiểm tra (K ý  và ghi rõ họ tên) Đại\r\n   d iện đơn vị đ ượ c\r\n  kiểm tra (ký tên, đóng d ấ u)   HƯỚNG\r\nDẪN CHẤM ĐIỂM 1.  Nguyên tắc chấm điểm - Chỉ chấm điểm với các tiêu chí có\r\nthực hiện - Các nội dung không quy định bắt buộc\r\nthực hiện đối với nhà trường thì trừ điểm chu ẩn  và tổng điểm - Thực hiện đầy đủ tiêu chí được 100%\r\nmức điểm chuẩn, thực hiện chưa đầy đủ được 50% mức điểm chuẩn 2.  Đánh giá kết quả: - Tổng điểm tối đa là 100 điểm a)  Trường đạt loại\r\nTốt:  có tổng mức điểm đạt  ≥  90\r\nđiểm và đạt từ  ≥  80% điểm chuẩn của từng nhóm tiêu chí bắt\r\nbuộc. - Các nhóm tiêu chí bắt buộc gồm: + Bảo đảm các điều kiện về cơ sở vật\r\nchất, cấp  thoát  nước vệ sinh môi t r ường, an toàn thực phẩm, điều kiện chăm sóc sức khỏe học sinh \r\n(32,0 điểm trở lên) ; + Bảo đảm môi trường thực thị chính\r\nsách và xây dựng các mối quan hệ xã hội trong trường học, liên kết cộng đồng  (8,0\r\nđiểm trở lên) ; + Tổ chức các hoạt động quản lý, bảo\r\nvệ và chăm sóc sức khỏe học sinh  (16,0  điểm trở lên) ; + Tổ chức các hoạt động truyền thông,\r\ngiáo dục sức khỏe  (12,0 điểm trở lên) . b)  Trường đạt loại\r\nKhá : từ 70 - <90% tổng mức điểm chuẩn; Có một trong các nhóm tiêu chí bắt buộc\r\nkhông đạt 70% mức điểm chuẩn. c)  Trường đạt loại\r\nTrung bình:  từ 50 - <70% tổng mức điểm chuẩn; Có một trong các nhóm tiêu chí bắt buộc\r\nkhông đạt 50% mức điểm chuẩn. d ) Trường Không đạt:  có dưới 50% tổng mức điểm chuẩn     Khổ\r\n  giấy A4 (21cm  x  29,7cm) Phụ lục 0 4 MẪU ĐÁNH GIÁ CÔNG TÁC Y TẾ TRƯỜNG\r\nHỌC (Áp dụng cho cơ s ở  giáo dục  phổ\r\nthông ) (Ban hành kèm theo Thông t ư  liên\r\ntịch s ố 13 /2016/TTLT-BYT-BGDĐT ngày  12 t háng 5 năm 2016 quy định công t á c y tế trường\r\nhọc của Bộ trưởng Bộ Y tế v à  Bộ tr ưở ng Bộ Gi á o dục và Đào tạo) Tên trường ………………….. CỘNG\r\n  HÒA XÃ HỘI CHỦ NGHĨA VIỆT NAM \r\n  Độc lập - Tự do - Hạnh phúc  \r\n  --------------- ………..,  ngày  …….  tháng  ……  năm 20 ……   ĐÁNH\r\nGIÁ CÔNG TÁC Y T Ế  TRƯỜNG HỌC (Áp dụng cho cơ sở giáo dục  phổ thông ) I. Thông tin chung 1.  Tên cơ sở được đánh giá: Trường  ……………………………………………….. xã/phường  ……………………  quận/huyện  ……………………..  Tỉnh/ thành phố  ………………… 2.  T ổ ng s ố\r\n lớp:  ………………………….. Tổng số học sinh  ……………. Tổng số giáo viên: ……….. 3.  Hình thức đánh giá:     Tự đánh giá □           Đánh giá của cơ quan quản lý □ 4.  Thời gian:      Ngày       tháng    \r\n  năm 20 II.  Kết qu ả  đánh giá TT Nội\r\n  dung đánh giá Điểm\r\n  chuẩn Điểm  đ ạ t I Công tác tổ chức và kế hoạch 5.0   1.1. Ban Chăm sóc sức khỏe học sinh 2.0     Có  Quyết \r\n  định thành lập, phân công trách nhiệm các thành viên 1.0     Định kỳ  tổ chức  họp Ban Chăm sóc sức khỏe và đ ề  ra\r\n  nhiệm vụ cụ thể cho từng học kỳ (tối thiểu 1 lần/học kỳ) 1.0   1.2. Kế hoạch  hoạt động YTTH hàng năm  và giai đoạn 3.0     Có bản  kế hoạch  hoạt động YTTH theo năm học được phê duyệt 1.0     Nội dung b ả n  kế hoạch  được xây dựng đủ các nội dung  về  YTTH theo quy định 1.0     Có b ố  trí kinh\r\n  phí đ ể  thực hiện nhiệm vụ YTTH hàng năm 1.0   II Bảo đảm các điều ki ệ n về cơ sở v ậ t chất 10   2.1. Phòng học 2.0     Diện tích trung bình không dưới  1 ,25m 2 /1 học sinh (đ ố i với tiểu học)  1 ,5m 2 /1 học sinh (đối với trung học) 0.5     Phòng học được thi ế t k ế  2 cửa ra vào, một cửa ở đ ầ u lớp, một cửa ở cuối lớp; cửa đi có 2 cánh, chiều rộng không nhỏ hơn\r\n   1 ,0m và mở ra phía hành lan g 0.3     Các phòng học không được thông nhau\r\n  và được ngăn cách với các phòn g  có nguồn gây ô nhiễm tiếng\r\n  ồn, khói bụi ,  hơi khí độc hoặc mùi khó ch ị u 0.3     Phòng học thông t hoán g, mát  về \r\n  mùa hè, ấm  về  mùa đông; có hệ thống\r\n  thông gió nhân tạo như quạ t  trần, quạt tường, quạt thông\r\n  gió; nồng độ khí C O 2 \r\n  trong phòng học không quá 0,1% 0.4     Phòng học yên tĩnh, ti ế ng  ồ n nên không quá 55 dBA theo mức âm tương\r\n  đương 0.5   2.2 Phòng học bộ môn vật lý,  hóa  học, sinh học 1.0     Diện tích t ố i\r\n  thi ể u cho 1 học sinh đ ố i với  cấp  trung học cơ sở là  1 ,85m 2 ,\r\n  đ ố i với cấp trung học phổ thông là 2m 2 0.2     Chi ề u cao từ\r\n  3,30m trở lên; chi ề u ngang có kích thước t ố i thi ể u 7,2m, tỷ lệ giữa chiều dài và chiều rộng\r\n  không lớn hơn 2; có phòng chuẩn bị với diện tích từ 12m 2  đến 27m 2 \r\n  và được b ố  trí liền kề, có cửa liên thông với phòng học\r\n  bộ môn 0.2     B ố  trí 2 cửa\r\n  ra vào phía đ ầ u và cu ố i phòng, chi ề u rộng cửa đ ả m bảo yêu cầu  thoát  hiểm 0.2     Thông t hoán g, n ồ ng độ khí C O 2  không quá 0,1% và n ồ ng độ các chất  hóa  học khác\r\n  trong không khí nằm trong giới hạn cho phép 0.2     Có bảng nội quy và hướng dẫn an\r\n  toàn được viết rõ ràng, cụ thể, đầy đủ và được treo ở nơi dễ đọc 0.2   2.3 Phòng học bộ môn công ng hệ thông tin 0.5     Diện tích t ố i thi ể u cho 1 học sinh đ ố i với  cấp  ti ể u học và trung học cơ sở là 2,25m 2 ,\r\n  đối với cấp trun g  học phổ thông là 2,45m 2 0.3     Phòng học c ầ n\r\n  được thông khí t ố t, n ồ ng độ C O 2  không quá 0,1 %, đảm bảo an toàn về điện\r\n  và an toàn điện từ trường cho học sinh theo quy định 0.2   2.4 Bàn gh ế 3.5   2.4.1 Bàn gh ế  phòng học 2.5     Sử dụng bàn gh ế  không quá 2 ch ỗ  ng ồ i, bàn\r\n  và gh ế  rời nhau, các góc cạnh nh ẵ n\r\n  và an toàn 1.0     Có đủ 6 cỡ bàn gh ế  I, II, III, IV, V, VI tương ứng với chi ề u cao của\r\n  học sinh theo quy định tại Thông tư liên tịch số 26/2011/BGD&ĐT-BKHCN-BYT\r\n  ngày 16/6/2011 về hướng dẫn tiêu chuẩn bàn ghế học sinh trường tiểu học, trường\r\n  trung học cơ sở và trung học phổ thông và được kê theo đúng quy định 1.5   2.4.2 Bàn gh ế  phòng học bộ môn vật lý,  hóa  học,\r\n  sinh học 0.5     Là loại chuyên dụng, đáp ứng được\r\n  các yêu c ầ u đặc thù của bộ môn, có hệ thống điện, nước, khí\r\n  ga theo yêu cầu sử dụng, đảm bảo an toàn cho học sinh khi tiến hành làm thí\r\n  nghiệm 0.5   2.4.3 Bàn gh ế  phòng học bộ môn công ng hệ thông tin 0.5     Là loại chuyên dụng, đáp ứng được\r\n  các yêu c ầ u đặc thù  của \r\n  bộ môn 0.5   2.5 Bảng phòng học, phòng học bộ môn 1.0     Sử dụng bảng ch ố ng l óa  và đảm bảo độ tương phản giữa n ề n bảng và ch ữ  viết 0.5     Chi ề u cao của\r\n  bảng từ  1 ,2m -  1 ,5m, chi ề u rộng bảng không quá 3,2m, phù hợp với chiều rộng phòng học và  đ ược treo theo đúng quy định 0.1     Bảng có màu xanh lá cây hoặc m ầ u đen (n ế u vi ế t b ằ ng ph ấ n tr ắ ng), mầu trắng\r\n  (nếu viết b ằ ng bút dạ màu đen) 0.1     Bảng treo ở giữa tường, mép dưới bảng\r\n  cách n ề n phòng học từ 0,8m đến  1 m 0.3   2.6 Chi ế u\r\n  s á ng 2.0   2.6.1 Chi ế u\r\n  sáng phòng học 1.0     Hướng l ấ y ánh\r\n  sáng tự nhiên là hướng nam hoặc đông nam (cửa sổ ở phía không có hà nh  lang) về phía tay trái của học sinh khi ng ồ i học;\r\n  tỷ lệ t ổ ng diện tích cửa s ổ  (vùng l ấ y ánh sáng) trên diện tích phòng học không dưới 1/5 0.2     Phòng học có  hệ thống  chi ế u sáng nhân\r\n  tạo, các bón g  đèn có chụp chống lóa; bóng đèn trên trần\r\n  treo thấp hơn quạt trần, thành dãy song song với tường có cửa sổ, cách tường\r\n  từ 1,2 đến  1 ,5m, có công tắc riêng cho từng dã y 0.2     Vùng học tập có hệ số chiếu sáng đồng\r\n  đều và không dưới 1/2, độ rọi không dưới 300 Lux 0.5     Đ è n chi ế u sáng bảng được l ắ p đặt song song với tường treo\r\n  bảng, cách tường 0,6m và cao hơn mép trên của bảng 0,3m 0.1   2.6.2 Chi ế u\r\n  sáng phòng học bộ môn vật lý,  hóa  học,\r\n  sinh học 0.5     Bảo đảm các y ê u\r\n  c ầ u v ề  chi ế u\r\n  sáng; hướng lấy ánh sáng tự nhiên từ phía tay trái khi học sinh ngồi hướng\r\n  lên bảng; sử dụng hệ thống chiếu sáng nhân tạo hỗn hợp (chiếu sáng chung và\r\n  chiếu sáng cục bộ); độ rọi trên mặt phẳng làm việc không dưới 300 Lux 0.5   2.6.3 Chi ế u\r\n  sáng phòng học bộ môn công ng hệ thông tin 0.5     Chi ế u sáng trên\r\n  bàn máy tính không dưới 300 Lux 0.5   III Bảo đảm các điều kiện về cấp  thoát  n ướ c\r\n  và vệ sinh môi trường 10   3.1 Cấp  nước ăn u ố ng và sinh hoạt 3.0     Nước uống bảo đảm tối thiểu bình quân\r\n  mỗi học sinh trong một ca học có 0,5 lít về mùa hè và 0,3 lít về mùa đông 1.0     Nước sinh hoạt bảo đảm t ố i thi ể u 4 lít cho một học sinh trong một ca học;\r\n  nếu dùng hệ thống cấp nước bằng đường ống thì m ỗ i vòi sử\r\n  dụng tối đa cho 200 học sinh trong một ca học 0.5     Khu nội trú của trường học có đủ nước\r\n  sạch đ ể  học sinh sử dụng trong ăn u ố ng và sinh hoạt hằng ngày, bảo đảm tối thiểu 100 lít cho một học sinh\r\n  trong 24 giờ 0.5     Chất  lượng nước bảo đảm theo các quy định của Bộ Y t ế 0.5     Gi ế ng nước, b ể\r\n   nước, chum, vại nước (n ế u có) có n ắ p đậy, độ cao bảo đảm an toàn cho trẻ khi  sử dụng  theo quy định 0.5   3.2 Công trình vệ sinh 4.0     Khu vệ sinh được b ố  trí hợp lý, đáp ứng yêu c ầ u sử dụng của học sinh\r\n  và giáo viên, không làm ô nhiễm môi trường 0.2     Có khu vực vệ sinh riêng cho học\r\n  sinh và giáo viên, riêng biệt cho nam và nữ 0.5     Mỗi khu vệ sinh nhà tiêu, nhà tiểu,\r\n  khu rửa tay có nước sạch, xà phòng hoặc dung dịch sát khuẩn 1.0     Loại hình nhà tiêu sử dụng bảo đảm\r\n  các yêu c ầ u  về  xây\r\n  dựng, sử dụng và bảo  quả n theo quy định của Bộ Y t ế  ( Q CVN 01: 2011/BYT) 0.5     S ố  lượng thi ế t bị: 01 ti ể u nam, 01 xí và 01 chậu rửa cho từ\r\n  20-30 học sinh. Đối với học sinh nữ tối đa 20 học sinh/1 chậu xí. 1.0     L ố i vào khu vệ\r\n  sinh không được đ ố i diện với l ố i\r\n  vào phòng học, phòng bộ môn. Chiều cao l ắ p đặt các thiết\r\n  bị vệ sinh phù hợp với nhu cầu sử dụng và lứa tuổi học sinh 0.2     Có bảng nội quy nhà vệ sinh 0.2     Tùy  theo loại nhà tiêu mà đảm bảo đủ  chất \r\n  độn, nước dội, gi ấ y vệ sinh, thùng rác hợp vệ sinh 0.4   3.3 Thu gom v à  xử lý ch ấ t thải 3.0     Có hệ thống cống rãnh  thoát  nước mưa ,  nước thải\r\n  sinh hoạt, không có nước ứ đọng xung quanh trường lớp 1.0     Có thùng chứa rác và phân loại rác\r\n  thải 1.0     Có h ợ p đ ồ ng với các cơ sở đủ  điều kiện \r\n  thu gom, xử lý  chất  thải, rác thải sinh\r\n  hoạt hoặc tự thu gom, xử lý  chất  thải,\r\n  rác thải theo quy định 1.0   IV Bảo đảm các điều kiện về an toàn\r\n  thực phẩm 10   4.1 Nhà ăn, căng tin 4.0     Thông t hoán g, đủ ánh sáng, cửa sổ có lưới chống chuột, ruồi nhặng, côn\r\n  trùng 0.5     Tường, tr ầ n\r\n  nhà b ằ ng ph ẳ ng, nh ẵ n, thuận tiện làm vệ sinh 0.5     B àn, gh ế , dụng\r\n  cụ, phương tiện làm b ằ ng vật liệu d ễ  cọ rửa 0.5     Khu vực ăn uống t hoán g mát, đủ bàn ghế và các trang thiết bị để\r\n  ngăn côn trùng 0.5     Dụng cụ chứa thức ăn và sử dụng đ ể  ăn u ố ng được làm b ằ ng vật\r\n  liệu dễ làm vệ sinh và không thôi nhiễm yếu t ố  độc hại 0.5     Có phương tiện bảo quản, lưu giữ thực\r\n  phẩm 0.5     Có đủ phương tiện, trang thi ế t bị phục vụ làm vệ sinh, khử trùng 0.5     Có nguồn nước sạch và ch ỗ  rửa tay với xà phòng hoặc dung dịch sát khuẩn 0.5   4.2 Nhà b ế p 2.0     Có khu sơ ch ế  nguyên\r\n  liệu, khu ch ế  bi ế n n ấ u nướng, khu bảo quản thức ăn, khu ăn uống, kho nguyên liệu, bảo quản\r\n  thực phẩm 0.5     Nơi ch ế  bi ế n thức ăn được thi ế t k ế \r\n  theo nguyên t ắ c 1 chi ề u, có đủ dụng\r\n  cụ chế biến, bảo quản, sử dụng riêng với thực phẩm sống và thức ăn chín 1.0     Có lưu m ẫ u thức\r\n  ăn theo quy định 0.5     Đ ố i với các\r\n  trường không tự cung  cấp  thức ăn: Có ký\r\n  hợp  đồ ng với các cơ sở có giấy chứng nhận đủ điều kiện\r\n  an toàn thực phẩm để cung cấp suất ăn cho học sinh 2.0   4.3 Kho chứa thực  phẩm 1.0     Bảo đảm lưu thông không khí, đủ ánh\r\n  sáng, cửa s ổ  có lưới ch ố ng chuột và\r\n  côn trùng 0.3     Tường, tr ầ n\r\n  nhà, sàn nhà nh ẵ n, b ằ ng ph ẳ ng, thuận tiện cho việc làm vệ sinh và khử trùng 0.2     Có phương tiện, dụng cụ đ ể  phân loại, bảo quản và lưu giữ thực  phẩm 0.5   4.4 Người làm việc tại nhà ăn, căn g  tin 3.0     Có gi ấ y chứng\r\n  nhận tập hu ấ n  về \r\n  an toàn thực  phẩm 1.0     Có gi ấ y ch ứng\r\n   nhận sức khỏe theo quy định  của \r\n  Bộ Y t ế 1.0     Người trực ti ế p\r\n  làm việc tại nhà ăn, nhà b ế p mặc trang phục bảo hộ\r\n  riêng, đội mũ, đi găng tay chuyên dụng, đeo kh ẩ u trang 1.0   V Bảo đảm môi trường thực thị\r\n  chính sách và xây dựng các m ố i quan hệ xã hội\r\n  trong trường học, liên kết cộng đồng 10   5.7 Thực hiện các ch í nh sách, quy định và ch ế  độ chăm sóc sức\r\n  khỏe học sinh trong trường học 4.0     Có quy định và thực hiện vệ sinh\r\n  môi trường, vệ sinh cá nhân 0.5     Có quy định và thực hiện phòng ch ố ng tai nạn thương tích 0.5     Có quy định và thực hiện bảo đảm an\r\n  toàn thực  phẩm 0.5     Có  quy\r\n  định  và thực hiện dinh d ưỡ ng hợp lý 0.5     Có quy định và thực hiện tăng cường\r\n  hoạt động th ể  lực 0.5     Có quy định  cụ thể  trách nhiệm của giáo viên và người chăm sóc 0.5     Có quy ch ế  ph ố i hợp giữa nhà trường, gia đình và cộng đ ồ ng  về  chăm sóc và  bảo\r\n  vệ  sức khỏe học sinh 0.5     Có tổ chức chương trình dạy học phù\r\n  hợp lứa tuổi, b ả o đảm thời gian nghỉ ngơi, vui chơi, tạo\r\n  môi trường thuận lợi cho học sinh cùng tham gia 0.5   5.2 Xây dựng m ố i quan hệ giữa th ầ y cô giáo với học sinh\r\n  và học sinh với học sinh 3.0     Th ầ y cô giáo\r\n  và người chăm sóc học sinh không vi phạm các nội quy ứng xử, tôn trọng và\r\n  không  đ ối x ử  thô bạo với học sinh;\r\n  thực hiện bình đẳng giới, dân tộc, tôn giáo, không phân biệt đ ố i xử 2.0     Học sinh có hoàn cảnh khó khăn và học\r\n  sinh khó hòa nhập được phát hiện và  giúp  đỡ 1.0   5.3 Xây dựng m ố i liên hệ giữa nhà trường với gia đình và cộng đ ồ ng trong chăm sóc sức khỏe học sinh 3.0     Trường học có hướng d ẫ n cha mẹ học sinh bảo đảm các  điều kiện \r\n  học tập, rèn luyện cho con em mình tại nhà 0.5     Trường học vận động sự ủng hộ của\r\n  chính quy ề n, ban ngành, đoàn thể tại  đ ịa phương hỗ trợ nguồn lực tạo điều kiện cho hoạt động y tế trường học 1.0     Giáo viên và học sinh tích cực tham\r\n  gia các phong trào, hoạt động thể thao văn hóa của địa phương, tạo s ự  gắn kết giữa trường học và chính quyền, đoàn thể địa phương 0.5     Trường học phối hợp với cơ quan y tế\r\n  địa phương  tổ chức  các hoạt động chăm\r\n  sóc sức khỏe cho học sinh 1.0   VI Bảo đảm các điều kiện về chăm\r\n  sóc sức khỏe cho học sinh 10   6.1 Phòng y t ế  trường học 5.0     Có phòng y t ế  riêng,\r\n  bảo đảm diện tích triển khai hoạt động chuyên môn 1.0     Có vị trí thuận tiện cho công tác\r\n  sơ cứu,  cấp  cứu 0.5     Có ít  n hất 01\r\n  giường khám bệnh và lưu bệnh nhân 0.5     Có bàn, ghế, tủ dụng cụ và thiết bị\r\n  làm việc thông thường 1.0     Có thu ố c thi ết  y ế u phù hợp đ ể  phục vụ cho\r\n  việc chăm sóc sức khỏe học sinh trong thời gian học tập và sinh hoạt tại trường 1.0     Có s ổ  khám bệnh,\r\n  s ổ  theo dõi t ổ ng h ợ p t ì nh trạng sức khỏe học sinh, sổ theo dõi sức\r\n  khỏe học sinh theo quy định 1.0   6.2 Nhân viên YTTH 5.0     Nhân viên YTTH chuyên trách có\r\n  trình độ t ố i thi ể u y sĩ trình độ\r\n  trung cấp 2.0     Trường hợp trường học chưa có nhân\r\n  viên y t ế  hoặc nhân viên y t ế  chưa\r\n  đáp ứng trình độ chuyên môn theo quy định thì trường học ký hợp đồng với Trạm\r\n  Y tế xã hoặc cơ sở khám bệnh, ch ữ a bệnh từ hình thức\r\n  phòng khám đa khoa tr ở  lên để chăm sóc sức khỏe học sinh 2.0     Nhân viên y t ế  trường học phải được thường xuyên cập nhật ki ế n\r\n  thức chuyên môn y tế thông qua các hình thức hội thảo, tập huấn, đào tạo ,\r\n   bồi dưỡng n g hiệp vụ chuyên môn do n g ành y tế, ngành giáo dục tổ chức để triển khai được các nhiệm vụ theo\r\n  quy  đ ịnh 3.0   VII Quản lý, bảo vệ, chăm sóc sức khỏe\r\n  học sinh 20     Có thực hiện  kiểm tra  sức khỏe cho học sinh vào đ ầ u năm học, bao gồm: đo chiều cao, cân nặng, huyết áp, nhịp tim, thị lực. 2.0     Có theo dõi chỉ s ố  kh ố i cơ th ể  (BMI) và tình\r\n  trạng dinh dưỡng của học sinh để tư vấn về dinh dưỡng hợp lý và hoạt động thể\r\n  lực cho học sinh 2.0     Có thường xuyên theo dõi sức khỏe học\r\n  sinh, phát hiện giảm thị lực, cong vẹo cột sống, bệnh răng miệng, rối loạn\r\n  tâm sinh lý và các bệnh tật khác để xử trí, chuyển đến  cơ sở  khám bệnh, chữa bệnh theo quy định và\r\n  áp dụng chế độ học tập ,  rèn luyện phù hợp với tình trạng\r\n  sức khỏe 2.0     Có ph ố i hợp với\r\n  các cơ sở y t ế  có đủ  điều  kiện đ ể\r\n   tổ chức  khám, điều\r\n  trị theo các chuyên khoa cho học sinh 1.0     Thực hiện sơ cứu, cấp cứu (nếu có)\r\n  theo quy định của Bộ Y tế 1.0     Tư v ấ n các v ấ n đ ề  liên quan đ ế n bệnh tật,\r\n   phát triển  th ể  chất  và tinh thần của học sinh\r\n  cho giáo viên, cha mẹ hoặc người giám hộ; hướng d ẫ n cho\r\n  học sinh biết tự chăm sóc sức khỏe; trường hợp trong trường học có học sinh\r\n  khuyết tật thì tư vấn,  hỗ trợ  cho học\r\n  sinh khuyết tật h òa  nhập 1.0     Có hướng dẫn tổ chức bữa ăn học đường\r\n  bảo đảm dinh dưỡng hợp lý, đa dạng thực phẩm, phù hợp với đ ố i tượng và lứa tuổi đối với các trường có học sinh nội trú, bán trú 1.0     Có phối hợp với cơ sở y tế địa\r\n  phương trong việc tổ chức các chiến dịch tiêm chủng, uốn g  vắc xin phòng bệnh 1.0     Thông báo định kỳ t ố i thi ể u 01 l ầ n/năm học và\r\n  khi c ầ n thi ế t  về  tình hình sức  khỏe  của học sinh cho cha mẹ hoặc người giám hộ của học sinh 1.0     Có lập và ghi ch é p đầy đủ vào sổ khám bệnh, sổ theo dõi sức khỏe học sinh, sổ theo dõi\r\n  tổng hợp tình trạng sức khỏe học sinh 2.0     C ó  thường\r\n  xuyên  kiểm tra , giám sát các  điều kiện  học tập, vệ sinh trường lớp, an toàn\r\n  thực phẩm, cung cấp nước uống, xà phòng rửa tay 2.0     C ó  chủ động\r\n  tri ể n khai các biện pháp phòng, ch ố ng\r\n  dịch theo hướng dẫn tại Thông tư số  46/2010/TT-BYT  ngày 29/12/2010 của Bộ Y tế\r\n  và các hướng dẫn khác của cơ quan y tế 2.0     Có  tổ\r\n  chức  tri ể n khai các chương trình y t ế , phong trào vệ sinh phòng bệnh, tăng cường hoạt động th ể  lực, dinh dưỡng hợp lý, phòng chống tác hại thuốc lá, tác hại rượu\r\n  bia 2.0   VIII H oạt động truy ề n thông, giáo dục sức khỏe 15     Có biên soạn,  sử dụng  các tài liệu truy ề n thông giáo dục sức khỏe với nội dung phù hợp cho từng nhóm đối tượng\r\n  và điều kiện cụ thể của từng địa phương 1.0     Có nội dung truy ề n thông, giáo dục sức khỏe  về \r\n  các biện pháp (1) phòn g  chống dịch, bệnh truyền nhiễm;\r\n  (2) phòn g  chống ngộ độc thực phẩm; (3) phòng chống tai nạn\r\n  thương tích; (4) dinh dưỡng và hoạt động thể lực; (5) phòng chống bệnh tật học\r\n  đường; (6) chăm sóc răng miệng; (7) chăm sóc mắt cho học sinh  (mỗi nội d u ng 1,0 điểm) 7.0     Có l ồ ng ghép\r\n  các nội dung giáo dục sức khỏe, phòng ch ố ng bệnh tật\r\n  trong các giờ giảng 1.0     Có  tổ\r\n  chức  cho học sinh thực hành các hành vi (1) vệ sinh cá nhân; (2) vệ\r\n  sinh môi trường; (3) dinh dưỡng hợp lý; (4) rèn luyện thể lực; (5) chăm sóc\r\n  răng miệng; (6) chăm sóc mắt thông qua các hình thức, mô hình phù hợp  (m ỗ i nội dung 1,0 điểm) 6.0   IX Thống kê báo cáo và đánh giá 10     H ằ ng năm có\r\n  báo cáo thực hiện công tác y t ế  trường học khi  kết  thúc năm học theo quy định 3.0     Hằng năm có tự tổ chức đánh giá\r\n  công tác y tế trường học theo quy định 5.0     Có sử dụng  kết quả  đánh giá đ ể  xây dựng  kế hoạch 2.0   Tổng\r\n  điểm 100   Kết  q uả\r\nđánh giá và xếp loại 1.  T ổ ng điểm  đ ạt: ………………điểm 2.  Các tiêu chí bắt buộc:     Đạt □        \r\n     Không đạt □ 3. X ếp loại:                          Tốt □             \r\n Khá             □      Trung bình □       Không đạt □   Đại\r\n  diện đoàn kiểm tra (K ý  và ghi rõ họ tên) Đại\r\n   d iện đơn vị đ ượ c\r\n  kiểm tra (ký tên, đóng d ấ u)   HƯỚNG\r\nDẪN CHẤM ĐIỂM 1.  Nguyên tắc chấm điểm - Chỉ chấm điểm với các tiêu chí có\r\nthực hiện - Các nội dung không quy định bắt buộc\r\nthực hiện đối với nhà trường thì trừ điểm chu ẩn  và tổng điểm - Thực hiện đầy đủ tiêu chí được 100%\r\nmức điểm chuẩn, thực hiện chưa đầy đủ được 50% mức điểm chuẩn 2.  Đánh giá kết quả: - Tổng điểm tối đa là 100 điểm a)  Trường đạt loại\r\nTốt:  có tổng mức điểm đạt  ≥  90\r\nđiểm và đạt từ  ≥  80% điểm chuẩn của từng nhóm tiêu chí bắt\r\nbuộc. - Các nhóm tiêu chí bắt buộc gồm: + Bảo đảm các điều kiện về cơ sở vật\r\nchất, cấp  thoát  nước vệ sinh môi t r ường, an toàn thực phẩm, điều kiện chăm sóc sức khỏe học sinh \r\n(32,0 điểm trở lên) ; + Bảo đảm môi trường thực thị chính\r\nsách và xây dựng các mối quan hệ xã hội trong trường học, liên kết cộng đồng  (8,0\r\nđiểm trở lên) ; + Tổ chức các hoạt động quản lý, bảo\r\nvệ và chăm sóc sức khỏe học sinh  (16,0  điểm trở lên) ; + Tổ chức các hoạt động truyền thông,\r\ngiáo dục sức khỏe  (12,0 điểm trở lên) . b)  Trường đạt loại\r\nKhá : từ 70 - <90% tổng mức điểm chuẩn; Có một trong các nhóm tiêu chí bắt buộc\r\nkhông đạt 70% mức điểm chuẩn. c)  Trường đạt loại\r\nTrung bình:  từ 50 - <70% tổng mức điểm chuẩn; Có một trong các nhóm tiêu chí bắt buộc\r\nkhông đạt 50% mức điểm chuẩn. d ) Trường Không đạt:  có dưới 50% tổng mức điểm chuẩn Lưu trữ Ghi chú  Ý kiến Facebook  Email In \r\n            Thông báo khi VB này bị sửa đổi, bổ sung, có hoặc hết hiệu lực\r\n             \r\n            VĂN BẢN THAY THẾ \r\n            SO SÁNH VĂN BẢN THAY THẾ \r\n            VĂN BẢN SONG NGỮ Tin, bài liên quan: Quy định chi tiết về công tác y tế trường học MỤC LỤC VĂN BẢN\r\n                                 MINISTRY OF\r\n  HEALTH - MINISTRY OF EDUCATION AND TRAINING \r\n  -------- THE SOCIALIST\r\n  REPUBLIC OF VIETNAM \r\n  Independence - Freedom - Happiness \r\n  ---------------- No:13/2016/TTLT-BYT-BGDDT\r\n   Hanoi, May 12,\r\n  2016   JOINT CIRCULAR REGULATIONS\r\nON HEALTHCARE ACTIVITIES IN SCHOOLS Pursuant to the Government’s Decree\r\nNo. 63/2012/ND-CP  on functions, entitlement, responsibilities and organizational\r\nstructures of Ministry of Health dated August 31, 2012; Pursuant to the Government’s Decree No. 32/2008/ND-CP \r\non functions, entitlement, responsibilities and organizational structures of\r\nthe Ministry of Education and Training dated March 19, 2008; Pursuant to the Directive No. 23/2006/CT-TTg  on\r\nhealthcare in educational institutions dated July 12, 2006 by the Prime\r\nMinister; The Minister of health and Minister of Education\r\nand Training hereby issues this Joint Circular stipulating regulations on\r\nhealthcare activities in schools. Chapter I GENERAL PROVISIONS Article 1. Scope and regulated\r\nentities .................................................. .................................................. .................................................. Hãy đăng nhập hoặc đăng ký Thành viên  Pro tại đây  để xem toàn bộ văn bản tiếng Anh. Bạn Chưa Đăng Nhập Thành Viên! \r\n                Vì chưa Đăng Nhập nên Bạn chỉ xem được  Thuộc tính  của văn bản.\r\n                 \r\n                Bạn chưa xem được  Hiệu lực của Văn bản ,  \r\n                    Văn bản liên quan ,  Văn bản thay thế ,  Văn bản gốc ,  Văn\r\n                            bản tiếng Anh ,... \r\n            Nếu chưa là Thành Viên, mời Bạn Đăng ký Thành viên  \r\n                tại đây Bạn Chưa Đăng Nhập Thành Viên! \r\n                Vì chưa Đăng Nhập nên Bạn chỉ xem được  Thuộc tính  của văn bản.\r\n                 \r\n                Bạn chưa xem được  Hiệu lực của Văn bản ,  \r\n                    Văn bản liên quan ,  Văn bản thay thế ,  Văn bản gốc ,  Văn\r\n                            bản tiếng Anh ,... \r\n            Nếu chưa là Thành Viên, mời Bạn Đăng ký Thành viên  \r\n                tại đây Bạn Chưa Đăng Nhập Thành Viên! \r\n                Vì chưa Đăng Nhập nên Bạn chỉ xem được  Thuộc tính  của văn bản.\r\n                 \r\n                Bạn chưa xem được  Hiệu lực của Văn bản ,  \r\n                    Văn bản liên quan ,  Văn bản thay thế ,  Văn bản gốc ,  Văn\r\n                            bản tiếng Anh ,... \r\n            Nếu chưa là Thành Viên, mời Bạn Đăng ký Thành viên  \r\n                tại đây Bạn Chưa Đăng Nhập Thành Viên! \r\n                Vì chưa Đăng Nhập nên Bạn chỉ xem được  Thuộc tính  của văn bản.\r\n                 \r\n                Bạn chưa xem được  Hiệu lực của Văn bản ,  \r\n                    Văn bản liên quan ,  Văn bản thay thế ,  Văn bản gốc ,  Văn\r\n                            bản tiếng Anh ,... \r\n            Nếu chưa là Thành Viên, mời Bạn Đăng ký Thành viên  \r\n                tại đây \r\n                                Thông tư liên tịch 13/2016/TTLT-BYT-BGDĐT quy định về công tác y tế trường học do Bộ Y tế, Bộ Giáo dục và Đào tạo ban hành Tải Văn bản tiếng Việt Tải Văn bản tiếng Anh Tải Văn bản gốc Bạn Chưa Đăng Nhập Thành Viên! \r\n                Vì chưa Đăng Nhập nên Bạn chỉ xem được  Thuộc tính  của văn bản.\r\n                 \r\n                Bạn chưa xem được  Hiệu lực của Văn bản ,  \r\n                    Văn bản liên quan ,  Văn bản thay thế ,  Văn bản gốc ,  Văn\r\n                            bản tiếng Anh ,... \r\n            Nếu chưa là Thành Viên, mời Bạn Đăng ký Thành viên  \r\n                tại đây Bạn Chưa Đăng Nhập Thành Viên! \r\n                Vì chưa Đăng Nhập nên Bạn chỉ xem được  Thuộc tính  của văn bản.\r\n                 \r\n                Bạn chưa xem được  Hiệu lực của Văn bản ,  \r\n                    Văn bản liên quan ,  Văn bản thay thế ,  Văn bản gốc ,  Văn\r\n                            bản tiếng Anh ,... \r\n            Nếu chưa là Thành Viên, mời Bạn Đăng ký Thành viên  \r\n                tại đây Bạn Đang Đăng Nhập Thành Viên Free! \r\n                Vì Đăng Nhập Thành Viên  Free  nên Bạn chỉ xem được\r\n                Thuộc tính và Nội dung của văn bản.\r\n                 \r\n                Bạn chưa xem được  Nội dung toàn văn ,  Hiệu lực của Văn bản ,  Văn bản liên quan ,  Văn bản thay thế , \r\n                            Văn bản gốc ,  Văn bản tiếng Anh ,... \r\n            Nếu muốn làm Thành Viên  Basic / Pro , mời Bạn Chuyển\r\n            Đổi loại Thành Viên  \r\n                tại đây. Bạn Đang Đăng Nhập Thành Viên  Basic ! \r\n                Vì Đăng Nhập Thành Viên  Basic  nên Bạn chỉ xem\r\n                được Thuộc tính, Nội dung của văn bản,  Nội dung toàn\r\n                    văn ,  Hiệu lực của Văn bản ,  Văn bản\r\n                        liên quan ,  Văn bản thay thế , \r\n                            Văn bản gốc .\r\n                 \r\n                Bạn chưa xem được  Văn bản tiếng Anh , ... \r\n            Nếu muốn làm Thành Viên  Pro , mời Bạn Chuyển Đổi\r\n            loại Thành Viên  \r\n                tại đây. \r\n        Thông tư 13/2016/TTLT-BYT-BGDĐT hướng dẫn thực hiện công tác y tế ...\r\n     \r\n        Chọn văn bản so sánh thay thế:\r\n        \r\n     CÁC NỘI DUNG SỬA ĐỔI,\r\n            HƯỚNG DẪN NỘI DUNG Văn bản bị thay thế Văn bản thay thế     Chú thích:  \r\n        Rà chuột vào nội dụng văn bản để sử dụng. <Nội dung>  = Nội dung thay\r\n        thế tương ứng; <Nội dung>  =\r\n        Không có nội dung thay thế tương ứng; <Nội dung>  = Không có\r\n        nội dung bị thay thế tương ứng; <Nội dung>  = Nội dung được sửa đổi, bổ\r\n        sung. \r\n          Click trái  để xem cụ thể từng nội dung cần so sánh\r\n        và cố định bảng so sánh. Click phải  để xem những nội dung sửa đổi, bổ sung. Double click  để xem tất cả nội dung không có thay\r\n        thế tương ứng. Tắt so sánh [X]  để\r\n        trở về trạng thái rà chuột ban đầu. Tạo thư mục Địa chỉ: \r\n                        17 Nguyễn Gia Thiều, P.6, Q.3, TP.HCM\r\n                     Điện thoại: \r\n                        (028) 3930 3279 (06 lines) _  Fax:  (028) 3930 3009\r\n                     E-mail: i n f o@ThuVienPhapLuat.vn\r\n                     Trang chủ Các Gói Dịch Vụ Online Phần mềm  THƯ VIỆN PHÁP LUẬT Hướng Dẫn Sử Dụng Giới Thiệu Liên Hệ Lưu trữ Đăng ký Thành viên  Thỏa Ước Dịch Vụ Tra cứu pháp luật Tra cứu\r\n                    Công văn Tìm kiếm luật sư Tra cứu Tiêu Chuẩn Việt Nam Cộng đồng ngành luật  Biểu thuế WTO Bảng giá\r\n                    đất Xin chân thành cảm ơn Thành viên    đã sử dụng www.ThuVienPhapLuat.vn\r\n     \r\n                        Họ & Tên: \r\n                        Email: \r\n                        Điện thoại: \r\n                        Nội dung: \r\n            Bạn hãy nhập mật khẩu đang sử dụng và nhập mật khẩu mới 2 lần để chắc rằng bạn nhập\r\n            đúng.\r\n         \r\n            Tên truy cập: \r\n            Mật khẩu cũ: \r\n            Mật khẩu mới: \r\n        Nhập lại:\r\n         \r\n            Bạn hãy nhập e-mail đã sử dụng để đăng ký thành viên.\r\n         \r\n            E-mail: \r\n            Email người nhận: \r\n            Tiêu đề Email: \r\n            Nội dung: \r\n                        Họ & Tên: \r\n                        Email: \r\n                        Điện thoại: \r\n                        Nội dung: \r\n          Thông báo cho tôi khi Văn bản   bị sửa đổi, bổ sung, có hoặc hết hiệu lực.\r\n         \r\n            Email nhận thông báo: \r\n          Ghi chú cho Văn bản  .\r\n        "
        },
        {
          "title": "Translate",
          "relevance": "0",
          "url": "http://www.biodivn.com/2014/06/phuong-phap-dieu-tra-giam-sat-da-dang-sinh-hoc.html",
          "content": "BIODIVN  Đa Dạng Sinh học và Bảo Tồn Việt Nam Trang chủ Tin tức, Sự kiện Tin tức Sự kiện Đa dạng Hệ Thực vật Hệ Động vật Tài nguyên Thực vật Bảo tồn Khái niệm chung Đa dạng Sinh học Con người và Tự nhiên Suy thoái loài Quản lý và Bảo tồn Nghiên cứu và Giáo dục Vấn đề Việt Nam Danh Lục đỏ Việt Nam Thực vật Động vật Các loài Bị đe doạ Tin tức Bảo tồn Loài mới Hệ Thực vật Hệ Động vật Hệ Nấm Tài liệu Thực vật Động vật Quản lý - Bảo tồn Kinh tế - Môi trường Trẻ em - Học Sinh Ảnh Thực vật Góc Thiếu Nhi Quỹ, Học Bổng Các quỹ, học bổng Lý lịch Khoa học-CV Giới thiệu Biodivn trên Facebook Nhận bài mới qua Email Contact Us \nEmail: biodivn@gmail.com\n (biodivn.blogspot.com) a. Nguyên tắc chọn loài và sinh cảnh điều tra giám sát ĐDSH  Việc xây dựng rừng đặc dụng trên thế giới ở Việt Nam nhằm mục tiêu chính là bảo  vệ tài nguyên và môi trường. Tuy nhiên, ngoài mục tiêu tổng quát, phần lớn các VQG  và KBTTN không chỉ có một số loài, mà có hàng trăm (thậm chí hàng nghìn) loài động thực vật khác nhau và hoạt động điều tra giám sát ĐDSH  không thể thực hiện đối với hàng trăm, hàng nghìn loài sống trong các sinh cảnh, kiểu rừng khác nhau. Vì vậy,  chọn loài và sinh cảnh để thực hiện chương trình điều tra và giám sát ĐDSH là việc cần phải làm trước. Không thể đưa ra cụ thể các loài hay các sinh cảnh hoặc HST cần điều gia  giám sát của tất cả  hệ thông rừng đặc dụng Việt Nam vì tính đặc thù của các khu rừng đặc dụng rất khác nhau. Đối với một số  VQG  hay KBTTN, có thể loài  và sinh cảnh được chọn để điều tra giám sát cũng chính là những mục đích mà luận chứng kinh tế kỹ thuật (LCKTKT)  hoặc dự án đào tạo xây dựng (DAĐTXD) VQG hoặc KBTTN đó nhằm đạt tới.  Các VQG và KBTTN được xây dựng ngoài mục tiêu chung là bảo tồn tài nguyên thiên nhiên và môi trường,  mục đích chủ yếu của nó là để bảo vệ các HST cần thiết cho rất nhiều loài thực và động vật tiêu biểu của Việt Nam thì nội dung cơ bản của hoạt đồng điều tra giám sát sẽ là:  -           Xác định sinh cảnh chính đã tạo nên toàn bộ HST rừng ở đó;  -           Xác định các loài chỉ thị (hoặc loài đặc trưng) đại diện cho mỗi dạng sinh cảnh;  -           Theo dõi sự biến động của loài chỉ thị đó và những nguyên nhân chính (mối đe doạ) đã gây nên sự biến động;  -           Tìm ra các giải pháp để giảm thiểu mối đe doạ nói trên.  Các VQG và KBTTN được xây dựng, ngoài mục tiêu chung là bảo tồn thiên nhiên và môi trường, mục tiêu cụ thể của việc xây dựng nhằm bảo vệ một loài động vật và thực vật quan trọng đang bị đe doạ tuyệt chủng thì nhiệm vụ và nội dung điều tra giám sát ĐDSH sẽ là:  -           Xác định phân bố, hiện trạng quần thể loài đó;  -           Xác định mối quan hệ giữa quần thể loài đó với sinh cảnh sống và những yếu tố ảnh hưởng đến sự tồn tại và phát triển của quần thể loài đó;  -           Giám sát các xu hướng biến đổi kích thước quần thể loài đó;  -           Tìm các biện pháp thích hợp để ngăn ngừa hoặc giảm thiểu các mối đe doạ đó.  Như vậy vấn đề mà các VQG và KBTTN cần quan tâm là xác định các dạng sinh cảnh và các loài chính để đưa vào chương trình giám sát. Các bước cơ bản được tiến hành như sau:  -           Xây dựng bản đồ sinh cảnh chính. Cac dạng sinh cảnh hay kiểu rừng chính được xác định dựa trên các cơ sở:  +       Các thông tin có trong bản LCKTKT hoặc DAĐTXD VQG hay KBTTN;  +      Đã được khoanh vẽ trên bản đồ hiện trạng tài nguyên rừng của VQG hoặc KBTTN.  -           Chuyển tải các thông tin được ghi trong LCKTKT hoặc DAĐTXD VQG hay KBTTN hoặc từ các đợt kiểm tra gần nhất vào bản đồ phân loại sinh cảnh (cả các thông tin về phân bố động thực vật của VQG hoặc KBTTN trước và hiện nay). Thông tin chuyển tải vào bản đồ phải dựa trên chương trình điều tra giám sát ĐDSH vạch ra.  -           Chọn loài giám sát được lựa chọn trên các nguyên tắc:  +     Dễ quan sát hoặc bẫy bắt (động  vật kiếm ăn ngày,  loài thực vật chỉ thị hoặc được người dân chú ý khai thác);  +     Không phải là loài hiếm và cũng không phải là loài quá phong phú, nếu sự phong phú của loài đó là nhờ có mặt của con người.  Loài hiếm nên có chương trình điều tra giám sát riêng;  +     Chọn các loài ăn chuyên hoặc một số loài thức ăn đặc biệt hay nhóm loài có chung các nhu cầu sinh thái (thức ăn, nơi ở).  b. Thiết kế  chương trình điều tra giám sát ĐDSH  Một nguyên tắc quan trọng đối với việc thực hiện một chương trình điều tra và giám sát ĐDSH là phải tuyệt đối tuân thủ khi lặp lại. Điều này có ý nghĩa rằng việc tiến hành chương trình điều tra giám sát ĐDSH trong các lần tiếp theo phải đúng qui trình (thời gian, nhân lực, phương pháp)  như đã thực hiện ở lần đầu. Chỉ khi tuân thủ các nguyên tắc đó thì các số liệu của các đợt điều tra giám sát mới hữu ích và có thể so sánh với nhau trên mọi phương diện  Để thiết lập một kế hoạch điều tra giám sát ĐDSH cần:  -           Hiểu biết chi tiết về các loài và các sinh cảnh trong khu trên các phương diện: phân bố, yếu tố đe doạ, mức độ đe doạ và diễn biến tình trạng loài qua các năm;  -           Xác lập nội dung, phương pháp thực hiện; quyết định loài, sinh cảnh hoặc mối đe doạ nào cần được chú ý;  -           Xác định nguồn lực thực hiện ( con người, thời gian và tài chính).  Yêu cầu quan trọng nhất để thực hiện tốt một chương trình điều tra giám sát ĐDSH là lập tuyến điều tra giám sát. Tuyến điều tra giám sát được lập cần thoả mãn một số điều kiện sau:  -           Đi qua được nhiều sinh cảnh đại diện đã lựa chọn;  -           Các tuyến không cắt nhau và khoảng cách  giữa chúng là tốt nhất ( >100m);  -           Khả năng ghi nhận loài giám sát là cao nhất;  -           Dễ nhận biết nhất ( được đánh dấu, dấu không bị mất do mưa gió).  Kỹ thuật giám sát trên tuyến là yêu cầu quan trọng thứ 2, cụ thể:  -           Nhóm cán bộ điều tra giám sát được cần tổ chức ổn định ( không  hoặc rất hạn chế thay đổi nhân sự) và có được những kiến thức và kỹ năng cơ bản sau:  +     Nhận biết loài nhanh và chính xác,  đặc biệt là các loài được lựa chọn để giảm sát tại địa bàn;  +       Có những hiểu biết cơ bản về đặc điểm sinh học, sinh thái loài giám sát;  +       Có kỹ năng khảo sát và  ghi chép thông tin.  Để có thể chia sẻ thông tin và so sánh số liệu  khi cần thiết, việc thống nhất mẫu biểu ghi chép các kết quả điều tra giám sát ĐDSH trên tuyến là cực kỳ quan trọng  c. Thời gian lặp lại của chương trình điều tra giám sát ĐDSH  Thời gian lặp lại của chương trình điều tra giám sát ĐDSH hay là giãn cách giữa 2 lần giám sát phụ thuộc các điều kiện cơ bản sau:  -           Nguồn nhân lực;  -           ái lực của các yếu tố đe doạ ( bởi con người);  -           Đặc điểm sinh học của  nhóm loài giám sát trong quan hệ với khí hậu khu vực;  -           Nguồn tài chính hỗ trợ.  Thường các chương trình điều tra giám sát ĐDSH được thiết kế theo chu kỳ lặp lại 3, 6 hoặc 12 tháng. Những VQG và KBTTN có đủ nhân lực và  sự tác động của con người luôn xảy ra thì nên xây dựng chương trình điều tra giám sát ĐDSH có thời gian lặp lại là 3 tháng theo 4 mùa của năm. Chương trình điều tra giám sát ĐDSH lặp lại sau 3 tháng giúp các nhà quản lý cập nhật thông tin và triển khai  và triển khai các giải pháp bảo tồn ĐDSH  kịp thời nhất. Đối với VQG và KBTTN chưa đủ nguồn lực, sức ép của con người không lớn hoặc thời tiết   chia 2 mùa, có thể xây dựng chương trình điều tra giám sát có thời gian lặp lại là 6 tháng. Tuy nhiên, thời gian giám sát lặp lại càng lâu  thì tính cập nhật thông tin thấp và  việc  xác định  chính xác các nguyên nhân hay yếu tố gây biến động ĐDSH thiếu chính xác và khó tìm được giải pháp  bảo tồn thích hợp  d. Sự tham gia của người dân trong điều tra giám sát ĐDSH  Sự tham gia  của  người dân được coi là yếu tố quan trọng đối với sự thành công của hoạt động bảo tồn ĐDSH.  Bài học kinh nghiệm  cho thấy ở VQG và KBTTN nào, sự tham gia của người dân là tự nguyện và tích cực thì công tác bảo tồn ĐDSH ở nơi đó sẽ có kế quả. Ngược lại, nơi chưa có sự hợp tác quản lý của người dân với ban quản lý VQG và KBTTN thì sức ép của con  người lên tài nguyên luôn là vấn đề đặt ra. Kết quả  phỏng vấn người dân địa phương ở 7 VQG và người dân vào công tác quảnl ý rừng ặc dụng là chưa có hoặc có thì chỉ là một sôits bảo tồn viên thôn  bản.  Sự tham gia của người dân trong điều tra giám sát ĐDSH không giống như hoạt động bảo tồn mà chủ yếu là phát huy những kinh nghiệm săn bắt trước đây và kinh nghiêm đi rừng của họ. Những kinh nghiệm đó giúp chương trình điều tra giám sát ĐDSH  thuận lợi và có thể cho kết quả tốt. Mặt khác, các hoạt động điều tra giám sát ĐDSH cũng góp phần giúp họ nâng cao nhận thức về bảo tồn ĐDSH. (biodivn.blogspot.com). Tài liệu tham khảo: Đa dạng Sinh học và Bảo tồn, Bộ Tài nguyên và Môi trường, 2005.  Xem thêm:  Đa dạng Sinh học và Bảo tồn Tình hình nghiên cứu đa dạng sinh học ở Việt Nam Email This BlogThis! Share to Twitter Share to Facebook Chúng ta biết chính xác loài Tê giác java ở Việt nam tuyệt chủng như thế nào! \n   Vào tháng 4 năm 2010. Xác cá thể Tê giác cuối cùng của Việt Nam đã được tìm thấy tại Vườn Quốc Gia Cát Tiên. Đây là câu chuyện của người ...\n Popular Posts Nguyên nhân gây suy thoái Đa dạng Sinh học Việt Nam Suy thoái Đa dạng Sinh học ở Việt Nam Nguyên nhân và Hậu quả suy thoái Đa dạng sinh học Sức ép gia tăng dân số đến tài nguyên và môi trường Các Vườn Quốc Gia và Khu Bảo tồn TN ở Việt Nam Cấu trúc và chức năng Hệ Sinh thái Giới sinh vật là gì, phân chia sinh giới thế nào? Định nghĩa về đa dạng sinh học Các nguyên nhân gây Suy thoái Đa dạng Sinh học \"Cái chết đang sống\" của Hoàng đàn lạng sơn Cupressus tonkinensis Labels Angiosperm Phylogeny Group APG APG IV Australia Authors Ba Ria - Vung Tau Bac Giang Bac Kan Binh Phuoc Biodiversity Bryophyta BSEN Cambodia Cao Bang Conifers Conservation Sources CV KH Cycads Da Nang Dak Lak Dak Nong Dien Bien Dong Nai Events Fauna Biodiversity Fauna Book Fauna Conservation Fauna New species fish Flora Biodiversity Flora books Flora Conservation Flora New Species Fund Gia Lai Ha Giang Ha Noi Ha Tinh Hai Phong Hoa Binh Hoya insect Jusminum Khanh Hoa Kids Books Kids Corner Kien Giang Kinh te - Moi truong books Kon Tum Lai Chau Lam Dong Lang Son Lao Cai Laos Liliopsida Magnoliopsida Mammalia Marine Mot la mam Mushroom Nam Dinh News Nghe An Nha Trang Ninh Binh Ninh Thuan Ocean Oleaceae Orchidaceae Paleontology Phu Tho Plant resources Quan ly Bao Ton books Quang Binh Quang Nam Quang Ngai Quang Ninh Quang Tri Redlist Reproductive Biology Reptile Son La Southern China Squamata Tay Ninh Te giac Thai Nguyen Thanh Hoa Thua Thien Hue Tp Ho Chi Minh Tuyen Quang Vinh Phuc Yen Bai Mục Biodiversity BSEN Cambodia Conservation Sources Fauna Book insect Laos News Southern China Trang Home Tin Tức Động vật Loài mới Động vật Đa dạng Động vật Bảo tồn Thực vật Loài mới Thực vật Đa dạng Thực vật Bảo tồn Chào mừng đến với Trang BiodiVn Mục Conifers Cycads Flora books Hoya Mushroom Orchidaceae Paleontology Plant resources Redlist"
        },
        {
          "title": "Học máy",
          "relevance": "1",
          "url": "https://vi.wikipedia.org/wiki/H%E1%BB%8Dc_m%C3%A1y",
          "content": "Bách khoa toàn thư mở Wikipedia Bạn có  tin nhắn mới  ( thay đổi gần đây ). \n\t\t\t\t\tBước tới:\t\t\t\t\t menu , \t\t\t\t\t tìm kiếm Học máy , có tài liệu gọi là  Máy học , ( tiếng Anh :  machine learning ) là một lĩnh vực của  trí tuệ nhân tạo  liên quan đến việc nghiên cứu và xây dựng các kĩ thuật cho phép các hệ thống \"học\" tự động từ dữ liệu để giải quyết những vấn đề cụ thể. Ví dụ như các máy có thể \"học\" cách phân loại  thư điện tử  xem có phải  thư rác (spam)  hay không và tự động xếp thư vào thư mục tương ứng. Học máy rất gần với  suy diễn thống kê  (statistical inference) tuy có khác nhau về thuật ngữ. Học máy có liên quan lớn đến  thống kê , vì cả hai lĩnh vực đều nghiên cứu việc phân tích dữ liệu, nhưng khác với thống kê, học máy tập trung vào sự phức tạp của các giải thuật trong việc thực thi tính toán. Nhiều bài toán suy luận được xếp vào loại bài toán  NP-khó , vì thế một phần của học máy là nghiên cứu sự phát triển các giải thuật suy luận xấp xỉ mà có thể xử lý được. Học máy có hiện nay được áp dụng rộng rãi bao gồm  máy truy tìm dữ liệu ,  chẩn đoán y khoa , phát hiện  thẻ tín dụng giả , phân tích  thị trường chứng khoán , phân loại các  chuỗi DNA ,  nhận dạng tiếng nói  và  chữ viết ,  dịch tự động ,  chơi trò chơi  và  cử động rô-bốt  ( robot locomotion ). Mục lục 1 Định nghĩa 2 Biểu diễn 3 Tính phổ quát 4 Tương tác với con người 5 Tương quan với Khai phá dữ liệu 6 Các loại giải thuật 7 Các chủ đề về máy học 8 Xem thêm 9 Tham khảo 10 Liên kết ngoài 10.1 Tài nguyên chung 10.2 Tạp chí và Hội thảo 10.3 Nhóm nghiên cứu 10.4 Phần mềm Định nghĩa [ sửa  |  sửa mã nguồn ] Dưới góc nhìn của  trí tuệ nhân tạo , động lực chính học máy bởi là nhu cầu thu nhận tri thức (knowledge acquisition). Thật vậy, trong nhiều trường hợp ta cần kiến thức chuyên gia là khan hiếm (không đủ chuyên gia ngồi phân loại lừa đảo thẻ tín dụng của tất cả giao dịch hàng ngày) hoặc chậm vì một số nhiệm vụ cần đưa ra quyết định nhanh chóng dựa trên xử lý dữ liệu khổng lồ (trong mua bán chứng khoán phải quyết định trong vài khoảng khắc của giây chẳng hạn) và thiếu ổn định thì buộc phải cần đến máy tính. Ngoài ra, đại đa số dữ liệu sinh ra ngày nay chỉ phù hợp cho máy đọc (computer readable) tiềm tàng ngưồn kiến thức quan trọng. Máy học nghiên cứu cách thức để mô hình hóa bài toán cho phép máy tính tự động hiểu, xử lý và học từ dữ liệu để thực thi nhiệm vụ được giao cũng như cách đánh giá giúp tăng tính hiệu quả. Tom Mitchell , giáo sư nổi tiếng của Đại học Carnegie Mellon University -  CMU  định nghĩa cụ thể và chuẩn mực hơn như sau: \"Một chương trình máy tính CT được xem là học cách thực thi một lớp nhiệm vụ NV thông qua trải nghiệm KN, đối với thang đo năng lực NL nếu như dùng NL ta đo thấy năng lực thực thi của chương trình có tiến bộ sau khi trải qua KN\" (máy đã học). [1] Biểu diễn [ sửa  |  sửa mã nguồn ] Biểu diễn (tiếng Anh: representation) là một trong những vấn đề quan trọng của học máy. Biểu diễn ở đây có thể hiểu làm sao ghi mã (encode) những thông tin của thế giới thật giúp hoàn thành nhiệm vụ một cách hiệu quả và đầy đủ nhất có thể. Thông tin ở đây bao hàm cả thông tin về dữ liệu đầu vào, đầu ra hay các trạng thái của hệ thống; cũng như cách đánh giá hiệu quả của chương trình. Thông thường, trong học máy người ta hay xây dựng các mô hình sử dụng những  biến ngẫu nhiên  cho việc biểu diễn dữ liệu và nội trạng thái của hệ thống.  Ví dụ:  dùng biến ngẫu nhiên để biểu thị cho tính chất của email là spam (tương ứng giá trị 0) hay là bình thường (tương ứng 1). Mối tương quan giữa các biến ngẫu nhiên này có thể sử dụng ví dụ như  mô hình xác suất dạng đồ thị  để miêu tả. Mặt khác, để đo hiệu quả có thể dùng các  hàm thiệt hại  (hay  hàm tiện ích , trong tiếng Anh là  loss function  và  utility function  tương ứng). Tính phổ quát [ sửa  |  sửa mã nguồn ] Một trong những trọng tâm khác của học máy là đạt được tính phổ quát (tiếng Anh: generalization), nói cách khác là tính chất của chương trình có thể làm việc tốt với dữ liệu mà nó chưa gặp bao giờ (tiếng Anh: unseen data). Một chương trình chỉ hiệu quả với dữ liệu đã gặp nhìn chung không có nhiều tính hữu dụng. Lấy ví dụ về xếp thư điện tử tự động như trên, một hệ thống tự động sau khi trải qua quá trình học từ dữ liệu (\"training\") có thể suy diễn một số nguyên tắc riêng (chẳng hạn như xem xét nội dung: nếu thư được viết bằng tiếng Anh mà chứa một số từ như \"porn\", \"sell\", \"good product\" hoặc người gửi đến từ Somalia trong khi người nhận ở Hà Nội không thân quen nhau) để quyết định xem có phải là  thư rác  hay không. Tuy nhiên, nếu như trong dữ liệu bài giảng ( training data ) có ngôn ngữ khác trong thực tế (tiếng Việt thay vì tiếng Anh) hoặc thậm chí không phải dạng thuần văn bản (dạng ảnh khiến cho bóc tách nội dung khó hơn hoặc không thể) thì rất có thể máy sẽ dự báo không chính xác nữa. Một số chương trình có thể tự động cập nhật trong thời gian thực (ví dụ như người sử dụng có chỉ ra rằng thư bị sắp xếp sai danh mục). Tương tác với con người [ sửa  |  sửa mã nguồn ] Một số hệ thống học máy nỗ lực loại bỏ nhu cầu trực giác của con người trong việc phân tích dữ liệu, trong khi các hệ thống khác hướng đến việc tăng sự cộng tác giữa người và máy. Không thể loại bỏ hoàn toàn tác động của con người vì các nhà thiết kế hệ thống phải chỉ định cách biểu diễn của dữ liệu và những cơ chế nào sẽ được dùng để tìm kiếm các đặc tính của dữ liệu. Học máy có thể được xem là một nỗ lực để tự động hóa một số phần của  phương pháp khoa học . Một số nhà nghiên cứu học máy tạo ra các phương pháp bên trong các khuôn khổ của  thống kê Bayes . Tương quan với Khai phá dữ liệu [ sửa  |  sửa mã nguồn ] Khai phá dữ liệu  và học máy là hai khái niệm hay bị nhầm lẫn. Hai lĩnh vực này nhìn chung gần với nhau và đôi khi dùng chung nhiều phương pháp, công cụ nhưng khác biệt chính là ở mục tiêu: Khai phá dữ liệu: thường mục tiêu là tìm kiếm những thông tin, tri thức hoàn toàn mới tiềm năng có ích trong nguồn dữ liệu. Học máy: dự đoán một số thông tin của dữ liệu dựa trên những đặc tính đã biết. Các loại giải thuật [ sửa  |  sửa mã nguồn ] Các  thuật toán  học máy được phân loại theo kết quả mong muốn của thuật toán. Các loại thuật toán thường dùng bao gồm: Học có giám sát —trong đó, thuật toán tạo ra một hàm ánh xạ dữ liệu vào tới kết quả mong muốn. Một phát biểu chuẩn về một việc học có giám sát là bài toán  phân loại : chương trình cần học (cách xấp xỉ biểu hiện của) một hàm ánh xạ một vector  [ X 1 , X 2 , … X N ] {\\displaystyle [X_{1},X_{2},\\ldots X_{N}]}  tới một vài lớp bằng cách xem xét một số mẫu dữ liệu - kết quả của hàm đó. Học không giám sát —mô hình hóa một tập dữ liệu, không có sẵn các ví dụ đã được gắn nhãn. Học nửa giám sát —kết hợp các ví dụ có gắn nhãn và không gắn nhãn để sinh một hàm hoặc một bộ phân loại thích hợp. Học tăng cường —trong đó, thuật toán học một chính sách hành động tùy theo các quan sát về thế giới. Mỗi hành động đều có tác động tới môi trường, và môi trường cung cấp thông tin phản hồi để hướng dẫn cho thuật toán của quá trình học. Chuyển đổi —tương tự học có giám sát nhưng không xây dựng hàm một cách rõ ràng. Thay vì thế, cố gắng đoán kết quả mới dựa vào các dữ liệu huấn luyện, kết quả huấn luyện, và dữ liệu thử nghiệm có sẵn trong quá trình huấn luyện. Học cách học —trong đó thuật toán học  thiên kiến quy nạp  của chính mình, dựa theo các kinh nghiệm đã gặp. Phân tích hiệu quả các thuật toán học máy là một nhánh của ngành  thống kê , được biết với tên  lý thuyết học điện toán . Các chủ đề về máy học [ sửa  |  sửa mã nguồn ] Danh sách các chủ đề của môn học này: Mô hình hóa  các hàm mật độ xác suất điều kiện :  hồi quy  và  phân loại Mạng nơ-ron Máy học cực độ (Extreme learning machine) Cây quyết định Lập trình biểu thức gen Lập trình di truyền Hồi quy quá trình Gauss Phân tích biệt thức tuyến tính k láng giềng gần nhất Độ dài thông điệp tối thiểu Cảm tri nguyên Hàm cơ sở xuyên tâm Máy vector hỗ trợ (Support Vector Machine) Mô hình hóa các  hàm mật độ xác suất  qua các  mô hình phát sinh :\n Thuật toán cực đại kì vọng Các  mô hình đồ họa  gồm  mạng Bayes  và  mạng Markov Ánh xạ topo phát sinh Các kỹ thuật suy luận xấp xỉ đúng:\n Chuỗi Markov phương pháp Monte Carlo Phương pháp biến thiên Tối ưu hóa : hầu hết các phương pháp trên đều sử dụng tối ưu hóa hoặc là các thể hiện của các thuật toán tối ưu hóa. Xem thêm [ sửa  |  sửa mã nguồn ] Trí tuệ nhân tạo Trí tuệ điện toán Khai phá dữ liệu Nhận dạng mẫu Các ẩn bản quan trọng trong học máy (khoa học máy tính) Các ấn bản quan trọng trong học máy (thống kê) Rô-bốt tự hành Lập trình suy diễn lôgic Tham khảo [ sửa  |  sửa mã nguồn ] Bishop C. M. (1995).  Neural Networks for Pattern Recognition , Nhà in Đại học Oxford.  ISBN 0-19-853864-2 Richard O. Duda, Peter E. Hart, David G. Stork (2001)  Pattern classification  (ấn bản lần 2), Wiley, New York,  ISBN 0-471-05669-3 . MacKay D. J. C. (2003).  Information Theory, Inference, and Learning Algorithms , Nhà in Đại học Cambridge.  ISBN 0-521-64298-1 Sholom Weiss và Casimir Kulikowski (1991).  Computer Systems That Learn , Morgan Kaufmann.  ISBN 1-55860-065-5 ^ * Mitchell, T. (1997).  Machine Learning , McGraw Hill.  ISBN 0-07-042807-7 , p.2. Liên kết ngoài [ sửa  |  sửa mã nguồn ] Tài nguyên chung [ sửa  |  sửa mã nguồn ] UCI description MLnet Mailing List Kmining List of machine learning, data mining and KDD scientific conferences Book \" Intelligent Systems and their Societies \" của  Walter Fritz Links from Open Directory Project Eruditionhome  - nơi chứa nhiều mục đề về Học máy MLpedia  - Từ điển bách khoa wiki dành riêng cho chủ đề Học máy Tạp chí và Hội thảo [ sửa  |  sửa mã nguồn ] Journal of Machine Learning Research Machine Learning Journal Machine Learning papers  tại CiteSeer NIPS: Neural Information Processing Systems ICML: International Conference on Machine Learning Nhóm nghiên cứu [ sửa  |  sửa mã nguồn ] Machine Learning  tại Đại học Hebrew Machine Learning and Natural Language Processing  tại Đại học Freiburg Machine Learning and Data Mining in Bioinformatics Group  tại TU München Machine Learning and Biological Computation Group  tại Đại học Bristol Machine Learning and Applied Statistics  của Microsoft Research Department of Knowledge Technologies  của Học viện Jozef Stefan Statistical Multimedia Learning Group  tại Đại học British Columbia Machine Learning Systems Group  tại Jet Propulsion Laboratory, Học viện Kỹ thuật California Department of Empirical Inference  tại Viện Max Planck về điều khiển học sinh học, Tübingen Machine Learning Group  tại Đại học Toronto Intelligent Data Analysis Group  tại Fraunhofer FIRST, Berlin Machine Learning Group  tại Đại học Tự do Bruxelles Phần mềm [ sửa  |  sửa mã nguồn ] Chương trình mạng nơ ron đa lớp (Multi Layer Neural Network) và mạng nơ ron tự tổ chức (Self Organizing Maps) có giải thích bằng tiếng Việt. Sử dụng phần mềm mạng nơ ron 3 lớp Spice-MLP Sử dụng phần mềm mạng tự tổ chức Spice-SOM Hướng dẫn sử dụng mạng nơ ron trong các ứng dụng thực tế  trong đó có minh họa phân loại ảnh khuôn mặt, ảnh người đi bộ, ảnh xe hơi, dự báo chứng khoán và một số ví dụ khác SPIDER  - một hộp công cụ học máy hoàn chỉnh cho Matlab PRTools  PRTools là một gói phần mềm hoàn chỉnh khác tương tự SPIDER và được cài trong Matlab. SPIDER có vẻ có nhiều hỗ trợ mức thấp, nhưng các công cụ của PRTools có phần đa dạng hơn. PRTools có sách và tài liệu tốt. Cả SPIDER và PRTools được cung cấp miễn phí trên mạng cho các ứng dụng phi thương mại. Orange , bộ chương trình học máy với các script viết bằng Python và giao diện lập trình đồ họa YALE  là một công cụ mạnh miễn phí cho Học máy và  Khai phá dữ liệu Weka Machine Learning Software Matlab  MATLAB có hỗ trợ hộp công cụ cho nhiều công cụ học máy. Hiện giờ hộ công cụ Tin sinh học đã có Support Vector Machines và các bộ phân loại KNN (k láng giềng gần nhất). Hộp công cụ thống kê thực hiện biệt thức tuyến tính và phân loại bằng cây quyết định. Hộp công cụ mạng nơ-ron là một bộ công cụ hoàn chỉnh để cài đặt mạng nơron. Trong thời gian gần đây, các phương pháp mới để đánh giá hiệu quả của các bộ phân loại và để thẩm định chéo đã làm Matlab trở nên hấp dẫn hơn đối với học máy. MLC++  là thư viện lớp C++ dành cho học có giám sát MDR  là một gói phần mềm nguồn mở dành cho việc phát hiện các tương tác thuộc tính bằng phương pháp  rút gọn thứ nguyên đa thừa số  (MDR). x t s Những lĩnh vực chính của  khoa học máy tính Các nền tảng toán học Logic toán học   · Lý thuyết tập hợp   · Lý thuyết số   · Lý thuyết đồ thị   · Lý thuyết kiểu   · Lý thuyết thể loại   · Giải tích số   · Lý thuyết thông tin   · Đại số   · Nhận dạng mẫu   · Nhận dạng tiếng nói   · Toán học tổ hợp   · Đại số Boole   · Toán rời rạc Lý thuyết phép tính Độ phức tạp Kolmogorov   · Lý thuyết Automat   · Lý thuyết tính được   · Lý thuyết độ phức tạp tính toán   · Lý thuyết điện toán lượng tử Các cấu trúc dữ liệu \nvà  các giải thuật Phân tích giải thuật   · Thiết kế giải thuật   · Hình học tính toán   · Tối ưu hóa tổ hợp Các ngôn ngữ lập trình \nvà  Các trình biên dịch Các bộ phân tích cú pháp   · Các trình thông dịch   · Lập trình cấu trúc   · Lập trình thủ tục   · Lập trình hướng đối tượng   · Lập trình hướng khía cạnh   · Lập trình hàm   · Lập trình logic   · Lập trình máy tính   · Lập trình mệnh lệnh   · Lập trình song song   · Lập trình tương tranh   · Các mô hình lập trình   · Prolog   · Tối ưu hóa trình biên dịch Tính song hành , Song song , \nvà các hệ thống  phân tán Đa xử lý   · Điện toán lưới   · Kiểm soát song hành   · Hiệu năng hệ thống   · Tính toán phân tán Công nghệ phần mềm Phân tích yêu cầu   · Thiết kế phần mềm   · Các phương pháp hình thức   · Kiểm thử phần mềm   · Quy trình phát triển phần mềm   · Các phép đo phần mềm   · Đặc tả chương trình   · LISP   · Mẫu thiết kế   · Tối ưu hóa phần mềm Kiến trúc hệ thống Kiến trúc máy tính   · Tổ chức máy tính   · Các hệ điều hành   · Các cấu trúc điều khiển   · Cấu trúc bộ nhớ lưu trữ   · Vi mạch   · Thiết kế ASIC   · Vi lập trình   · Vào/ra dữ liệu   · VLSI design   · Xử lý tín hiệu số Viễn thông \nvà  Mạng máy tính Audio máy tính   · Chọn tuyến   · Cấu trúc liên kết mạng   · Mật mã học Các cơ sở dữ liệu \nvà  Các hệ thống thông tin Hệ quản trị cơ sở dữ liệu   · Cơ sở dữ liệu quan hệ   · SQL   · Các giao dịch   · Các chỉ số cơ sở dữ liệu   · Khai phá dữ liệu   · Biểu diễn và giao diện thông tin   · Các hệ thống thông tin   · Khôi phục dữ liệu   · Lưu trữ thông tin   · Lý thuyết thông tin   · Mã hóa dữ liệu   · Nén dữ liệu   · Thu thập thông tin Trí tuệ nhân tạo Lập luận tự động   · Ngôn ngữ học tính toán   · Thị giác máy tính   · Tính toán tiến hóa   · Các hệ chuyên gia    · Học máy   · Xử lý ngôn ngữ tự nhiên   · Robot học Đồ họa máy tính Trực quan hóa   · Hoạt họa máy tính   · Xử lý ảnh Giao diện người-máy tính Khả năng truy cập máy tính   · Giao diện người dùng   · Điện toán mang được   · Điện toán khắp mọi nơi   · Thực tế ảo Khoa học tính toán Cuộc sống nhân tạo   · Tin sinh học   · Khoa học nhận thức   · Hóa học tính toán   · Khoa học thần kinh tính toán   · Vật Lý học tính toán   · Các giải thuật số   · Toán học kí hiệu Chú ý: khoa học máy tính còn có thể được chia thành nhiều chủ đề hay nhiều lĩnh vực khác dựa theo  Hệ thống xếp loại điện toán ACM . x t s Những Chuyên ngành chính của  Tin học  •   Phần cứng  •   Phần mềm Công nghệ thông tin Cuộc sống nhân tạo Đa xử lý Điện toán lưới Đồ họa máy tính Hệ chuyên gia Hệ thống thông tin quản lý Hoạt họa máy tính Khoa học nhận thức Khoa học tính toán Khoa học thần kinh tính toán Khoa học thông tin Kiểm soát song hành Kiến trúc hệ thống Lập luận tự động Ngôn ngữ hình thức Ngôn ngữ học tính toán Người máy Robot học Thực tế ảo Tính toán song song Tối ưu hóa trình biên dịch Tổ chức máy tính Trí tuệ nhân tạo Từ điển học Tương tranh Vật lý học tính toán Hệ thống thông tin An toàn thông tin Cơ sở dữ liệu đa phương tiện Cơ sở dữ liệu thông minh Dữ liệu lớn Hệ cơ sở tri thức Hệ dựa trên logic Hệ gợi ý Hệ thích nghi dựa trên ngữ cảnh Hệ thống hướng tác tử Hệ thống thông minh Hệ thống thông tin địa lý Hệ trợ giúp quyết định Kỹ nghệ dữ liệu Kỹ nghệ tri thức Logic mờ Phân tích dữ liệu Phân tích và thiết kế hệ thống Quản trị dự án Quản trị tri thức Thiết kế và quản trị dữ liệu Tích hợp dữ liệu Tính toán hiệu năng cao Web ngữ nghĩa Xử lý thông tin mờ Khoa học máy tính Cơ sở dữ liệu phân tán Hệ quản trị cơ sở dữ liệu Hệ thống đa lõi Hệ thống truyền thông Hình học tính toán Hóa học tính toán Học máy Khai phá dữ liệu Lập trình song song Lý thuyết mã hóa Lý thuyết tính toán Ngôn ngữ và phương pháp dịch Nguyên lý ngôn ngữ lập trình Quy hoạch ràng buộc Sinh học tính toán  ( Tin sinh học ) Thiết kế và phân tích thuật toán Tìm kiếm thông tin Tính toán khoa học Tính toán kí hiệu Tính toán phân tán Tính toán tiến hóa Tính toán tự nhiên Tối ưu hoá tổ hợp Xử lý song song Kỹ thuật máy tính Đa phương tiện Định vị vệ tinh  ( GNSS ) Giao diện người dùng Ghép nối máy tính Hệ nhúng Hệ thống thời gian thực Hiệu năng hệ thống Kiến trúc máy tính Lập trình đôi Lập trình đồ họa Lập trình hệ thống Lý thuyết nhận dạng Mạng nơ-ron Nhận dạng tiếng nói Phân tích tín hiệu Thị giác máy tính Thiết kế IC Thoại IP Tổng hợp giọng nói Tương tác người–máy tính Vi xử lý Xử lý ảnh Xử lý dữ liệu đa phương tiện Xử lý ngôn ngữ tự nhiên Xử lý tiếng nói Xử lý tín hiệu số Kỹ nghệ phần mềm Bảo trì phần mềm Các phương pháp hình thức Chất lượng phần mềm Đảm bảo chất lượng phần mềm Đánh giá phần mềm Đo lường và quản trị phần mềm Độ tin cậy và chịu lỗi phần mềm Kiểm thử phần mềm Kiến trúc doanh nghiệp Kiến trúc phần mềm Kinh tế công nghệ phần mềm Kỹ nghệ hướng dịch vụ Lập trình linh hoạt Mẫu thiết kế Mô hình hóa phần mềm Phân tích hệ thống Phân tích thiết kế hướng đối tượng  ( UML ) Phân tích yêu cầu phần mềm Phát triển phần mềm Quản lý cấu hình phần mềm Quản lý dự án phần mềm Quản lý kỹ thuật phần mềm Quy trình phát triển phần mềm  ( Vòng đời phát hành phần mềm ) Thiết kế phần mềm Triển khai phần mềm Tối ưu hóa phần mềm Mạng máy tính An ninh mạng An ninh trong giao dịch điện tử Đánh giá hiệu năng mạng  ( QoS ) Điện toán đám mây Định tuyến Hệ phân tán Kỹ thuật truyền thông Lý thuyết thông tin Mạng không dây Mạng thế hệ mới Mạng thiết bị di động Mạng thông tin quang Mật mã học Mô phỏng mạng Nhận dạng Quản trị mạng Thiết bị truyền thông và mạng Thiết kế mạng Tính toán khắp nơi và di động Trung tâm dữ liệu Truyền thông di động Truyền thông đa phương tiện Truyền thông số Vệ tinh thông tin Viễn thông  ( Mạng viễn thông ) Ước lượng tín hiệu và hệ thống Web thế hệ mới Tin học kinh tế x t s Giám đốc công nghệ thông tin  ·  Tin học kinh tế  ·  Quản lý công nghệ thông tin Quản lý ITIL  &  ITSM Định hướng phát triển Phát triển nhân lực Quản lý bảo mật Quản lý chất lượng Quản lý công nghệ Quản lý dự án Quản lý mua sắm Quản lý ngân sách Quản lý nguồn lực Quản lý phát hành Quản lý rủi ro Quản lý tài sản Quản lý thay đổi Quản lý tích hợp Quản lý tổ chức Quản lý truyền thông Quản lý tuân thủ Quản lý vấn đề Thiết kế giải pháp Xây dựng chiến lược Xây dựng chính sách Quản lý mạng Ảo hóa Mạng campus Mạng diện rộng Mạng nội bộ Mạng riêng ảo STP VLAN IVR VTP Quản trị hệ thống Hoạt động vận hành Bảo trì thiết bị Bảo vệ hệ thống Đối phó sự cố Kế hoạch dự phòng Hoạt động kỹ thuật Hỗ trợ kỹ thuật Kiểm soát truy cập Kiểm tra hệ thống Xác thực người dùng Hoạt động an toàn An ninh nhân sự An ninh hệ thống Nhận thức an toàn Rủi ro hệ thống Quản lý hệ thống Bàn dịch vụ Quản lý cấu hình Quản lý công suất Quản lý dịch vụ Quản lý hạ tầng Quản lý khôi phục Quản lý người dùng Quản lý sự cố Quản lý tính liên tục Quản lý tính sẵn sàng Tổ chức công việc Tổ chức hỗ trợ Kỹ năng lãnh đạo Kỹ năng cộng tác nhóm Kỹ năng đàm phán Kỹ năng giải quyết vấn đề Kỹ năng giao tiếp Kỹ năng gọi thoại Kỹ năng huấn luyện Kỹ năng lắng nghe Kỹ năng phân công ủy thác Kỹ năng phỏng vấn tuyển dụng Kỹ năng quản lý thời gian Kỹ năng tạo động lực Kỹ năng tư duy Kỹ năng thiết kế quy trình Kỹ năng thuyết trình Kỹ năng viết tài liệu kỹ thuật Ứng dụng Chính phủ điện tử Giáo dục trực tuyến Hoạch định tài nguyên doanh nghiệp Kinh doanh điện tử  ( Mua sắm trực tuyến    · Thương mại điện tử    · Tiếp thị trực tuyến ) Kinh doanh thông minh Quản lý quan hệ khách hàng Quản lý tri thức Các lĩnh vực liên quan Kinh tế Luật pháp Tài chính Kế toán Kinh doanh Tổ chức Xã hội Quản lý Quản trị kinh doanh \n\t\t\t\t\t\tLấy từ “ https://vi.wikipedia.org/w/index.php?title=Học_máy&oldid=32001081 ”\t\t\t\t\t Thể loại :  Học máy Trí tuệ nhân tạo Học tập Điều khiển học Thể loại ẩn:  Trang sử dụng liên kết tự động ISBN Trình đơn chuyển hướng Công cụ cá nhân Chưa đăng nhập Thảo luận cho địa chỉ IP này Đóng góp Mở tài khoản Đăng nhập Không gian tên Bài viết Thảo luận Biến thể Các hiển thị Đọc Sửa đổi Sửa mã nguồn Xem lịch sử Khác Tìm kiếm Xem nhanh Trang Chính Bài viết chọn lọc Tin tức Bài viết ngẫu nhiên Thay đổi gần đây Phản hồi lỗi Quyên góp Tương tác Hướng dẫn Giới thiệu Wikipedia Cộng đồng Thảo luận chung Giúp sử dụng Liên lạc Công cụ Các liên kết đến đây Thay đổi liên quan Các trang đặc biệt Liên kết thường trực Thông tin trang Khoản mục Wikidata Trích dẫn trang này In/xuất ra Tạo một quyển sách Tải về dưới dạng PDF Bản để in ra Tại dự án khác Wikimedia Commons Ngôn ngữ khác العربية অসমীয়া Azərbaycanca Bahasa Indonesia Български Català Čeština Dansk Deutsch Eesti Ελληνικά English Español Euskara فارسی Français 한국어 Հայերեն हिन्दी Íslenska Italiano עברית ಕನ್ನಡ Latviešu Lietuvių Magyar Македонски മലയാളം मराठी Nederlands 日本語 Norsk Norsk nynorsk Polski Português Русский Shqip Simple English Slovenščina Српски / srpski Srpskohrvatski / српскохрватски Suomi Svenska Tagalog தமிழ் ไทย Türkçe Українська 中文 Sửa liên kết  Trang này được sửa đổi lần cuối lúc 07:53 ngày 14 tháng 10 năm 2017. Văn bản được phát hành theo  Giấy phép Creative Commons Ghi công–Chia sẻ tương tự ; có thể áp dụng điều khoản bổ sung. Với việc sử dụng trang web này, bạn chấp nhận  Điều khoản Sử dụng  và  Quy định quyền riêng tư . \nWikipedia® là thương hiệu đã đăng ký của  Wikimedia Foundation, Inc. , một tổ chức phi lợi nhuận. Quy định quyền riêng tư Giới thiệu Wikipedia Lời phủ nhận Nhà phát triển Tuyên bố về cookie Phiên bản di động"
        },
        {
          "title": "Nhiều trường ở TP HCM gắn camera giám sát học sinh",
          "relevance": "0",
          "url": "https://vnexpress.net/tin-tuc/giao-duc/nhieu-truong-o-tp-hcm-gan-camera-giam-sat-hoc-sinh-3177687.html",
          "content": "Rao vặt VnExpress International – Vietnam and ASEAN news 24h qua  RSS   Giáo dục Chủ nhật, 5/4/2015  |  06:00 GMT+7 \n\tThời gian gần đây liên tiếp xuất hiện video học sinh đánh nhau, trong đó nhiều trường hợp ẩu đả trong lớp học. Các trường tại TP HCM đang tăng cường lắp camera để giám sát học sinh, song việc này gây nhiều ý kiến trái chiều trong chính những người làm giáo dục và phụ huynh. Hiệu trưởng, bảo vệ trường THCS Lý Phong quan sát học sinh qua màn hình camera. Ảnh:  N.Duy. \n\tÔng Bùi Gia Hiếu, Hiệu trưởng trường THPT Nhân Việt (quận Tân Phú) cho biết, kể từ khi thành lập, trường đã gắn 48 camera để theo dõi hoạt động của học sinh và ngăn ngừa bạo lực học đường. Việc quan sát màn hình camera được các giám thị thực hiện hằng ngày chứ không đợi đến lúc xảy ra sự cố rồi mới coi lại. \n\tBà Trần Thị Thu Ngân, Hiệu trưởng trường THCS Lý Phong (quận 5) cho biết, có 14 camera gắn ở hành lang phòng chức năng, phòng vi tính, thư viện, hành lang nhà vệ sinh… của trường. \"Đây là những nơi thường xảy ra va chạm, dễ đánh nhau, hút thuốc, đùa giỡn quá trớn, thậm chí cả làm nơi hẹn hò của học sinh. Nhờ gắn camera nên những việc trên hầu như không còn. Trường đang tính gắn thêm camera nhưng chưa có kinh phí”, bà Ngân nói. \n\tTrường THPT Marie Curie (quận 3) gắn mỗi tầng 2 camera. Ngoài ra, nhà thi đấu, sân bóng, sân trường cũng được gắn camera để quan sát học sinh. Trường THCS Kim Đồng (quận 5) cũng gắn 32 camera dọc các hành lang lớp học và những góc khuất nơi cầu thang. \n\tTrường THCS Ba Đình (quận 5) đã lắp hơn 10 camera từ năm 2008. Thời gian gần đây, trường lắp thêm một số nơi khác nhằm ngăn ngừa bạo lực học đường bao gồm hành lang phía sau lầu hai, cổng trường, hành lang đi vào nhà vệ sinh… \n\tTrường THCS Trần Bội Cơ (quận 5) đã lắp hơn 20 camera để theo dõi tất cả hoạt động trong trường. Hằng ngày, luôn có giám thị túc trực bên màn hình camera. Nếu phát hiện điều gì bất thường sẽ đến ngăn chặn kịp thời.  \n\tViệc các trường gắn camera giám sát được một số phụ huynh, học sinh ủng hộ. Chị Trần Thị Cúc (đường Trần Bình Trọng, quận 5) cho rằng nhà trường gắn camera, phụ huynh sẽ an tâm hơn. Nếu có chuyện gì xảy ra với học sinh, phụ huynh sẽ cùng nhà trường xem lại camera để xử lý. \n\tCòn em Lê, học sinh trường THPT Nhân Việt thì cho hay, có camera em không sợ mất đồ dùng học tập, trường cũng có vẻ văn minh, hiện đại hơn. Kỹ thuật viên bảo dưỡng camnera tại trường  THCS Lý Phong .  Ảnh:  N.Duy. \n\tTuy nhiên, dưới góc độ quản lý giáo dục, ông Đỗ Minh Hoàng, Chánh văn phòng Sở Giáo dục và Đào tạo TP HCM, khẳng định Sở không chủ trương và không khuyến khích các trường lắp camera bởi đó là phương pháp phản khoa học. Nó sẽ gây ức chế cho giáo viên, tạo tâm lý không tốt cho học sinh. Đó cũng không phải là biện pháp hữu hiệu để hạn chế bạo lực học đường. \n\t\"Muốn hạn chế bạo lực học đường phải tăng cường giáo dục đạo đức, lối sống cho học sinh; phối hợp chặt giữa nhà trường, gia đình và xã hội để uốn nắn, ngăn chặn mầm mống bạo lực học đường”, ông Hoàng nói. \n\tThầy Bùi Gia Hiếu cũng nhìn nhận, camera dễ gây ức chế cho thầy cô khi giảng bài. Muốn ngăn chặn bạo lực học đường, phải giải quyết gốc rễ của vấn đề là giáo dục cách ứng xử, giải quyết mâu thuẫn cho học sinh chứ không phải đợi camera ghi nhận rồi xử lý hậu quả. \n\tÔng Nguyễn Thành Nhân, Cố vấn cao cấp Trung tâm đào tạo tài năng trẻ châu Á - Thái Bình Dương, cũng cho là cần thay đổi hình thức giáo dục, bắt đầu từ việc giảm chương trình đào tạo, tăng thực hành, dã ngoại… “Chỉ khi nào học sinh được trải nghiệm tốt về tinh thần thì mới giảm được tình trạng bạo lực”, ông nói. Nguyễn Duy Xem nhiều nhất \n\t\t\t\t\t\t\t\tNữ thủ khoa Triết từ chối cơ hội học thẳng tiến sĩ\t\t\t\t\t\t\t   \t\t\t\t\t\t\t \n             \n\t\t\t\t\t\t\t\tCầu nào ở Việt Nam lớn nhất Đông Dương đầu thế kỷ 20?\t\t\t\t\t\t\t  \n\t\t\t\t\t\t\t\tBốn câu đố luyện IQ khiến bạn bối rối\t\t\t\t\t\t\t  \n\t\t\t\t\t\t\t\tBảy nữ sinh ở Hà Nội bị kỷ luật vì đánh bạn học\t\t\t\t\t\t\t  \n\t\t\t\t\t\t\t\tThư tình Obama gửi bạn gái thời đại học được công khai\t\t\t\t\t\t\t  Ý kiến bạn đọc  ( 0 ) Mới nhất  |\r\n           Quan tâm nhất Xem thêm Ý kiến của bạn Gửi 20 /1000 Ý kiến của bạn Gửi 20 /1000    Tags \n                        gắn camera                      \n                        bạo lục học đường                      \n                        phản khoahọc                      \n                        học sinh đánh nhau                      \n                        hò hẹn trong trường học                      Tin khác \n                    Những phát minh khó tin của trẻ biến thành hiện thực                   \n                    Thông điệp truyền cảm hứng vượt qua tật nói lắp của thầy giáo Anh                                      \n             \n                    Thầy giáo quân hàm xanh ở bản vùng cao Lào Cai                   \n              Cầu nào ở Việt Nam lớn nhất Đông Dương đầu thế kỷ 20?             \n              Đo mức độ am hiểu giới từ tiếng Anh             \n              Đáp án bài toán chọn bóng thách thức suy luận             \n               Thử thách đuổi hình bắt chữ bằng tiếng Anh              \n               Bị bắt nạt, nam sinh Brazil bắn chết hai bạn học              \n               Học sinh miền núi Quảng Trị mình trần băng sông đến trường              Xem nhiều nhất Nữ thủ khoa Triết từ chối cơ hội học thẳng tiến sĩ     Cầu nào ở Việt Nam lớn nhất Đông Dương đầu thế kỷ 20?     Bốn câu đố luyện IQ khiến bạn bối rối     Bảy nữ sinh ở Hà Nội bị kỷ luật vì đánh bạn học     Thư tình Obama gửi bạn gái thời đại học được công khai     Hiệu trưởng ngồi phòng rộng, trẻ học phòng chật     Đo độ am hiểu Sài Gòn qua 5 câu hỏi về nhà máy điện đầu tiên     Nguyên Thứ trưởng Y tế: 'Nghề Y có quyền lực, dễ dẫn đến cạm bẫy'     Cô giáo dạy Địa lý bằng tiếng Anh     Đo mức độ am hiểu giới từ tiếng Anh     Tranh luận bỏ hay giữ hội phụ huynh \n                TP HCM chấn chỉnh lạm thu, điều lệ hội phụ huynh             Sở Giáo dục và các quận huyện sẽ kiểm tra tình trạng lạm thu, các khoản đóng góp của Ban đại diện cha mẹ học sinh. \n                Ban đại diện cha mẹ ở Sài Gòn trả tiền cho phụ huynh             \n                Trường 'cực chẳng đã' mới xi-nhan hội phụ huynh thu tiền             \n                Giáo viên ấm ức vì bị cho khuất tất với hội phụ huynh             Học tiếng Anh \n                Đo mức độ am hiểu giới từ tiếng Anh             Bạn có chắc chắn không bao giờ dùng sai giới từ tiếng Anh? Hãy làm bài trắc nghiệm dưới đây để kiểm tra. \n                Thử thách đuổi hình bắt chữ bằng tiếng Anh             \n                Thử tài suy luận với năm câu đố vui tiếng Anh             \n                Đừng cố 'nói tiếng Anh như gió'             Trắc nghiệm \n                Cầu nào ở Việt Nam lớn nhất Đông Dương đầu thế kỷ 20?             Ở thời điểm khánh thành, cây cầu này còn là một trong những cầu lớn nhất thế giới. \n                Đo độ am hiểu Sài Gòn qua 5 câu hỏi về nhà máy điện đầu tiên             \n                Đèn đường ở Sài Gòn có từ bao giờ?             \n                Năm câu hỏi về 'rạp hát Tây' đầu tiên ở Sài Gòn             Du học \n                Cơ hội nhận học bổng 50% tại Triển lãm du học trung học             Tham gia Triển lãm du học do Capstone Vietnam tổ chức, bạn có cơ hội phỏng vấn học bổng trực tiếp với 15 trường đến từ các nước tiên tiến.  ... \n                Chương trình visa ưu tiên CES Canada được gia hạn             \n                Để bớt thấp thỏm, hãy tìm thầy tốt cho con du học Mỹ             \n                Cơ hội giành học bổng toàn phần của các trường THPT công lập Mỹ             Chia sẻ bài viết qua email Bài viết Nhiều trường ở TP HCM gắn camera giám sát học sinh Giáo dục Nhập mã xác nhận: Hoàn tất Trang chủ  Tìm kiếm\r\n         Video Thời sự Góc nhìn Thế giới Kinh doanh Giải trí Thể thao Pháp luật Giáo dục Sức khỏe Gia đình Du lịch Khoa học Số hóa Xe Cộng đồng Tâm sự Cười Ảnh Infographics Rao vặt Shop VnExpress Pay VnExpress Quà tặng Tải VnExpress App\r\n             Trang chủ Tải VnExpress App Video Thời sự Góc nhìn Thế giới Kinh doanh Giải trí Thể thao Pháp luật Giáo dục Sức khỏe Gia đình Du lịch Khoa học Số hóa Xe Cộng đồng Tâm sự Cười Ảnh Infographics Rao vặt Shop VnExpress Pay VnExpress Quà tặng Liên hệ quảng cáo Đường dây nóng Liên hệ tòa soạn 0123.888.0123  (HN) -  0129.233.3555  (TP HCM) Thuộc Bộ Khoa học Công nghệ. © Copyright 1997 VnExpress, All rights reserved. ® VnExpress giữ bản quyền nội dung trên website này. VnExpress tuyển dụng  Liên hệ:  Quảng cáo  /  Tòa soạn Đường dây nóng:  0123.888.0123  (HN) -  0129.233.3555  (TP HCM) \n        ×\n     \n           Phiên bản:  VnExpress International – Vietnam and ASEAN news Trang chủ APEC VIETNAM 2017 Video Thời sự Góc nhìn Thế giới Kinh doanh VEPF 2017 Giải trí Thể thao Pháp luật Giáo dục Sức khỏe Gia đình Du lịch Khoa học Số hóa Xe Cộng đồng Tâm sự Ảnh  Infographics  Cười Rao vặt 24h qua Liên hệ Tòa soạn Thông tin Tòa soạn \n            Thuộc Bộ Khoa học Công nghệ.\n             © Copyright 1997 VnExpress, All rights reserved ® VnExpress giữ bản quyền nội dung trên website này. \n            Hotline: 0123.888.0123  (Hà Nội) 0129.233.3555  (TP HCM) "
        },
        {
          "title": "[ML] Giải thích 10 thuật ngữ chính trong Machine Learning",
          "relevance": "1",
          "url": "http://tech.3si.vn/2016/04/04/ml-giai-thich-10-thuat-ngu-chinh-trong-machine-learning/",
          "content": "3SI SERVICES COMMUNITY CONTACT Blog_check10 b_c_10 AUTHOR : Nam Vu Tags:  machine-learning «  [ML] Classification – Part 3 [Microsoft Azure] Deploy PHP website lên Azure App Service  »  Comments are closed. Administration Login Recent Posts THIẾT BỊ BAY KHÔNG NGƯỜI LÁI: CÔNG NGHỆ & CÁC HƯỚNG ỨNG DỤNG MỚI Giải pháp thúc đẩy ứng dụng Công nghệ Thông tin trong sản xuất nông nghiệp thông minh tại Thanh Hóa [NodeJS] Lựa chọn mới cho nền tảng server side stdClass LÀ GÌ? LÀM THẾ NÀO ĐỂ CÓ PROPERTIES ĐỘNG TRONG PHP? What is stdClass? And Dynamic Properties in PHP? DevOps Thiết kế RESTful APIs CodeFights – Hướng dẫn chi tiết cách giải các dạng bài tập CodeFights – Nơi những tráng sĩ coder hội tụ Database versioning Categories Advertising  (2)\n Agile  (1)\n Algorithm  (21)\n Android Development Patterns  (5)\n Bản tin iOS  (30)\n Business  (5)\n Coding  (45)\n Entertainement  (15)\n Framework  (27)\n Fun  (9)\n Journal  (4)\n Lesson & Practice  (14)\n Machine Learning  (17)\n Marketing  (6)\n Media  (1)\n Mobility  (14)\n Photography  (1)\n Primary  (11)\n Process  (6)\n Social Marketing  (1)\n Uncategorized  (8)\n Web Applications  (6)\n Web Design  (2)\n WordPress  (1)\n Archives October 2017  (2) July 2017  (1) May 2017  (2) April 2017  (3) March 2017  (3) January 2017  (3) December 2016  (4) November 2016  (1) July 2016  (2) June 2016  (2) May 2016  (3) April 2016  (10) March 2016  (6) February 2016  (3) January 2016  (12) December 2015  (8) November 2015  (6) October 2015  (4) September 2015  (6) August 2015  (10) May 2015  (2) April 2015  (7) March 2015  (22) February 2015  (5) Tag #react-native Ajit Johnson Android AnimatedVectorDrawable Apple TV asynchronous backend battery CSS Database Encode frontend Fuji Fujifilm hiếp ảnh iOS Java javascript machine-learning Microsoft Azure Nodejs package-by-feature photography PHP Process Realm Retro runtime Rxjava shopapp smart agriculture SQL Server structure SVG Swift Tip tvOS VectorDrawable Vintage Web X-E1 X-E2 X-T1 X100T Xephinh Meta Log in Entries  RSS Comments  RSS WordPress.org Copyright © 2015 3S Intersoft JSC. All Rights Reserved\r\n"
        },
        {
          "title": "Support Vector Machine trong học máy - Một cái nhìn đơn giản hơn",
          "relevance": "0",
          "url": "https://viblo.asia/p/support-vector-machine-trong-hoc-may-mot-cai-nhin-don-gian-hon-XQZkxoQmewA",
          "content": "Posts Questions Discussions Announcements  No announcement yet. All Announcements  Sign In/Sign up\n     +2 Pham Van Toan  Follow Published Dec 6th, 1:02 am Support Vector Machine trong học máy - Một cái nhìn đơn giản hơn Học máy Algorithm Machine Learning  Dec 6th, 1:02 am\n          1249  3  0  Report\n     Xin chào các bạn, nếu như các bạn có theo dõi các bài viết trước của mình về các mô hình hồi quy thì chúng ta có thể dễ dàng nhận thấy được sự đơn giản và dễ áp dụng của phương pháp hồi quy, nhất là trong các bài toán dự đoán (prediction). Tuy nhiên chính sự đơn giản đó của mô hình làm cho hiệu quả của thuật toán chưa thật sự được như mong muốn. Có rất nhiều phương pháp cho hiệu quả tốt hơn các phương pháp hồi quy, và một trong số đó là  Support Vector Machine (SVM)  mà mình sẽ giới thiệu thật kĩ trong bài viết này. Tuy nhiên, để tránh nhàm chán với những yếu tố học thuật trong bài này chúng ta sẽ tìm hiểu  SVM  theo cách mà người ta vẫn hay kiểm tra học sinh tiểu học theo dạng  cô hỏi - trò đáp . OK chúng ta bắt đầu thôi Khái niệm - SVM là gì Nhìn hình ảnh chúng ta cũng có thể đoán được mục đích của nó đúng không.  SVM  sử dụng để tìm ra một  siêu phẳng  (hyperplane) - chính là cái đường cong cong như hình trên đó. Nhưng thử tưởng tượng trong không gian nhiều chiều hơn chẳng hạn, nó có thể là một mặt cầu, mặt bầu dục... Tóm lại mục đích của cái siêu phẳng đó là phân tách tập dữ liệu thành hai phần riêng biệt - tư tưởng của  bài toán phân lớp . Ví dụ như ảnh trên, chúng ta có một mặt bàn đựng hai loại quả lê và táo. Siêu phẳng phân tách đống quả này thành hai lớp, bản chất là đi tìm một hàm toán học phụ thuộc tọa độ của một quả trên mặt bàn. Có nghĩa là khi nhét một quả mới vào trên mặt bàn, dựa vào tọa độ của nó ta có thể biết được nó là quả táo hay quả lê nhờ vào việc nó nằm bên phải hay bên trái của siêu phẳng. Có thể hiểu đơn giản như thế. Tuy nhiên các phần sau bắt đầu phức tạp rồi đấy, các bạn chuẩn bị tinh thần nha. Ánh xạ tập dữ liệu vào không gian nhiều chiều Trở lại với ví dụ trên của chúng ta, nếu như các quả táo và lê không năm quá đan xen nhạu thì chúng ta hoàn toàn có thể dùng một cái que ( siêu phẳng ) phân tách chúng. Tuy nhiên, thực tế không phải đơn giản như thế, có nghĩa là các quả táo và quả lê nằm tại các vị trí rất  lung tung  trên mặt bàn và rất khó có thể tìm được một cái que như thế để phân tách giữa chúng.  Vậy thì làm thế nào bây giờ???  Một cách giải quyết đó là vận dụng tư tưởng của  trò chơi tung hứng . Giả sử chúng ta trong một cơn tức giận hất tung chiếc bạn đựng táo và lê lên trời, các quả táo và lê bay lơ lửng trên không trung. Lúc này chúng đã ở các vị trí khác nhau và chúng ta hoàn toàn có thể dùng một mặt cong tưởng tượng để phân tách giữa chúng. Ví dụ như mặt phẳng xanh bên đưới đây. Các bạn sẽ nghĩ là có cái gì đó  chém gió  ở đây phải không? Không hề đâu, trò chơi tung hứng trong thực tế tương đương với việc chuyển đổi từ không gian hai chiều ( mặt bàn ) sang không gian nhiều chiều hơn ( không trung ). SVM thực hiện điều này một cách rất tự nhiên thông qua Kernel. Mình không hề chém gió chút nào đâu. Chúng ta cũng có thể hình dung dễ hơn việc  tung bóng  này của SVM thực hiện ra sao trong video dưới đây: {@youtube: https://youtu.be/3liCbRZPrZA}\n SVM thực hiện điều này như thế nào? Như chúng ta đã thảo luận ở các phần trên, bản chất của phương pháp SVM là chuyển không gian dữ liệu ban đầu thành một không gian mới hữu hạn chiều mà ở đó cho khả năng phân lớp dễ dàng hơn. Một quả bất kì nằm trên mặt bàn sẽ được gắn với một tọa độ cụ thể. Ví dụ, quả táo nằm cách mép trái 2cm và cách mép dưới 5cm được thể hiện trên trục tọa độ (x, y) tương ứng là  (2, 5) . x và y chính là tọa độ trong không gian hai chiều của quả táo. Khi đưa lên chiều thứ 3 là  z(x, y) , ta có thể tính được tọa độ của z trong không gian 3 chiều dựa vào tọa độ x,y ban đầu. Điểm làm SVM hiệu quả hơn các phương pháp khác chính là việc sử dụng  Kernel Method  giúp cho SVM không còn bị giới hạn bởi việc phân lớp một cách tuyến tính, hay nói cách khác các siêu phẳng có thể được hình thành từ các hàm phi tuyến. Margin trong SVM là gì? Margin là khoảng cách giữa siêu phẳng đến 2 điểm dữ liệu gần nhất tương ứng với các phân lớp. Trong ví dụ quả táo quả lê đặt trên mặt bán, margin chính là khoảng cách giữa cây que và hai quả táo và lê gần nó nhất. Điều quan trọng ở đây đó là phương pháp SVM luôn cô gắng  cực đại hóa  margin này, từ đó thu được một siêu phẳng tạo khoảng cách xa nhất so với 2 quả táo và lê. Nhờ vậy, SVM có thể giảm thiểu việc phân lớp sai ( misclassification ) đối với điểm dữ liệu mới đưa vào. Ưu điểm của SVM là gì? Là một kĩ thuật phân lớp khá phổ biến, SVM thể hiện được nhiều ưu điểm trong số đó có việc tính toán hiệu quả trên các tập dữ liệu lớn. Có thể kể thêm một số ưu điểm của phương pháp này như: Xử lý trên không gian số chiều cao : SVM là một công cụ tính toán hiệu quả trong không gian chiều cao, trong đó đặc biệt áp dụng cho các bài toán phân loại văn bản và phân tích quan điểm nơi chiều có thể cực kỳ lớn Tiết kiệm bộ nhớ : Do chỉ có một tập hợp con của các điểm được sử dụng trong quá trình huấn luyện và ra quyết định thực tế cho các điểm dữ liệu mới nên chỉ có những điểm cần thiết mới được lưu trữ trong bộ nhớ khi ra quyết dịnh Tính linh hoạt  - phân lớp thường là phi tuyến tính. Khả năng áp dụng Kernel mới cho phép linh động giữa các phương pháp tuyến tính và phi tuyến tính từ đó khiến cho hiệu suất phân loại lớn hơn. Nhược điểm của SVM là gì? Bài toán số chiều cao : Trong trường hợp số lượng thuộc tính ( p ) của tập dữ liệu lớn hơn rất nhiều so với số lượng dữ liệu ( n ) thì SVM cho kết quả khá tồi Chưa thể hiện rõ tính xác suất : Việc phân lớp của SVM chỉ là việc cố gắng tách các đối tượng vào hai lớp được phân tách bởi siêu phẳng SVM. Điều này chưa giải thích được xác suất xuất hiện của một thành viên trong một nhóm là như thế nào. Tuy nhiên hiệu quả của việc phân lớp có thể được xác định dựa vào khái niệm  margin  từ điểm dữ liệu mới đến siêu phẳng phân lớp mà chúng ta đã bàn luận ở trên. Kết luận SVM là một phương pháp hiệu quả cho bài toán phân lớp dữ liệu. Nó là một công cụ đắc lực cho các bài toán về xử lý ảnh, phân loại văn bản, phân tích quan điểm. Một yếu tố làm nên hiệu quả của SVM đó là việc sử dụng  Kernel function  khiến cho các phương pháp chuyển không gian trở nên linh hoạt hơn. Tham khảo Support Vector Machine Tutorial SVM Exercise Pham Van Toan @pham.van.toan Follow  1193\n          74\n          15\n         \n                            Clip this post\n                         Have problems with  Học máy, Algorithm or Machine Learning ?  Ask on Viblo » Comments  No comments yet.\n         +2 • • • Facebook Viblo Hot authors Resources Posts Questions Videos Tags Authors Help Terms  RSS Feed Browser extension Atom plugin Feedback FAQ \n                    © 2017  Viblo . All rights reserved.\n                 \n        Viblo - Free service for technical knowledge sharing\n     Feedback \n                        Your feedback will be shared with our product delivery teams, and taken into consideration\n                        for future development.\n                     Name Email Send \n                Sign In\n             \n                Sign Up\n             Join us to find useful information required to improve your skills \n            Forgot your password?\n         Sign In Free  service for technical knowledge  sharing Join us to find useful information required to improve your skills \n                    I agree to  Viblo Terms of Service Sign Up or"
        },
        {
          "title": "Máy bay được theo dõi như thế nào",
          "relevance": "0",
          "url": "https://vnexpress.net/tin-tuc/khoa-hoc/may-bay-duoc-theo-doi-nhu-the-nao-2963103.html",
          "content": "Rao vặt VnExpress International – Vietnam and ASEAN news 24h qua  RSS   Khoa học Thứ năm, 13/3/2014  |  15:01 GMT+7 \n\t\t\t\t\tVị trí của hộp đen và hệ thống tiếp sóng trên một chiếc máy bay. Ảnh:  BBC \n\tBộ phận tiếp sóng là gì \n\tKiểm soát không lưu kết hợp các thông số xác định vị trí của radar cơ bản với các tín hiệu được cung cấp bởi các bộ tiếp sóng (nhận và phát tín hiệu) của các máy bay, để tạo nên bức tranh chi tiết của giao thông hàng không. \n\tTất cả các loại máy bay thương mại đều được trang bị bộ tiếp sóng ở khu vực buồng lái. Bộ tiếp sóng sẽ tự động truyền tín hiệu điện tử về mặt đất khi nhận được tín hiệu radio. \n\tCác loại hệ thống tiếp sóng cơ bản nhất thường chỉ gửi tín hiệu về độ cao và mã máy bay. Tuy nhiên, các trạm radar vẫn có thể xác định được tốc độ và hướng bay của một chiếc máy bay bằng cách kiểm tra quá trình truyền tiếp. \n\tVùng bao quát của radar thường kéo dài khoảng 240 km tính từ bờ biển. Khi bay qua biển, phi hành đoàn giữ liên lạc với bộ phận kiểm soát trên mặt đất và các máy bay khác bằng cách sử dụng radio tần số cao. \n\tBộ tiếp sóng có thể được tắt bằng tay khi máy bay đang ở trên không trung. Tuy nhiên, trong trường hợp của chiếc máy bay MH370, nguyên nhân mất tín hiệu do hành động cố ý của con người hay ảnh hưởng của một hiện tượng thời tiết khắc nghiệt vẫn đang còn là một ẩn số. \n\t\"Được rồi, đã rõ\" là tín hiệu radio cuối cùng mà bộ phận kiểm soát không lưu nhận được cho thấy mọi việc diễn ra bình thường cho đến trước khi chiếc máy bay hoàn toàn biến mất. \n\t\t\t\t\tSơ đồ mô phỏng quá trình nhận tín hiệu và phát tín hiệu từ trạm kiểm soát không lưu và máy bay. Ảnh:  BBC \n\tChuyện gì xảy ra nếu bộ phận tiếp sóng không hoạt động \n\tKhi bộ tiếp sóng ngừng gửi tín hiệu, máy bay vẫn có thể được theo dõi bằng cách sử dụng một phương pháp tương tự như dò radar được phát triển từ những năm 1930. \n\tHệ thống radar sơ cấp thực hiện chức năng theo dõi và rà soát mọi thứ trên bầu trời có thể cho thấy rằng tín hiệu radio vẫn được truyền đi. Vì vậy, radar chỉ có thể chỉ ra vị trí tương đối của một chiếc máy bay mà không thể xác định được nó. Ngày nay, phương pháp này được sử dụng chủ yếu với vai trò là hệ thống hỗ trợ cho radar thứ cấp. \n\tTheo các quan chức của Malaysia, việc theo dõi dấu vết máy bay bằng radar sơ cấp có thể cung cấp thông tin về hành trình của chiếc máy bay mất tích. Tuy nhiên, những dữ liệu này cần được các chuyên gia phân tích một cách chi tiết. Hệ thống định vị toàn cầu  \n\tCác máy bay thực tế có thể được trang bị hệ thống định vị toàn cầu (GPS). Trong khi đó, mạng lưới kiểm soát không lưu của thế giới vẫn gần như hoàn toàn chỉ dựa vào hoạt động của hệ thống radar. \n\tVới GPS, phi công có thể xác định được vị trí của máy bay trên bản đồ. Dữ liệu này hiện không được cung cấp cho bộ phận kiểm soát không lưu. \n\tMột số chiếc máy bay hiện đại nhất thế giới có thể kết nối ngược dữ liệu GPS với các trung tâm theo dõi vệ tinh. Tuy nhiên, việc xử lý một số lượng lớn dữ liệu sẽ rất tốn kém và các hệ thống này thường chỉ được sử dụng ở những khu vực hẻo lánh mà không có phạm vi hoạt động của radar. \n\tChuyên gia hàng không Chris Yates cho biết hệ thống Giám sát tự động phụ thuộc – phát quảng bá (ADS-B), công nghệ mới trong hoạt động giám sát và quản lý không lưu, đang sử dụng dữ liệu GPS trong hoạt động theo dõi máy bay. \n\tADS-B sẽ điều hành các hệ thống cung cấp hình ảnh \"radar giả\" về chiếc máy bay trong chuyến bay. Những hình ảnh này sẽ được cơ quan tìm kiếm máy bay trực tuyến sử dụng và đưa vào hệ thống vẽ bản đồ. \n\tMáy bay sử dụng hệ thống ADS-B để xác định vị trí qua một vệ tinh, sau đó phát tín hiệu vị trí đến một máy bay khác và đến trạm thông tin trên mặt đất. \n\tTại Mỹ, tất cả các máy bay đều được yêu cầu phải trang bị hệ thống ADS-B từ nay tới tháng 1/2020, . Hệ thống này được dự đoán sẽ thay thế các hoạt động tìm kiếm dấu vết máy bay dựa trên radar trong tương lai. \n\tChiếc máy bay của Malaysia Airlines không còn xuất hiện trên hệ thống website theo dõi máy bay cùng thời điểm nó biến mất khỏi màn hình kiểm soát không lưu và không có dữ liệu GPS nào được ghi nhận để có thể tìm ra tung tích. \n\t\t\t\t\tMột mảnh vỡ từ chiếc máy bay Air France 417 được tìm thấy ở Đại Tây Dương. Ảnh:  AP Hệ thống ACARS \n\tKhi chiếc máy bay Air France 447 rơi xuống Đại Tây Dương vào năm 2009, Hệ thống Báo cáo và Liên lạc (ACARS) đã giúp các nhà điều tra tìm ra đầu mối về những sự cố xảy ra khiến máy bay gặp nạn. \n\tNhờ hệ thống ACARS, máy tính trên máy bay có thể liên lạc với máy tính trên mặt đất và cung cấp thông tin về tình trạng hoạt động của các hệ thống khi đang bay. Các tin nhắn sẽ được truyền bằng tín hiệu radio hoặc tín hiệu điện tử qua vệ tinh và có thể bao quát nhiều vấn đề liên quan đến tình trạng của chiếc máy bay đó, từ động cơ máy móc đến khu vệ sinh.  Các thông tin quan trọng sẽ được gửi đến bộ phận theo dõi trên mặt đất và quá trình hỗ trợ sẽ được tiến hành nhanh chóng hơn. \n\tNhờ ACARS, các chuyên gia đã phát hiện lỗi kiểm soát tốc độ của máy bay Air France trong sự cố năm 2009. Đây là nguyên nhân khiến phi hành đoàn mất phương hướng và chiếc máy bay gặp nạn sau đó chỉ vài phút. \n\tTuy nhiên, trong trường hợp chiếc máy bay mất tích của Malaysia Airlines, các nhà điều tra cho biết vẫn chưa có dữ liệu nào liên quan được ghi nhận cho đến nay. \n\t\t\t\t\tDữ liệu ghi lại trong hộp đen có thể giúp tìm ra nguyên nhân máy bay gặp nạn. Ảnh minh họa:  NTSB Hộp đen \n\tTheo các chuyên gia hàng không, sự biến mất bí ẩn của MH370 chỉ có thể được lý giải khi các dữ liệu ghi lại trong hộp đen của chiếc máy được phục hồi. Tuy nhiên, trong trường hợp chiếc máy bay rơi xuống biển, việc phục hồi các dữ liệu này không hề dễ dàng. \n\tKhi ở dưới nước, hộp đen sẽ tự phát các tín hiệu siêu âm. Tuy nhiên, những tín hiệu này chỉ được phát hiện ở một phạm vi hẹp. Các đội tìm kiếm chỉ có thể phát hiện tín hiệu này nếu tiếp cận đủ gần với vị trí máy bay gặp nạn. \n\tHộp đen không được trang bị các thiết bị phát định vị GPS, do đó việc tìm kiếm nó thường gây nhiều khó khăn cho các đội cứu hộ. Thùy Linh  (theo  BBC )  Máy bay MH370  \n                          Malaysia bàn với công ty Mỹ để nối lại tìm kiếm MH370                           (20/10) \n                          Người phụ nữ Australia nhặt được đôi dép nghi thuộc về hành khách MH370                           (19/12) \n                          Bộ mô phỏng ở nhà cơ trưởng MH370 hiển thị đường bay Ấn Độ Dương                           (28/7) \n                          Tìm thấy mảnh vỡ nghi của MH370 ở Tanzania                           (26/6) \n                          Chiến dịch tìm kiếm MH370 có thể sắp kết thúc                           (17/5)  Xem thêm  Xem nhiều nhất \n\t\t\t\t\t\t\t\tBằng chứng con người vẫn nhận thức được sau khi chết\t\t\t\t\t\t\t   \t\t\t\t\t\t\t \n             \n\t\t\t\t\t\t\t\tMỹ đánh bại Nhật trong đại chiến robot khổng lồ\t\t\t\t\t\t\t   \t\t\t\t\t\t\t \n             \n\t\t\t\t\t\t\t\tChó và lợn rừng tử chiến trong cuộc đấu đẫm máu ở Indonesia\t\t\t\t\t\t\t   \t\t\t\t\t\t\t \n             \n\t\t\t\t\t\t\t\tHài cốt nghìn năm còn nguyên da phát lộ sau bão ở Ireland\t\t\t\t\t\t\t   \t\t\t\t\t\t\t \n             \n\t\t\t\t\t\t\t\tTỷ phú Mỹ tiết lộ về quái vật đào hầm thứ hai\t\t\t\t\t\t\t   \t\t\t\t\t\t\t \n             Ý kiến bạn đọc  ( 0 ) Mới nhất  |\r\n           Quan tâm nhất Xem thêm Ý kiến của bạn Gửi 20 /1000 Ý kiến của bạn Gửi 20 /1000    Tags \n                        máy bay                      \n                        Malaysia                      \n                        hộp đen                      \n                        mất tích                      \n                        Malaysia Airlines                      \n                        MAS                      \n                        MH370                      Tin khác \n                    Công ty Mỹ đưa trạm không gian tự phồng lên Mặt Trăng                                      \n             \n                    Voi ngang nhiên 'tập giãn cơ' giữa đường                   \n                    Chó robot có thể di chuyển trên nhiều địa hình                                      \n             \n              Bị báo săn truy sát, linh dương đâm sầm vào ôtô                          \n             \n              Siêu núi lửa Mỹ có thể phun trào sớm hơn dự kiến                          \n             \n              Làm cách nào để ốc nhanh hết sạn nhớt?             \n               Trung Quốc sắp xây xong cầu vượt biển dài nhất thế giới                            \n             \n               Nghĩa địa của những tàu vũ trụ rơi xuống Trái Đất                            \n             \n               12  loài vật có khả năng ngụy trang thiên tài trong tự nhiên                            \n             Hỏi - Đáp \n                Làm cách nào để ốc nhanh hết sạn nhớt?             Xin mọi người mách giúp cách làm cho các loại ốc nhả hết chất nhớt và sạn cát một cách nhanh nhất? (Văn Thông) \n                Tại sao chúng ta bị bóng đè?             \n                Tại sao một số loài vẫn sống dù máu bị đóng băng?             \n                Điều gì xảy ra nếu có hai Mặt Trăng quay quanh Trái Đất?             Công nghệ mới \n                Mặt đường thông minh tự vạch ký hiệu cho người đi bộ                          \n             Công nghệ mới giúp mặt đường có thể cảm ứng và tự hiển thị các ký hiệu theo nhu cầu của người tham gia giao thông. \n                Công nghệ biến đồ vật trong nhà thành điều khiển TV             \n                Nông trại rau nằm sâu 33 m dưới lòng London                          \n             \n                Nga thử nghiệm mẫu môtô bay 8 cánh mới                          \n             Môi trường \n                Bị tàn phá, rừng nhiệt đới bắt đầu thải carbon nhiều hơn hấp thụ             Những cánh rừng nhiệt đới không còn hấp thụ nhiều CO2 nữa mà đang thải ra carbon do cây cối mục ruỗng sau khi bị chặt hạ. \n                Ô nhiễm không khí khiến người Trung Quốc giảm thọ ba năm             \n                Khí thải khiến giông bão nguy hiểm hơn             \n                Tự sản xuất năng lượng, người phụ nữ Nhật không tốn tiền điện                          \n             Chia sẻ bài viết qua email Bài viết Máy bay được theo dõi như thế nào Khoa học Nhập mã xác nhận: Hoàn tất Trang chủ  Tìm kiếm\r\n         Video Thời sự Góc nhìn Thế giới Kinh doanh Giải trí Thể thao Pháp luật Giáo dục Sức khỏe Gia đình Du lịch Khoa học Số hóa Xe Cộng đồng Tâm sự Cười Ảnh Infographics Rao vặt Shop VnExpress Pay VnExpress Quà tặng Tải VnExpress App\r\n             Trang chủ Tải VnExpress App Video Thời sự Góc nhìn Thế giới Kinh doanh Giải trí Thể thao Pháp luật Giáo dục Sức khỏe Gia đình Du lịch Khoa học Số hóa Xe Cộng đồng Tâm sự Cười Ảnh Infographics Rao vặt Shop VnExpress Pay VnExpress Quà tặng Liên hệ quảng cáo Đường dây nóng Liên hệ tòa soạn 0123.888.0123  (HN) -  0129.233.3555  (TP HCM) Thuộc Bộ Khoa học Công nghệ. © Copyright 1997 VnExpress, All rights reserved. ® VnExpress giữ bản quyền nội dung trên website này. VnExpress tuyển dụng  Liên hệ:  Quảng cáo  /  Tòa soạn Đường dây nóng:  0123.888.0123  (HN) -  0129.233.3555  (TP HCM) \n        ×\n     \n           Phiên bản:  VnExpress International – Vietnam and ASEAN news Trang chủ APEC VIETNAM 2017 Video Thời sự Góc nhìn Thế giới Kinh doanh VEPF 2017 Giải trí Thể thao Pháp luật Giáo dục Sức khỏe Gia đình Du lịch Khoa học Số hóa Xe Cộng đồng Tâm sự Ảnh  Infographics  Cười Rao vặt 24h qua Liên hệ Tòa soạn Thông tin Tòa soạn \n            Thuộc Bộ Khoa học Công nghệ.\n             © Copyright 1997 VnExpress, All rights reserved ® VnExpress giữ bản quyền nội dung trên website này. \n            Hotline: 0123.888.0123  (Hà Nội) 0129.233.3555  (TP HCM) "
        },
        {
          "title": "Tản mạn về mảnh bằng Ph.D",
          "relevance": "0",
          "url": "https://www.cse.buffalo.edu/~hungngo/Vietnamese/phd.html",
          "content": "Tản mạn về mảnh bằng Ph.D Những năm gần đây có\nkhá nhiều sinh viên Việt  Nam  du học ở Mỹ và nhiều nước khác,\nbằng nhiều con đường khác nhau. Người có học bổng, chức trợ giảng (teaching assistant - TA), hoặc\ntrợ nghiên cứu (research assistant - RA), người thì du học tự túc.  Tôi\nkhông nhớ chính xác là đã đọc ở đâu đó rằng có hơn nghìn du học sinh mỗi năm\nsang Mỹ. Nhiều người trong số họ  theo  học tiến sĩ\n(Ph.D). \nThông tin về làm thế nào để xin học bổng, TA, RA, xin  thư \ngiới thiệu, cách viết dự định cá nhân (personal statement), vân vân đầy rẫy\ntrên các mailing lists trên Internet. \nThế nhưng, một số câu hỏi quan trọng mà tôi ít thấy sinh viên hỏi là: \"tại\nsao lại học Ph.D?\", \"có đáng bỏ thời gian học Ph.D hay không?\",\n\"làm thế nào để đánh giá mảnh bằng Ph.D?\", \"tôi có đủ khả năng\nđể học Ph.D hay không?\", \"học Ph.D xong rồi làm gì? \", \nvân vân. \nCó lẽ ta cần một luận  án  ... Ph.D để trả lời phần nào\nthỏa đáng các câu hỏi trên. Cũng có lẽ có ai đó trong các ngành giáo dục hay\ntâm lý học đã làm rồi. Về mặt kinh tế thì một người bạn cho tôi biết đã có cả\nmớ công trình nghiên cứu về “cái giá của giáo dục” (returns to education). \nTrong bài viết này, tôi thử lạm bàn  lan  man xung quanh\ncác câu hỏi trên.  Bài viết hoàn toàn không mang tính hàn lâm\n(academic), nghĩa là sẽ không có các con số thống kê, bảng phân tích, để ủng hộ\nmột (vài) luận điểm nào đó. Sẽ không có tham khảo đến\ncác nguồn thông tin tín cẩn và các thứ tương tự. Tác\ngiả chỉ dựa trên các kinh nghiệm, quan sát, và suy nghĩ cá nhân, sau gần chục\nnăm học và \"hành nghề\" Ph.D ở Mỹ. Tôi chắc\nlà một cá nhân khác trong hoàn cảnh của tôi sẽ có không ít ý kiến bất đồng. Tôi cũng không có tham vọng nói\nhết được những cóp nhặt kinh nghiệm của mình. Ngoài\nra, tôi sẽ nhấn mạnh nhiều hơn mặt trái của việc học Ph.D. \nÐiều tôi hy vọng là qua bài viết này, tôi có thể giúp cho các sinh viên (cùng\ngia đình) sẽ và đang học Ph.D ít nhiều chuẩn bị tinh thần cho đoạn đường chông\ngai nhưng thú vị này; hy vọng chỉ ra được một góc nhìn khác về Ph.D so với quan\nniệm  chung  của xã hội.   1.     \n Ph.D là  gì ?   Trước hết\nta hãy thử bàn về mảnh bằng Ph.D từ cái nhìn hàn lâm. Ph.D là viết tắt của chữ Doctor of\nPhilosophy. Học bậc Ph.D, cao nhất trong các học bậc,\nđầu tiên xuất hiện ở Ðức, sau đó được Mỹ và nhiều nước phương tây khác sử dụng. Bằng Ph.D đầu tiên của Mỹ xuất hiện khoảng cuối thế kỷ 19. (Ơ Y' cho đến những năm 1980 mới có bằng Ph.D.) \nTừ Ph.D có gốc  latin  là Philosophiae Doctor.  Chữ doctor nghĩa là \"thầy\" (teacher), và \"chuyên\ngia\", \"chức trách\" (authority).  Chữ philosophy (triết\nhọc) có nguồn gốc từ thời trung cổ (medieval) ở Châu Ảu, khi mà các trường đại\nhọc có bốn chuyên khoa (faculty) chính: thần học (theology), luật học (law), y\nhọc (medicine), và triết học (philosophy).  Philosophy ở đây\ndùng để chỉ các ngành học không dẫn đến một nghề nghiệp thực tế nhất định của\nthời đó như người của nhà thờ, luật sư, và bác sĩ. \nÐến nay thì không phải Ph.D nào cũng liên quan đến philosophy, cho dù lấy  theo  nghĩa bóng nhất của từ này.  Tuy nhiên\nchữ doctor vẫn mang đầy đủ ý nghĩa của nó. Ở phương\nTây, trong nghi thức giao tiếp người ta gọi một người có bằng Ph.D là doctor. Hầu hết các trường đại học đều đòi hỏi toàn bộ giảng viên và\ncác giáo sư có bằng Ph.D. Ða số các nhà nghiên cứu ở các phòng nghiên cứu\nchuyên nghiệp đều có bằng Ph.D. \nTuy vậy, điều ngược lại không đúng: không phải tất cả các Ph.D đều có thể làm\ngiảng viên, giáo sư, hay nghiên cứu viên.  Có những Ph.D thậm\nchí chẳng bằng một kỹ sư thông thường.  Cũng có khá nhiều Ph.D, sau khi\n\"hành nghề\" một thời gian thì lên chức, hoặc chuyển sang làm salesman\nhoặc làm quản lý, vân vân. Ta sẽ quay lại đề tài này sau. Cái nhìn hiện đại của Ph.D như sau.  Ðể hoàn tất Ph.D, sinh\nviên phải đạt được hai mục tiêu chính: (a) hoàn toàn tinh thông một ngành (hoặc\nphân ngành) nào đó, và (b) góp phần mở rộng khối kiến thức của nhân loại về\nngành đó. Mục tiêu (b) là cái lõi để phân biệt bậc Ph.D với các bậc học\nkhác.  Ph.D không phải là cái bằng \"nhai lại\": đọc nhiều, thi\nlấy điểm cao là xong.  Một Ph.D đúng nghĩa phải có một vài\ncông trình và ý tưởng nghiên cứu của riêng mình (originality). Về mặt lý thuyết thì là thế. Thực tế\nra sao?   2.     \n \"Nghề\" Ph.D: đoạn trường cũng lắm\nchông gai   Ở Mỹ, là\nsinh viên sau đại học (graduate student) cũng là một nghề.  (Tôi không dùng từ \"nghiên cứu sinh\" vì\nkhông phải graduate student nào cũng làm nghiên cứu thực thụ, nhất là các sinh\nviên đang học thạc sĩ.) Các graduate  students  thường\nlàm TA hoặc RA, với mức lương khoảng 900USD đến 1200USD một tháng (sau thuế),\ntiền học được bao.  Sống tằn tiện thì mức lương này vừa đủ một\nngười sống.  Thường thì các gradudate students sống chui rúc trong một\ncăn hộ nhỏ bé nào đó (dĩ nhiên là có ngoại lệ, đa phần do  may \nmắn), hầu hết thời gian dùng ở các phòng lab (phòng thí nghiệm hoặc phòng máy\ntính) và thư viện. Tối về đến nhà là  lăn  ra ngủ để rồi\nsáng mài mèo con lại hớn hở bút chì bánh mì lên đường. \nKể chuyện cuộc sống gradudate  students  thì có lẽ cần\nmột tiểu thuyết vài trăm trang. Ðiều tôi muốn đề cập là: trong hoàn cảnh làm\nviệc căng thẳng như vậy, một sinh viên thông thường thỉnh thoảng sẽ phải tự đặt\ncâu hỏi \"có đáng không?\"  Nhất là khi công việc học\ntập và nghiên cứu không trôi chảy. Mà kể cả khi nó\nhoàn toàn trôi chảy, tính về các mặt kinh tế, tinh thần, thời gian, và ...\nphilosophy, câu hỏi trên vẫn hoàn toàn hợp lệ. \nVề mặt kinh tế thì lương trung bình của Ph.D ra trường có nhỉnh hơn thạc sĩ\n(M.S) và bậc đại học (B.S) một chút, nhưng sự khác biệt này không khỏa lấp được\nlỗ lã cho thu nhập đã mất trong khoảng thời gian làm Ph.D: trung bình từ 4 đến\n5 năm. Tính tổng số USD kiếm được cho mỗi giờ học tập thì Ph.D là hạng bét\n(tính tương đối  theo  từng ngành học). Về mặt tinh thần thì làm việc căng thẳng và cật lực trong một\nthời gian dài trong một môi trường cạnh tranh tương đối công bằng nhưng khắc\nnghiệt (!) hoàn toàn có thể ảnh hưởng xấu đến tâm lý cá nhân.  Ðiều này\nđặc biệt đúng với sinh viên du học: thiếu thốn các nhu cầu văn hóa và tinh thần\ncơ bản của quê hương, cơ hội tìm bạn tình hoặc bạn đời bị giảm thiểu (với phái\nnam), vân vân. Không phải hiếm mà người ta hay thấy bọn Ph.D hơi ... gàn gàn.\nCông bằng mà nói, gradudate students do thiếu thốn văn hóa hay tìm cách nghiên\ncứu học hỏi thêm cái này cái khác ngoài ngành của mình (nhạc, thơ, lịch sử,\nchính trị, triết  học, ...) , cho nên bọn gàn cũng có\nthể rất đa tài.  Ở Mỹ thì địa vị xã hội của một Ph.D cũng\nchẳng hơn gì các nghành nghề khác là mấy. Yếu tố tinh thần này rất quan trọng. Có không ít các gradudate students cần đến 8, 9 năm mới làm xong\nPh.D. Nhiều năm trời \"ở mãi kinh kỳ với bút nghiên\", ngoảnh đi ngoảnh\nlại chưa làm được gì ra hồn mà đã ngoài 30.  Khi thị trường việc cho Ph.D\nbị thuyên giảm thì người ta rơi vào cái vực muôn thuở: \"về hay ở\",\n\"về thì đâm đầu vào đâu? \". Nhiều năm làm việc với mức lương vừa đủ sống, các Ph.D mới ra\ntrường hoàn toàn không dành giụm được gì, chưa nói đến việc nợ thẻ tín dụng kha\nkhá. Dù các nhà chức trách đã có kế hoạch đãi ngộ nhân\ntài, chế độ này vẫn còn xa rời thực tế. Ðầu tư tinh\nthần và thời gian của một Ph.D quá nhiều để có thể hài lòng với một công việc\nmột vài triệu đồng một tháng.  Họ sẽ phải tự hỏi: nếu xưa mình không đi\nhọc thì bây giờ cũng có thể đã phây phây lương vài triệu một tháng?  Vậy cả chục năm trời bỏ ra công cốc à? Tôi\nđã nhập nhằng yếu tố tinh thần và kinh tế, nhưng đôi khi ta không tách rời\nchúng được. Một khía cạnh khác của yếu tố tinh thần là sức ép của gia\nđình và người thân.  \"Người ta 4 năm đã xong Ph.D, vợ con nhà cửa\nđàng hoàng, bọn không Ph.D thì cũng giám đốc với trưởng phòng, xây nhà to cửa\nrộng cho bố cho mẹ; còn mày bây giờ ngoài 30 mà vẫn cứ lông bông tay trắng.  Ông chẳng ra ông, thằng chẳng ra thằng\". Về mặt triết học mà nói thì có đáng học Ph.D không? \nCâu hỏi này phụ thuộc rất nhiều vào bản thân sinh viên: đi học Ph.D để làm gì?\nTa sẽ quay lại điểm này trong phần tới. \nBây giờ hãy giả dụ cô/anh Ph.D yêu dấu của ta tìm được một công việc ổn định ở\nnước ngoài, quyết định ở lại tích lũy tư bản giúp gia đình và tích lũy kinh\nnghiệm  để    sau \nnày, cách này hay cách khác, (về) giúp quê hương. Có hai nhánh công việc chính\ncho một Ph.D mới ra trường: (a) làm việc ở một phòng nghiên cứu chuyên nghiệp\nnào đó, và (b) một chân giảng viên hoặc giáo sư ở một trường đại học. \n(Hai công việc này có thể chỉ có được sau một vài năm làm postdoc  nữa .  Ta hãy cứ gộp luôn postdoc vào tổng thời gian cho\ntiện, mặc dù lương postdoc khá hơn lương gradudate students.) Lương bổng và giá trị của vị trí mới phụ thuộc hoàn toàn vào\nviệc người ta đánh giá Ph.D như thế nào. Tôi sẽ bàn về\nviệc này trước.  Tôi cũng có ý nói lan man vềđề tài \"định trị\nPh.D\" sau khi đọc một bản tin ở VNExpress thấy trong nước người ta có nói\nvề đánh giá Ph.D loại \"giỏi, khá, trung bình\" (sau một buổi họp nào\nđó). Phạm vi \"định trị Ph.D\" của tôi chủ yếu áp dụng cho các nghành\nkỹ thuật và khoa học tự nhiên như điện, điện tử, khoa học máy tính, toán, lý, ... \nKhi xưa thì giá trị của một Ph.D mới ra trường tùy thuộc vào giá trị công trình\nnghiên cứu trong luận văn tốt nghiệp.  Sau khi ra trường thì\ndoctor mới sẽ phát triển công trình này thành một vài bài báo đăng ở các tạp\nchí (journals) và hội nghị (conference) chuyên ngành.  Các bài báo này\nđều được phê bình (reviewed) bởi các chuyên gia đã trưởng thành trong cùng\nngành.  Các bài báo không đóng góp gì nhiều hoặc vớ vẩn sẽ\nkhông được nhận đăng. \nHiện nay thì áp lực đăng báo (publication) của graduate students khi còn đang\nhọc lớn hơn gấp bội. Một công việc kha khá ở một trường đại học hay phòng\nnghiên cứu danh tiếng thường nhận doctor mới với hơn chục bài báo. Trung bình\nmột giáo sư trẻ mới ra trường trong ngành khoa học máy tính có đến khoảng 3-5\njournal papers và cả chục conference papers. Dĩ nhiên số lượng là thứ yếu, chất lượng mới quan trọng. Một công trình chất lượng cao sẽ được nhiều người biết đến\nrất sớm, và có thể nói không ngoa là nó quan trọng hơn cả trăm bài báo dạng ...\n\"bổ củi\". (Bổ củi là tính từ dân gian trong\ngiới khoa học Việt  Nam  để chỉ các bài báo thường thường\nbậc trung, ai làm mãi rồi cũng xong.) \nÐối với Ph.D ở Mỹ thì điểm học trung bình khi học Ph.D hầu như không mang ý\nnghĩa gì cả, ngoại trừ điểm tối thiểu để có thể được tiếp tục học, khoảng chừng\n3.3 đến 3.5 trên 4.0, tùy theo trường.  Số lượng và chất lượng\ncác bài báo và các công trình nghiên cứu khác (một ứng dụng máy tính chẳng hạn)\nmới là tiêu chí đánh giá Ph.D. Không có chuyện người ta xếp loại Ph.D trung\nbình, yếu, giỏi, khá, vân vân.  Lý do chính là: làm chuyện này hầu như là\nvô vọng.  Ai có đủ thẩm quyền và thời gian để đánh giá. Kể cả giáo sư hướng dẫn chưa chắc đã biết hết về phân ngành\nmà sinh viên của mình làm, huống gì người ngoài. Có\nrất nhiều công trình đăng báo vài năm hoặc vài chục năm sau người ta mới thấy\nhết giá trị của nó. Cũng có cả tỉ công trình lúc mới\nđăng thì ai cũng xúm vào khen, nhưng vài năm sau thì lặng tăm. Dĩ nhiên có khá nhiều các công trình mà người trong ngành đọc\nbiết ngay là \"dỏm\" hay \"xịn\". Nhưng\nvấn đề chính là không ai có thời gian xếp loại và định trị Ph.D. Ở Mỹ, kinh tế\nthị trường tương đối công bằng. Ph.D giỏi sẽ được đồng\nnghiệp biết đến, tìm được việc ở các trường đại học và phòng nghiên cứu danh\ntiếng, vân vân. Cũng có thể có Ph.D giỏi không tìm\nđược việc, hoặc Ph.D dỏm \"lọt lưới\" cung cầu. Các trường hợp này đều là ngoại lệ hiếm hoi. Lại nói thêm về đăng báo.  Ta hãy nhớ mục tiêu (b) của\nPh.D: đóng góp vào khối kiến thức của nhân loại. Ph.D mà không có bài báo nào\nthì có 10 Ph.D cũng hoàn vô nghĩa,  theo  nghĩa tinh\nkhiết nhất của chữ Ph.D. Chí ít, Ph.D phải chia xẻ các thu lượm và nghiên cứu\ncủa mình với đồng nghiệp ở một vài hội nghị và journal danh tiếng nào đó. Các nhà xuất bản khoa học ở phương Tây cũng làm kinh tế. Có rất nhiều các hội nghị và journals hạng bét, bài vớ va vớ\nvẩn cũng đăng vào được. Chỉ có người trong ngành mới\nbiết được hội nghị và journal nào có uy tín. Mà kể cả\nở các nơi có uy tín này ta vẫn có thể tìm thấy các bài báo tồi. Tóm lại, công việc \"định trị Ph.D\" hoàn toàn không\nđơn giản chút nào. Áp lực phải đăng báo đè rất nặng lên\nvai các gradudate students.  Ngược lại, cảm giác công trình của mình được\nđồng nghiệp công nhận và đánh giá cao cũng rất tuyệt vời! \nTrong 5, 6 năm đầu sau khi ra trường, bất kể công việc là giáo sư hay nghiên\ncứu viên, áp lực viết báo và xin tiền làm nghiên cứu còn nặng hơn khi còn là\nsinh viên nữa. \n(Ở đây ta loại trừ các trường hợp người ta chỉ muốn có Ph.D để  theo  đuổi nghề giảng viên (lecturer) nào đó.  Có lẽ phải khẳng định rằng mục tiêu này cũng cao quí như các mục\ntiêu \"cạnh tranh khắc nghiệt\" khác.) Nếu Ph.D trẻ không khẳng định được mình trong 5, 6 năm đầu\ntiên này thì thường là sẽ không giữ được công việc của mình. Có lẽ bạn đọc cũng có thể tưởng tượng được áp lực này nặng như thế\nnào.  Các bài báo đều là các công trình sáng tạo mà trước đó chưa có ai\nlàm, chưa có ai nghĩ ra (chí ít là về nguyên tắc). Làm thế nào mà ai đó có thể\nđảm bảo một năng suất sáng tạo nhất định trong một thời gian dài như vậy?  Có đáng bỏ ngần ấy thời gian và công sức cho một mục tiêu mà phần\nthưởng về cả kinh tế, tinh thần, triết học, sức khỏe đều khá mập mờ? 3.     \n Tại sao lại học Ph.D? Có nên học Ph.D không? Ta thử ghi ra đây một phần\nnhỏ các lý do: \na) Bạn bè đều đi nước ngoài học sau đại học. \nb) Ðược xã hội nể trọng, oách ra phết. \nc) Ðể học được kiến thức tiên tiến. \nd) Không rõ lắm. Từ bé học đã giỏi, thì cứ tiếp tục học. \ne) Có lẽ là con đường duy nhất để cải thiện đời sống gia đình và cá nhân. \nf) Ðể mở tầm mắt ra những chân trời mới. \ng) Ðể sau này về làm giáo sư đại học. \nh) Ðể được làm nghiên cứu khoa học. \ni) Ðể thay đổi thế giới quan. \n...... \nz) Tất cả các lý do trên. Và z phẩy) Không làm Ph.D thì làm gì? \nÐối với đa số gradudate  students  và\ngraduate-students-tương-lai thì câu trả lời là một tập con khá lớn của vài tá\ncâu trả lời mà ai cũng có thể nghĩ ra. \nTa hãy thử phân tích vài chọn lựa quan trọng nhất. Làm Ph.D để mở mang\nkiến thức . Ðây là một mục tiêu rất quan trọng và\nmang tính cá nhân.  Mark Twain từng nói: \"đừng để trường lớp cản trở\ncon đường giáo dục của bạn\" (Don't let school get in the way of your\neducation). Trường lớp không phải là con đường duy nhất đến  Rome  của tri thức. Tuy vậy, trong hoàn cảnh lạc hậu của\nmột nước thế giới  thứ  \n ba  như Việt  Nam  ta, thì ra nước ngoài học thêm là con đường hữu lý. \nCâu hỏi chính mà ta nên đặt ra là chỉ nên học M.S thôi, hay là học cả Ph.D. Chỉ\nvề kiến thức mà nói, thì hai năm M.S cũng đủ cho một sinh viên thông minh sau\nđó tự học.  Làm Ph.D cũng đa phần là tự học thôi. Làm Ph.D để có một cuộc sống tốt đẹp hơn,\nđược xã hội nể trọng hơn; vì bạn bè ai cũng học Ph.D; có bằng Ph.D rất oách; từ\nbé đã học giỏi thì cứ tiếp tục học; vân vân . \nMột Ph.D thực thụ sẽ cho bạn biết rằng các lý do loại này đều là sai lầm to\nlớn!  Tôi hoàn toàn không có ý định \"giảng đạo\" về\nchọn lựa cá nhân của ai. Tôi cũng không nói động cơ\n\"hám bằng cấp\" hay \"oai oách\" là sai trái. Ðó là chọn lựa của từng cá nhân.  Ðiểm tôi muốn nói là các\nđộng cơ loại này sẽ không thể giúp sinh viên hoàn thành tốt việc học Ph.D. Việc\nhay so sánh mình với bạn bè và người khác sẽ tạo nên áp lực tinh thần không thể\nchịu nổi trong khi học.  Yêu thích \"tiếng tăm\" cũng\nvậy.  \"Học giỏi\",  theo  nghĩa ở ta, là\nthi thố điểm cao và \"nhai lại\" những gì được dạy, cho nên học giỏi\nchưa chắc đã liên quan mấy đến khả năng sáng tạo - khả năng sống còn của Ph.D. Từ khóa dẫn đến thành công của sinh viên Ph.D phải là  “đam mê\" .  Ðam mê học hỏi và\nsáng tạo trong một phân ngành nhất định! Trừ những người thật sự xuất chúng thì\nđa số chúng ta sẽ không thể làm thành công Ph.D ở một ngành nào đó chỉ vì\n\"xã hội cần nó\", hay \"nó kiếm ra tiền\". Nếu chỉ đam mê học hỏi không thôi thì cũng không đáng bỏ ra\nngần ấy thời gian để làm Ph.D. Ta hoàn toàn có thể làm M.S rồi tự đọc, tự học\nthêm. \nTất cả các thành quả như chức vụ, danh tiếng, oai oách, vân vân đều phải, và\nnên, là sản phẩm phụ của quá trình  theo  đuổi nỗi đam\nmê sáng tạo và mở mang tri thức này. Ðấy là nói về \"động lực\" học Ph.D. Thế còn\n\"khả năng\" thì sao? Quá trình học Ph.D lên\nxuống như hình sin. Sẽ có bao nhiêu trở ngại kinh tế,\ntinh thần phải vượt qua.  Một trong những trở ngại lớn nhất là: sau một\nvài thất bại trong nghiên cứu, các sinh viên sẽ phải tự hỏi \"ta có đủ khả\nnăng làm Ph.D không nhỉ?\" Ðam mê và khả năng tạo thành cái vòng luẩn quẩn.  Ta có\nxu hướng đam mê cái mà ta giỏi, và ta thường xuất sắc ở công việc mà ta đam mê.\n Nhảy vào được cái vòng này là hành trình cá nhân. Có lẽ không ai trả lời thay ta được. 4.       Phụ huynh: xin đừng gây áp lực tâm lý Không ít các\nbậc phụ huynh mà tôi được dịp quan sát đặt rất nhiều kỳ vọng vào con em mình về\ncon đường hàn lâm.  Họ đầu tư tiền\nbạc và thời gian, nuôi niềm hy vọng ngày nào đó sẽ có một \"trạng\nnguyên\" vinh quy bái tổ, nở mày nở mặt với hàng xóm láng giềng và bè bạn.\nChuyện này có ở tất cả các học bậc, không riêng gì Ph.D. Tuy vậy, áp lực ở Ph.D\nlớn hơn khá nhiều vì graduate students sẽ phải cạnh tranh với các sinh viên\nxuất sắc trên toàn thế giới. \nTôn trọng tri thức và học tập là điều tốt, và bằng cấp là một thước đo tương\nđối chính xác của tri thức. Nhưng nó không phải là thước đo duy nhất. Ðó là\nchưa nói đến các câu hỏi như: đạt được tri thức loại gì thì mới được coi là\n\"thành nhân\"? Khó mà có thể đo lường xem một Ph.D và một anh đạp xích\nlô ai có \"đóng góp\" nhiều hơn cho xã hội, hay ai \"hạnh\nphúc\" hơn ai, theo bất kỳ nghĩa nào của các từ này. Có một ranh giới rất\nbé giữa \"tôn trọng tri thức\" và \"hám bằng cấp\". \nHy vọng tôi đã hay sẽ thuyết phục được bạn rằng Ph.D cũng thượng vàng hạ cám.  Một Ph.D về khoa học máy tính chẳng hạn, nếu làm nghiên cứu về một\nphân ngành chẳng ai quan tâm, đăng vài bài báo ở các chỗ linh tinh, thì sẽ từ\ntừ xa rời dòng chảy chính của tri thức nhân loại. Có\nkhông ít Ph.D về khoa học máy tính lập trình không ra hồn, thua hẳn một kỹ sư\nthông thường, chính là vì lý do này. Tôi lại triết lý 3-xu rồi. Ðiều tôi\nmuốn nói là niềm \"hy vọng\" của các bậc phụ huynh tạo áp lực cực lớn\nảnh hưởng đến kết quả học tập và nghiên cứu của sinh viên.  Trong khi\nchọn lựa nghề nghiệp tương lai đáng lẽ nên là chọn lựa cá nhân! 5.     \n Ðạt được Ph.D chỉ là bước đầu Còn khá\nnhiều điểm khác tôi muốn nói, nhưng bài đã dài. Lấy Ph.D chỉ là bước đầu rất nhỏ\ncủa một nghề nghiệp, cũng như bao nhiều nghề nghiệp khác. Có Ph.D có thể đồng nghĩa với những phần thưởng đáng quí về kinh tế\nvà tinh thần về cả mặt xã hội lẫn cá nhân, nhưng bù lại cái giá phải trả về mọi\nmặt cũng cao không kém. \"Nghề\" Ph.D chẳng\ncao quí hơn nhiều nghề khác, mà thời gian và công sức bỏ ra lại nhiều hơn khá\nnhiều. Cuộc sống và các chọn lựa cá nhân lẽ dĩ nhiên là phức tạp. \nTôi hy vọng qua bài viết này các bạn trẻ có thể có một cái nhìn và suy nghĩ cẩn\ntrọng hơn trước khi  theo  đuổi \"con đường đau\nkhổ\" này. Ta không thể  theo  nó chỉ vì các ảo\ntưởng danh tiếng, bằng cấp và tiền bạc. Ðầu tư như vậy không có lãi! \nMột trong những điều kiện cần cho nghề này là khả năng  theo \nđuổi nỗi đam mê nghiên cứu và sáng tạo trong một thời gian dài. Bằng Ph.D chỉ\nlà một bước cỏn con trong hành trình chông gai nhưng thú vị này. Nó hoàn toàn\nkhông phải là con đường duy nhất. Thứ bảy, 29 tháng 11, 2003. NQH"
        }
      ]
    },
    {
      "description": "Tìm hiểu về vấn đề overfitting trong Machine Learning, bất kỳ trnag web nào nêu nguyên nhân, khái niệm, hậu quả và các cách khắc phục overfitting đều là kết quả phù hợp",
      "query": "Vấn đề Overfiting trong Machine Learning",
      "sites": [
        {
          "content": "Công nghệ Lập trình Lập trình ứng dụng Lập trình web Tools & Tips Sự kiện Chuyên gia nói Tâm sự coder Devvui Tìm kiếm Sign in Đăng nhập tài khoản Tài khoản mật khẩu của bạn Forgot your password? Get help Password recovery Khởi tạo mật khẩu email của bạn Mật khẩu đã được gửi vào email của bạn. Tech Talk Điều gì sẽ xảy ra khi cập nhật phần mềm mỗi… Trí tuệ nhân tạo của Google tự đánh cờ vây với… 8 xu hướng công nghệ sẽ thống trị trong giai đoạn… Những kĩ năng cần có của một developer thành công Bản update Windows 10 Fall Creators không tương thích với laptop… Tất cả Lập trình ứng dụng Lập trình web Framework Microsoft .Net 4.7.1 có gì mới? 7 lí do để loại bỏ React’s Functional Components Tại sao C # là một trong những ngôn ngữ lập… Vì sao JavaScript là ngôn ngữ lập trình quyến rũ Getting Started with Entity Framework 6 Code First using MVC 5 Thử một lần code mà không dùng If xem nào? 5 cách giúp lập trình viên tăng năng suất làm việc Lập trình viên, liệu bạn đã đủ “hấp dẫn” trong mắt… [TÀI LIỆU] Beginning Mobile App Development with React Native “Ở Việt Nam, cơ hội để thực sự làm về Trí… [ Học bổng] Học tiếng Nhật miễn phí cùng ” Akira”-… VWS2017 – Xây dựng Decentralized web trên nền tảng Blockchain Họp báo ra mắt sự kiện Vietnam Web Summit 2017 [Miễn phí] Tham gia sự kiện App Analytics Tools: con đường… 4 sai lầm có thể khiến các lập trình viên rời… Lương kỹ sư IT đi Nhật có thể lên đến 165… Phỏng vấn chuyên gia Machine Learning từ AdAsia về ứng dụng… 5 điều phiền toái nhất của CSS Thương hiệu cá nhân Dev quan trọng hơn bạn tưởng Tâm sự một coder: hãy dũng cảm thành thật với con… Web Dev: Cố gắng hoàn hảo sẽ cản trở bạn Thay đổi code, thay đổi thế giới Thương hiệu cá nhân Dev quan trọng hơn bạn tưởng Những tình huống “đứng hình” trong JavaScript Trang Chủ Công nghệ Từ Over-fitting trong Machine Learning đến những triết lý cuộc đời Từ Over-fitting trong Machine Learning đến những triết lý cuộc đời January 12, 2017 237 Chia sẻ Facebook Tweet Hà Nội một ngày cuối đông Chào các bạn, chúng ta chào đón nhau bằng một nụ cười thật tươi và chúc bạn một ngày mới với những niềm vui mới. Các bạn thân mến của tôi, có lẽ rằng với rất nhiều người, khoa học tự nhiên vốn là khoa học của những con số, những công thức, những giải thuật và vô hình chung người ta đã quan niệm rằng nó tách biệt hoàn toàn với khoa học xã hội – khoa học của các triết lý, khoa học của nhân sinh quan, thế giới quan… Chúng vốn dĩ đã là hai ngành khoa học chẳng thể tách rời nhưng có lẽ vì một lý do nào đó mà rất nhiều người đã không nhìn ra rằng có một sợi dây vô hình song rất bền chặt gắn kết chúng lại với nhau, rằng từ những con số tưởng chừng rất khô khan luôn luôn tiềm tàng một triết lý sống mà đôi khi chúng ta phải mất cả một cuộc đời để trải nghiệm. Điều mà tôi muốn nói đến ngày hôm nay không phải là một vấn đề mới, thậm chí còn là cũ mèm.  Over filting  – một thuật ngữ có lẽ đã quá quen thuộc đối với những người nghiên cứu các giải thuật thống kê và đặc biệt là lĩnh vực  Machine Learning  nhưng ẩn sâu trong nó là những bài học về nhân sinh thật sự kì diệu. Và bạn….chính bạn chứ không phải ai khác, bạn đã sẵn sàng cho con đường tìm ra mối liên hệ rất tự nhiên mà lại kì diệu đó chưa??? Bạn hiểu thế nào về Over-fitting Bản chất của  Machine Learning  chính là việc  sử dụng máy tính để mô hình hóa dữ liệu  rồi từ các mô hình đó chúng ta đưa ra các dự đoán cho tương lai. Chúng ta có thể sử dụng rất nhiều các thuật toán khác nhau phục vụ cho việc mô hình hóa dữ liệu của chúng ta, tìm kiếm ra các mối tương quan ẩn chứa trong tập dữ liệu mà chúng ta thu thập được trong quá khứ (chúng ta gọi đó là tập dữ liệu  huấn luyện ). Suy nghĩ của đại đa số người chính là một mô hình  tốt nhất  trên tập dữ liệu huấn luyện thì cũng sẽ tốt nhất khi áp dụng vào thực tế (tức là khi chạy trên tập dữ liệu  kiểm tra ). Điều này đôi khi là một sai lầm, bằng chứng là cho thấy có rất nhiều mô hình cho kết quả rất tốt đối với tập dữ liệu hiện tại nhưng lại hoàn toàn không thể đem áp dụng trên thực tế vì kêt quả dự đoán quá thấp. Hiện tượng mà mô hình dữ liệu  làm việc tốt  trên tập dữ liệu mẫu nhưng lại  bất lực  trước việc dự đoán dữ liệu thực tế được gọi là  Over Fitting  mà theo ngôn ngữ kiếm hiệp người ta gọi là  tẩu hỏa nhập ma . Chúng ta cùng xem xét một ví dụ như sau: Bài toán dự đoán độ hài lòng của các cặp đôi sau hôn nhân  Bằng các cuộc khảo sát nhiều cặp vợ chồng, các nhà nghiên cứu tâm lý học đã thu được một biểu đồ thể hiện được mức độ hài lòng của các cặp vợ chồng sau khi kết hôn. Chúng ta có thể thấy, nhìn chung thì mức độ hài lòng suy giảm theo thời gian, nhưng mối liên quan với thời gian không hẳn tuân theo phương trình đường thẳng. Trong 3 năm đầu, mức độ suy giảm khá nhanh, nhưng sau đó tăng trong năm thứ 4 và 5; sau 5 năm thành hôn thì mức độ hài lòng lại suy giảm nữa. Việc cần làm của những nhà nghiên cứu bây giờ là tìm ra  mô hình  hay  phương trình  tốt nhất để mô tả sự tương quan trong tập dữ liệu đó. Nếu gọi mức độ hài lòng là  y  và thời gian là  t  thì bản chất của chúng ta là đi tìm sự phụ thuộc  y = f(t) . Dễ thấy được ngay mô hình đơn giản nhất là mô hình  hồi quy tuyến tính , tức là đi tìm tham số  a  và  b  sao cho  y = a + b.t . Sử dĩ nó đơn giản vì phương trình chúng ta chỉ phụ thuộc vào duy nhất một tham số  b  liên quan đến thời gian  t  mà thôi. Mô hình này giải thích được 90% sự khác biệt của dữ liệu như hình dưới (hồi quy tuyến tính là  đường thẳng liền ): Tuy nhiên chúng ta có thể thấy được rằng mức độ hài lòng có sự biến thiên không tuân theo quy luật tuyến tính, điều này thể hiện ở việc mức độ hài lòng tăng vào năm thứ 4-5 và giảm sau đó. Điều này làm ta nghĩ đến một mô hình có bậc cao hơn. Bằng các kĩ thuật nâng bậc của mô hình với sự trợ giúp của các ngôn ngữ lập trình như  R  hay  Python  chúng ta dễ dàng tìm được mô hình bậc 2 với 2 tham số dạng: Mô hình này thể hiện bằng  đường nét đứt  như biểu đồ trên. Mô hình này thật sự tốt, nó giải thích được  93%  sự dị biệt của dữ liệu. Tuy nhiên chúng ta vẫn chưa coi thế là đủ, tư tưởng  được đằng chân lân đằng đầu  khiến chúng ta kì vọng có được mô hình tốt hơn giải thích được  100%  phương sai của biến  y . Việc này cũng chẳng khó khăn với các ngôn ngữ lập trình như  R  chỉ chưa đầy 3 phút chúng ta có thể tìm được một mô hình với 9 tham số, giải thích gần như hoàn toàn sự dị biệt của dữ liệu. Mô hình này thể hiện bằng đường cong in đậm trong hình trên.  Tuy nhiên , mục đích của mô hình được tạo ra không phải để giải thích dữ liệu hiện tại, mà để dự đoán được các dữ liệu tương lai sẽ như thế nào. Vì tương lai là thứ mà chúng ta chưa thể đoán biết được. Vậy câu hỏi đặt ra là 3 mô hình trên (1 tham số, 2 tham số, và 9 tham số) thì mô hình nào dự báo tốt nhất ??? Không ngạc nhiên khi mô hình 1 tham số tiên lượng mức độ hài lòng tiếp tục giảm trong năm 11, còn mô hình 2 tham số cũng tiên lượng giảm nhưng giảm một chút thôi. Nhưng điều kì lạ là mô hình 9 tham số tiên lượng rằng năm thứ 11 sau thành hôn thì mức độ hài lòng giảm như là xe hơi lao dốc xuống núi! Đành rằng mức độ hài lòng có thể suy giảm, nhưng không thể nào giảm đột ngột như mô hình 9 tham số dự báo như thế. Và chúng ta cần phải xem xét lại. Liệu rằng đã có gì đó không đúng chăng??? Nghịch lý : Mô hình giải thích được nhiều dữ liệu trong quá khứ nhất lại là mô hình tiên lượng tồi nhất! Người mà bạn chọn lựa làm bạn đời là ai??? Mỗi con người trong chúng ta đều có những tiêu chuẩn riêng để lựa chọn người sẽ đi cùng ta suốt cả cuộc đời. Chúng ta thôi không bàn đến những triết lý của  ngôn tình  như  chỉ cần tình yêu là đủ  ở đây. Tình yêu là chuyện của con tim, còn tiêu chuẩn để lựa chọn là chuyện của lý trí. Một người thiếu trái tim – người đó chết. Một người mất đi lý trí – người đó chẳng thể sống được. Vậy nên đứng trước những sự lựa chọn lớn lao của cuộc đời, chúng ta cần tuân thủ rất nhiều nguyên tắc, rất nhiều tiêu chuẩn. Tuy nhiên, thực tế lại chứng minh rằng chúng ta đừng lựa chọn quá nhiều, đừng áp đặt lên người khác quá nhiều tiêu chuẩn.  Over fitting  thể hiện trong việc chọn lựa bạn đời là việc chúng ta cố gắng áp đặt quá nhiều tiêu chuẩn cho người bạn đời của chúng ta nhưng lại quên mất rằng, tất cả những tiêu chuẩn đó chỉ được xem xét trong quá khứ và hiện tại, mà biểu hiện của con người đó ở tương lai như thế nào không ai có thể biết trước được. Và bạn có muốn người bạn đời của mình giống như một mô hình dữ liệu, dù rất tốt khi xem xét ở hiện tại nhưng hoàn toàn bất lực với những thay đổi ở tương lai không. Vậy nên: --------------------  Thông tin cho Dev  -------------------- Không có tiêu chuẩn để lựa chọn – bạn đang đánh cược với tương lai. Có quá nhiều tiêu chuẩn để lựa chọn – chính tương lai đang đánh cược với bạn Quá khứ chỉ là kỉ niệm. Đừng sống mãi trong chiếc bong bóng của riêng mình Vấn đề của  Over fitting  không hẳn chỉ là một vấn đề trong ngành khoa học máy tính. Đôi khi chúng ta thấy thật sự nó rất đời thường, rất nhân sinh. Có lẽ không ít người trong mỗi chúng ta đang ở trong một chiếc bong bóng vô hình mà không hay biết. Không ít người trong số chúng ta thường có xu hướng nói nhiều về quá khứ hơn là nhắc đến tương lai. Có rất nhiều người thích lặp đi lặp lại những chuyện  Xưa rồi Diễm , rằng ngày xưa tôi đã từng rất giỏi, ngày xưa tôi đã từng rất hạnh phúc, ngày xưa tôi đã….. Stop . Chúng ta là những con người đang sống ở hiện tại và tương lai là những điều chưa ai có thể biết trước. Đừng mải mê trong quá khứ vàng son và cũng đừng bi lụy vì một quá khứ lầm lỗi. Dù bạn là ai, bạn có quyền được quyết định chính tương lai của bạn và thay đổi để sống tốt hơn, sống đẹp hơn chưa bao giờ là muộn cả. Và nếu như ranh giới giữa những hạnh phúc hay khổ đau của quá khứ với những điều kì diệu của tương lai cũng mong manh như một bong bóng xà phòng, thị bạn đã sẵn sàng thoát ra khỏi chiếc bong bóng của mình đễ ngắm nhìn một thế giới tốt đẹp hơn chưa??? Đừng quá cầu toàn. Mọi thứ có thể thay đổi và quan trọng là biết thích nghi Bạn có tin là có những thứ đến Google cũng không có câu trả lời cho bạn không??? Đúng là như vậy đấy, tôi đang nói riêng về phương diện tìm kiếm tri thức thôi, chúng ta dã là quá nhỏ bé rồi. Đến cả Google cũng không thể nói rằng biết hết  100%  về mọi lĩnh vực thì huống gì chúng ta. Tôi biết có rất nhiều người rất cầu toàn, sự cầu toàn không phải là một cái gì đó sai trái, nó giúp cho con người ta biết chu toàn bổn phận và trách nhiệm của bản thân mình một cách nghiêm khắc hơn. Tuy nhiên, sự cầu toàn thái quá dễ dẫn con người ta đến cảm giác tiêu cực, cảm giác bị thất bại khi có một mục tiểu không hoàn thành. Có một điều bạn phải chấp nhận rằng: Chúng ta không bao giờ có thể là người hoàn hảo Có lẽ bạn thấy một con người rất giỏi về lập trình phần mềm và bạn ngưỡng mộ người đó vì đó là chuyên môn của bạn, nhưng chính bản thân người đó lại rất yếu kém ở một mảng khác ngoài chuyên môn ví dụ như nấu ăn chẳng hạn. Cuộc sống luôn cần những khoảng trống để lấp đầy, nếu một ai đó mà chẳng có một khoảng trống nào để lấp đầy thì họ thật cô đơn biết bao??? Đừng thần tượng hóa ai cả. Hi vọng càng nhiều thì thất vọng càng lớn Tôi biết có rất nhiều người thực sự rất đáng để chúng ta ngưỡng mộ, có rất nhiều người thực sự rất có khả năng trong một lĩnh vực nào đó và họ xứng đáng được xã hội công nhận. Tuy nhiên có rất nhiều người mắc một căn bệnh mà giới trẻ bây giờ gọi là bênh  Fan cuồng . Tôi còn nhớ một câu chuyện kể rằng khi  Bi Rain  – một diễn viên, một ca sĩ nổi tiếng của Hàn Quốc – trong một lần sang lưu diễn tại Việt Nam đã thu hút rất nhiều Fan hâm mộ trẻ đến cổ vũ. Việc đó thì cũng chẳng có gì là lạ cho đến một ngày người ta đưa lên mạng một đoạn video khi  Bi Rain  rời đi thì một đám đông các Fan cuồng chạy lên chiếc ghế mà anh ta ngồi thi nhau  hít hà  chiếc ghế đó như thể hít một thứ sinh khí từ trời ban xuống vậy. Thật là ngô nghê buồn cười, sự thần tượng hóa một ai đó giống như việc chúng ta đang cố gắng tập trung một cách thái quá vào dữ liệu mà chúng ta thu thập được mà quên đi mất bản chất của vấn đề. Thậm chí chính sự thần tượng hóa đó lại gây ra những hệ lụy vô cùng to lớn khi chúng ta biết được sự thật rằng trên thực tế có thể thần tượng của ta không được như chúng ta mong đợi.  Thần tượng hóa  là một thể hiện của  Over fitting  trong đời sống con người, nó có thể làm con người ta lạc quan hơn, nhưng cũng có thể làm cho người ta thất vọng hơn khi quay về với thực tại. Vậy nên, tốt nhất là  Đừng thần tượng hóa ai cả Và cuối cùng. Hãy học cách sống trung dung Tôi biết rằng khi đứng trước một vấn đề của cuộc sống, có rất nhiều người hay có thói quen suy nghĩ rất nhiều, đôi khi là phức tạp hóa vấn đề đến mức tối đa. Tuy nhiên suy nghĩ nhiều quá có thể giúp chúng ta giải thích được những gì mình quan sát trong quá khứ (và hiện tại), nhưng nó không hẳn giúp ích chúng ta trong quyết định cho tương lai mà có thể làm cho tình hình rối lên. Trái ngược lại có những người chẳng suy nghĩ, hoặc suy nghĩ rất ít trước một vấn đề. Thái độ  mặc đời trôi  của những người như vậy cũng không cho chúng ta một cách giải quyết hiệu quả cho tương lai. Hiện tượng đó tương tự như  under-fitting  trong khoa học dữ liệu, nó bỏ sót và tiên lượng kém chính xác. Thành ra, nghệ thuật của mô hình hoá các mối liên quan là tìm một mô hình không có quá nhiều tham số mà cũng không có quá ít tham số. Nghệ thuật này cũng là nghệ thuật sống:  tìm cách sống trung dung. . Hãy sống ôn hòa, bình thản trước mọi vấn đề của cuộc sống, không tầm thường hóa nhưng cũng không cường điều hóa vấn đè. Cần phải giữ cho ý nghĩ và việc làm luôn luôn ở mức trung hòa, không thái quá, không bất cập và phải cố gắng ở đời theo nhân, nghĩa, lễ, trí, tín, cho thành người quân tử –  Sách Trung Dung – Tử Tư Lời kết Từ một vấn đề tưởng chừng đã quá quen thuộc như  Over fitting  vẫn luôn tiềm ẩn trong nó những triết lý nhân sinh mà chúng ta đáng học hỏi mà cuối cùng tựu chung lại đó là cách sống trung dung trước mọi vấn đề xảy ra trong cuộc đời. Đứng trước những sự việc xảy ra bạn sẽ chọn cách nhìn nhận nó như thế nào.  Và đứng trước cả vũ trụ rộng lớn – bạn sẽ chọn bạn là ai??? Facebook Twitter Bài viết trước Hướng dẫn tạo Cut-out Border với CSS Bài kế tiếp Lựa chọn hàng đầu về tuyển dụng IT tại Việt Nam “Tôi liều có tính toán” – anh Nguyễn Duy Vĩ nói gì? November 14, 2016 Thắc mắc về UX không biết hỏi ai thì hãy hỏi ở đây! September 29, 2016 “Làm PM, theo anh không cần biết về code, nhưng phải hiểu về SQL, database, những khái niệm cơ bản của code” December 24, 2016 Nếu việc yêu máy tính là sai trái, thì tôi cũng chẳng muốn đúng... July 28, 2017 Lương cao ư? Mấy ai hiểu cay đắng của lập trình viên Việt Nam February 17, 2017 Làm giàu với nghề lập trình viên – Hào quang đầy ảm đạm August 8, 2017 “Ở Việt Nam, cơ hội để thực sự làm về Trí tuệ nhân tạo còn quá ít, trong khi những thứ mang hình thù... October 21, 2017 [ Học bổng] Học tiếng Nhật miễn phí cùng ” Akira”- chắp cánh ước mơ October 19, 2017 VWS2017 – Xây dựng Decentralized web trên nền tảng Blockchain October 19, 2017 VỀ CHÚNG TÔI • Giấy phép thiết lập Mạng xã hội số 569/GP-BTTTT do Bộ Thông Tin và Truyền Thông cấp.\r\n• Cơ quan chủ quản: Công ty Cổ phần Applancer.\r\nTrụ sở: 179 Đường Nguyễn Đình Chính, Phường 11, Quận PN, TP.HCM  Liên hệ:  hieuld@applancer.net - Tel: 028 6264 5022 THEO CHÚNG TÔI",
          "relevance": "0",
          "title": "Từ Over-fitting trong Machine Learning đến những triết lý cuộc đời",
          "url": "https://techtalk.vn/tu-over-fitting-trong-machine-learning-den-nhung-triet-ly-cuoc-doi.html"
        },
        {
          "content": "Tìm kiếm trang web này Trang chủ LY NAM PHONG Lưu Tuấn Anh Nguyễn Văn Hải Các công cụ xử lý Trích lọc tiếng Việt từ HTML DongDu Download dữ liệu Giới thiệu về các nghiên cứu mới [Máy học]Learning Combination Features with L1 Regularization [phân loại]Text Categorization with All Substring Features  [in Japanese] Automatic Tree and String Based Wrapper Generation for Semi-structured Documents Extracting Structured Data from Web Pages Online Feature Selection using Grafting Kiến thức cơ bản về xử lý ngôn ngữ tự nhiên Khởi đầu NLP với Python Liblinear-thư viện học máy Lựa chọn đặc trưng (Feature selection) Machine Learning trong NLP Mô hình ngôn ngữ NLP là gì ? Phân nhóm dữ liệu (Clustering) Thuật toán tách từ (Tokenizer) Xử lý tiếng Việt bằng Python (1) Ứng dụng Pointwise để tách từ Nghiên cứu của tác giả Bài toán thêm dấu cho tiếng Việt Việt hoá Mecab Nhập môn Linux SHELL là gì SHELL mạnh nhất : zsh Tài nguyên ngôn ngữ tiếng Việt Khái yếu về corpus Khái yếu về từ điển Kế hoạch xây dựng tự động corpus từ nguồn Web Đặc trưng của tiếng Việt Tạp đàm seminar là gì Sơ đồ trang web Hoạt động gần đây của trang web Tác giả trang anh tháng hai 7, 2012 Kiến thức cơ bản về xử lý ngôn ngữ tự nhiên ‎ > ‎ Machine Learning trong NLP ‎ > ‎\n   Thuật toán Cây quyết định Trong lý thuyết quyết định, một cây quyết định là một đồ thị những quyết định và những kết quả có khả năng của chúng (bao gồm cả giá phải trả và độ rủi ro) được sử dụng để tạo ra một đường đi tới đích [4]. Cây quyết định là một dạng đặc biệt của cấu trúc cây được xây dựng để trợ giúp việc ra quyết định. Trong lĩnh vực học máy, cây quyết định là một mô hình dự đoán, có nghĩa là từ việc quan sát các item để rút ra kết luận về giá trị đích của item đó. Mỗi nút bên trong tương đương với một biến, mỗi cung đi tới một nút con tương ứng với giá trị có thể của biến đó. Các là tương ứng với giá trị đích được dự đoán cho các biến. Kỹ thuật học máy sử dụng việc xây dựng cây quyết định trên tập dữ liệu được gọi là học cây quyết định hay đơn giản chỉ là cây quyết định. Học cây quyết định cũng là một phương pháp rất thông dụng trong khai phá dữ liệu. Trong đó cây quyết định mô tả cấu trúc cây mà ở đó các lá đại diện cho các lớp và các nhánh cây biểu diễn sự kết hợp của các đặc trưng dẫn dắt tới việc phân lớp. Một cây quyết định có thể được học bằng cách chia tập nguồn thành các tập con dựa trên giá trị các thuộc tính kiểm tra [4], [5]. Quá trình này được lặp lại trên từng tập con thu được. Quá trình đệ quy sẽ kết thúc khi không thể chia tiếp được nữa hoặc khi từng phần tử của tập con được gán với một lớp đơn. Cây quyết định được mô tả bằng cách tính toán xác suất có điều kiện. Cây quyết định cũng có thể được mô tả như là một kỹ thuật tính toán và hỗ trợ toán học, kỹ thuật này hỗ trợ việc mô tả, phân loại và khái quát tập dữ liệu đưa vào. Dữ liệu đưa vào dạng ghi có dạng: (x, y) = (x1, x2, … ,xk, y ) Biến phụ thuộc y là biến mà chúng ta cố gắng để biết, phân lớp hay tổng quát hóa, còn các biến x1, x2,… là các biến giúp ta thực hiện công việc đó. Trong bài toán phân lớp văn bản, x là vector đặc trưng, y là phân lớp cần tìm.  So với các phương pháp khác trong Data Mining, phương pháp cây quyết định có những ưu điểm nổi bất như: - Rất dễ hiểu và dễ giải thích: mọi người đều có thể hiểu mô hình cây quyết định qua một số giải thích tổng quát ban đầu. - Dữ liệu dùng cho cây quyết định chỉ là những dữ liệu căn bản hoặc có thể không cần thiết. Một số kỹ thuật khác có thể đòi hỏi dữ liệu chuẩn, tạo các biến giả và loại bỏ đi các giá trị trống. - Có khả năng xử lý cả dữ liệu thực và dữ liệu mập mờ. Một số kỹ thuật khác chỉ sử dụng những tập dữ liệu đặc biệt chẳng hạn như mạng nơron có thể chỉ sử dụng các biến là số. - Có thể kiểm chứng mô hình bằng cách thử thống kê. - Có khả năng thực hiện tốt đối với dữ liệu lớn trong thời gian ngắn: một lượng lớn dữ liệu có thể được phân tích bằng máy tính cá nhân trong thời gian ngắn đủ để người sử dụng đưa ra quyết định dựa trên sự phân tích đó. Tuy nhiên sử dụng phương pháp cây quyết định có thể xảy ra hiện tượng overfit, tức là tồn tại một giả thuyết h phù hợp với tập ví dụ huấn luyện nhưng tiên đoán không chính xác bằng giả thuyết h’ ít phù hợp với tập ví dụ huấn luyện hơn so với h. Để giải quyết vấn đề này chúng ta phải dùng cách chặt bớt cây (pruning), bỏ bớt đi các nhánh dữ liệu nhiễu và dư thừa… Một vấn đề khác nữa của phương pháp cây quyết định là sự không an định của thuật toán. Tức là, dù chỉ 1 sự thay đổi nhỏ như thêm đỉnh, giảm đỉnh, thêm noise, ... thì kết quả của thuật toán sẽ khác đi rất nhiều.  Với những ưu, khuyết điểm như thế, cây quyết định cũng không phải là 1 phương pháp thường được sử dụng trong bài toán phân loại văn bản.  Comments Sign in | Recent Site Activity | Report Abuse | Print Page | Powered By Google Sites",
          "relevance": "0",
          "title": "Thuật toán Cây quyết định",
          "url": "http://viet.jnlp.org/kien-thuc-co-ban-ve-xu-ly-ngon-ngu-tu-nhien/machine-learning-trong-nlp/thuat-toan-cay-quyet-dinh"
        },
        {
          "content": "SEPTENI TECHNOLOGY Developer's Blog The SEPTENI TECHNOLOGY Blog CONNECT WITH US: Categories Agile Android Angular JS Automation Box BDD Business Company Continuous Integration Culture Data Science Design Design Pattern Diary Domain-Driven Design Frontend Developer Functional Programming Kotlin Life skills Machine Learning Marketing Mobile Games None Offshore Process React JS Recruitment Ruby Advanced Ruby Tutorial Box Scala Scrum Security Security Testing SQA TechNote Featured posts Sorry. No data so far. Archives September 2017 August 2017 July 2017 June 2017 May 2017 April 2017 March 2017 February 2017 January 2017 December 2016 November 2016 October 2016 September 2016 August 2016 July 2016 June 2016 May 2016 April 2016 March 2016 February 2016 January 2016 December 2015 November 2015 October 2015 September 2015 August 2015 July 2015 June 2015 May 2015 April 2015 March 2015 February 2015 January 2015 December 2014 November 2014 October 2014 September 2014 August 2014 July 2014 June 2014 May 2014 April 2014 March 2014 February 2014 January 2014 December 2013 November 2013 October 2013 September 2013 August 2013 July 2013 June 2013 May 2013 April 2013 March 2013 \t\tCopyright ©  2017  SEPTENI TECHNOLOGY Developer's Blog  All rights reserved.\r\n\t",
          "relevance": "1",
          "title": "Related posts",
          "url": "http://labs.septeni-technology.jp/technote/ml-10-regularization-overfitting-and-underfitting/"
        },
        {
          "content": "Tôi là Duyệt \nMachine Learning is Fun! (Vietnamese version)\n \nChuỗi bài viết  \"Machine Learning is Fun!\"  này mình lược dịch từ bài viết gốc của tác giả ageitgey. Mình tin chắc có rất nhiều bạn đã và đang quan tâm đến Machine Learning hiện nay. \"Machine Learning is Fun!\" chắc chắn sẽ mang cho bạn đến cho bạn cái nhìn từ cơ bản đến chuyên sâu nhất về thế giới Machine Learning. Bài gốc:  https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471 \nBạn đã từng nghe rất nhiều người nói về Machine Learning, nhưng chúng chỉ là các thông tin rất mù mờ? \nChuỗi bài viết này dành cho những ai muốn tìm hiểu về Machine Learning nhưng chưa biết bắt đầu từ đâu. Tôi cá rằng có rất nhiều người đã cố gắng đọc  bài viết về Machine Learning này trên Wikipedia , và họ phải rất vọng và bỏ cuộc vì những định nghĩa giải thích trong đó. \nMachine Learning là gì? Machine Learning  là một ý tưởng về một thuật toán tổng quát chung có thể nói cho bạn biết vài điều về các khía cạnh khác nhau của bộ dữ liệu, mà bạn không cần phải viết bất cứ dòng code đặc biệt nào để giải quyết vấn đề. Thay vì bạn viết code, bạn đổ dữ liệu vào các thuật toán và chúng sẽ tự xây dựng các logic dựa vào dữ liệu đó. \nVí dụ, một loại thuật toán cơ bản đó là phân lớp (classification algorithm), thuật toán này cho phép chia dữ liệu thành nhiều nhóm khác nhau. Một thuật toán dùng để nhận dạng chữ số viết tay (recognize handwritten numbers) có thể được sử dụng để phân loại email (thành  spam  và  không spam ), mà không cần phải code lại. \nHai bài toán trên không một thuật toán, nhưng khác dữ liệu. Thuật toán Machine Learning này là black-box, có thể được sử dụng để giải quyết nhiều bài toán phân lớp khác nhau. \n\"Machine Learning\" là một thuật ngữ chung bao hàm rất nhiều thuật toán như trên. \nHai loại thuật toán Machine Learning \nThuật toán của Machine Learning được chia thành hai nhóm lớn - học giám sát ( supervised learning ) và học không giám sát ( unsupervised learning ). Sự khác nhau giữa hai nhóm này đơn giản, nhưng rất quan trọng. \nSupervised Learning \nGiả sử bạn là một người làm về bất động sản. Công ty của bạn phát triển nhanh, bạn tuyển hàng loạt thực tập. Nhưng có một vấn đề - bạn có thể dễ dàng nhìn lướt qua và đánh giá chính xác giá trị của một ngôi nhà, nhưng với các thực tập viên không có kinh nghiệm, họ không biết mỗi căn trị giá bao nhiêu. \nĐể giúp đỡ các bạn thực tập này, bạn quyết định viết một ứng dụng nhỏ có thể ước lượng được giá của một ngôi nhà dựa vào  diện tích, khu vực lân cận, ...  \nVà bạn ghi lại thông tin của mọi căn nhà được bán trong thành phố, trong vòng 3 tháng. Với mỗi ngôi nhà, bạn ghi lại mọi thông tin:  số phòng ngủ, diện tích (feet vuông), neighborhood, ...  Nhưng quan trọng nhất là  giá (price)  cuối cùng của căn nhà được bán: Chúng ta có được \"training data\" \nVới dữ liệu \"training data\" như trên, chúng ta muốn viết một ứng dụng có thể ước tính được giá của một căn nhà tương tự khác: Chúng ta muốn sử dụng training data để dự đoán giá của những ngôi nhà khác. \nĐây được gọi là  supervised learning . Bạn biết được giá mỗi căn nhà được bán đi, nói cách khác, bạn biết được câu trả lời của bài toán, và có thể từ đây suy ra được logic của vấn đề. \nĐể xây dựng ứng dụng này, bạn cho training data về mỗi ngôi nhà này vào một thuật toán machine learning. Thuật toán sẽ cố gắng tìm ra loại tính toán nào để các con số có thể work. \nNó giống như việc tìm các toán tử trong bài tập toán hồi lớp 1 chúng ta vẫn hay được học: \nTừ bảng trên, bạn có thể tìm ra được các phép toán nào để có được kết quả bên phải? Bạn biết bạn có nghĩa vụ phải \"làm gì đó\" với những con số ở bên trái để có được câu trả lời ở bên phải. \nVới  supervised learning , bạn đang để máy tính phải tìm ra những quan hệ đó cho bạn. Và một khi bạn biết những phép toán nào cần để giải một số bài toán trên, bạn có thể giải bất kỳ bài toán khác cùng loại! \nUnsupervised Learning \nQuay lại với ví dụ bất động sản, giả sử bạn  không biết  thông tin gì về  giá . Chỉ với thông tin về diện tích, vị trí, ... bạn vẫn có thể làm được vài thứ hay ho. Đây được gọi là  unsupervised learning . Mặc dù bạn không thể dự đoán được giá nhà, bạn vẫn có thể làm vài chuyện hay khác với Machine Learning \nBài toán này giống như ai đó đưa cho bạn một đống hồ sơ và bảo rằng  \"Tao không biết mấy con số này có nghĩa gì, nhưng mà tao nghĩ mày có lẽ sẽ tìm ra được một pattern nào đó, hoặc chia thành các nhóm, hoặc là một cái gì khác. OK?\" \nVậy chúng ta có thể làm gì với data này? Với người mới bắt đầu, bạn có thể có một thuật toán để tự động chia dữ liệu thành các nhóm thị trường khác nhau.  Có thể bạn sẽ tìm ra rằng người mua nhà ở khu vực gần trường đại học thích mua nhà nhỏ với nhiều phòng ngủ, nhưng người mua nhà ở vùng ngoại ô thích nhà có 3 phòng ngủ hơn, diện tích lớn hơn.  Biết được những loại khách hàng này sẽ giúp ích rất nhiều tới chiến lược kinh doanh.  \nMột điều thú vị nữa là bạn có thể làm là tự động xác định các ngôi nhà đặc biệt khác biệt (outlier). Bạn có thể tập trung những người bán hàng giỏi nhất vào những ngôi nhà thuộc dạng outlier này (biệt thự lớn, giá trị cao), vì mức hoa hồng sẽ cao hơn.   \nSupervised learning sẽ được tập trung giới thiệu ở những phần còn lại của bài viết này, nhưng không có nghĩa là unsupervised learning vô dụng hơn hoặc không thú vị bằng. Thực tế, unsupervised learning ngày càng quan trọng hơn bởi vì nó có thể được áp dụng mà không cần có nhãn (label) mà vẫn cho được kết quả đúng. Note: có rất  nhiều loại thuật toán machine learning khác . Nhưng supervised learning và unsupervised learning là hai nhóm cơ bản tốt nhất khi bắt đầu nghiên cứu vào machine learning. \nCó đúng là việc ước lượng (estimate) được giá của một ngôi nhà là \"learning\" hay không? \nVới con người, bộ não của chúng ta có thể tiếp cận mọi vấn đề và học cách giải quyết nó mà không cần chỉ dẫn chi tiết nào. Nếu bạn bán nhà trong một thời gian dài, theo bản năng bạn có thể cảm giác được đâu là giá chính xác của một căn nhà. Mục tiêu của ngành AI là lặp lại được điều đó với máy tính. \nNhưng hiện tại các thuật toán của machine learning của máy tính vẫn chưa đủ tốt, chúng chỉ giải quyết được những vấn đề rất cụ thể và giới hạn. Chúng ta có lẽ nên định nghĩa  \"learning\"  ở đây là  \"tìm ra được cách giải một bài toán cụ thể dựa vào vài ví dụ\". \nKhông may  \"tìm ra được cách giải một bài toán cụ thể dựa vào vài ví dụ\"  không phải là cái tên hay, thay vào đó mọi người gọi ngắn gọn là  \"Machine Learning\" . \nLet’s write that program! \nVậy, làm thế nào để có thể code được chương trình cho ví dụ trên? Bạn có thể suy nghĩ một chút trước khi đọc phần tiếp theo. \nNếu bạn không biết gì về machine learning, theo cách thông thường bạn có thể cố viết ra một số quy luật cơ bản để ước lượng giá nhà như sau: def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):\n  price = 0\n  # In my area, the average house costs $200 per sqft\n  price_per_sqft = 200\n  if neighborhood == \"hipsterton\":\n    # but some areas cost a bit more\n    price_per_sqft = 400\n  elif neighborhood == \"skid row\":\n    # and some areas cost less\n    price_per_sqft = 100\n  # start with a base price estimate based on how big the place is\n  price = price_per_sqft * sqft\n  # now adjust our estimate based on the number of bedrooms\n  if num_of_bedrooms == 0:\n    # Studio apartments are cheap\n    price = price — 20000\n  else:\n    # places with more bedrooms are usually\n    # more valuable\n    price = price + (num_of_bedrooms * 1000)\n return price \nCho dù bạn bỏ ra thêm hàng giờ nữa để code chương trình này, chương trình của bạn vẫn sẽ không bao giờ hoàn hảo và khó để bảo trì. \nTại sao bạn không để máy tính giúp bạn tự tìm ra kết quả? def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):\n  price = <computer, please do some math for me>\n  return price \nMột ý tưởng để giải quyết vấn đề này là bạn biết được  price  được tính bằng tổ hợp của  number of bedrooms ,  square footage  và  neighborhood . Nếu bạn có thể chỉ ra mỗi thành phần tác động đến price cuối cùng như thế nào, vậy có lẽ sẽ tồn tại một tỉ lệ nào đó để kết hợp 3 chỉ số trên để có được  price . \nVới suy nghĩ trên, bạn có thể rút lại chương trình ban đầu thành như này: def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):\n price = 0\n # a little pinch of this\n price += num_of_bedrooms * .841231951398213\n # and a big pinch of that\n price += sqft * 1231.1231231\n # maybe a handful of this\n price += neighborhood * 2.3242341421\n # and finally, just a little extra salt for good measure\n price += 201.23432095\n return price \nChúng ta có những con số đặc biệt sau:  0.841231951398213 ,  1231.1231231 ,  2.3242341421  và  201.23432095 . Đây sẽ là các trọng số  weights . Nếu bạn có thể tìm ra được weights hoàn hảo vừa khớp cho mọi căn nhà, chương trình của chúng ta có thể predict được giá nhà! \nMột cách ngớ ngẩn để tìm ra weight tốt nhất mà mình có thể làm là: Bước 1:  Đặt tất cả weight là  1.0 def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):\n  price = 0\n  # a little pinch of this\n  price += num_of_bedrooms * 1.0\n  # and a big pinch of that\n  price += sqft * 1.0\n  # maybe a handful of this\n  price += neighborhood * 1.0\n  # and finally, just a little extra salt for good measure\n  price += 1.0\n  return price Bước 2:  Với dữ liệu training data mọi căn nhà mà bạn biết giá, sử dụng chương trình trên để đoán giá và xem thử giá này cách xa với giá thực tế bao nhiêu: Sử dụng function để predict giá nhà \nVí dụ với dòng đầu tiên, thực tế nó được bán với giá $250,000, nhưng function của chúng ta đoán nó có giá $178,000, chênh lệch $72,000. \nBây giờ bạn cộng tổng các bình phương giá chênh lệch của mỗi căn nhà trong tập dữ liệu. Giả sử như bạn có 500 căn nhà, và tổng bình phương chênh lệch từ hàm đoán giá của chúng ta so với thực tế là $86,123,373 - con số này thể hiện mức độ \"lỗi\" của chương trình của chúng ta. \nTiếp đến ta lấy tổng đó chia cho 500 để lấy trung bình độ chênh lệnh của mỗi ngôi nhà. Gọi nó là độ lỗi trung bình  cost function . \nNếu bạn có được cost bằng 0 nếu thay đổi các weights, function của bạn sẽ hoàn hảo. Có nghĩa là trong mọi trường hợp, function sẽ đoán chính xác giá của mọi ngôi nhà được đưa vào theo dữ liệu. \nVà đây cùng là nhiệm vụ của chúng ta: làm sao để có được cost thấp nhất có thể bằng cách thử các weights khác nhau. Bước 3:  Lặp lại bước 2 với  mọi tổ hợp weights có thể có . Tìm ra tổ hợp nào giúp ta có được cost gần 0 nhất. Một khi bạn tìm ra được tập weights này, bài toán được giải quyết! \nThật đơn giản đúng không? Bạn lấy một vài dòng dữ liệu, thực hiện 3 bước đơn giản và bạn có được chương trình có thể đoán được giá của mọi căn nhà. \nNhưng khoan, hay xem lại. Có một vài điều sẽ khiến bạn ngạc nhiên: Phương pháp \"ngớ ngẫn\" trên có thể vượt lên đánh bại các chuyên gia. Function của bạn có được một cách ngớ ngẫn. Nó không biết \"square feet\" hay \"bedrooms\" là gì. Tất cả những gì nó biết là tìm ra con số để có được kết quả đúng. Bạn cũng sẽ không biết tại sao các weights đó lại giúp function trả về kết qủa đúng. Vì thế bạn cũng sẽ không thể hiểu để chứng minh rằng nó hoạt động chính xác.  Tưởng tượng rằng thay vì bạn truyền giá trị cho các tham số \"sqft\" hay \"num_of_bedrooms\", predict function có thể nhận một mảng các con số. Ví dụ như mỗi con số là độ sáng của pixel ảnh được chụp từ camera gắn ở trước xe hơi của bạn. Chương trình thay vì dự đoán giá nhà, bạn có thể sẽ có chương trình \"degrees_to_turn_steering_wheel\" (điều chỉnh vô lăng xe hơi). Bạn mới vừa viết ra chương trình để xe hơi tự lái!! \nThật điên rồi đúng không :v  \nThử \"mọi tổ hợp weights có thể có\" ở bước 3 \nDĩ nhiên bạn hoàn toàn có thể thử mọi tổ hợp số, hoặc là bạn sẽ có kết quả tốt, hoặc là bạn sẽ thử suốt đời mới xong.  \nĐể tránh điều này, toán học có nhiều  cách làm \"thông minh\"  để nhanh chóng tìm ra các weights này mà không cần phải thử quá nhiều lần. Đây là một cách: \nĐầu tiên, viết một biểu thức đơn giản để biểu diễn cost ở bước 2 ở trên: cost function. \nBây giờ chúng ta viết lại dưới dạng một biểu thức với ký hiệu toán machine learning (không hiểu không sao, có thể bỏ qua): θ biểu diễn cho weights, J(θ) có nghĩa là cost của weight hiện có. \nCông thức trên biểu diễn độ sai của function ước lượng price của chúng ta với tập weights θ.  \nNếu chúng ta vẽ hết tất cả giá trị của biểu thức J(θ) với các weights có thể có ứng với number_of_bedrooms và sqft, biểu đồ sẽ có thể có dạng như sau: Trục đứng thể hiện cost. Đồ thị của cost function sẽ có dạng hình cái bát. \nTrên hình trên, điểm thấp nhất màu xanh ứng với nơi có cost thấp nhất - vì thế chương trình sẽ có độ lỗi thấp nhất. Điểm càng cao sẽ có độ lệch/lỗi càng cao. Vì thế nếu chúng ta có thể tìm được weights đưa chúng ta đến điểm thấp nhất trên đồ thị, chúng ta sẽ tìm ra được câu trả lời! \nVì thế, chúng ta cần điều chỉnh weights, giống như việc \"đi xuống thung lũng\" trong đồ thị để tìm được điểm thấp nhất. Nếu như chúng ta điều chỉnh từng chút một và luôn đi xuống, ta sẽ tìm được điểm cực tiểu mà không cần phải thử quá nhiều weights. \nNếu bạn còn nhớ trong môn Đại số, nếu bạn đạo hàm một hàm số, bạn sẽ biết được hàm số đồng biến (hướng lên) hay nghịch biến (hướng xuống) tại mọi điểm tiếp tuyến, có nghĩa là bạn sẽ biết được độ dốc của mọi điểm trên đồ thị. \nVì vậy, nếu chúng ta đạo hàm cost function, tính giá trị tại điểm đạo hàm đó, trừ giá trị này với từng weight, từng bước như vậy sẽ giúp ta di chuyển dần đến điểm thấp nhất của đồ thị. \nỞ trên là một cách tốt nhất để tìm weights tối ưu, người ta gọi nó là  batch gradient descent . Nếu bạn quan tâm muốn đi sâu hơn, có thể đọc tiếp bài viết này:  http://hbfs.wordpress.com/2012/04/24/introduction-to-gradient-descent/ . \nNhững thứ có thể bị skip qua nhanh \nThuật toán 3 bước được giới thiệu ở trên được gọi là  multivariate linear regression . Bạn có thể ước lượng được biểu thức để có thể fit tất cả dòng dữ liệu đang có. Sau đó bạn dùng biểu thức để đoán giá của những căn nhà khác, dựa vào giá của những căn có trong dữ liệu qúa khứ. Đây là một ý tưởng rất tuyệt, và có thể triển khai được trong thực tế. \nNhưng ý tưởng tôi đã trình bày chỉ đúng với trường hợp đơn giản, sẽ có vài ngoại lệ. Vì giá nhà đất không phải lúc nào cũng theo một đường, một quy luật cố định. \nMay mắn là có nhiều thuật toán khác có thể giải quyết vấn đề này, xử lý được dữ liệu phi tuyến tính như  neural networks  hoặc  SVMs  với  kernels . Hoặc cũng có thể sử dụng linear regression khéo léo hơn, cho phép tạo ra đường fit nhất với dữ liệu. \nTương tự, tôi cũng bỏ qua trường hợp  overfitting . Đây là trường hợp các weights khớp và đoán đúng mọi căn nhà trong dữ liệu training, nhưng không đúng với những căn khác ngoài tập dữ liệu. \nCó nhiều cách để giải quyết vấn đề này như  regularization  và sử dụng  cross-validation  trên data set. Học cách để giải quyết những vấn đề trên cũng là một mục tiêu để ứng dụng thành công machine learning.  \nMachine Learning toàn năng? \nMột khi nhìn thấy cách mà machine learning giải quyết những vấn đề rất phức tạp một cách dễ dàng (như nhận diện khuôn mặt, chữ viết), người ta thường sẽ nghĩ rằng machine learning sẽ giải quyết được mọi vấn đề nếu như có đủ data. \nNhưng, hãy nhớ rằng machine learning chỉ có thể hoạt động nếu vấn đề có-thể-giải-quyết được với dữ liệu bạn đang có. \nVí dụ, bạn xây dựng mô hình dự đoán giá nhà dựa vào các loại chậu cây trồng trước nhà, nó sẽ không bao giờ hoạt động. Giá nhà và chậu cây chẳng liên quan gì nhau cả. \nVì vậy hãy nhớ nếu một chuyên gia  không  thể sử dụng dữ liệu của giải quyết vấn đề, thì máy tính cũng vậy. \nLàm thế nào để tìm hiểu nhiều hơn về Machine Learning \nTác giả đã tạo một khóa học  từng bước một về chuỗi bài viết này, kể cả viết code . \nNếu bạn muốn đi sâu hơn, hãy thử khóa học  Machine Learning trên Coursera của Andrew Ng . Ngoài ra, bạn cũng có thể thử đủ loại thuật toán trong thư viện  Scikit-learn  của Python. Đăng ký nhận bài viết mới qua email ❤  by  duyetdev",
          "relevance": "0",
          "title": "Tôi là Duyệt",
          "url": "http://blog.duyet.net/2017/08/machine-learning-is-fun.html#.WbIy1R9SDIU"
        },
        {
          "content": "\ntuan's blog\n \"Đại số đạo đức\" Nhà\ntiến hoá học Charles Darwin là một người rất cẩn thận. Trước khi đi đến quyết định\nthành hôn, ông ngồi xuống suy tính và cân nhắc lợi và hại của hôn nhân, và những\nsuy nghĩ này được viết xuống trong cuốn nhật kí năm 1838. Ông liệt kê những lợi\ních của hôn nhân như có người đồng hành trong đời, có người chit-chat như nghe\nnhạc, và có người chăm sóc khi về già. Nhưng ông cũng nghĩ đến những điều bất lợi\ncủa hôn nhân như mất thì giờ ghê gớm, mất tự do đi đây đó mà ông muốn, thêm lo\nlắng bởi con cái, và có ít tiền hơn để ... mua sách ( Hình 1 ). Tất cả chỉ giới hạn trong 1 trang giấy.  Cuối\ncùng, ông quyết định thành hôn, và ông viết xuống kết luận này bằng kí hiệu QED\nmà có lẽ nhiều người trong chúng ta đều biết. Ông thành hôn với người em họ là\nEmma Wedgwood vào ngày 29/1/1839. Hai người có đến 10 người con. Có thể nói là\nmột cuộc hôn nhân thành công.  Những\nsuy nghĩ và cân nhắc của Darwin có thể xem là một sự tính toán. Benjamin\nFranklin gọi những \"tính toán\" định tính là \" Moral Algebra \" -- đại số đạo đức.\nNhiều người trong chúng ta có lẽ nghĩ rằng những quyết định quan trọng trong đời\ncần phải dựa vào đại số đạo đức càng nhiều càng tốt. Thoạt đầu mới nghe qua thì\ncũng có lí, vì đúng là suy nghĩ càng nhiều, chúng ta càng có nhiều lựa chọn và\ntừ đó đi đến quyết định tối ưu. Nhưng trong thực tế thì không hẳn như thế: suy\nnghĩ nhiều chưa chắc đã tốt. Over-fitting là gì?  Để\nhiểu vấn đề này, chúng ta có thể xem việc cân nhắc của Darwin như là một cách\nmô hình dữ liệu. Mô hình dữ liệu có nghĩa là tìm phương trình để mô tả các mối\ntương quan dựa trên dữ liệu thực tế, mà tiếng Anh gọi là \"model\nfitting\". Có nhiều mô hình có thể sử dụng để mô tả một mối liên quan. Vấn\nđề là làm sao tìm mô hình tốt nhất. Nếu mối liên quan cần 2 tham số, mà mô hình\ndùng 1 tham số thì được gọi là  under-fitting ;\nnhưng nếu mô hình dùng đến 5 tham số thì sẽ xảy ra tình trạng  over-fitting . Tương tự, suy nghĩ nhiều\nquá sẽ dẫn đến over-fitting, hay nói theo ngôn ngữ dân gian là \" tẩu hoả nhập ma .\" Có\nthể minh hoạ cho vấn đề over-fitting qua một ví dụ về mối tương quan giữa mức độ\nhài lòng trong cuộc sống và thời gian 10 năm sau thành hôn. Các nhà nghiên cứu\ntâm lí học thực hiện một nghiên cứu trên một số cặp vợ chồng và hỏi họ về sự\nhài lòng trong đời sống sau khi thành hôn (1), và biểu đồ dưới đây ( Hình 2 ) trình bày mối tương quan đó.\nNhưng có thể thấy, nhìn chung thì mức độ hài lòng suy giảm theo thời gian,\nnhưng mối liên quan với thời gian không hẳn tuân theo phương trình đường thẳng.\nTrong 3 năm đầu, mức độ suy giảm khá nhanh, nhưng sau đó tăng trong năm thứ 4\nvà 5; sau 5 năm thành hôn thì mức độ hài lòng lại suy giảm nữa.  Vấn\nđề của chúng ta là tìm một phương trình (hay mô hình) tốt nhất để mô tả mối\nliên quan đó. Gọi mức độ hài lòng là y, và thời gian sau thành hôn là t, mô\nhình đơn giản nhất là hồi qui tuyến tính đơn giản (tức chỉ có 1 tham số) dưới dạng\ny = a + b*t (trong đó a là điểm khởi đầu, và b là tham số liên quan đến t). Mô\nhình này mô tả khá tốt mối liên quan (xem  Hình\n3 , đường đứt đoạn). Mô hình này giải thích được 90% sự khác biệt của dữ liệu.\n Nhưng\nvì trong thực tế mức độ hài lòng tăng vào năm thứ 4-5 và giảm sau đó, nên có lẽ\nmô hình tốt hơn là mô hình đa thức bậc hai, hay nói trắng ra là phương trình bậc\nhai: y = a + b*t + c*t^2 (trong đó t^2 là t bình phương và c là tham số mới\nliên quan đến t^2). Mô hình 2 tham số này (đường đứt đoạn dài) quả thật tốt hơn\nmô hình tuyến tính 1 tham số. Nhưng mô hình 2 tham số này giải thích được 93%\nphương sai của y, tức là vẫn còn 7% chưa giải thích được.  Nhưng\nchúng ta thử \"thừa thắng xông lên\" tìm mô hình giải thích 100% phương\nsai của y xem sao. Với máy tính và chương trình R, chỉ cần 2 phút là chúng ta\nđã có mô hình 9 tham số có thể giải thích gần 100% phương sai của y. Mô hình\nnày thật là tốt, tốt nhất so với hai mô hình đơn giản kia.  Nhưng\nchúng ta đừng quên rằng một mục đích khác của mô hình hoá dữ liệu là tiên lượng\n-- tiên lượng tương lai. Tương lai là cái mà chúng ta chưa quan sát được. Câu hỏi\nlà 3 mô hình trên (1 tham số, 2 tham số, và 9 tham số) thì mô hình nào dự báo tốt\nnhất cho năm 11, 12, v.v.  Không ngạc\nnhiên khi mô hình 1 tham số tiên lượng mức độ hài lòng tiếp tục giảm trong năm\n11, còn mô hình 2 tham số cũng tiên lượng giảm nhưng giảm một chút thôi.  Nhưng điều kì lạ là mô hình 9 tham số tiên lượng\nrằng năm thứ 11 sau thành hôn thì mức độ hài lòng giảm như là xe hơi lao dốc xuống\nnúi! Đành rằng mức độ hài lòng có thể suy giảm, nhưng không thể nào giảm đột ngột\nnhư mô hình 9 tham số dự báo như thế. Có cái gì nghịch lí ở đây:  mô hình giải thích nhiều dữ liệu nhất lại là\nmô hình tiên lượng tồi nhất !  Thật\nra, \"hiện tượng\" mà mô hình giải thích mối tương quan tốt nhất nhưng\ntiên lượng tồi nhất không phải là mới, vì nó đã được các nhà khoa học thống kê\nhọc phát hiện từ xưa (thời Mosteller) và đặt tên là over-fitting. Đối nghịch với\nover-fitting là under-fitting. Mô hình hồi qui tuyến tính 1 tham số được xem là\nunder-fitting. Mô hình 9 tham số được gọi là over-fitting. Có lẽ mô hình đa thức\nbậc 2 là tối ưu nhất. Tôi nói \"có lẽ\" là vì chúng ta chưa làm xét\nnghiệm để xác định mô hình nào là tối ưu; chúng ta mới dùng cảm quan và trực\ngiác mà thôi.  \"Mặc áo\" cho dữ liệu  Như\nvậy, over-fitting xảy ra khi mô hình có nhiều tham số hơn cần thiết. Tình trạng\nnày cũng giống y chang việc chọn quần áo. Nếu quần áo quá chật hay quá rộng đều\ntạo ấn tượng không tốt, mà còn bất tiện. Nhưng chọn quần áo vừa thân hình một\ncá nhân, ngoài sở thích cá nhân, còn là một nghệ thuật. Do đó, việc mô hình dữ\nliệu khoa học cũng có thể ví von như là mặc áo cho dữ liệu.  Với\ncách ví von đó, tôi nghĩ có thể xem nhà thống kê học như là người thợ may. Người\nthợ may, trước khi tiến hành cắt vải, phải đo lường cẩn thận (hay nói theo ngôn\nngữ nhà nghề là lấy ni, tấc) để có thể cắt vải đúng kích thước. Nhà thống kê học\ncũng như thế: trước khi chọn mô hình cho dữ liệu, họ phải xem xét phân bố của dữ\nliệu, đơn vị đo lường, chuẩn hoá dữ liệu (standardization), và đánh giá các mối\nliên quan, trước khi chọn mô hình thích hợp. Các thông số của bộ quần áo chính\nlà tham số của mô hình thống kê. Cái software để ước tính tham số chính là cái\nmáy may.   Nhưng\nngười thợ may có kinh nghiệm còn phải xem xét đến khía cạnh thẩm mĩ, họ phải\nnhìn người khách hàng, cân nhắc giữa sắc diện và cấu trúc thân thể, để đi đến màu\nsắc của vải, chọn chất liệu, để sau cùng có một bộ đồ không chỉ vừa vặn mà còn\nthanh nhã. Tương tự, nhà thống kê học có kinh nghiệm còn phải quan tâm đến cách\ntrình bày dữ liệu một cách đẹp đẽ và trang nhã qua thiết kế biểu đồ. Biểu đồ phải\ncó phẩm chất tốt, có màu sắc và font chữ ai cũng đọc được (chứ không phải loại\nbiểu đồ Excel). Nhà thống kê học còn phải quan tâm đến ý nghĩa của kết quả phân\ntích (chứ không chỉ cho ra những kết quả vô nghĩa). Cái khác biệt giữa một nhà\nkhoa học và một kĩ thuật viên là ở chỗ này: nhà khoa học phải hiểu biết ý nghĩa\ncủa kết quả phân tích, còn kĩ thuật viên thì chỉ quan tâm đến sự chính xác và mẹo\ntính toán nhanh hơn. (Ở đây, không nói ai quan trọng hơn ai, vì cả hai đều quan\ntrọng).   Ý nghĩa cuộc sống của over-fitting  Tình\ntrạng này cũng giống như suy nghĩ nhiều quá có thể giúp chúng ta giải thích được\nnhững gì mình quan sát trong quá khứ (và hiện tại), nhưng nó không hẳn giúp ích\nchúng ta trong quyết định cho tương lai mà có thể làm cho tình hình rối lên.\nNhưng under-fitting thì lại bỏ sót và tiên lượng kém chính xác.  Thành ra, nghệ thuật của mô hình hoá các mối\nliên quan là tìm một mô hình không có quá nhiều tham số mà cũng không có quá ít\ntham số . Nghệ thuật này cũng là nghệ thuật sống:  tìm cách sống trung dung .  \"Trung\ndung\", Tử Tư khuyên người  quân tử \nnên giữ cách sống trung hòa, không thái quá. Do đó, tôi nghĩ mô hình tốt nhất\ntrong khoa học cũng giống như mẫu người \"quân tử\" của Tử Tư vậy. Người\nquân tử hành xử giữ thế trung bình giữa hai thái cực, thì mô hình tốt cũng\nchính là mô hình nằm giữa hai thái cực qua đơn giản và quá phức tạp.  Over-fitting\ncó ý nghĩa trong vấn đề sùng bái thần tượng mà tiếng Anh họ gọi là  idolatry . Vào thời xa xưa, người ta sùng\nbái thần thánh và nặn tượng cho họ. Dù chỉ là đất sét thôi, nhưng dân chúng ai\ncũng vái lạy vì người ta nghĩ các tượng đất sét này là biểu tượng của thần\nlinh. Trong các thể chế toàn trị như Cuba và Romania, chúng ta hay thấy nạn\nsùng bái thần tượng, mà theo đó người dân được dạy và buộc phải xem họ như là\nthần thánh, là cha mẹ dân tộc.  Sùng bái\nthần tượng giúp cho người ta cảm thấy thoải mái vì có cảm hứng và có người để\ncầu cạnh (và cầu nguyện) nhưng nó làm người ta quên đi hiện tình. Sự sùng bái\nthần tượng là một hình thức của over-fitting trong đời sống tâm linh.\nOver-fitting cũng giống như là một hình thức thần tượng hoá dữ liệu (dân trong\nnghề gọi là \"data idolatry\"), và đó cũng là hệ quả của việc tập trung\nthái quá vào những gì chúng ta có thể đo lường được, nhưng thiếu tập trung vào\nvấn đề.  Tình\ntrạng over-fitting còn có thể giải thích tại sao những thực phẩm ngon miệng\n[nói chung] lại có hại cho sức khoẻ. Những chất dinh dưỡng cơ bản như đường,\nmuối, mỡ đều rất cần thiết cho sự tiến hoá của nhân loại qua hàng triệu năm,\nnhưng ngày nay lại có tác hại đến sức khoẻ con người. Ăn nhiều muối có thể làm\ntăng huyết áp, và dẫn đến mấy bệnh tim mạch nguy hiểm. Ăn nhiều mỡ và đường thì\nkhói nói ai cũng biết là có hại cho sức khoẻ. Ngày xưa, khi thực phẩm khan\nhiếm, thì những chất dinh dưỡng đó là những \"luxury\" của cuộc sống,\nvà chúng là biểu tượng của cách ăn uống lành mạnh. Nhưng khi công nghệ chế biến\nhoàn thiện thì các chất dinh dưỡng đó trở nên thừa thải và con người tiêu thụ\nnhư không có ngày mai -- một hiện tượng tiêu thụ thái quá. Thế là từ những\n\"good guys\", các chất dinh dưỡng đó và nhiều thực phẩm khác được xem\nlà \"bad guys\" trong cuộc sống hiện đại.  Over-fitting\ncũng có thể giải thích tại sao VN đứng hạng cao trong PISA hay các kì thi\nOlympic, mà hệ thống giáo dục bị chính người trong nước đánh giá thấp. Ở Việt\nNam không chỉ có kĩ nghệ dạy và học thêm (ngoài giờ), mà còn có hẳn những chương\ntrình luyện thi để chiếm các giải thưởng quốc tế.  Đó là những chương trình dạy và học tủ để\nchuẩn bị cho những kì thi có cấu trúc cố định. Chiến lược tập trung vào một\nnhóm nhỏ và một số môn học giúp cho Việt Nam có nhiều giải thưởng quốc tế,\nnhưng đứng trên bình diện quần thể thì đa số học sinh không hưởng lợi gì đáng\nkể từ những kĩ nghệ đó. Chiến lược luyện gà chọi cũng giống như tập trung tìm\nmột mô hình nhằm giải thích các mối liên quan phức tạp trong một môi trường có\nkiểm soát, nhưng khi ứng dụng mô hình cho tương lai hay cho một quần thể lớn\nthì mô hình đó hoàn toàn thất bại. Đó chính là sự thất bại của  ứng dụng một mô hình over-fitting cho một\nquần thể độc lập. Tình trạng over-fitting cũng giải thích tại sao việc nhồi\nnhét kiến thức cho học sinh, sinh viên không hẳn là tốt cho tương lai của họ,\nvà giải thích tại sao sinh viên VN có thể học tốt ở bậc thấp, nhưng càng lên\ncao thì sinh viên VN càng kém.  Các\nbạn có thể nghĩ thêm về ý nghĩa của over-fitting trong các khía cạnh đời sống\nchính trị - xã hội khác, kể cả trong quyết định hôn nhân. Hôn nhân, nói cho\ncùng là một thí nghiệm tình cảm xã hội. Cái khó của thí nghiệm này là có quá\nnhiều thông tin mà chúng ta chưa có phương tiện để đo chính xác, và có khá\nnhiều nhiễu. (Chẳng hạn như làm sao chúng ta đo lường chính xác được sự hài\nlòng của \"đối tượng\". Trong cuộc cạnh tranh tình cảm thì có khá nhiều\nyếu tố nhiễu làm chúng ta mất tập trung.) Trong điều kiện nhiễu thông tin và\nthiếu chính xác, mô hình rất dễ trở nên over-fitting.  Nói cách khác, chúng ta không nên suy nghĩ\nnhiều quá trước quyết định hôn nhân.  Nếu\nkhông suy nghĩ nhiều quá thì suy nghĩ ít? Không phải. Một cách khắc phục tình\ntrạng over-fitting là suy nghĩ có kiểm soát, có định hướng. Nói theo ngôn ngữ\nthống kê học là mô hình cần phải có yếu tố để kiểm soát các tham số, không có\nchúng quá lạc quan, và mô hình đó có tên chung là \" regularized\nmodel\" (2). Nhưng có lẽ giải pháp thực tế hơn là suy nghĩ theo mô thức có\ntên phức tạp là \"heuristics\", có thể hiểu là \"xấp xỉ\". Nhà\ntâm lí học Daniel Kahneman gọi đó là \"suy nghĩ nhanh\" mà tôi có lần\nđiểm sách \"Thinking, fast and slow\". Trong tình huống phức tạp, đa\nyếu tố, phương pháp suy nghĩ nhanh, ngạc nhiên thay, có hiệu quả diệu kì. Lí\nthuyết này (heuristics) đã từng đem lại giải Nobel cho nhà kinh tế Harry\nMarkowitz năm 1990. Markowitz cho biết khi đối đầu với tình huống quá phức tạp,\nông tìm cách suy nghĩ xấp xỉ heuristics.  Quay\nlại vấn đề quyết định hôn nhân, bài học từ \"over-fitting\" là đừng suy\nnghĩ nhiều quá, mà suy nghĩ trung dung thôi. Suy nghĩ trung dung là dùng ít\nthông tin hơn nhưng và thông tin có chất lượng cao (gọi là substantial data)\nnhưng vẫn đạt được độ chính xác tốt.  Trường\nhợp của Darwin mà tôi đề cập trong phần đầu minh chứng cho điều này. Cái yếu tố\nquan trọng nhất trong phân tích của Darwin là ông thấy trước một viễn ảnh chỉ\nlàm việc, làm việc, và làm việc mà không có con cái hay \"người đồng\nhành\" chia ngọt xẻ bùi, còn yếu tố có ít tiền hơn để mua sách chỉ là yếu\ntố nhiễu mà thôi.  ==== (1)\nVấn đề over-fitting sẽ được bàn trong workshop sắp tới về machine learning ở ĐH\nTĐT. Chúng ta sẽ bàn về ý nghĩa của over-fitting trong các mô hình phổ biến\ntrong khoa học với vài ví dụ thú vị. Nhưng đây là chủ đề khác mà chúng tôi\nsẽ bàn trong khoá học Machine Learning vào tháng 1 tới đây.  \nPosted by\n Tuan Nguyen \nat\n 2:09 PM \nLabels:\n over-fitting ,\n thống kê học 1 comment: Thông Trần \nsaid...\n \nCảm ơn giáo sư đã có một bài viết rất hay và bổ ích, ai đọc xong bài này chắc chắn sẻ có những trải nghiệm thú vị. Chúc Giáo sư bước sang năm mới sức khỏe và có nhiều bài viết giá trị chia sẻ những hiểu biết uyên thâm, lô gic, sáng tạo, mang tính thời sự cho các độc giả nhé!\n \nJanuary 4, 2017 at 11:40 PM\n Post a Comment \nSubscribe to:\n Post Comments (Atom) Vài nối kết Về tác giả trang blog Một số sách mới của tôi  Biết nguy cơ gãy xương?  Nhóm nghiên cứu cơ xương ở VN Bayesian statistics and MD Science based medicine ASBMR Trang dành cho sách của tôi \"Ám sát cá nhân\" Chuyện thời sự Việt Nam là nơi tốt nhất để sống: Một cách hiểu khác Du lịch VN: tại sao \"một đi không trở lại\"? Bác sĩ cần môn nào hơn cả Văn học? Nghĩ về bằng cấp tiến sĩ của ông Hà Văn Thắm và chuyện xa hơn Nội Bài và Tân Sơn Nhất là các phi trường tồi nhất ở Á châu! Số liệu về người Việt ở Mĩ Dự thảo xếp hạng đại học ở Việt Nam: những điểm cần bàn thêm Tiến sĩ: xứ người thừa, xứ ta thiếu Yêu nước và tự hào Khi niềm tin vào khoa học bị sứt mẻ Thân phận của những \"rùa biển\" Việt Nam Giáo sư là gì, ai là giáo sư? Giải Nobel y sinh học 2015: Ý nghĩa về bệnh nhiễm và y học cổ truyền Cuộc đời và sự nghiệp của Đồ U U (Tu Youyou) Terror in Little Saigon (Khủng bố ở Sài Gòn Nhỏ) Kỉ niệm đi đây đó Ghi chép Seattle 10/2015 Một thoáng Hàn Quốc Nhật kí Nam Dương Ghi chép trên xứ chùa tháp 1 – Casino Ghi chép trên xứ chùa tháp 2 – Cái nghèo và chính trị Ghi chép trên xứ chùa tháp 3 – Siem Reap và Nụ cười Angkor Ghi chép trên xứ chùa tháp 4 – Một thoáng Angkor Wat Ghi chép trên xứ chùa tháp 5 – Kì bí Angkor Thom Ghi chép trên xứ chùa tháp 6: Phnom Penh – chút gì để nhớ Ghi chép trên xứ chùa tháp 7 – Cảm nghĩ về du lịch Campuchea Người Nga ở Nha Trang Nhật kí Nha Trang Vài ghi chép ở An Giang 22/12/2013 Nhật kí viết ở Khon Kaen 1 Nhật kí Khon Kaen 2: Một buổi giảng ở Thái Lan Nhật kí Khon Kaen 3: Một buổi giảng ở Thái Lan Nhật kí Khon Kaen 4: Dự đám cưới người Thái văn nghệ văn gừng Lời tự thuật của tác giả bài thơ \"Màu Tím Hoa Sim\" Thời của thánh thần Bảy nỗi buồn tháng bảy Ngô Đình Nhu và tướng Võ Nguyên Giáp Hoàng Cầm - Suy nghĩ lúc nằm im Bàn về nhạc sến Hữu Loan với tình yêu trong Màu tím hoa sim Cái âm điệu tủi thân, bi đát Trịnh Công Sơn: Người lắng âm vọng nhân sinh Biết ái tình ở dòng sông Hương Đàm Vĩnh Hưng và \"Thương hoài ngàn năm\" Nghe Cẩm Ly ca Thơ Hàn Mạc Tử qua cái nhìn của chuyên gia tâm thần Đối phó với bệnh tật bằng thái độ thích hợp Chiều về trên sông Lê Đạt (1929 - 2008) Hồn của cây Trần Dần và Tố Hữu Nguyễn Bính và thời gian ở Cà Mau Bà Huyện Thanh Quan Phạm Duy và bà mẹ Gio Linh Hữu Loan bây giờ Lệ Thu - tiếng hát để đời Tiếng hát Phương Thanh Vài hàng ghi nhanh sau khi nghe Cẩm Ly hát Bàn về thơ Quang Dũng Bàn về ca từ trong các bài hát mới Vài cảm nhận về nghệ thuật cải lương Trộm nhìn nhau Bàn về thơ và nhạc Nguyễn Tất Nhiên Cao Xuân Hạo Nguyễn Ngọc Tư - gom góp những buồn vui Phạm Tiến Duật Tản Mạn về Nguyễn Bính Võ Nguyên Giáp Ngô Phan Lưu Ái Vân - Hồng nhan trắc trở... Em đến thăm anh một chiều mưa Phạm Quỳnh: tiểu luận viết bằng tiếng Pháp Sống Thời đại và Tinh thần Đức Phật Lan man nhớ … Nhớ Sài Gòn Cảm xúc mới trong không gian mới Chậm rơi chiều miền Kinh Bắc bài đã gửi lên mạng \n\n        ► \n      \n \n2017\n (81) \n\n        ► \n      \n \nOctober\n (7) \n\n        ► \n      \n \nSeptember\n (1) \n\n        ► \n      \n \nAugust\n (4) \n\n        ► \n      \n \nJuly\n (8) \n\n        ► \n      \n \nJune\n (14) \n\n        ► \n      \n \nMay\n (9) \n\n        ► \n      \n \nApril\n (14) \n\n        ► \n      \n \nMarch\n (10) \n\n        ► \n      \n \nFebruary\n (12) \n\n        ► \n      \n \nJanuary\n (2) \n\n        ▼ \n      \n \n2016\n (106) \n\n        ▼ \n      \n \nDecember\n (15) \"Over-fitting\" và ý nghĩa thực tế trong đời sống Qui luật 37% (và ứng dụng cho việc tìm người yêu) Thông báo thứ 3 (sau cùng) về lớp học Machine Lear... Nhạc boléro trên đài truyền hình Vĩnh Long Lại bàn về PISA: khái niệm effect size \"Biases\" trong kiểm định giáo dục Yếu tố nào ảnh hưởng đến điểm PISA 2015? Kết quả PISA 2015: một cách hiểu khác Bạn đã đọc Nguyễn Thanh Việt (Viet Thanh Nguyen) c... Ước tính cỡ mẫu và Machine Learning Hành trình đến tự do - Nhà văn Mai Thảo Cập nhật lớp học về \"Machine Learning\" tháng 1/201... Nghĩ lan man về \"Kĩ thuật cao và tiêu hoá Hà Nội\" Cập nhật khoá học về Machine Learning 1/2017 Đánh giá khoa học \n\n        ► \n      \n \nNovember\n (10) \n\n        ► \n      \n \nOctober\n (18) \n\n        ► \n      \n \nSeptember\n (7) \n\n        ► \n      \n \nAugust\n (12) \n\n        ► \n      \n \nJuly\n (6) \n\n        ► \n      \n \nJune\n (10) \n\n        ► \n      \n \nMay\n (2) \n\n        ► \n      \n \nMarch\n (3) \n\n        ► \n      \n \nFebruary\n (15) \n\n        ► \n      \n \nJanuary\n (8) \n\n        ► \n      \n \n2015\n (233) \n\n        ► \n      \n \nDecember\n (16) \n\n        ► \n      \n \nNovember\n (31) \n\n        ► \n      \n \nOctober\n (26) \n\n        ► \n      \n \nSeptember\n (23) \n\n        ► \n      \n \nAugust\n (11) \n\n        ► \n      \n \nJuly\n (18) \n\n        ► \n      \n \nJune\n (14) \n\n        ► \n      \n \nMay\n (13) \n\n        ► \n      \n \nApril\n (23) \n\n        ► \n      \n \nMarch\n (20) \n\n        ► \n      \n \nFebruary\n (14) \n\n        ► \n      \n \nJanuary\n (24) \n\n        ► \n      \n \n2014\n (195) \n\n        ► \n      \n \nDecember\n (25) \n\n        ► \n      \n \nNovember\n (46) \n\n        ► \n      \n \nOctober\n (39) \n\n        ► \n      \n \nSeptember\n (22) \n\n        ► \n      \n \nAugust\n (19) \n\n        ► \n      \n \nJuly\n (34) \n\n        ► \n      \n \nJanuary\n (10) \n\n        ► \n      \n \n2013\n (96) \n\n        ► \n      \n \nDecember\n (8) \n\n        ► \n      \n \nNovember\n (9) \n\n        ► \n      \n \nOctober\n (11) \n\n        ► \n      \n \nSeptember\n (8) \n\n        ► \n      \n \nAugust\n (7) \n\n        ► \n      \n \nJuly\n (15) \n\n        ► \n      \n \nJune\n (11) \n\n        ► \n      \n \nMay\n (1) \n\n        ► \n      \n \nApril\n (2) \n\n        ► \n      \n \nMarch\n (21) \n\n        ► \n      \n \nFebruary\n (1) \n\n        ► \n      \n \nJanuary\n (2) \n\n        ► \n      \n \n2012\n (38) \n\n        ► \n      \n \nNovember\n (19) \n\n        ► \n      \n \nOctober\n (19) \n\n        ► \n      \n \n2011\n (112) \n\n        ► \n      \n \nDecember\n (10) \n\n        ► \n      \n \nJune\n (3) \n\n        ► \n      \n \nMay\n (24) \n\n        ► \n      \n \nApril\n (29) \n\n        ► \n      \n \nMarch\n (10) \n\n        ► \n      \n \nFebruary\n (22) \n\n        ► \n      \n \nJanuary\n (14) \n\n        ► \n      \n \n2010\n (145) \n\n        ► \n      \n \nDecember\n (3) \n\n        ► \n      \n \nNovember\n (6) \n\n        ► \n      \n \nOctober\n (2) \n\n        ► \n      \n \nJuly\n (1) \n\n        ► \n      \n \nMay\n (21) \n\n        ► \n      \n \nApril\n (23) \n\n        ► \n      \n \nMarch\n (34) \n\n        ► \n      \n \nFebruary\n (34) \n\n        ► \n      \n \nJanuary\n (21) \n\n        ► \n      \n \n2009\n (347) \n\n        ► \n      \n \nDecember\n (22) \n\n        ► \n      \n \nNovember\n (34) \n\n        ► \n      \n \nOctober\n (42) \n\n        ► \n      \n \nSeptember\n (34) \n\n        ► \n      \n \nAugust\n (20) \n\n        ► \n      \n \nJuly\n (24) \n\n        ► \n      \n \nJune\n (29) \n\n        ► \n      \n \nMay\n (26) \n\n        ► \n      \n \nApril\n (27) \n\n        ► \n      \n \nMarch\n (23) \n\n        ► \n      \n \nFebruary\n (21) \n\n        ► \n      \n \nJanuary\n (45) \n\n        ► \n      \n \n2008\n (306) \n\n        ► \n      \n \nDecember\n (9) \n\n        ► \n      \n \nNovember\n (41) \n\n        ► \n      \n \nOctober\n (32) \n\n        ► \n      \n \nSeptember\n (38) \n\n        ► \n      \n \nAugust\n (39) \n\n        ► \n      \n \nJuly\n (19) \n\n        ► \n      \n \nJune\n (16) \n\n        ► \n      \n \nMay\n (38) \n\n        ► \n      \n \nApril\n (40) \n\n        ► \n      \n \nMarch\n (2) \n\n        ► \n      \n \nFebruary\n (9) \n\n        ► \n      \n \nJanuary\n (23) \n\n        ► \n      \n \n2007\n (145) \n\n        ► \n      \n \nDecember\n (31) \n\n        ► \n      \n \nNovember\n (23) \n\n        ► \n      \n \nOctober\n (29) \n\n        ► \n      \n \nSeptember\n (5) \n\n        ► \n      \n \nAugust\n (16) \n\n        ► \n      \n \nJuly\n (7) \n\n        ► \n      \n \nJune\n (12) \n\n        ► \n      \n \nMay\n (19) \n\n        ► \n      \n \nApril\n (3) chủ nhà Hình này thật ra là hình họa bằng máy tính (do \"Hoạ sĩ\" NĐN vẽ từ một hình tôi chụp khi qua Phà Vàm Cống năm 2005. Chẳng hiểu sao tôi thích tấm hình này, chẳng phải vì nó hay ho gì cả, nhưng nhìn để nhớ đến hôm qua Phà, nhìn sông nước mênh mông mà ước gì có ngày một cây cầu bắc ngang sông để nối liền Kiên Giang và An Giang với các tỉnh khác trong vùng Tây Nam Bộ. khách từ mọi nơi luân hồi (ảnh: nguyễn đình nguyên) Kĩ năng khoa học Bài báo khoa học: Cách viết tựa đề  Bài báo khoa học: Cách viết dẫn nhập (introduction) Bài báo khoa học: Cách viết phần phương pháp  Bài báo khoa học: Cách viết phần kết quả  Bài báo khoa học: Cách viết phần bàn luận  Bài báo khoa học: Cách viết phần abstract  Cách trả lời bình duyệt (response to peer review) Bài báo khoa học: Nguyên tắc IDEA   Bài báo khoa học: Văn phong tiếng Anh  Cách viết “cover letter” cho tập san khoa học Những lỗi phổ biến trong trình bày bằng PowerPoint Kĩ năng trình bày: Cách soạn powerpoint slide Kĩ năng trình bày: Cách nói trong hội nghị Kĩ năng trình bày: Cách mô tả biểu đồ, hình ảnh và số liệu Kĩ năng trình bày: Cách ứng đáp trong hội nghị khoa học Kĩ năng trình bày: Cách làm chủ tọa hội nghị Kĩ năng trình bày: Điệu bộ Biểu đồ trong bài báo khoa học 1: nguyên tắc Biểu đồ trong bài báo khoa học 2: yếu tố dối Biểu đồ trong bài báo khoa học 3: tỉ số dữ liệu trên mực in Biểu đồ trong bài báo khoa học 4: mật độ dữ liệu Văn phong khoa học: thứ tự từ Kinh nghiệm tự học tiếng Anh Mười nguyên lí để tăng khả năng công bố bài báo khoa học Danh sách bài giảng về phân tích dữ liệu trên youtube.com Luận án tiến sĩ: độ dài và tài liệu tham khảo Cách viết đề cương nghiên cứu  Đánh giá tập san khoa học: chỉ số eigenfactor Đánh giá khoa học Sự trỗi dậy của Mã Lai trong khoa học Tập san SCIE chất lượng thấp hơn SCI? Tập san Asian Social Science SCIRP: Nhà xuất bản dỏm Tạp san dỏm đang làm vẩn đục khoa học Tiêu chí để nhận dạng tập san khoa học dỏm Đánh giá khoa học: con số và những hiểu lầm tai hại Năng suất khoa học Việt Nam (2009-2013) Nhận dạng những công trình nghiên cứu có ảnh hưởng lớn Danh tiếng có ảnh hưởng đến citation! Dự thảo xếp hạng đại học ở Việt Nam: những điểm cần bàn thêm Thước đo MỚI về ảnh hưởng của bài báo khoa học Đánh giá năng lực nghiên cứu khoa học Hệ số tác động (Impact Factor) chưa chết! Đánh giá một nhà khoa học qua những thước đo nào? Khoa học thời toàn cầu hoá: Lạm phát tác giả bài báo khoa học Những chuyện khó tin trong nghiên cứu khoa học ở VN Phân biệt khoa học cơ bản và khoa học ứng dụng Lan man chuyện khoa học: chất và lượng Tài trợ khoa học: công bằng và khoa học Đơn vị \"đầu đàn\" của Khoa học Việt Nam nghiên cứu như thế nào? Viện hàn lâm khoa học xã hội (VASS) giáo dục và khoa học Những “thước đo” năng lực của một nhà khoa học Quá trình phát triển giáo dục đại học ở Nhật và n... Trình độ học vấn của bộ trưởng Việt Nam, Mĩ và Úc Không cần lượng, chỉ cần chất Một buổi sáng ở Đại học Khoa học Tự nhiên Bình luận: \"Đào tạo tiến sĩ - 'Chất' và 'lượng'\" Cẩn thận với kết tội qua xét nghiệm DNA Khoa học VN đang ở đâu trong năm 2009 ? Bộ trưởng khoa học đạo văn ? Chỉ số H trong nghiên cứu khoa học Thưởng cho nghiên cứu khoa học như thế nào? Kết quả thi tốt nghiệp THPT 2009 Quốc nạn loạn chức danh, học vị Chín lí do cho công bố quốc tế Những ngộ nhận về học vị tiến sĩ Nhận định của Tuyên giáo về hợp tác đào tạo Việt-Mĩ Về ý kiến của một người Mĩ về giáo dục ở Việt Nam Học tiến sĩ ở Việt Nam Khoa học và công nghệ tác động đến phát triển kinh... Chung quanh vấn đề xét phong chức danh GS/PGS Vài nhận xét về kết quả phong hàm giáo sư 2009 Bàn về tiêu chuẩn phong chức danh GS/PGS Con số thống kê mù chữ có ý nghĩa gì ? Xếp hạng đại học: cần minh bạch hóa phương pháp Xếp hạng đại học Ước vọng 200 Suy nghĩ về “cái gốc trong sách giáo khoa” Chuyện buồn giáo dục Tản mạn Ngày nhà giáo Vài kinh nghiệm về đề bạt chức danh khoa bảng Thưởng cho nghiên cứu khoa học như thế nào? Tôn vinh cái gì? Thói háo danh hay thành tích học thuật? ĐH đẳng cấp quốc tế: Đường dài xa ngái ... Nhân sự khoa bảng trong đại học Việt Nam Học sinh cần học điều gì ? Di sản tri thức và bất cập tâm thức Trang web Đại học Y Dược TPHCM Thi cử trung học ở Úc Có nên bỏ thi trung học? Ngành toán học Việt Nam Bàn về chính sách đối với Việt kiều Chỉ số H trong nghiên cứu khoa học Địa đàng ở phương Đông (Eden in the East) Khoa học và Phật giáo Nguồn gốc của văn hóa đạo văn Bàn về “cái mới” trong luận án tiến sĩ Chất lượng nghiên cứu dịch tễ học ở Việt Nam Chỉ số H của tôi Xã hội hóa nghiên cứu khoa học như thế nào? Sách giáo khoa của ta chẳng giống ai ? Website Đại học y dược TPHCM Mô hình đại học mới ở nước ta Ghi nhanh: khoa học và chính trị Chuột và khoa học – nhặt sạn Sẽ và có thể Hoài nghi thông tin khoa học! Chất lượng giáo dục: từ thầy đến trò Vị thế của khoa học VN ở đâu? Khoa học VN đang ở đâu? Khoa học Việt Nam qua những công bố quốc tế Có cần công bố quốc tế? Dịch sang tiếng Anh họ đăng tất! Bàn về thi tốt nghiệp trung học và ... Làm khoa học và chuyện bôi trơn Những điều khó tin về “Bảy điều khó tin nhất trong... Đại học \"khát\" giảng viên Nghiên cứu khoa học: thừa tiền! Giải pháp nào cho thông tin khoa học ở nước ta Đạo văn: 2 công trình khoa học giống nhau 90 % ! Cục đất sét và thần tượng Về “Bài toán 1000 trí thức Việt kiều” - phần 2 Về “Bài toán 1000 trí thức Việt kiều”, phần 1 Việt Nam có bao nhiêu nhân lực khoa học công nghệ 60% tiến sĩ Mĩ không hơn tiến sĩ Việt Nam Bàn về tiêu chuẩn đào tạo tiến sĩ Gian nan đi tìm tiến sĩ Lo ngại về chất lượng tiến sĩ Báo động \"đỏ\" về tiến sĩ \"ra lò\" không chất Nhiều tiêu chuẩn đào tạo tiến sĩ còn \"lãng mạn\" VN có bao nhiêu GS, PGS trình độ quốc tế? Chất lượng giáo dục: từ thầy đến trò 60% tiến sĩ Mĩ không hơn tiến sĩ Việt Nam Khoa học VN đang ở đâu? Vị thế của khoa học VN ở đâu? Sẽ và có thể Hoài nghi thông tin khoa học! Xếp hạng đại học: cần minh bạch hóa phương pháp Xếp hạng đại học Ước vọng 200 Suy nghĩ về “cái gốc trong sách giáo khoa” Chuyện buồn giáo dục Tản mạn Ngày nhà giáo Con số thống kê mù chữ có ý nghĩa gì ? y khoa và y tế Cẩn thận với kết tội qua xét nghiệm DNA Những sai sót trong cứu y học ở Việt Nam 200 năm Darwin Trời cho nắng, sao ta chưa phơi! Vitamin D và cảm cúm Vitamin D và gãy xương Loãng xương ở đàn ông vertebroplasty và kyphoplasty Ăn chay như là một trị liệu Ăn chay và loãng xương Y đức và nghiên cứu y học Nhìn lại dịch cúm heo 1976 Đầu độc bằng dioxin Khẩu trang và H1N1 Trị liệu bằng tế bào gốc: triển vọng và quan tâm Mối quan hệ giữa giới y khoa và kĩ nghệ dược Gien du côn Lại gian lận trong nghiên cứu y khoa! Nhà vệ sinh: chuyện nhỏ mà không nhỏ Nghiên cứu ứng dụng tế bào gốc Nhà vệ sinh: chuyện quan trọng Y đức: cười buồn Hiệu quả của tamiflu và relenza Lại bàn về văn hóa sợ hãi qua vụ cúm H1N1 Thịt đỏ và nguy cơ tử vong Ăn chay và loãng xương Chữ viết của bác sĩ và tác hại Kì thị trong tiềm thức? Thế nào là “Cơ sở khoa học” ? Qui định cấp giấy phép lái xe dựa vào chiều cao: sai giả định Về qui định chiều cao và trọng lượng để cấp bằng lái xe: thiếu cở sở khoa học và kì thị Giải Nobel Y sinh học 2008 Xã hội hóa và an toàn thực phẩm Cần qui ước đạo đức cho kĩ nghệ thực phẩm Tuổi thọ của vua chúa Việt Nam Viết tiếp về di truyền trong thể thao Câu chuyện về y đức Thảo luận với Dr Olivé Y tế dự phòng Mắm tôm vô tội lần 2! Hậu “mắm tôm được minh oan” - bằng chứng khoa học,... Mắm tôm vô tội! Tại sao mắm tôm “vô tội” Dịch tả: Nước quan trọng hơn mắm tôm! Phản biện cần thông tin khoa học Được xin lỗi thì đã...oan khiên! Xin lỗi … mắm tôm Cầu tiêu và vệ sinh Qui ước Ingelfinger Có bao nhiêu trẻ em VN suy dinh dưỡng? Tai nạn y khoa trong bệnh viện Boris Yeltsin từng được một bác sĩ Mĩ cứu mạng Cần hiểu đúng về ung thư và nguy cơ ung thư Bưởi không gây ung thư vú Bưởi không gây ung thư - phản hồi The Trouble With Animal Models Giáo sư Mario R. Capecchi Giải nobel y học và lợi ích cho người bệnh người nổi tiếng và những vấn đề chuyện xã hội Tại sao người Việt Nam ít nói “cảm ơn”? Tướng Nguyễn Sơn Chuyện doping của Ngân Thương: cần minh bạch Tây nguyên và phát triển bền vững (phần II) Tây nguyên và phát triển bền vững (phần I) Đường về miền Tây Chuyện chưa biết về Nhà văn Sơn Nam Nhà văn Sơn Nam Đi tìm cái tôi đã mất (Nguyễn Khải) Kế hoạch cho ngày tàn của Phật giáo Trí thức và quan chức Học giả Nguyễn Hiến Lê Hàm Nghi nghệ sĩ Đại tướng Nguyễn Chí Thanh mất ở đâu? Chất độc da cam và Việt Nam Nguyễn Trung Trực Điểm tựa của ông Võ Văn Kiệt Hàng không Việt Nam một chút vụng về Sao ông lại nỡ ... phần 2 Câu chuyện tiếng Anh - Sao ông lại nỡ ... Võ Văn Kiệt Tôi ngượng nhưng không phải ngượng cho mình Văn hóa và phát triển trong thế kỉ 21 Luận bàn về trí thức Tội làm hư dân Công bằng, dân chủ dưới con mắt Phật giáo Cái \"trí\" của quan ngày nay Có một vị Bồ Tát thật trên đời Chiếc long bào của vị Hoàng Đế cởi truồng! Tản mạn cuối tuần: Quyền phê phán và trí thức Minh oan cho vua Duy Tân Chả bao giờ thấy nàng cười Một người viết báo kể chuyện Sài Gòn 30/4 Não trạng của những người thiếu tự tin Mất công bằng trong y tế Chuyện về lao động VN ở Mã Lai 315 người chết và chỉ 0,09% Phán quyết bất công Vụ rét đậm: Bộ Y tế chậm quá Chương trình Bông sen Vàng Buồn thương bìm bịp kêu chiều Về thủ phủ Miền Tây sông nước Đủ kiểu \"trói\" bệnh nhân! Thẳng tay móc túi người bệnh Lại nón bảo hiểm: xem qua các lí giải phổ biến Chất độc da cam, trách nhiệm, và nhân đạo Phê phán phải dựa trên cơ sở thực tế Lại chuyện cổ phần hóa bệnh viện Có nên cổ phần hóa bệnh viện công Chuyện nhỏ nhưng ý nghĩa không nhỏ Hải quan Canada Lên Bạch Mai chống độc ! Bàn chuyện tiếng Anh trong website của Tòa đại sứ Câu chuyện của Huỳnh Mai Bi kịch của cô gái nghèo hiếu thảo Bà Mười Xiềm Bẽ bàng nghề tiếp thị rượu bia! \nSimple theme. Theme images by  5ugarless . Powered by  Blogger .\n",
          "relevance": "0",
          "title": "'Over-fitting' và ý nghĩa thực tế trong đời sống",
          "url": "http://tuanvannguyen.blogspot.com/2016/12/over-fitting-va-y-nghia-thuc-te-trong.html"
        },
        {
          "content": "\r\n\t\t\t                            \r\n        Chào mừng đến với BIS\r\n         Đăng nhập  \r\n         |  Đăng ký \r\n        |  Trợ giúp \r\n\t\t\t\t\t                                    \r\n\t\t\t\t\t\t                                         trong \r\n\t\t\t\t\t\t                                         Data Mining and Business Intelligence... Data Mining and Business Intelligence... (Entire Site) Tìm kiếm BIS  »  Data Mining and Business Intelligence  »  Data Mining and Business Intelligence  »  Các phương pháp học máy (Machine Learning)  Các phương pháp học máy (Machine Learning)  Bài cuối 06-25-2013 11:27 AM của  thanhthi . 5 trả lời. Trang 1 trong số 1 (6 nội dung)  \r\n\t\t\t\t        \r\n\t\t\t\t                Sắp xếp bài viết:\r\n\t\t\t\t                 Cũ đến mới Mới đến cũ Trước Tiếp theo \r\n\t\t\t\t\t\t\t\t        04-10-2012 05:51 PM    \r\n\t\t\t\t\t\t\t\t     TienPhong Tham gia 03-04-2012 Điểm 110 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Các phương pháp học máy (Machine Learning)\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Trong lĩnh vực Học máy có các phương\r\npháp học sau: 1) Học có giám sát (supervised\r\nlearning) 2) Học không có giám sát\r\n(unsupervised learning) 3) Học bán giám sát\r\n(semi-supervised learning) 4) Học tăng cường (reinforcement\r\nlearning) Không phải ngẫu nhiên người ta lại\r\nphân chia và đặt tên cho các phương pháp học như vậy. Sau một thời gian tìm đọc tài liệu\r\nvề học máy, cố gắng hiểu được bản chất và phân biệt được sự giống và khác nhau\r\ncủa các phương pháp học này, tôi đưa ra một bài toán ví dụ sau: Cho một công-ten-nơ chứa đầy hoa\r\nquả. Nhiệm vụ phải chia số quả này thành các nhóm đúng với loại quả đó. (xoài\r\nra xoài, cam ra cam, táo ra táo,…) 1) Với Học có giám sát\r\n(supervised learning) Là kỹ thuật học sử dụng cho các bài\r\ntoán phân lớp (Classification) Để thực hiện được bài toán trên,\r\ntrước tiên cần phải có 2 điều kiện: Điều kiện 1: phải biết trước số\r\nnhãn lớp cần phân loại, tức là phải biết trong công-ten-nơ đó có nhưng loại quả\r\ngì. Giả sử trong công-ten-nơ đó có 5 loại quả là xoài, cam, táo, ổi, đào (đây\r\nchính là 5 loại nhãn lớp). Điều kiện 2: phải có tập đặc trưng\r\ncủa mỗi loại quả, ví dụ các đặc trưng là: hình dáng, màu sắc, trọng lượng, độ cứng\r\nmềm, v.v… Tập đặc trưng này có được thông qua học một tập dữ liệu huấn luyện\r\n(chính là các công-ten-nơ của các chuyến hàng trước đó) Khi thực hiện phân loại các loại\r\nquả trong công-ten-nơ đang xét, dựa vào đặc trưng của các loại quả (điều kiện\r\n2), quả sẽ được đưa vào 1 trong 5 nhóm đã biết (điều kiện 1). 2) Học không có giám sát\r\n(unsupervised learning) Là kỹ thuật học sử dụng cho các bài\r\ntoán phân cụm, gom cụm (Clustering)   Để thực hiện được bài toán trên,\r\ncần phải có tập đặc trưng của mỗi loại quả. Tập đặc trưng này có được cũng thông\r\nqua học một tập dữ liệu huấn luyện (như điều kiện 2 của Học có giám sát).   Điểm khác của Học không giám sát\r\nso với Học có giám sát là: trước khi phân cụm, không biết trong công-ten-nơ đang\r\nxét có bao nhiêu loại quả và đó là những loại quả gì. Khi thực hiện phân cụm, dựa vào đặc\r\ntrưng của mỗi loại quả, sẽ đưa quả đang xét vào nhóm (cụm) có đặc trưng tương đồng\r\nvới nó nhất. Khi đó, 2 quả bất kỳ ở cùng cụm sẽ tương đồng nhau, 2 quả khác cụm\r\nsẽ khác biệt nhau. * Nhận xét Giống nhau: Cả\r\nhai phương pháp học 1) và 2) đều cần phải có một tập huấn luyện (training data\r\nset) để hệ thống có thể “học” và rút ra được các đặc trưng dùng cho việc gán nhãn. Khác nhau: Phương\r\npháp 1) cần biết trước đầu ra chính là số nhãn lớp. Phương pháp 2) không cần biết\r\ntrước đầu ra  (là số cụm và nhãn) để phân\r\ncụm.   Trên đây là cách hiểu của tôi về\r\nphương pháp Học có giám sát và Học không giám sát. Nhân đây tôi xin đưa một thắc\r\nmắc để các thành viên và những ai quan tâm đưa ra ý kiến để cùng thảo luận. Thắc mắc: Học bán giám sát là gì?\r\nHọc tăng cường là gì? Phân biệt sự khác nhau và giống nhau giữa các phương pháp\r\nhọc bán giám sát và học tăng cường. Cho ví dụ minh hoạ? Từ khóa đại diện:  Machine Learning ,  Phân cụm dữ liệu. ,  Học máy ,  Phân lớp dữ liệu ,  Khai phá dữ liệu Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        06-18-2012 09:41 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  phamvu Tham gia 06-18-2012 Điểm 35 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Các phương pháp học máy (Machine Learning)\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ \r\n\t\t\t\t\t\t\t\t\t\t    Em thấy k - means là học ko giám sát, trong khi đó xác định k cụm là do người dùng xác định hoặc có thể default, vậy số k đã được biết trước.Vậy thuật toán k - means phải là học nửa giám sát?\r\n\t\t\t\t\t\t\t\t\t\t    \r\n\t\t\t\t\t\t\t\t\t     phamvu Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        09-02-2012 01:19 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  TienPhong Tham gia 03-04-2012 Điểm 110 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Các phương pháp học máy (Machine Learning)\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Đúng k means là thuật toán học không giám sát vì nó được dùng để phân cụm cho các dữ liệu chưa biết nhãn. Để dễ phân biệt bạn chú ý là việc biết (cho) trước k cụm không liên quan gì đến dữ liệu đầu vào (hay dữ liệu huấn luyện - training data set). Với học có giám sát dữ liệu đầu vào đều đã được gán nhãn trước, phương pháp học này dùng giải quyết bài toán phân lớp (classification). Với học không giám sát, không biết trước nhãn của dữ liệu (hay dữ liệu đầu vào đều chưa được gán nhãn), phương pháp này dùng để giải quyết bài toán phân cụm (clustering). Còn với học nửa giám sát, sử dụng tập dữ liệu đầu vào gồm cả dữ liệu đã gán nhãn và dữ liệu chưa gán nhãn, trong đó dữ liệu gán nhãn thường (rất) ít và dữ liệu chưa gán nhãn thường (rất) nhiều. Hai kỹ thuật tiêu biểu cho học nửa giám sát là Self-training và Co-training, bạn có thể tìm kiếm trên internet. Chúc vui vẻ! Điểm chủ đề: 20 \r\n\t\t\t\t\t\t\t\t        05-17-2013 12:49 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  TienPhong Tham gia 03-04-2012 Điểm 110 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Các phương pháp học máy (Machine Learning)\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Có nhiều bạn gửi mail hỏi mình về Machine Learning, các bạn thông cảm vì đi làm bận nên không trả lời riêng các bạn, và cũng để mọi người có thể cùng trao đổi mình sẽ post lên đây để mọi người chia sẻ những kiến thức về ML.   Trước tiên mình nêu một số định nghĩa rất quan trọng trong ML.   Định nghĩa học máy Học máy (hay máy học – Machine learning) là một thành phần quan trọng của trí tuệ nhân tạo nhằm nghiên cứu và phát triển các phương pháp, kỹ thuật giúp cho các hệ thống hay máy tính có khả năng học (Tiến Phong).   Vấn đề quá vừa dữ liệu (Over-fitting) Thuật ngữ over-fitting ra đời dùng để chỉ một hiện tượng xuất hiện trong quá trình khai phá dữ liệu sử dụng phương pháp học máy. Hiện tượng này gây khó khăn đáng kể cho việc thiết kế, xây dựng hệ thống ngay từ bước chuẩn bị dữ liệu cho đến bước kiểm thử hệ thống. Khi hiện tượng over-fitting xảy ra sẽ làm cho hiệu quả của hệ thống giảm xuống và kết quả thu được từ hệ thống không còn độ tin cậy cao. Có thể định nghĩa hiện tượng over-fitting như sau:   Định nghĩa quá vừa dữ liệu Một hàm mục tiêu hay một giả thiết học được  h , sẽ được gọi là over-fitting (quá vừa dữ liệu) với một tập dữ liệu huấn luyện nếu tồn tại một hàm mục tiêu khác là  h’  sao cho: h’  kém phù hợp hơn, đạt độ chính xác kém hơn so với  h  trên tập dữ liệu huấn luyện, nhưng  h’  lại đạt độ chính xác cao hơn  h  đối với toàn bộ tập dữ liệu (bao gồm cả tập dữ liệu liệu huấn luyện và tập dữ liệu kiểm tra)   Ví dụ quá vừa dữ liệu Giả sử gọi D là tập toàn bộ các dữ liệu có thể có, Training_D là tập các dữ liệu huấn luyện Giả sử Err_D(h) là mức lỗi mà giả thiết h sinh ra đối với tập D, và Err_Training_D(h) là mức lỗi mà giả thiết h sinh ra đối với tập Training_D. Nếu tồn tại một giả thiết khác là h’ sao cho: Err_Training_D(h) < Err_Training_D(h’) và Err_D(h) > Err_D(h’) Thì khi đó h được coi là quá vừa dữ liệu trên tập huấn luyện Training_D.   Nguyên nhân quá vừa dữ liệu Vấn đề over-fitting thường do các nguyên nhân: - Lỗi (nhiễu) trong tập huấn luyện phát sinh trong quá trình thu thập, xây dựng tập dữ liệu. - Số lượng dữ liệu của tập huấn luyện quá nhỏ, không đại diện cho toàn bộ tập dữ liệu có thể có hay toàn bộ phân bố dữ liệu của bài toán.   Bản chất của học máy dưới góc nhìn của xác suất thống kê. Có thể mô hình hoá một vấn đề máy học như sau: Cho một dãy  l  quan sát:  (x 1 , y 1 ), (x 2 , y 2 ), … , (x l , y l ) .  Trong đó: - x 1 , x 2 , …, x l  là các mẫu, x i R n . Các mẫu x i  được phát sinh ngẫu nhiên theo một hàm phân bố xác suất F(x) nào đó mà ta không biết. - y i  là các kết quả học tương ứng với mẫu x i , y i R. Thường thì y là kết quả của một hàm f(x) nào đó – đơn trị. Tuy nhiên trong trường hợp tổng quát thì y không đơn trị. Do đó y được xác định theo một hàm phân bố điều kiện F(y|x) mà ta cũng không biết. - Bây giờ cho một mẫu x*, vấn đề của máy học là xác định một hàm f 0 (x) mà có thể ước lượng tốt nhất giá trị y* tương ứng. Như vậy theo lý thuyết tương quan trong thống kê thì f 0 (x) tốt nhất theo lý thuyết phải là kỳ vọng của y theo x theo phân bố F(y|x).f 0 (x) còn được gọi là phương trình hồi quy. Với x tuân theo phân bố F(x), y tuân theo phân bố có điều kiện F(y|x) thì hàm phân bố của cặp (x, y) là F(x, y) = F(x)F(y|x). Có thể thấy xác suất để có dãy  (x 1 , y 1 ), (x 2 , y 2 ), … , (x l , y l )  là tích  F(x 1 , y 1 )F(x 2 , y 2 )…F(x l , y l ). Tuy nhiên, ở đây ta không biết F(x) lẫn F(y|x) nên không thể xác định chính xác kỳ vọng này. Tất cả dữ liệu mà ta biết chỉ là dãy hữu hạn các mẫu quan sát  (x 1 , y 1 ), (x 2 , y 2 ), … , (x l , y l ) . Nhiệm vụ của máy học là xác định chính xác nhất có thể được hàm f 0 (x) dựa trên các dữ liệu hữu hạn này. Trong trường hợp khi y   R, tức đây là vấn đề hồi quy (regression).  Trong trường hợp bài toán phân lớp (classification) thì y  {-1, 1} là trường hợp nhận dạng hai lớp, nếu y i  = -1 thì x i  thuộc lớp thứ nhất (không được quan tâm), còn y i  = 1 thì x i  thuộc lớp thứ 2 (lớp được quan tâm)   Một số phương pháp học máy Trong lĩnh vực học máy có nhiều phương pháp học khác nhau, trong phần này đề cập đến 3 phương pháp học được sử dụng phổ biến nhất, gồm có: học không giám sát, học bán/ nửa giám sát và học có giám sát.   Phương pháp học không giám sát (Unsupervised Learning) *  Khái niệm học không giám sát Học không giám sát là học với tập dữ liệu huấn luyện ban đầu hoàn toàn chưa được gán nhãn. Học không giám sát là phương pháp học sử dụng cho lớp bài toán gom cụm, phân cụm (Clustering).   *  Nội dung phương pháp học không giám sát - Để thực hiện phân cụm, trước tiên cần một tập dữ liệu huấn luyện (training dataset) – là một tập các ví dụ học (training examples/instances). Trong đó, mỗi ví dụ học chỉ chứa thông tin biểu diễn (ví dụ: một vector các giá trị thuộc tính), mà không có bất kỳ thông tin gì về nhãn lớp hoặc giá trị đầu ra mong muốn (expected output). - Áp dụng một thuật toán học không có giám sát (ví dụ k-Means) để học hàm/mô hình mục tiêu (trong trường hợp này là hàm phân cụm ứng với thuật toán được chọn). - Sử dụng một phương pháp thử nghiệm (có thể kết hợp với một tập dữ liệu có gán nhãn) để đánh giá hiệu năng/chất lượng của hàm mục tiêu học được.   *  Một số thuật toán học không giám sát Có rất nhiều thuật toán học không giám sát được ra đời và phát triển nhằm giải quyết bài toán phân cụm phục vụ khai thác hiệu quả nguồn dữ liệu chưa gán nhãn nhiều và rất đa dạng. Việc lựa chọn sử dụng thuật toán nào tuỳ thuộc vào dữ liệu và mục đích của từng bài toán. Trong đó các thuật toán thường được sử dụng như: k-means, HAC  (Hierarchical Agglomerative Clustering) , SOM  (Self-Organizing Map) , DBSCAN, FCM,... (chi tiết các thuật toán này có thể tìm kiếm trên Internet)   Phương pháp học bán giám sát (Semi-Supervised Learning) Trong thực tế, để có được một tập dữ liệu có chất lượng và đã được gán nhãn của một lĩnh vực, thường được thực hiện thủ công bằng tay bởi người có nhiều kinh nghiệm về lĩnh vực đó. Vì vậy, dữ liệu đã được gán nhãn thường ít và đắt. Trong khi đó, dữ liệu chưa được gán nhãn lại rất nhiều và phong phú. Phương pháp học bán giám sát (hay học nửa giám sát) được đặt ra để tận dụng cả hai nguồn dữ liệu này.   *  Khái niệm học bán giám sát Học bán giám sát là học với tập dữ liệu huấn luyện gồm cả dữ liệu đã được gán nhãn và dữ liệu chưa được gán nhãn. Tuỳ vào từng mục đích cụ thể, học bán giám sát có thể được áp dụng cho bài toán phân lớp hoặc phân cụm.   *  Nội dung phương pháp học bán giám sát -  Nội dung chính của học bán giám sát là hệ thống  sử dụng  một tập học (training set) gồm 2 phần: các ví dụ học có nhãn, thường với số lượng (rất) ít, và các ví dụ học không có nhãn, thường với số lượng (rất) nhiều. Thực tế cho thấy khi sử dụng kết hợp dữ liệu không có nhãn với một lượng nhất định dữ liệu có nhãn có thể tăng độ chính xác đáng kể. -  Một thuật toán học bán giám sát được sử dụng (ví dụ Self-training) sẽ học các ví dụ có nhãn, sau đó  tiến hành gán nhãn cho một số (có lựa chọn) các ví dụ không có nhãn - một cách hợp lý, có đánh giá chất lượng công việc  hay  độ chính xác. Tiếp theo, chọn các ví dụ vừa được gán nhãn có độ tin cậy cao (vượt trên một ngưỡng chọn trước) đưa vào kết hợp với tập dữ liệu có nhãn, tạo thành một tập dữ liệu huấn luyện mới. -  Áp dụng một phương pháp kiểm thử (có thể kết hợp với một tập dữ liệu đã biết trước nhãn) để đánh giá hiệu năng/độ chính xác của mô hình.   *  Một số thuật toán học bán giám sát Một số thuật toán thường được sử dụng gồm có: thuật toán Cực đại kỳ vọng (EM - Expectation Maximization), SVM truyền dẫn (TSVM - Transductive Support Vector Machine), Self-training, Co-training và các phương pháp dựa trên đồ thị (graph-based). Việc lựa chọn thuật toán nào dựa trên một số định hướng: nếu các lớp dữ liệu có tính phân cụm cao thì nên dùng EM với mô hình hỗn hợp sinh; nếu đã sử dụng SVM thì mở rộng thành TSVM; khi khó nâng cấp mô hình học có giám sát đã có, thì nên dùng self-training; nếu các đặc trưng của dữ liệu phân chia tự nhiên thành hai phần riêng rẽ thì nên dùng Co-training; còn nếu hai mẫu dữ liệu có đặc trưng tương tự nhau hướng tới một lớp thì sử dụng phương pháp dựa trên đồ thị.   Trong số các thuật toán học bán giám sát thông dụng, có 2 thuật toán tiêu biểu là Self-training và Co-training. - Thuật toán Self-training: Self-training là kỹ thuật học bán giám sát được sử dụng khá phổ biến do tận dụng được nguồn dữ liệu chưa gán nhãn lớn và ban đầu chỉ cần lượng nhỏ dữ liệu đã gán nhãn. Nội dung chính của Self-training là lặp nhiều lần phương pháp học có giám sát. Gọi     D: là tập các dữ liệu đã được gán nhãn.           C : là tập các dữ liệu chưa gán nhãn. Thuật toán Self-training thực hiện như sau: Lặp  (cho đến khi C =  Æ ): i. Huấn luyện bộ phân lớp có giám sát  h  trên tập D ii. Sử dụng  h  để phân lớp dữ liệu trong tập C iii. Tìm tập con C’  Í  C có độ tin cậy cao nhất: D + C’  Þ  D ; C – C’  Þ  C. Ban đầu huấn luyện bộ phân lớp bằng cách cho bộ phân lớp học một tập dữ liệu huấn luyện đã được gán nhãn (tập này thường nhỏ so với tập dữ liệu chưa gán nhãn). Dùng bộ phân lớp đã được huấn luyện, phân lớp cho các dữ liệu chưa được gán nhãn. Trong số dữ liệu mới được gán nhãn, chọn các dữ liệu có độ tin cậy cao (lớn hơn một ngưỡng nào đó) kèm với nhãn vừa gán, đem bổ sung vào tập dữ liệu huấn luyện ban đầu. Sau đó, bộ phân lớp được học lại trên tập huấn luyện mới (gồm dữ liệu đã gán nhãn ban đầu và dữ liệu do bộ phân lớp mới gán nhãn) và thuật toán được lặp lại. Sau mỗi vòng lặp, bộ phân lớp sẽ bổ sung một số mẫu dữ liệu có độ tin cậy cao nhất cùng với dự đoán phân lớp của chúng vào tập dữ liệu huấn luyện. Tên gọi Self-training xuất phát từ việc sử dụng dự đoán của nó để huấn luyện chính nó.   - Thuật toán Co-training: Thuật toán Co-training dựa trên giả thuyết rằng các đặc trưng của tập dữ liệu huấn luyện có thể được phân chia thành 2 tập con (trường hợp lý tưởng là hai tập con này thoả mãn điều kiện độc lập nhau - conditional independent). Nội dung chính của thuật toán như sau: + Dùng 2 bộ phân lớp phù hợp để học 2 tập con tương ứng (mỗi tập con huấn luyện một bộ phân lớp). + Mỗi bộ phân lớp thực hiện phân lớp cho các dữ liệu chưa gán nhãn, thu được kết quả là tập dữ liệu chưa gán nhãn kèm theo nhãn dự đoán của chúng. Trong tập kết quả của bộ phân lớp 1, chọn ra những mẫu dữ liệu (kèm nhãn đã dự đoán) có độ tin cậy cao nhất bổ sung vào tập huấn luyện của bộ phân lớp 2 và ngược lại. + Mỗi bộ phân lớp được học lại tập dữ liệu huấn luyện (gồm dữ liệu gán nhãn ban đầu và dữ liệu gán nhãn mới bổ sung từ kết quả của bộ phân lớp kia). Quá trình được lặp lại cho đến khi tập dữ liệu chưa gán nhãn rỗng hoặc số vòng lặp đạt tới một ngưỡng được xác định trước.   Thuật toán Co-training: (1). Huấn luyện hai bộ phân lớp:   f  (1)  từ (X l  (1) , Y l ), f  (2)  từ (X l  (2) , Y l ).  (2). Phân lớp các mẫu dữ liệu chưa gán nhãn X u  với f  (1)  và f  (2)  tách biệt nhau. (U là tập các mẫu dữ liệu chưa gán nhãn) (3).  Chèn thêm vào f  (1)  k-most-confident (x, f  (1) (x)) tới các dữ liệu đã gán nhãn của f  (2) . (4). Chèn thêm vào f  (2)  k-most-confident (x, f  (2)  (x)) tới các dữ liệu đã gán nhãn của f  (1) . (5). Lặp lại các quá trình trên.   Thuật toán Co-training trên có thể viết như sau: L: là tập các mẫu dữ liệu đã gán nhãn U: là tập các mẫu dữ liệu chưa gán nhãn (1). L có thể phân chia thành hai tập con L 1  và L 2  (trường hợp lý tưởng thì L 1  và L 2  độc lập nhau). (2). Cho bộ phân lớp h 1  học L 1  (hay L 1  huấn luyện bộ phân lớp h 1 ) Cho bộ phân lớp h 2  học L 2  (hay dùng L 2  huấn luyện bộ phân lớp h 2 ) (3). Dùng h 1  phân lớp cho U thu được tập U 1 ’ kèm nhãn dự đoán của chúng. Dùng h 2  phân lớp cho U thu được tập U 2 ’ kèm nhãn dự đoán của chúng. (4). Từ U 1 ’ chọn ra u 1  mẫu dữ liệu kèm theo nhãn của nó, có độ tin cậy cao nhất. Bổ sung u 1  vào L 2 . Khi đó, L 2  + u 1  => L 2 . Từ U 2 ’ chọn ra u 2  mẫu dữ liệu kèm theo nhãn của nó, có độ tin cậy cao nhất. Bổ sung u 2  vào L 1 . Khi đó, L 1  + u 2  => L 1 . (5). Dùng L 1  mới huấn luyện bộ phân lớp h 1  (hay h 1  học L 1 ) Dùng L 2  mới huấn luyện bộ phân lớp h 2  (hay h 2  học L 2 ) (6). Lặp lại từ bước (3). cho đến khi tập U rỗng hoặc số vòng lặp đạt đến ngưỡng xác định trước.   Có thể viết rút gọn bằng cách bỏ bước (5). ở trên. Bước (6). đổi thành bước (5): Lặp lại từ bước (2). cho đến khi tập U rỗng hoặc số vòng lặp đạt đến ngưỡng xác định trước.   Phương pháp học có giám sát (Supervised Learning) *  Khái niệm học có giám sát : Học có giám sát là học với tập dữ liệu huấn luyện ban đầu hoàn toàn được gán nhãn từ trước. Học có giám sát là phương pháp học sử dụng cho lớp bài toán phân lớp, phân loại (Classification).   *  Nội dung phương pháp học có giám sát : -  Để thực hiện phân lớp, trước tiên phải chuẩn bị một tập dữ liệu huấn luyện (trainning data set), để có  tập dữ liệu huấn luyện  phải thực hiện gán nhãn cho dữ liệu ban đầu, đây được gọi là quá trình thu thập tập huấn luyện.  -  Lựa chọn một thuật toán phân lớp (ví dụ SVM) xây dựng bộ phân lớp để  học  tập dữ liệu huấn luyện. Hay nói cách khác, dùng tập dữ liệu huấn luyện để huấn luyện bộ phân lớp. Thuật ngữ  học có giám sát  được hiểu là  học  tập dữ liệu đã được gán nhãn trước (các dữ liệu kèm theo nhãn tương ứng này coi như đã được giám sát bởi người thực hiện gán nhãn). -  Sử dụng một tập dữ liệu kiểm tra (test data set) đã được gán nhãn trước, để kiểm tra tính đúng đắn của bộ phân lớp. Sau đó, có thể dùng bộ phân lớp để phân lớp cho các dữ liệu mới.   *  Một số thuật toán học có giám sát : Một số thuật toán thường được lựa chọn khi xây dựng bộ phân lớp gồm có: máy vector hỗ trợ (Support Vector Machine – SVM); k láng giềng gần nhất (K Nearest Neighbours – KNN); tiếp cận xác suất thống kê (Naïve Bayes – NB); Cây quyết định (Decision Tree – DT); sử dụng mạng nơron (Neural Network – Nnet); dựa trên vector trọng tâm (Centroid–base vector); hay tuyến tính bình phương nhỏ nhất (Linear Least Square Fit – LLSF). (Chi tiết các thuật toán này có thể tham khảo trên Internet). Từ khóa đại diện:  Machine Learning ,  Học máy ,  máy học Điểm chủ đề: 20 \r\n\t\t\t\t\t\t\t\t        05-17-2013 01:16 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  TienPhong Tham gia 03-04-2012 Điểm 110 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Các phương pháp học máy (Machine Learning)\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Mình đính chính một vài lỗi soạn thảo nhỏ: Trong phần “Bản chất của học máy dưới góc nhìn của xác suất thống kê”  - x 1 , x 2 , …, x l  là các mẫu,  x i  thuộc R n .  Các mẫu x i  được phát sinh ngẫu nhiên theo một hàm phân bố xác suất F(x) nào đó mà ta không biết. - y i  là các kết quả học tương ứng với mẫu  x i , y i  thuộc R . Thường thì y là kết quả của một hàm f(x) nào đó – đơn trị. Tuy nhiên trong trường hợp tổng quát thì y không đơn trị. Do đó y được xác định theo một hàm phân bố điều kiện F(y|x) mà ta cũng không biết. … Trong trường hợp khi  y thuộc R , tức đây là vấn đề hồi quy (regression).  Trong trường hợp bài toán phân lớp (classification) thì  y thuộc{-1, 1}  là trường hợp nhận dạng hai lớp, nếu y i  = -1 thì x i  thuộc lớp thứ nhất (không được quan tâm), còn y i  = 1 thì x i  thuộc lớp thứ 2 (lớp được quan tâm).   Phần “ Thuật toán Self-training thực hiện như sau:” Lặp (cho đến khi C =  rỗng ): … iii. Tìm tập con C’  <ký hiệu là con thực sự của>  C có độ tin cậy cao nhất: D + C’  =>  D; C – C’  =>  C. Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        06-25-2013 11:27 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  thanhthi Tham gia 06-25-2013 Điểm 20 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Các phương pháp học máy (Machine Learning)\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ  Em có bài toán thế này mong các anh giúp đỡ - Dữ liệu đầu vào là các chỉ số của 1 mã chứng khoán theo ngày: Giá mở đóng cửa, chỉ số trung bình trượt 5 ngày, 25 ngày ... (đầu vào khoảng 10 chiều ) - và được gán nhãn là Tăng hoặc giảm - Em muốn dùng SVM để dự đoán ngày tiếp theo sẽ tăng hay giảm thì em cần phải làm như nào ạ, có thể dùng Weka với libsvm được không ạ? hoặc tool nào có thể giải quyết được bài toán này ạ ?   Các anh, các chị nào biết vui lòng chỉ giáo cho em với, em đang cần rất gấp  Cảm ơn các anh chị nhiêu ạ ! Nếu có tài liệu mong anh chị gửi cho em vào mail giúp em ạ    Trần Thị Thanh:  thanhtd112@gmail.com Điểm chủ đề: 20 Trang 1 trong số 1 (6 nội dung)  ©2008-2017 Business Intelligence Solution. All rights reserved.",
          "relevance": "1",
          "title": "Các phương pháp học máy (Machine Learning)",
          "url": "http://bis.net.vn/forums/t/619.aspx"
        },
        {
          "content": "Latest by category Q1. Quick Notes 1 15. Overfitting 11. Feature Engineering 2. Phân nhóm các thuật toán Machine Learning 1. Giới thiệu về Machine Learning Latest FundaML.com 33. Đánh giá hệ thống phân lớp (1/2) 32. Naive Bayes Classifier Viết và nhận xét các bài báo khoa học 31. Maximum Likelihood và Maximum A Posteriori Con đường học Toán của tôi 30. Ôn tập Xác Suất Q2. Transfer Learning 29. Linear Discriminant Analysis Q1. Quick Notes 1 28. Principal Component Analysis (2/2) 27. Principal Component Analysis (1/2) 26. Singular Value Decomposition 25. Matrix Factorization Collaborative Filtering 24. Neighborhood-Based Collaborative Filtering 23. Content-based Recommendation Systems 22. Multi-class SVM 21. Kernel SVM 20. Soft Margin SVM 19. Support Vector Machine 18. Duality 17. Convex Optimization Problems 16. Convex sets và convex functions 15. Overfitting 14. Multi-layer Perceptron và Backpropagation 13. Softmax Regression 12. Binary Classifiers 11. Feature Engineering 10. Logistic Regression 9. Perceptron Learning Algorithm 8. Gradient Descent (2/2) 7. Gradient Descent (1/2) 6. K-nearest neighbors 5. K-means Clustering - Applications 4. K-means Clustering 3. Linear Regression 2. Phân nhóm các thuật toán Machine Learning 1. Giới thiệu về Machine Learning Machine Learning cơ bản  About Index Tags Categories Archive Math Copyrights ebook Search Trong trang này: 1. Giới thiệu 2. Validation 2.1. Validation 2.2. Cross-validation 3. Regularization 3.1. Early Stopping 3.2. Thêm số hạng vào hàm mất mát 3.3. \\(l_2\\) regularization Ví dụ về Weight Decay với MLP 3.4. Tikhonov regularization 3.5. Regularizers for sparsity 3.6. Regularization trong sklearn 4. Các phương pháp khác 5. Tóm tắt nội dung 6. Tài liệu tham khảo Overfitting không phải là một thuật toán trong Machine Learning. Nó là một hiện tượng không mong muốn thường gặp, người xây dựng mô hình Machine Learning cần nắm được các kỹ thuật để tránh hiện tượng này. 1. Giới thiệu Đây là một câu chuyện của chính tôi khi lần đầu biết đến Machine Learning. Năm thứ ba đại học, một thầy giáo có giới thiệu với lớp tôi về Neural Networks. Lần đầu tiên nghe thấy khái niệm này, chúng tôi hỏi thầy mục đích của nó là gì. Thầy nói, về cơ bản, từ dữ liệu cho trước, chúng ta cần tìm một hàm số để biến các các điểm đầu vào thành các điểm đầu ra tương ứng, không cần chính xác, chỉ cần xấp xỉ thôi. Lúc đó, vốn là một học sinh chuyên toán, làm việc nhiều với đa thức ngày cấp ba, tôi đã quá tự tin trả lời ngay rằng  Đa thức Nội suy Lagrange  có thể làm được điều đó, miễn là các điểm đầu vào khác nhau đôi một! Thầy nói rằng “những gì ta biết chỉ là nhỏ xíu so với những gì ta chưa biết”. Và đó là những gì tôi muốn bắt đầu trong bài viết này. Nhắc lại một chút về Đa thức nội suy Lagrange: Với \\(N\\) cặp điểm dữ liệu \\((x_1, y_1), (x_2, y_2), \\dots, (x_N, y_N)\\) với các \\(x_i\\) kháu nhau đôi một, luôn tìm được một đa thức \\(P(.)\\) bậc không vượt quá \\(N-1\\) sao cho \\(P(x_i) = y_i, ~\\forall i = 1, 2, \\dots, N\\). Chẳng phải điều này giống với việc ta đi tìm một mô hình phù hợp (fit) với dữ liệu trong bài toán  Supervised Learning  hay sao? Thậm chí điều này còn tốt hơn vì trong Supervised Learning ta chỉ cần xấp xỉ thôi. Sự thật là nếu một mô hình  quá fit  với dữ liệu thì nó sẽ gây phản tác dụng! Hiện tượng  quá fit  này trong Machine Learning được gọi là  overfitting , là điều mà khi xây dựng mô hình, chúng ta luôn cần tránh. Để có cái nhìn đầu tiên về overfitting, chúng ta cùng xem Hình dưới đây. Có 50 điểm dữ liệu được tạo bằng một đa thức bậc ba cộng thêm nhiễu. Tập dữ liệu này được chia làm hai, 30 điểm dữ liệu màu đỏ cho training data, 20 điểm dữ liệu màu vàng cho test data. Đồ thị của đa thức bậc ba này được cho bởi đường màu xanh lục. Bài toán của chúng ta là giả sử ta không biết mô hình ban đầu mà chỉ biết các điểm dữ liệu, hãy tìm một mô hình “tốt” để mô tả dữ liệu đã cho.  Underfitting và Overfitting với Polynomial Regression ( Source code ).\n Với những gì chúng ta đã biết từ bài  Linear Regression , với loại dữ liệu này, chúng ta có thể áp dụng  Polynomial Regression . Bài toán này hoàn toàn có thể được giải quyết bằng Linear Regression với dữ liệu mở rộng cho một cặp điểm \\((x, y)\\) là \\((\\mathbf{x}, y)\\) với \\(\\mathbf{x} = [1, x, x^2, x^3, \\dots, x^d]^T\\) cho đa thức bậc \\(d\\). Điều quan trọng là chúng ta cần tìm bậc \\(d\\) của đa thức cần tìm. Rõ ràng là một đa thức bậc không vượt quá 29 có thể  fit  được hoàn toàn với 30 điểm trong training data. Chúng ta cùng xét vài giá trị \\(d = 2, 4, 8, 16\\). Với \\(d = 2\\), mô hình không thực sự tốt vì mô hình dự đoán quá khác so với mô hình thực. Trong trường hợp này, ta nói mô hình bị  underfitting . Với \\(d = 8\\), với các điểm dữ liệu trong khoảng của training data, mô hình dự đoán và mô hình thực là khá giống nhau. Tuy nhiên, về phía phải, đa thức bậc 8 cho kết quả hoàn toàn ngược với  xu hướng của dữ liệu . Điều tương tự xảy ra trong trường hợp \\(d = 16\\). Đa thức bậc 16 này  quá fit  dữ liệu trong khoảng đang xét, và  quá fit , tức  không được mượt  trong khoảng dữ liệu training. Việc  quá fit  trong trường hợp bậc 16 không tốt vì mô hình đang cố gắng mô tả  nhiễu  hơn là dữ liệu. Hai trường hợp đa thức bậc cao này được gọi là  Overfitting . Nếu bạn nào biết về Đa thức nội suy Lagrange thì có thể hiểu được hiện tượng sai số lớn với các điểm nằm ngoài khoảng của các điểm đã cho. Đó chính là lý do phương pháp đó có từ “nội suy”, với các trường hợp “ngoại suy”, kết quả thường không chính xác. Với \\(d = 4\\), ta được mô hình dự đoán khá giống với mô hình thực. Hệ số bậc cao nhất tìm được rất gần với 0 (xem kết quả trong  source code ), vì vậy đa thưc bậc 4 này khá gần với đa thức bậc 3 ban đầu. Đây chính là một mô hình tốt. Overfitting là hiện tượng mô hình tìm được  quá khớp  với dữ liệu training. Việc  quá khớp  này có thể dẫn đến việc dự đoán nhầm nhiễu, và chất lượng mô hình không còn tốt trên dữ liệu test nữa.  Dữ liệu test được giả sử là không được biết trước, và không được sử dụng để xây dựng các mô hình Machine Learning . Về cơ bản, overfitting xảy ra khi mô hình quá phức tạp để mô phỏng training data. Điều này đặc biệt xảy ra khi lượng dữ liệu training quá nhỏ trong khi độ phức tạp của mô hình quá cao. Trong ví dụ trên đây, độ phức tạp của mô hình có thể được coi là bậc của đa thức cần tìm. Trong  Multi-layer Perceptron , độ phức tạp của mô hình có thể được coi là số lượng hidden layers và số lượng units trong các hidden layers. Vậy, có những kỹ thuật nào giúp tránh Overfitting? Trước hết, chúng ta cần một vài đại lượng để đánh giá chất lượng của mô hình trên training data và test data. Dưới đây là hai đại lượng đơn giản, với giả sử \\(\\mathbf{y}\\) là đầu ra thực sự (có thể là vector), và \\(\\mathbf{\\hat{x}}\\) là đầu ra dự đoán bởi mô hình: Train error:  Thường là hàm mất mát áp dụng lên training data. Hàm mất mát này cần có một thừa số \\(\\frac{1}{N_{\\text{train}}} \\) để tính giá trị trung bình, tức mất mát trung bình trên mỗi điểm dữ liệu. Với Regression, đại lượng này thường được định nghĩa:\n\\[\n\\text{train error}= \\frac{1}{N_{\\text{train}}} \\sum_{\\text{training set}} \\|\\mathbf{y} - \\mathbf{\\hat{y}}\\|_p^2\n\\]\nvới \\(p\\)  thường bằng 1 hoặc 2 . Với Classification, trung bình cộng của  cross entropy  có thể được sử dụng. Test error:  Tương tự như trên nhưng áp dụng mô hình tìm được vào  test data . Chú ý rằng, khi xây dựng mô hình, ta không được sử dụng thông tin trong tập dữ liệu test. Dữ liệu test chỉ được dùng để đánh giá mô hình. Với Regression, đại lượng này thường được định nghĩa:\n\\[\n\\text{test error}= \\frac{1}{N_{\\text{test}}} \\sum_{\\text{test set}} \\|\\mathbf{y} - \\mathbf{\\hat{y}}\\|_p^2\n\\] với \\(p\\) giống như \\(p\\) trong cách tính  train error  phía trên. Việc lấy trung bình là quan trọng vì lượng dữ liệu trong hai tập hợp training và test có thể chênh lệch rất nhiều. Một mô hình được coi là tốt (fit) nếu cả  train error  và  test error  đều thấp. Nếu  train error  thấp nhưng  test error  cao, ta nói mô hình bị overfitting. Nếu  train error  cao và  test error  cao, ta nói mô hình bị underfitting. Nếu  train error  cao nhưng  test error  thấp, tôi không biết tên của mô hình này, vì cực kỳ may mắn thì hiện tượng này mới xảy ra, hoặc có chỉ khi tập dữ liệu test quá nhỏ. Chúng ta cùng đi vào phương pháp đầu tiên 2. Validation 2.1. Validation Chúng ta vẫn quen với việc chia tập dữ liệu ra thành hai tập nhỏ: training data và test data. Và một điều tôi vẫn muốn nhắc lại là khi xây dựng mô hình, ta không được sử dụng test data. Vậy làm cách nào để biết được chất lượng của mô hình với  unseen data  (tức dữ liệu chưa nhìn thấy bao giờ)? Phương pháp đơn giản nhất là  trích  từ tập training data ra một tập con nhỏ và thực hiện việc đánh giá mô hình trên tập con nhỏ này. Tập con nhỏ  được trích ra từ training set  này được gọi là  validation set . Lúc này,  training set là phần còn lại của training set ban đầu . Train error được tính trên training set mới này, và có một khái niệm nữa được định nghĩa tương tự như trên  validation error , tức error được tính trên tập validation. Việc này giống như khi bạn ôn thi. Giả sử bạn không biết đề thi như thế nào nhưng có 10 bộ đề thi từ các năm trước. Để xem trình độ của mình trước khi thi thế nào, có một cách là bỏ riêng một bộ đề ra, không ôn tập gì. Việc ôn tập sẽ được thực hiện dựa trên 9 bộ còn lại. Sau khi ôn tập xong, bạn bỏ bộ đề đã để riêng ra làm thử và kiểm tra kết quả, như thế mới “khách quan”, mới giống như thi thật. 10 bộ đề ở các năm trước là “toàn bộ” training set bạn có. Để tránh việc học lệch, học tủ theo chỉ 10 bộ, bạn tách 9 bộ ra làm training set thật, bộ còn lại là validation test. Khi làm như thế thì mới đánh giá được việc bạn học đã tốt thật hay chưa, hay chỉ là  học tủ . Vì vậy,  Overfitting  còn có thể so sánh với việc  Học tủ  của con người. Với khái niệm mới này, ta tìm mô hình sao cho cả  train eror  và  validation error  đều nhỏ, qua đó có thể dự đoán được rằng  test error  cũng nhỏ. Phương pháp thường được sử dụng là sử dụng nhiều mô hình khác nhau. Mô hình nào cho  validation error  nhỏ nhất sẽ là mô hình tốt. Thông thường, ta bắt đầu từ mô hình đơn giản, sau đó tăng dần độ phức tạp của mô hình. Tới khi nào  validation error  có chiều hướng tăng lên thì chọn mô hình ngay trước đó. Chú ý rằng mô hình càng phức tạp,  train error  có xu hướng càng nhỏ đi. Hính dưới đây mô tả ví dụ phía trên với bậc của đa thức tăng từ 1 đến 8. Tập validation bao gồm 10 điểm được lấy ra từ tập training ban đầu. Hình 2: Lựa chọn mô hình dựa trên validation ( Source code ). Chúng ta hãy tạm chỉ xét hai đường màu lam và đỏ, tương ứng với  train error  và  validation error . Khi bậc của đa thức tăng lên,  train error  có xu hướng giảm. Điều này dễ hiểu vì đa thức bậc càng cao, dữ liệu càng được  fit . Quan sát đường màu đỏ, khi bậc của đa thức là 3 hoặc 4 thì  validation error  thấp, sau đó tăng dần lên. Dựa vào  validation error , ta có thể xác định được bậc cần chọn là 3 hoặc 4. Quan sát tiếp đường màu lục, tương ứng với  test error , thật là trùng hợp, với bậc bằng 3 hoặc 4,  test error  cũng đạt giá trị nhỏ nhất, sau đó tăng dần lên. Vậy cách làm này ở đây đã tỏ ra hiệu quả. Việc không sử dụng  test data  khi lựa chọn mô hình ở trên nhưng vẫn có được kết quả khả quan vì ta giả sử rằng  validation data  và  test data  có chung một đặc điểm nào đó. Và khi cả hai đều là  unseen data ,  error  trên hai tập này sẽ tương đối giống nhau. Nhắc lại rằng, khi bậc nhỏ (bằng 1 hoặc 2), cả ba error đều cao, ta nói mô hình bị  underfitting . 2.2. Cross-validation Trong nhiều trường hợp, chúng ta có rất hạn chế số lượng dữ liệu để xây dựng mô hình. Nếu lấy quá nhiều dữ liệu trong tập training ra làm dữ liệu validation, phần dữ liệu còn lại của tập training là không đủ để xây dựng mô hình. Lúc này, tập validation phải thật nhỏ để giữ được lượng dữ liệu cho training đủ lớn. Tuy nhiên, một vấn đề khác nảy sinh. Khi tập validation quá nhỏ, hiện tượng overfitting lại có thể xảy ra với tập training còn lại. Có giải pháp nào cho tình huống này không? Câu trả lời là  cross-validation . Cross validation  là một cải tiến của  validation  với lượng dữ liệu trong tập validation là nhỏ nhưng chất lượng mô hình được đánh giá trên nhiều tập  validation  khác nhau. Một cách thường đường sử dụng là chia tập training ra \\(k\\) tập con không có phần tử chung, có kích thước gần bằng nhau. Tại mỗi lần kiểm thử , được gọi là  run , một trong số \\(k\\) tập con được lấy ra làm  validata set . Mô hình sẽ được xây dựng dựa vào hợp của \\(k-1\\) tập con còn lại. Mô hình cuối được xác định dựa trên trung bình của các  train error  và  validation error . Cách làm này còn có tên gọi là  k-fold cross validation . Khi \\(k\\) bằng với số lượng phần tử trong tập  training  ban đầu, tức mỗi tập con có đúng 1 phần tử, ta gọi kỹ thuật này là  leave-one-out . Sklearn hỗ trợ rất nhiều phương thức cho phân chia dữ liệu và tính toán  scores  của các mô hình. Bạn đọc có thể xem thêm tại  Cross-validation: evaluating estimator performance . 3. Regularization Một nhược điểm lớn của  cross-validation  là số lượng  training runs  tỉ lệ thuận với \\(k\\). Điều đáng nói là mô hình polynomial như trên chỉ có một tham số cần xác định là bậc của đa thức. Trong các bài toán Machine Learning, lượng tham số cần xác định thường lớn hơn nhiều, và khoảng giá trị của mỗi tham số cũng rộng hơn nhiều, chưa kể đến việc có những tham số có thể là số thực. Như vậy, việc chỉ xây dựng một mô hình thôi cũng là đã rất phức tạp rồi. Có một cách giúp số mô hình cần huấn luyện giảm đi nhiều, thậm chí chỉ một mô hình. Cách này có tên gọi chung là  regularization . Regularization , một cách cơ bản, là thay đổi mô hình một chút để tránh overfitting trong khi vẫn giữ được tính tổng quát của nó (tính tổng quát là tính mô tả được nhiều dữ liệu, trong cả tập training và test). Một cách cụ thể hơn, ta sẽ tìm cách  di chuyển  nghiệm của bài toán tối ưu hàm mất mát tới một điểm gần nó. Hướng di chuyển sẽ là hướng làm cho mô hình  ít phức tạp hơn  mặc dù giá trị của hàm mất mát có tăng lên một chút. Một kỹ thuật rất đơn giản là  early stopping . 3.1. Early Stopping Trong nhiều bài toán Machine Learning, chúng ta cần sử dụng các thuật toán lặp để tìm ra nghiệm, ví dụ như Gradient Descent. Nhìn chung, hàm mất mát giảm dần khi số vòng lặp tăng lên. Early stopping tức dừng thuật toán trước khi hàm mất mát đạt giá trị quá nhỏ, giúp tránh overfitting. Vậy dừng khi nào là phù hợp? Một kỹ thuật thường được sử dụng là tách từ training set ra một tập validation set như trên. Sau một (hoặc một số, ví dụ 50) vòng lặp, ta tính cả  train error  và  validation error , đến khi  validation error  có chiều hướng tăng lên thì dừng lại, và quay lại sử dụng mô hình tương ứng với điểm và  validation error  đạt giá trị nhỏ. Hình 3: Early Stopping. Đường màu xanh là  train error , đường màu đỏ là  validation error . Trục x là số lượng vòng lặp, trục y là error. Mô hình được xác định tại vòng lặp mà  validation error  đạt giá trị nhỏ nhất.  ( Overfitting - Wikipedia ). Hình trên đây mô tả cách tìm điểm  stopping . Chúng ta thấy rằng phương pháp này khá giống với phương pháp tìm bậc của đa thức ở phần trên của bài viết. 3.2. Thêm số hạng vào hàm mất mát Kỹ thuật regularization phổ biến nhất là thêm vào hàm mất mát một số hạng nữa. Số hạng này thường dùng để đánh giá độ phức tạp của mô hình. Số hạng này càng lớn, thì mô hình càng phức tạp.  Hàm mất mát mới  này thường được gọi là  regularized loss function , thường được định nghĩa như sau:\n\\[\nJ_{\\text{reg}}(\\theta) = J(\\theta) + \\lambda R(\\theta)\n\\] Nhắc lại rằng \\(\\theta\\) được dùng để ký hiệu các biến trong mô hình, chẳng hạn như các hệ số \\(\\mathbf{w}\\) trong Neural Networks. \\(J(\\theta)\\) ký hiệu cho hàm mất mát ( loss function ) và \\(R(\\theta)\\) là số hạng  regularization . \\(\\lambda\\) thường là một số dương để cân bằng giữa hai đại lượng ở vế phải. Việc tối thiểu  regularized loss function , nói một cách tương đối, đồng nghĩa với việc tối thiểu cả  loss function  và số hạng  regularization . Tôi dùng cụm “nói một cách tương đối” vì nghiệm của bài toán tối ưu  loss function  và  regularized loss function  là khác nhau.  Chúng ta vẫn mong muốn rằng sự khác nhau này là nhỏ, vì vậy tham số regularization ( regularizaton parameter ) \\(\\lambda\\) thường được chọn là một số nhỏ để biểu thức regularization không làm giảm quá nhiều chất lượng của nghiệm. Với các mô hình Neural Networks, một số kỹ thuật regularization thường được sử dụng là: 3.3. \\(l_2\\) regularization Trong kỹ thuật này:\n\\[\nR(\\mathbf{w}) = \\|\\mathbf{w}\\|_2^2\n\\]\ntức norm 2 của hệ số. Nếu bạn đọc chưa quen thuộc với khái niệm norm, bạn được khuyến khích đọc  phần phụ lục này . Hàm số này có một vài đặc điểm đang lưu ý: Thứ nhất, \\(\\|\\mathbf{w}\\|_2^2\\) là một hàm số  rất mượt , tức có đạo hàm tại mọi , đạo hàm của nó đơn giản là \\(\\mathbf{w}\\), vì vậy đạo hàm của  regularized loss function  cũng rất dễ tính, chúng ta có thể hoàn toàn dùng các phương pháp dựa trên gradient để cập nhật nghiệm. Cụ thể:\n\\[\n\\frac{\\partial J_{\\text{reg}} }{\\partial \\mathbf{w}} = \\frac{\\partial J}{\\partial \\mathbf{w}} + \\lambda \\mathbf{w}\n\\] Thứ hai, việc tối thiểu \\(\\|\\mathbf{w}\\|_2^2\\) đồng nghĩa với việc khiến cho các giá trị của hệ số \\(\\mathbf{w}\\) trở nên nhỏ gần với 0. Với Polynomial Regression, việc các hệ số này nhỏ có thể giúp các hệ số ứng với các số hạng bậc cao là nhỏ, giúp tránh overfitting. Với Multi-layer Pereceptron, việc các hệ số này nhỏ giúp cho nhiều hệ số trong các ma trận trọng số là nhỏ. Điều này tương ứng với việc số lượng các hidden units  hoạt động  (khác không) là nhỏ, cũng giúp cho MLP tránh được hiện tượng overfitting. \\(l_2\\) regularization là kỹ thuật được sử dụng nhiều nhất để giúp Neural Networks tránh được overfitting. Nó còn có tên gọi khác là  weight decay .  Decay  có nghĩa là  tiêu biến . Trong Xác suất thống kê, Linear Regression với \\(l_2\\) regularization được gọi là  Ridge Regression . Hàm mất mát của  Ridge Regression  có dạng:\n\\[\nJ(\\mathbf{w}) = \\frac{1}{2} \\|\\mathbf{y} - \\mathbf{Xw}\\|_2^2 + \\lambda \\|\\mathbf{w}\\|_2^2\n\\]\ntrong đó, số hạng đầu tiên ở vế phải chính là hàm mất mát của Linear Regression. Số hạng thứ hai chính là phần regularization. Ví dụ về Weight Decay với MLP Chúng ta sử dụng  mô hình MLP giống như bài trước  nhưng dữ liệu có khác đi đôi chút. # To support both python 2 and python 3 from __future__ import division , print_function , unicode_literals import math import numpy as np import matplotlib.pyplot as plt np . random . seed ( 4 ) means = [[ - 1 , - 1 ], [ 1 , - 1 ], [ 0 , 1 ]] cov = [[ 1 , 0 ], [ 0 , 1 ]] N = 20 X0 = np . random . multivariate_normal ( means [ 0 ], cov , N ) X1 = np . random . multivariate_normal ( means [ 1 ], cov , N ) X2 = np . random . multivariate_normal ( means [ 2 ], cov , N ) Dữ liệu được tạo là ba cụm tuân theo phân phối chuẩn có tâm ở  [[-1, -1], [1, -1], [0, 1]] . Trong ví dụ này, chúng ta sử dụng số hạng regularization:\n\\[\n\\lambda R(\\mathbf{W}) = \\lambda \\sum_{l=1}^L \\|\\mathbf{W}^{(l)}\\|_F^2\n\\] với \\(\\|.\\|_F\\) là  Frobenius norm , là căn bậc hai của tổng bình phường các phẩn tử của ma trận. (Bạn đọc được khuyến khích đọc bài  MLP  để hiểu các ký hiệu). Chú ý rằng weight decay ít khi được áp dụng lên biases. Tôi thay đổi tham số regularization \\(\\lambda\\) và nhận được kết quả như sau:  Multi-layer Perceptron với Weight Decay ( Source code ).\n Khi \\(\\lambda = 0\\), tức không có regularization, ta nhận thấy gần như toàn bộ dữ liệu trong tập training được phân lớp đúng. Việc này khiến cho các class bị phân làm nhiều mảnh không được tự nhiên. Khi \\(\\lambda = 0.001\\), vẫn là một số nhỏ, các đường phân chia trông tự nhiên hơn, nhưng lớp màu xanh lam vẫn bị chia làm hai bởi lớp màu xanh lục. Đây chính là biểu hiện của overfitting. Khi \\(\\lambda\\) tăng lên, tức sự ảnh hưởng của regularization tăng lên (xem hàng dưới), đường ranh giới giữa các lớp trở lên tự nhiên hơn. Nói cách khác, với \\(\\lambda\\) đủ lớn, weight decay có tác dụng hạn chế overfitting trong MLP. Bạn đọc hãy thử vào trong  Source code , thay \\(\\lambda = 1\\) bằng cách thay dòng cuối cùng: mynet ( 1 ) rồi chạy lại toàn bộ code, xem các đường phân lớp trông như thế nào. Gợi ý:  underfitting . Khi \\(\\lambda\\) quá lớn, tức ta xem phần  regularization  quan trọng hơn phần  loss fucntion , một hiện tượng xấu xảy ra là các phần tử của \\(\\mathbf{w}\\) tiến về 0 để thỏa mãn regularization là nhỏ. Sklearn có cung cấp rất nhiều chức năng cho MLP , trong đó ta có thể lựa chọn số lượng hidden layers và số lượng hidden units trong mỗi layer, activation functions, weight decay,  learning rate, hệ số momentum, nesterovs_momentum , có early stopping hay không, lượng dữ liệu được tách ra làm validation set, và nhiều chức năng khác. 3.4. Tikhonov regularization \\[\n\\lambda R(\\mathbf{w}) = \\|\\Gamma \\mathbf{w}\\|_2^2\n\\] Với \\(\\Gamma\\) (viết hoa của gamma) là một ma trận. Ma trận \\(\\Gamma\\) hay được dùng nhất là ma trận đường chéo. Nhận thấy rằng \\(l_2\\) regularization chính là một trường hợp đặc biệt của Tikhonov regularization với \\(\\Gamma = \\lambda \\mathbf{I}\\) với \\(\\mathbf{I}\\) là ma trận đơn vị ( the identity matrix ), tức các phần tử trên đường chéo của \\(\\Gamma\\) là như nhau. Khi các phần tử trên đường chéo của \\(\\Gamma\\) là khác nhau, ta có một phiên bản gọi là  weighted \\(l_2\\) regularization , tức đánh trọng số khác nhau cho mỗi phần tử trong \\(\\mathbf{w}\\). Phần tử nào càng bị đánh trọng số cao thì nghiệm tương ứng càng nhỏ (để đảm bảo rằng hàm mất mát là nhỏ). Với Polynomial Regression, các phần tử ứng với hệ số bậc cao sẽ được đánh trọng số cao hơn, khiến cho xác suất để chúng gần 0 là lớn hơn. 3.5. Regularizers for sparsity Trong nhiều trường hợp, ta muốn các hệ số  thực sự  bằng 0 chứ không phải là  nhỏ gần 0  như \\(l_2\\) regularization đã làm phía trên. Lúc đó, có một regularization khác được sử dụng, đó là \\(l_0\\) regularization:\n\\[\nR(\\mathbf{W}) = \\|\\mathbf{w}\\|_0\n\\] Norm 0 không phải là một norm thực sự mà là giả norm. (Bạn được khuyến khích đọc thêm về  norms (chuẩn) ). Norm 0 của một vector là số các phần tử khác không của vector đó. Khi norm 0 nhỏ, tức rất nhiều phần tử trong vector đó bằng 0, ta nói vector đó là  sparse . Việc giải bài toán tổi thiểu norm 0 nhìn chung là khó vì hàm số này không  convex , không liên tục. Thay vào đó, norm 1 thường được sử dụng:\n\\[\nR(\\mathbf{W}) = \\|\\mathbf{w}\\|_1 = \\sum_{i=0}^d |w_i|\n\\] Norm 1 là tổng các trị tuyệt đối của tất cả các phần tử. Người ta đã chứng minh được rằng tối thiểu norm 1 sẽ dẫn tới nghiệm có nhiều phần tử bằng 0. Ngoài ra, vì norm 1 là một  norm thực sự  (proper norm) nên hàm số này là  convex , và hiển nhiên là liên tục, việc giải bài toán này dễ hơn việc giải bài toán tổi thiểu norm 0. Về \\(l_1\\) regularization, bạn đọc có thể đọc thêm trong  lecture note  này. Việc giải bài toán \\(l_1\\) regularization nằm ngoài mục đích của tôi trong bài viết này. Tôi hứa sẽ quay lại phần này sau. (Vì đây là phần chính trong nghiên cứu của tôi). Trong Thống Kê, việc sử dụng \\(l_1\\) regularization còn được gọi là  LASSO  (Least Absolute Shrinkage and Selection Operator)). Khi cả \\(l_2\\) và \\(l_1\\) regularization được sử dụng, ta có mô hình gọi là  Elastic Net Regression . 3.6. Regularization trong sklearn Trong  sklearn , ví dụ  Logistic Regression , bạn cũng có thể sử dụng các \\(l_1\\) và \\(l_2\\) regularizations bằng cách khai báo biến  penalty='l1'  hoặc  penalty = 'l2'  và biến  C , trong đó  C  là  nghịch đảo  của \\(\\lambda\\). Trong các bài trước khi chưa nói về  Overfitting và Regularization, tôi có sử dụng  C = 1e5  để chỉ ra rằng \\(\\lambda\\) là một số rất nhỏ. 4. Các phương pháp khác Ngoài các phương pháp đã nêu ở trên, với mỗi mô hình, nhiều phương pháp tránh overfitting khác cũng được sử dụng. Điển hình là  Dropout trong Deep Neural Networks mới được đề xuất gần đây . Một cách ngắn gọn, dropout là một phương pháp  tắt  ngẫu nhiên các units trong Networks.  Tắt  tức cho các unit giá trị bằng không và tính toán feedforward và backpropagation bình thường trong khi training. Việc này không những giúp lượng tính toán giảm đi mà còn làm giảm việc overffitng. Tôi xin được quay lại vấn đề này nếu có dịp nói  sâu về Deep Learning trong tương lai. Bạn đọc có thể tìm đọc thêm với các từ khóa:  pruning  (tránh overftting trong Decision Trees),  VC dimension  (đo độ phức tạp của mô hình, độ phức tạp càng lớn thì càng dễ bị overfitting). 5. Tóm tắt nội dung Một mô hình mô tốt là mộ mô hình có  tính tổng quát , tức mô tả được dữ liệu cả trong lẫn ngoài tập training. Mô hình chỉ mô tả tốt dữ liệu trong tập training được gọi là  overfitting . Để tránh overfitting, có rất nhiều kỹ thuật được sử dụng, điển hình là  cross-validation  và  regularization . Trong Neural Networks,  weight decay  và  dropout  thường được dùng. 6. Tài liệu tham khảo [1]  Overfitting - Wikipedia [2]  Cross-validation - Wikipedia [3]  Pattern Recognition and Machine Learning [4] Krogh, Anders, and John A. Hertz.  “A simple weight decay can improve generalization.”  NIPS. Vol. 4. 1991. [5] Srivastava, Nitish, et al.  “Dropout: A Simple Way to Prevent Neural Networks from  Overfitting”  Journal of Machine Learning Research 15.1 (2014): 1929-1958. Share Share Interactive Learning Facebook page Machine Learning cơ bản Forum Recommended books \"Pattern recognition and Machine Learning.\", C. Bishop  \"The Elements of Statistical Learning\", T. Hastie et al.   \"Computer Vision:  Models, Learning, and Inference\", Simon J.D. Prince  \"Convex Optimization\", Boyd and Vandenberghe Recommended courses \"Machine Learning\", Andrew Ng  CS224n: Natural Language Processing with Deep Learning CS231n: Convolutional Neural Networks for Visual Recognition CS246: Mining Massive Data Sets CS20SI: Tensorflow for Deep Learning Research  Introduction to Computer Science and Programming Using Python Others Top-down learning path: Machine Learning for Software Engineers Blog này được tạo như thế nào? Chúng tôi đã apply và học tiến sỹ như thế nào? (1/2) Chúng tôi đã apply và học tiến sỹ như thế nào? (2/2) 8 Inspirational Applications of Deep Learning Matrix calculus TensorFlow-Examples Eight Easy Steps To Get Started Learning Artificial Intelligence The 9 Deep Learning Papers You Need To Know About",
          "relevance": "1",
          "title": "Bài 15: Overfitting",
          "url": "https://machinelearningcoban.com/2017/03/04/overfitting/"
        },
        {
          "content": "Homepage About membership Sign in Get started Homepage hongthaiphi Blocked Unblock Follow Following Oct 18, 2016 ML for everyone Bạn nghe mọi người nói liên tục về machine learning nhưng bạn vẫn cảm thấy có một lớp sương mù che phủ ngăn cách bạn với ML? Bạn có chán việc gật đầu như bổ củi khi nói chuyện với đồng nghiệp về ML? Hãy thay đổi điều đó. Bài viết này dành cho tất cả những người tò mò về ML nhưng chưa biết bắt đầu từ đâu. Tôi nghĩ có rất nhiều người đã cố gắng đọc trên wikipedia, sau đó có thể phát cuồng vì khó hiểu, rồi đầu hàng, và chỉ ước có ai đó có thể giải thích mọi thứ một cách đơn giản hơn. Đây chính là thứ bạn cần. Mục đích là mọi người đều có thể hiểu — nên có thể bạn sẽ thấy nó rất khái quát, chung chung. Nhưng có sao đâu, miễn là sau khi hiểu những thứ đơn giản chung chung, bạn có hứng thú tiếp tục tìm hiểu sâu hơn, vậy là tôi thành công. Vậy Machine Learning là gì? Machine Learning là ý tưởng về những  thuật toán tổng quát  có thể nói cho bạn biết những thông tin thú vị dựa vào tập dữ liệu, và thú vị nhất ở chỗ, với những dữ liệu khác nhau, bạn không hề phải viết lại thuật toán. Thay vì viết code, bạn đưa dữ liệu,  thuật toán tổng quát  sẽ tự xây dựng các phương án để tìm ra kết quả thú vị cho bạn. Ví dụ, thuật toán phân lớp. Nó có thể xử lý những dữ liệu rất khác nhau. Với ML, thuật toán sử dụng để nhận dạng chữ viết tay có thể đem nhận dạng một bức thư có phải là thư rác hay không mà không cần phải thay đổi một dòng code nào. Hoàn toàn là một thuật toán, chỉ cần dữ liệu huấn luyện khác nhau, sẽ đem đến những kết quả khác nhau (và đều thoả mãn ý muốn của bạn). Kì diệu không? Thuật toán trong ML giống như một hộp đen, có thể tái sử dụng để phân loại rất nhiều thứ “Machine Learning\" là một thuật ngữ chung, bao hàm rất nhiều loại  thuật toán tổng quát . Hai trong số các thuật toán ML Bạn có thể tạm hiểu rằng các thuật toán trong ML được chia làm hai loại chính—  supervised learning (học có giám sát)  và  unsupervised learning (học thiếu giám sát) . Sự khác biệt giữa hai thuật toán này khá nhỏ, nhưng thực sự quan trọng. Supervised Learning — Học có giám sát Giả định rằng bạn là một đại lý bất động sản. Việc kinh doanh của bạn đang phát triển, bạn thuê một loạt các thực tập sinh để giúp việc cho bạn. Nhưng có một vấn đề — bản thân bạn có thể nhìn qua và xác định ngay giá trị của ngôi nhà, nhưng các nhân viên mới của bạn thì không có kinh nghiệm như bạn, không biết làm sao để định giá nhà. Để giúp nhân viên của bạn (và quan trọng là để có thời gian tận hưởng một kỳ nghỉ), bạn quyết định viết một ứng dụng nhỏ mà có thể ước tính giá trị của một ngôi nhà dựa trên diện tích, vị trí … của nó. Dựa vào những ngôi nhà tương tự đã bán. Vì vậy, bạn ghi lại thông tin mỗi khi có người bán một ngôi nhà ở thành phố của bạn trong 3 tháng. Đối với mỗi căn nhà, bạn ghi lại một loạt các chi tiết — số lượng phòng ngủ, kích thước bằng feet vuông, khu phố, vv Nhưng quan trọng nhất, bạn viết ra những giá bán cuối cùng: Đây gọi là “dữ liệu học tập\" (training data) Sử dụng dữ liệu học tập này, chúng ta kì vọng sẽ tạo ra một chương trình có thể ước tính được giá trị của tất cả các căn nhà trong thành phố của bạn: Ta cần sử dụng dữ liệu học để ước lượng giá của ngôi nhà này Điều này được gọi là  supervised learning — học có giám sát,  hay học từ kinh nghiệm cũ cũng được. Bạn biết giá của rất nhiều căn nhà đã bán trước đây, nói cách khác, bạn đã biết một số câu trả lời và bạn dùng kĩ thuật lần ngược để tìm ra công thức cho các câu hỏi trong tương lai. Để xây ứng dụng, bạn đưa dữ liệu về các ngôi nhà vào thuật toán ML. Thuật toán sẽ cố gắng tìm ra những loại toán cần thực hiện để ra được con số. Kiểu này giống như đưa cho bạn một loạt các số và bạn phải điền vào các phép toán để có kết quả đúng: Trong vòng nửa phút chắc bạn sẽ tìm ngay ra cách điền dấu vào chỗ trông để ra kết quả đúng? Trong  supervised learning , cũng giống như vậy, bạn đưa cho máy tính các con số, kết quả, máy tính sẽ tìm ra mối liên hệ của vế bên trái để có được kết quả ở vế phải. Và một khi đã có phương pháp giải quyết một vấn đề cụ thể, bạn có thể tổng quát hoá để tìm mối liên hệ cho bất kì dữ liệu đầu vào nào. Unsupervised Learning — Học không giám sát Hãy quay trở lại với ví dụ ban đầu của chúng ta về đại lý bất động sản. Nếu bạn không biết giá của căn nhà nào cả thì sao? Thậm chí ngay cả khi tất cả những gì bạn biết chỉ là kích thước, vị trí … của căn nhà, ML vẫn cho phép bạn tìm ra một thứ gì rất ấn tượng. Đó gọi là  unsupervised learning — học không có giám sát. Ngay cả khi bạn không có nhu cầu dự đoán, bạn vẫn có thể làm được những thứ thú vị với ML Giống như một ai đó cho bạn một danh sách các số trên một tờ giấy và nói: “Tôi không thực sự biết những gì những con số này có ý nghĩa gì nhưng có lẽ bạn có thể tìm ra nếu có một mẫu hoặc nhóm hoặc một cái gì đó — chúc may mắn” Vậy có thể làm gì với dữ liệu này? Để bắt đầu, bạn có thể có một thuật toán tự động xác định phân khúc thị trường khác nhau trong dữ liệu của bạn. Có lẽ bạn sẽ tìm ra rằng những người mua nhà trong khu phố gần các trường đại học địa phương thực sự thích ngôi nhà nhỏ với nhiều phòng ngủ, nhưng người mua nhà ở ngoại ô thích ngôi nhà 3 phòng ngủ với diện tích lớn. Hiểu biết về các loại khách hàng khác nhau có thể giúp các nỗ lực tiếp thị của bạn đi đúng đích hơn. Một điều thú vị mà bạn có thể làm là tự động xác định bất kỳ ngôi nhà ngoại hạng nào đó — ngôi nhà có một đặc tính nào đó khác xa các ngôi bình thường khác. Có thể là biệt thự khổng lồ và bạn có thể tập trung những người bán hàng tốt nhất của bạn trên những khu vực này vì bán những ngôi nhà kiểu này sẽ có hoa hồng rất lớn. Supervised learning-học có giám sát sẽ là thứ được tập trung nói đến từ giờ tới cuối bài này, nhưng không phải vì unsupervised learning kém thú vị hoặc kém hữu dụng. Trên thực tế, unsupervised learning ngày càng trở nên quan trọng với những thuật toán ngày một tốt hơn, chúng sử dụng những dữ liệu không cần gán nhãn nên sẽ giảm công sức thu thập dữ liệu hơn so với học có giám sát. Lưu ý: Có rất nhiều  thuật toán khác  trong ML. Nhưng để bắt đầu thì chỉ cần thế này là đẹp rồi. Khá hay, nhưng có thực sự là giá của các ngôi nhà được tính toán bằng cách “học”? Là một con người, não của bạn có thể tiếp cận hầu hết các tình huống và học tập từ đó mà không cần các chỉ dẫn cụ thể nào cả. Nếu bạn làm môi giới nhà đất trong một thời gian dài, bạn sẽ tự nhiên có “cảm giác\" rất chính xác về giá của một ngôi nhà, cách tốt nhất để quảng cáo, loại khách hàng sẽ quan tâm ngôi nhà ... Mục tiêu của nghiên cứu  Strong AI  là áp dụng khả năng này của con người vào cho máy tính. Nhưng các thuật toán ML hiện tại chưa đủ tốt —chúng chỉ làm việc khi tập trung vào các vấn đề rất cụ thể, đặc biệt. Có lẽ có một định nghĩa chính xác hơn cho từ “học”-”learning\" trong trường hợp này: “tìm một phương trình để giải quyết một vấn đề cụ thể dựa trên một số dữ liệu ví dụ”. Thật không may “ Tìm một phương trình để giải quyết một vấn đề cụ thể dựa trên một số dữ liệu ví dụ ”  không phải là một cái tên hay. Vậy nên chúng ta vẫn gọi nó là “Machine Learning”. Of course if you are reading this 50 years in the future and we’ve figured out the algorithm for Strong AI, then this whole post will all seem a little quaint. Maybe stop reading and go tell your robot servant to go make you a sandwich, future human. Tất nhiên nếu 50 năm nữa bạn đọc lại bài này — khi mà chúng ta đã tìm ra thuật toán cho “Strong AI\", có lẽ bạn sẽ thấy bài viết này có chút kì lạ. Thôi không đọc nữa và bảo robot giúp việc của bạn làm một cái bánh mì Hà Nội — anh bạn từ tương lai ạ. Thôi viết chương trình đó đi! Vậy, làm thế nào có thể viết một chương trình để ước tính giá trị của một ngôi nhà như trong ví dụ của chúng ta phía trên? Hãy suy nghĩ về nó trong một giây trước khi bạn đọc thêm. Nếu bạn không biết tí gì về ML, có thể bạn sẽ cố gắng viết ra một số quy tắc cơ bản để có thể ước tính giá của một ngôi nhà, như thế này: def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):   price = 0 # Ví dụ giá nhà trung bình là 200$/m2   price_per_sqft = 200 if neighborhood == \"hipsterton\":  # nội thành thì đắt     price_per_sqft = 400 elif neighborhood == \"skid row\":  # ngoại thành thì rẻ     price_per_sqft = 100 # tính giá dựa vào độ lớn của diện tích ngôi nhà   price = price_per_sqft * sqft # Giờ hãy ước đoán dựa trên số phòng ngủ   if num_of_bedrooms == 0: # Căn hộ kiểu studio thì rẻ     price = price — 20000   else:  # nhà có nhiều phòng ngủ sẽ đắt hơn     price = price + (num_of_bedrooms * 1000) return price Nếu bạn cứ cố nghịch hàng giờ, cuối cùng thế nào bạn cũng có một chương trình chạy khá ổn, tính ra kết quả tương đối chuẩn. Nhưng nó sẽ không bao giờ hoàn hảo và rất khó để bảo trì, sửa chữa khi giá cả thay đổi. Sẽ tốt hơn nếu máy tính có thể tìm ra cách để tự viết chương trình này cho bạn? Ai quan tâm nó làm thế nào, miễn là nó trả về một con số chuẩn xác: def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):   price = <computer, plz do some math for me> return price Một cach để nghĩ về vấn đề này là coi  giá — price  là một món hầm ngon với nguyên liệu là số phòng ngủ —  number of bedrooms , diện tích sàn —  square footage  và khu vực —  neighborhood . Nếu bạn có thể tìm ra cách các thành phần tác động lên giá cuối cùng, bạn sẽ tìm ra công thức để tính ra giá. Điều đó sẽ làm gọn hàm ban đầu của bạn (với cả đống  if ’s và  else ’s) thành một thứ đơn giản thế này: def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):  price = 0 # a little pinch of this  price += num_of_bedrooms *  .841231951398213 # and a big pinch of that  price += sqft *  1231.1231231 # maybe a handful of this  price += neighborhood *  2.3242341421 # and finally, just a little extra salt for good measure  price +=  201.23432095 return price Chú ý các số thần thánh được bôi đậm—  .841231951398213 ,  1231.1231231 , 2.3242341421 , và  201.23432095 . Đó là các trọng số —  weights . Nếu ta có thể tìm trọng số chính xác cho mọi căn nhà, hàm dự đoán giá của chúng ta sẽ hoàn hảo! Có một cách để tìm các trọng số, như sau: Bước 1: Bắt đầu bằng cách cho tất cả các trọng số bằng  1.0: def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):   price = 0 # a little pinch of this   price += num_of_bedrooms *  1.0 # and a big pinch of that   price += sqft *  1.0 # maybe a handful of this   price += neighborhood *  1.0 # and finally, just a little extra salt for good measure   price +=  1.0 return price Bước 2: Với tất cả các ngôi nhà đã có giá bán, bạn so sánh giá tính bằng hàm của bạn với giá thực tế Sử dụng hàm dự đoán tính ra giá nhà Ví dụ, ngôi nhà đầu tiên có giá bán thực sự là $250,000, nhưng hàm của bạn tính ra giá là $178,000, chúc mừng, bạn bị thiệt $72,000 cho ngôi nhà đầu tiên. Bây giờ tăng số lượng ngôi nhà trong tập dữ liệu của bạn lên. Giả sử bạn có 500 ngôi nhà trong danh sách các ngôi nhà đã bán, có giá, và tổng só tiền mà chương trình đã đoán sai là $86,123,373. Điều đó cho thấy hàm của bạn hiện tại đang sai sót lớn đến mức nào. Bây giờ, lấy tổng đó chia cho 500, để biết trung bình thì mỗi ngôi nhà bạn đã sai lệch bao nhiêu. Chúng ta gọi sai số trung bình này là chi phí — cost của hàm của bạn. Nếu chi phí này là 0, bạn đã tìm ra trọng số trong mơ, hàm của bạn chuẩn xác tuyệt đối. Điều đó có nghĩa là trong mọi trường hợp, hàm của bạn dự đoán hoàn hảo giá của ngôi nhà dựa trên dữ liệu đầu vào. Đó chính là mục đích của chúng ta— tìm ra chi phí thấp nhất bằng cách thử các trọng số khác nhau. Bước 3: Lặp lại Bước 2, lặp lại mãi với  các trọng số khác nhau . Tìm trọng số nào làm cho chi phí — cost gần bằng không. Khi đó, việc của bạn đã xong! Mind Blowage Time Đơn giản và khá đẹp, đúng không? Hãy suy nghĩ về những gì bạn vừa làm. Bạn lấy một số dữ liệu, đưa chúng qua ba bước, những bước đi thực là đơn giản, và cuối cùng bạn có một công thức có thể ước lượng giá cho một ngôi nhà bất kì. Woo, hay đấy chứ! Nhưng có và thứ có thể làm bạn điên đầu: Nghiên cứu trong nhiều lĩnh vực (như ngôn ngữ học / dịch) trong 40 năm qua đã cho thấy rằng những thuật toán học chung kiểu “khuấy số hầm” (tác giả dùng cụm từ “stir the number stew”) vượt phương pháp tiếp cận mà người ta cố gắng đạt được với các quy tắc rõ ràng. Cách tiêp cận ML kiểu “hơi ngu\" này cũng đủ để đánh bại các chuyên gia là con người. Hàm mà bạn tạo ra thực sự khá đần. Nó không hề biết “diện tích sàn” hay “phòng ngủ” là gì. Tất cả những gì nó cần để làm là những con số để có câu trả lời đúng. Rất có thể bạn sẽ chẳng biết chuyện gì xảy ra,  tại sao  một tập các trọng số lại đúng. Bạn chỉ viết một hàm mà bạn không thực sự hiểu nhưng bạn có thể chứng minh nó làm việc. Tưởng tượng rằng bạn thay các tham số “sqft” và “num_of_bedrooms”, hàm dự đoán của bạn lấy một tập các số. Giả sử mỗi số đại diện cho độ sáng của một điểm ảnh trong một hình chụp bằng camera gắn trên ô tô của bạn. Bây giờ hãy giả sử thay vì kết quả ra là giá nhà- “price”, đầu ra của hàm dự đoán sẽ là “góc để quay vô lăng\" — “degrees_to_turn_steering_wheel”. Bạn đã tự tạo ra một hàm để điều khiển chiếc xe của bạn ! Thực sự là điên đầu? Thực sự thì việc “thử mọi số” trong bước 3 là làm thế nào? Ok, tất nhiên là bạn sẽ không thử mọi trọng số có thể để tìm ra trọng số tốt nhất. Điều đó có thể khiến bạn làm việc mãi mãi mà vẫn không bao giờ xong. Để tránh đốt sức như thế, các nhà toán học đã tìm ra nhiều cách  thông minh  để tìm nhanh các trọng số tốt mà không phải thử mọi trọng số. Dưới đây là một cách: Đầu tiên, viết một phương trình đơn giản thể hiện Bước #2 ở trên: Đây chính là hàm chi phí — cost function . Bây giờ chúng ta hãy viết lại chính xác các phương trình tương tự, nhưng sử dụng một loạt các thuật ngữ toán học dùng trong ML (bạn có thể bỏ qua nó ngay): θ (theta) thể hiện trọng số hiện tại. J(θ) là chi phí tạo ra bởi trọng số hiện tại Phương trình này thể hiện chi phí (sai số) mà các trọng số của chúng ta đã tạo ra. If we graph this cost equation for all possible values of our weights for  number_of_bedrooms  and  sqft , we’d get a graph that might look something like this: Đồ thị của hàm chi phí trông như một cái bát. Trục tung thể hiện chi phí. . Trong biểu đồ này, điểm thấp nhất trong chỗ màu xanh là nơi chi phí là nhỏ nhất — đó là giá trị trọng số mà hàm của chúng ta sai ít nhất. Điểm cao nhất là trọng số làm cho chúng ta sai lầm nhất. Vậy nếu chúng ta có thể tìm ra các trọng số làm cho J(θ) ở điểm thấp nhất trong biểu đồ, chúng ta sẽ có câu trả lời! Chúng ta cần điều chỉnh trọng số sao cho tổng sai số “đi xuống đồi” trong đồ thị này là hướng tới điểm thấp nhất. Nếu chúng ta liên tục điều chỉnh các trọng số để tổng sai số nhỏ dần, chúng ta sẽ không phải thử toàn bộ các trọng số mà chỉ cần thử rất ít. Nếu bạn còn nhớ kiến thức tích phân, bạn có thể sẽ nhớ rằng nếu bạn có đạo hàm của một hàm, nó cho bạn biết độ dốc của tiếp tuyến của hàm tại bất kỳ điểm nào. Nói cách khác, nó cho chúng ta biết đó là cách xuống dốc cho bất kỳ điểm nào trên đồ thị của chúng ta. Chúng ta có thể sử dụng kiến ​​thức đó để đi bộ xuống dốc. Vì vậy, nếu chúng ta tính toán một đạo hàm riêng của hàm chi phí đối với mỗi trọng số, sau đó chúng ta thay đổi trọng số theo hướng nào đó. Điều đó sẽ giúp chúng ta đi gần hơn đến dưới chân đồi. Hãy làm điều đó và cuối cùng chúng ta sẽ đạt đến dưới cùng của quả đồi và có những giá trị tốt nhất có thể cho trọng số của chúng ta. (Nếu điều đó hơi khó hiểu, đừng lo lắng và tiếp tục đọc). Đó là một bản tóm tắt ở mức khái quát một cách để tìm ra trọng số tốt nhất cho hàm của bạn được gọi là  batch gradient descent . Đừng ngại  đọc kĩ  nếu bạn quan tâm vào việc học các chi tiết. Khi bạn sử dụng các thư viện ML để giải quyết một vấn đề thực sự, chúng sẽ làm các bước “xuống đồi” này cho bạn. Nhưng cũng tốt nếu bạn hiểu chuyện gì đang xảy ra. What else did you conveniently skip over? Bước thứ 3 ở thuật toán mà tôi đã mô ta gọi là  hồi quy tuyến tính đa biến — multivariate linear regression . Bạn đang ước lượng một phương trình phù hợp với dữ liệu của tất cả các ngôi nhà bạn đã biết giá. Sau đó, bạn sử dụng phương trình đó để đoán giá bán của ngôi nhà bạn chưa bao giờ thấy trước đây dựa vào các thông tin của ngôi nhà đó. Đó là một ý tưởng thực sự mạnh mẽ và bạn có thể giải quyết vấn đề “thực sự” với nó. Cách tiếp cận tôi vừa trình bày có thể làm việc trong các trường hợp đơn giản, nhưng không phải tất cả các trường hợp. Một lý do là giá nhà không phải lúc nào cũng đủ đơn giản để thể hiện bằng một đường liên tục. Nhưng rất may là có nhiều cách để xử lý vấn đề đó. Có rất nhiều thuật toán ML khác có thể xử lý dữ liệu phi tuyến tính (ví dụ như  neural networks  hoặc  SVMs  với  kernels ). Ngoài ra còn nhiều sách sử dụng hồi quy tuyến tính (linear regression) một cách khéo léo cho phép tạo ra các đường mô phỏng phức tạp. Trong mọi trường hợp, ý tưởng cơ bản vẫn là đi tìm các trọng số phù hợp. Ngoài ra, tôi đã bỏ qua vấn đề  overfitting . Rất dễ xảy ra trường hợp là bạn tìm ra các trọng số dự đoán hoàn hảo các dữ liệu học tập nhưng lại sai sót rất lớn khi dự đoán giá cho những ngôi nhà mới. Nhưng có nhiều cách để đối phó với điều này (như  regularization  và sử dụng một tập dữ liệu kiểm tra chéo —  cross-validation ). Làm thế nào để đối phó với vấn đề này là một phần quan trọng của việc áp dụng ML thành công hay không. Nói cách khác, trong khi các khái niệm cơ bản là khá đơn giản, cần nhiều kỹ năng và kinh nghiệm để áp dụng ML và nhận được kết quả hữu ích. Nhưng đó là một kỹ năng mà bất kỳ developer nào cũng có thể học được! ML có phải là ma thuật? Khi bạn bắt đầu thấy thật dễ để áp dụng ML vào việc giải quyết các vấn đề thực sự khó (như là nhận dạng chữ viết tay), bạn bắt đầu có cảm giác rằng bạn có thể sử dụng ML để giải quyết bất kì vấn đề gì, miễn là bạn có đủ dữ liệu. Chỉ cần đưa vào dữ liệu và ngồi xem máy tính tìm ra các phương trình phù hợp với dữ liệu đầu vào! Nhưng một thứ quan trọng cần nhớ, ML chỉ hoạt động đúng nếu vấn đề cần giải quyết thực sự có thể tìm ra đáp án bằng các dữ liệu mà bạn có. Ví dụ, nếu bạn tạo ra một mô hình dự đoán giá nhà dựa trên dữ liệu về các loại cây cảnh trồng trong ngôi nhà đó, nó sẽ không bao giờ hoạt động đúng. Không có bất kì mối liên hệ giữa cây cảnh trong ngôi nhà và giá bán của căn nhà. Dù bạn cố gắng thế nào đi nữa, máy tính không bao giờ suy ra mối qun hệ giữa chúng. Bạn chỉ có thể mô hình hoá một quan hệ khi nó tồn tại Hãy nhớ, nếu một chuyên gia không thể sử dụng dữ liệu để giải quyết vấn đề một cách thủ công, máy tính cũng không thể giúp bạn làm điều đó tự động. Thay vào đó, tập trung vào các vấn đề mà con người có thể giải quyết, nhưng mà nó sẽ là tuyệt vời nếu một máy tính có thể giải quyết nó nhanh hơn. Làm thế nào tìm hiểu thêm về Machine Learning Theo quan điểm của tôi, vấn đề lớn nhất với machine learning bây giờ là nó chủ yếu sống trong thế giới học thuật và các nhóm nghiên cứu thương mại. Không có nhiều tài liệu dễ hiểu dành cho đại chúng, muốn hiểu thì buộc phải trở thành chuyên gian. Rất may tình hình đang sáng hơn từng ngày. Khoá học miễn phí về  Machine Learning trên Coursera  của giáo sư Andrew Ng rất tuyệt vời. Tôi đặc biệt khuyên bạn nên tham nó nếu muốn tìm hiểu về ML. Tất cả các kỹ sư khoa học máy tính hoặc những người còn nhớ một ít về toán học đều nên học khoá học này. Ngoài ra, bạn có thể thử chơi bời với hàng đống thuật toán ML bằng cách tải và cài đặt  SciKit-Learn . Nó là một framework python kiểu “black box”, có hàng loạt các thuật toán ML cài sẵn, chỉ việc sử dụng. Đọc bài gốc của  Adam Geitgey  tại  đây Machine Learning For Everyone A single golf clap? Or a long standing ovation? By clapping more or less, you can signal to us which stories really stand out. Blocked Unblock Follow Following hongthaiphi Never miss a story from  hongthaiphi , when you sign up for Medium.  Learn more Never miss a story from  hongthaiphi Blocked Unblock Follow Get updates",
          "relevance": "0",
          "title": "ML for everyone",
          "url": "https://medium.com/@hongthaiphi/ml-for-everyone-fcd333cd811f"
        },
        {
          "content": "Bách khoa toàn thư mở Wikipedia \n\t\t\t\t\tBước tới:\t\t\t\t\t menu , \t\t\t\t\t tìm kiếm Bài viết này  cần thêm  chú thích nguồn gốc  để  kiểm chứng thông tin .  Mời bạn giúp  hoàn thiện bài viết này  bằng cách bổ sung chú thích tới  các nguồn đáng tin cậy . Các nội dung không có nguồn có thể bị nghi ngờ và xóa bỏ. Học có giám sát  là một kĩ thuật của ngành  học máy  để xây dựng một hàm ( function ) từ dữ liệu huấn luyện.  Dữ liệu huấn luyện  bao gồm các cặp gồm đối tượng đầu vào (thường dạng vec-tơ), và đầu ra mong muốn. Đầu ra của một hàm có thể là một giá trị liên tục (gọi là  hồi qui ), hay có thể là dự đoán một nhãn phân loại cho một đối tượng đầu vào (gọi là  phân loại ). Nhiệm vụ của chương trình học có giám sát là dự đoán giá trị của hàm cho một đối tượng bất kì là đầu vào hợp lệ, sau khi đã xem xét một số ví dụ huấn luyện (nghĩa là, các cặp đầu vào và đầu ra tương ứng). Để đạt được điều này, chương trình học phải tổng quát hóa từ các dữ liệu sẵn có để dự đoán được những tình huống chưa gặp phải theo một cách \"hợp lý\" (xem  thiên kiến qui nạp  -  inductive bias ). (So sánh với  học không có giám sát .) Học có giám sát có thể tạo ra hai loại mô hình. Phổ biến nhất, học có giám sát tạo ra một mô hình toàn cục ( global model ) để ánh xạ đối tượng đầu vào đến đầu ra mong muốn. Tuy nhiên, trong một số trường hợp, việc ánh xạ được thực hiện dưới dạng một tập các mô hình cục bộ (như trong phương pháp  lập luận theo tình huống  ( case-based reasoning ) hay  giải thuật láng giềng gần nhất ). Để có thể giải quyết một bài toán nào đó của học có giám sát (ví dụ: học để  nhận dạng chữ viết tay ) người ta phải xem xét nhiều bước khác nhau: Xác định loại của các ví dụ huấn luyện. Trước khi làm bất cứ điều gì, người kĩ sư nên quyết định loại dữ liệu nào sẽ được sử dụng làm ví dụ. Chẳng hạn, đó có thể là một ký tự viết tay đơn lẻ, toàn bộ một từ viết tay, hay toàn bộ một dòng chữ viết tay. Thu thập tập huấn luyện. Tập huấn luyện cần đặc trưng cho thực tế sử dụng của hàm chức năng. Vì thế, một tập các đối tượng đầu vào được thu thập và đầu ra tương ứng được thu thập, hoặc từ các chuyên gia hoặc từ việc đo đạc tính toán. Xác định việc biểu diễn các đặc trưng đầu vào cho hàm chức năng cần tìm. Sự chính xác của hàm chức năng phụ thuộc lớn vào cách các đối tượng đầu vào được biểu diễn. Thông thường, đối tượng đầu vào được chuyển đổi thành một vec-tơ đặc trưng, chứa một số các đặc trưng nhằm mô tả cho đối tượng đó. Số lượng các đặc trưng không nên quá lớn, do  sự bùng nổ tổ hợp  ( curse of dimensionality ); nhưng phải đủ lớn để dự đoán chính xác đầu ra. Xác định cấu trúc của hàm chức năng cần tìm và giải thuật học tương ứng. Ví dụ, người kĩ sư có thể lựa chọn việc sử dụng  mạng nơ-ron nhân tạo  hay  cây quyết định . Hoàn thiện thiết kế. Người kĩ sư sẽ chạy giải thuật học từ tập huấn luyện thu thập được. Các tham số của giải thuật học có thể được điều chỉnh bằng cách tối ưu hóa hiệu năng trên một tập con (gọi là tập  kiểm chứng  - validation  set) của tập huấn luyện, hay thông qua  kiểm chứng chéo  ( cross-validation ). Sau khi học và điều chỉnh tham số, hiệu năng của giải thuật có thể được đo đạc trên một tập kiểm tra độc lập với tập huấn luyện. Mục lục 1 Cực tiểu hóa rủi ro kinh nghiệm 2 Hướng tiếp cận và giải thuật 3 Ứng dụng 4 Vấn đề chung 5 Tham khảo 6 Liên kết ngoài Cực tiểu hóa rủi ro kinh nghiệm [ sửa  |  sửa mã nguồn ] Mục tiêu của việc học có giám sát một mô hình toàn cục là tìm ra một hàm  g , khi cho sẵn một tập các điểm có dạng ( x ,  g ( x )). Giả thiết rằng đã biết trước đặc điểm của hàm  g  đối với một tập điểm. Tập điểm đó đã được lấy mẫu  độc lập và có cùng phân bố  ( independent and identically distributed (i.i.d.) ) theo một xác suất phân bố  p  chưa biết từ một tập lớn hơn và có thể vô hạn. Ngoài ra, giả sử tồn tại một hàm  hàm tổn thất  ( loss function ) theo tác vụ  L  có dạng: L : Y × Y → R + {\\displaystyle L:Y\\times Y\\to {\\mathbb {R}}^{+}} trong đó  Y  là trùng với  miền xác định  của  g  và  L  ánh xạ tới các  số thực  không âm (có thể đặt thêm hạn chế cho  L ). Giá trị  L ( z ,  y ) là tổn thất nảy sinh khi đoán giá trị của  g  tại một điểm cho trước là  z  trong khi giá trị thực của nó là  y . Hàm  rủi ro f  được định nghĩa là  giá trị kỳ vọng  của hàm tổn thất và có công thức như sau: R ( f ) = ∑ i L ( f ( x i ) , g ( x i ) ) p ( x i ) {\\displaystyle R(f)=\\sum _{i}L(f(x_{i}),g(x_{i}))\\;p(x_{i})} nếu xác suất phân bố  p  là rời rạc (trường hợp xác suất phân bố liên tục cần một  tích phân xác định  ( definite integral ) và một  hàm mật độ xác suất . Mục tiêu là tìm một hàm  f *  trong số một lớp con cố định các hàm để cho rủi ro  R ( f * ) là  cực tiểu . Tuy nhiên, do thường chỉ biết được đặc điểm của hàm  g  cho một tập hữu hạn điểm ( x 1 ,  y 1 ),..., ( x n ,  y n ), người ta chỉ có thể xác định gần đúng rủi ro thực sự, ví dụ, với  rủi ro kinh nghiệm  ( empirical risk ): R ~ n ( f ) = 1 n ∑ i = 1 n L ( f ( x i ) , y i ) {\\displaystyle {\\tilde {R}}_{n}(f)={\\frac {1}{n}}\\sum _{i=1}^{n}L(f(x_{i}),y_{i})} Nguyên lý của  cực tiểu hóa rủi ro kinh nghiệm  là chọn hàm  f *  sao cho rủi ro kinh nghiệm là nhỏ nhất. Lý thuyết học bằng thống kê tìm hiểu xem việc cực tiểu hóa rủi ro kinh nghiệm có thể đạt được trong những điều kiện nào và có thể trông đợi các tính toán xấp xỉ tốt đến đâu. Hướng tiếp cận và giải thuật [ sửa  |  sửa mã nguồn ] học bằng phân tích  ( analytical learning ) mạng nơ-ron  nhân tạo\n Instantaneously trained neural networks kỹ thuật lan truyền ngược  ( backpropagation ) boosting thống kê Bayes lập luận theo tình huống  ( case-based reasoning ) học  cây quyết định inductive logic programming hồi quy Gauss  ( Gaussian process regression ) learning automata theory Minimum message length  ( cây quyết định , đồ thị quyết định, v.v.) naive Bayes classifier thuật toán láng giềng gần nhất probably approximately correct learning  (PAC) learning symbolic machine learning  algorithms subsymbolic machine learning  algorithms support vector machines Random Forests Ứng dụng [ sửa  |  sửa mã nguồn ] Tin sinh học Nhận dạng chữ viết tay Thu thập thông tin  ( information retrieval ) Nhận dạng đối tượng trong  computer vision Nhận dạng ký tự quang học Phát hiện spam Nhận dạng mẫu Nhận dạng tiếng nói Vấn đề chung [ sửa  |  sửa mã nguồn ] computational learning theory  (ngành toán học liên quan đến việc phân tích các thuật toán học máy) thiên kiến qui nạp  ( inductive bias ) overfitting  (hàm học được quá thích nghi với tập huấn luyện) version space Tham khảo [ sửa  |  sửa mã nguồn ] Liên kết ngoài [ sửa  |  sửa mã nguồn ] Chương trình mạng nơ ron đa lớp (Multi Layer Neural Network) và mạng nơ ron tự tổ chức (Self Organizing Maps) có giải thích bằng tiếng Việt. Sử dụng phần mềm mạng nơ ron 3 lớp Spice-MLP Sử dụng phần mềm mạng tự tổ chức Spice-SOM Hướng dẫn sử dụng mạng nơ ron trong các ứng dụng thực tế  trong đó có minh họa phân loại ảnh khuôn mặt, ảnh người đi bộ, ảnh xe hơi, dự báo chứng khoán và một số ví dụ khác \n\t\t\t\t\t\tLấy từ “ https://vi.wikipedia.org/w/index.php?title=Học_có_giám_sát&oldid=26626395 ”\t\t\t\t\t Thể loại :  Học máy Thể loại ẩn:  Trang thiếu chú thích trong bài Trình đơn chuyển hướng Công cụ cá nhân Chưa đăng nhập Thảo luận cho địa chỉ IP này Đóng góp Mở tài khoản Đăng nhập Không gian tên Bài viết Thảo luận Biến thể Các hiển thị Đọc Sửa đổi Sửa mã nguồn Xem lịch sử Khác Tìm kiếm Xem nhanh Trang Chính Bài viết chọn lọc Tin tức Bài viết ngẫu nhiên Thay đổi gần đây Phản hồi lỗi Quyên góp Tương tác Hướng dẫn Giới thiệu Wikipedia Cộng đồng Thảo luận chung Giúp sử dụng Liên lạc Công cụ Các liên kết đến đây Thay đổi liên quan Các trang đặc biệt Liên kết thường trực Thông tin trang Khoản mục Wikidata Trích dẫn trang này In/xuất ra Tạo một quyển sách Tải về dưới dạng PDF Bản để in ra Ngôn ngữ khác العربية Català Čeština Deutsch Ελληνικά English Español فارسی Français 한국어 Italiano עברית 日本語 Polski Русский Simple English Suomi ไทย Türkçe Українська 中文 Sửa liên kết  Trang này được sửa đổi lần cuối lúc 05:27 ngày 31 tháng 5 năm 2017. Văn bản được phát hành theo  Giấy phép Creative Commons Ghi công–Chia sẻ tương tự ; có thể áp dụng điều khoản bổ sung. Với việc sử dụng trang web này, bạn chấp nhận  Điều khoản Sử dụng  và  Quy định quyền riêng tư . \nWikipedia® là thương hiệu đã đăng ký của  Wikimedia Foundation, Inc. , một tổ chức phi lợi nhuận. Quy định quyền riêng tư Giới thiệu Wikipedia Lời phủ nhận Nhà phát triển Tuyên bố về cookie Phiên bản di động",
          "relevance": "0",
          "title": "Học có giám sát",
          "url": "https://vi.wikipedia.org/wiki/H%E1%BB%8Dc_c%C3%B3_gi%C3%A1m_s%C3%A1t"
        },
        {
          "content": " 090.220.9011  \n                                cuong@techmaster.vn Đăng nhập Đăng ký \n                Trang chủ\n             \n                Khóa học\n             \n                Thực tập\n             \n                Lịch khai giảng\n             \n                Blog\n             \n                Việc làm\n             \n                Phỏng vấn\n             \n                Chúng tôi  arrow_drop_down \n                Khách hàng - Đối tác\n             \n                Giảng viên\n             \n                Thiết bị đào tạo\n             \n                Quy định chung\n             \n                Tuyển dụng giảng viên\n             \n                Chơi thể thao\n             \n                Team building - Royal City\n             \n                Video hoạt động\n             \n                Thực tập\n             menu \n                Trang chủ\n             \n                Khóa học\n             \n                Thực tập\n             \n                Lịch khai giảng\n             \n                Blog\n             \n                Việc làm\n             \n                Phỏng vấn\n             \n                Chúng tôi  arrow_drop_down \n                Khách hàng - Đối tác\n             \n                Giảng viên\n             \n                Thiết bị đào tạo\n             \n                Quy định chung\n             \n                Tuyển dụng giảng viên\n             \n                Chơi thể thao\n             \n                Team building - Royal City\n             \n                Video hoạt động\n             \n                Thực tập\n             Trang chủ Blog Bức tranh tổng quan về thuật toán phân cụm close Danh mục bài viết \n                        Agile (13)\n                     \n                        Altassian (4)\n                     \n                        Android (75)\n                     \n                        Arduino (11)\n                     \n                        ASP.net (13)\n                     \n                        C++ (14)\n                     \n                        Cloud (6)\n                     \n                        công việc (142)\n                     \n                        CSS (19)\n                     \n                        Cuộc sống (129)\n                     \n                        Database (19)\n                     \n                        Design (43)\n                     \n                        Điện tử (35)\n                     \n                        Docker (1)\n                     \n                        Education (60)\n                     \n                        Eletronics (4)\n                     \n                        Facebook (2)\n                     \n                        Game (23)\n                     \n                        Git (8)\n                     \n                        Go (2)\n                     \n                        HTML (31)\n                     \n                        iOS (228)\n                     \n                        Java (115)\n                     \n                        JavaScript (59)\n                     \n                        John Vũ (7)\n                     \n                        Kiểm thử (9)\n                     \n                        lập trình (293)\n                     \n                        lập trình Android (32)\n                     \n                        lập trình ios (157)\n                     \n                        lập trình php (31)\n                     \n                        lập trình ứng dụng (93)\n                     \n                        lập trình ứng dụng iphone (42)\n                     \n                        Lập trình web (170)\n                     \n                        Linux (38)\n                     \n                        Mac (9)\n                     \n                        Machine Learning (11)\n                     \n                        Magento (4)\n                     \n                        Microservice (7)\n                     \n                        Microsoft (16)\n                     \n                        News (59)\n                     \n                        Nghề nghiệp (80)\n                     \n                        Node.js (71)\n                     \n                        Odoo (3)\n                     \n                        OpenCV (1)\n                     \n                        PHP (66)\n                     \n                        Project Management (21)\n                     \n                        Python (13)\n                     \n                        Ruby on Rails (54)\n                     \n                        Scala (15)\n                     \n                        SharePoint (6)\n                     \n                        STEM (10)\n                     \n                        Swift (86)\n                     \n                        TechMaster (130)\n                     \n                        Tips and tricks (18)\n                     \n                        Tutor (72)\n                     \n                        Twitter Boostrap (2)\n                     \n                        Uncategorized (26)\n                     \n                        ứng dụng (2)\n                     \n                        Unity3D (13)\n                     \n                        việc làm (126)\n                     \n                        VMware (1)\n                     \n                        WindowsPhone (43)\n                     \n                        WordPress (4)\n                     Bức tranh tổng quan về thuật toán phân cụm 28/05/2016 Bởi  Phan Đức Việt trong\n                                         Machine Learning Nếu bạn đang có ý định trở thành một Data Scientist (nhà khoa học dữ liệu) thì hiện tại đang là 1 thời điểm không hề tồi chút nào. Những con người kể cả khó tính nhất cũng sẽ đổ dồn sự chú ý khi bạn đề cập tới Big Data trong cuộc hội thoại, đám đông sẽ cảm thấy hào hứng khi được nghe bạn chém gió về Trí tuệ nhân tạo cũng như Học máy. Thậm chí những con số do Google cung cấp tại  đây  còn cho thấy: tất cả vẫn chưa có dấu hiệu dừng lại, chúng vẫn tiếp tục phát triển với tốc độ rất nhanh. Càng ngày càng có rất nhiều các giải thuật 'thông minh' đã được phát minh ra để giúp đỡ các nhà khoa học dữ liệu. Tất cả chúng nhìn chung đều có vẻ rất phức tạp, nhưng nếu chúng ta hiểu được và biết cách phối hợp một cách nhuần nhuyễn thì mọi việc sẽ trở nên dễ dàng hơn rất nhiều. Các khóa học về khai phá dữ liệu (Data Mining) hoặc học máy (Machine Learning) vẫn thường mở đầu bằng những ví dụ về phân cụm, lí do đơn giản bởi vì chúng rất thực tế và không quá khó hiểu. Bài toán phân cụm là 1 nhánh ứng dụng chính của lĩnh vực Unsupervised Learning (Học không giám sát), trong đó dữ liệu được mô tả trong bài toán không được dán nhãn (tức là không có đầu ra). Trong trường hợp này, thuật toán sẽ tìm cách phân cụm - chia dữ liệu thành từng nhóm có đặc điểm tương tự nhau, nhưng đồng thời đặc tính giữa các nhóm đó lại phải càng khác biệt càng tốt. Dữ liệu của chúng ta có thể là bất cứ thứ gì, chẳng hạn như dữ liệu về khách hàng: Thuật toán phân cụm sẽ rất hữu ích trong việc đánh giá và chia thành các nhóm người dùng khác nhau, rồi từ đó ta có thể đưa ra những chiến lược marketing phù hợp trên từng nhóm người dùng đó. Tham khảo khóa học  nhập môn Machine Learning  tại TechMaster K-Means Clustering Sau khi dạo qua những màn giới thiệu chung, đa số các khóa học Data Mining sẽ bắt đầu luôn với K-Means: 1 thuật toán tuy đơn giản nhưng lại khá hiệu quả và được sử dụng rộng khắp. Trước khi bắt tay vào làm, chúng ta cần phải xác định sẵn 2 thứ: đó là  hàm khoảng cách  được sử dụng (ví dụ như khoảng cách Euclid) và  số lượng nhóm  mong muốn (ta sẽ kí hiệu trong bài viết này là  k ) Mô phỏng quá trình phân cụm K-Means Thuật toán bắt đầu với việc chọn ra tâm của từng cụm. Chúng ta có thể đơn giản chọn k điểm ngẫu nhiên trong bộ, hoặc sử dụng một số hướng tiếp cận nào khác, nhưng nhìn chung ngẫu nhiên vẫn là cách tốt nhất. Rồi kế tiếp, luân phiên lặp lại 2 giai đoạn sau: Giai đoạn gán : gán từng phần tử trong bộ dữ liệu của chúng ta vào các cụm. Cách thức tiến hành đó là: với mỗi điểm, hãy tính khoảng cách từ điểm đó tới vị trí các tâm, sau cùng: tâm nào gần nhất thì gán vào cụm ứng với cái tâm đó Giai đoạn cập nhật : duyệt từng cụm, cập nhật lại tọa độ của tâm: Như đã biết, sau giai đoạn 1, chúng ta đã thu được k cụm ứng với dãy các điểm được gán cho từng cụm. Tọa độ tâm mới của cụm sẽ bằng trung bình cộng tọa độ các điểm trong cụm Sau càng nhiều vòng lặp, các tâm càng di chuyển chậm dần, và tổng khoảng cách từ mỗi điểm trong cụm tới tâm cụm lại càng nhỏ đi. Quá trình sẽ kết thúc cho tới khi hàm tổng khoảng cách hội tụ (tức là không có sự thay đổi nào xảy ra ở giai đoạn gán nữa). Lúc này tọa độ tâm vẫn sẽ bằng trung bình cộng các điểm hiện tại trong cụm, hay nói cách khác tâm sẽ không còn di chuyển tiếp nữa.  Chú ý thuật toán K-Means chỉ đảm bảo được quá trình này sẽ đưa hàm tổng khoảng cách hội tụ tới điểm cực tiểu địa phương, chứ không chắc chắn đó là giá trị nhỏ nhất của toàn bộ hàm số . Tuy nhiên, điều này là có thể chấp nhận được vì  KHÔNG  phải mô hình nào càng sát với bộ dữ liệu huấn luyện thì cũng sẽ càng tốt. Ta có thể nhận thấy rằng việc lựa chọn tâm lúc khởi điểm cũng có ảnh hưởng tới kết quả cuối cùng thu được, do đó đã nảy sinh rất nhiều ý kiến trái chiều về vấn đề này. Một ý tưởng đơn giản là cho chạy K-Means nhiều lần với mỗi bộ tâm ngẫu nhiên khác nhau, rồi sau đó chọn ra mô hình tốt nhất thông qua việc xét giá trị nhỏ nhất của các hàm tổng khoảng cách ứng với chúng. Một hướng tiếp cận khác trong việc chọn tâm ban đầu đó là chọn những điểm \"xa nhất\". Việc này có thể cho kết quả tốt hơn, tuy nhiên ta sẽ mắc phải vấn đề với những phần tử \"nhiễu\", đó là những phần tử nằm riêng lẻ một mình tách biệt với phần còn lại trong bộ dữ liệu. Do đó chúng sẽ tự lập ra 1 cụm riêng của chính mình. Có một cách giải quyết đã được phát minh để cân bằng đồng thời được cả 2 điều trên, nó có tên gọi là  K-Means++ : trong đó, tâm khởi đầu vẫn được chọn ngẫu nhiên, nhưng là  chọn lần lượt (thay vì đồng loạt)  và kèm theo  xác suất ngẫu nhiên tỉ lệ thuận với khoảng cách tới điểm tâm vừa chọn trước đó . Tức là, các điểm càng nằm phía xa sẽ có khả năng được chọn làm tâm kế tiếp càng lớn. Do đó, nếu có 1 nhóm các điểm, khả năng chỉ 1 điểm từ nhóm đó được chọn làm tâm cũng sẽ cao hơn. K-Means++ cũng đang được chọn sử dụng cho bước khởi tạo của thuật toán K-Mean trong thư viện  Scikit-learn  của Python. Nếu bạn đang lập trình Python, bạn có thể dùng ngay thư viện này. Đối với Java, thư viện  Weka  sẽ là 1 sự lựa chọn đáng để cân nhắc. Java (Weka) // Load some data\r\nInstances data = DataSource.read(\"data.arff\");\r\n\r\n// Create the model\r\nSimpleKMeans kMeans = new SimpleKMeans();\r\n\r\n// We want three clusters\r\nkMeans.setNumClusters(3);\r\n\r\n// Run K-Means\r\nkMeans.buildClusterer(data);\r\n\r\n// Print the centroids\r\nInstances centroids = kMeans.getClusterCentroids();\r\nfor (Instance centroid: centroids) {\r\n  System.out.println(centroid);\r\n}\r\n\r\n// Print cluster membership for each instance\r\nfor (Instance point: data) {\r\n  System.out.println(point + \" is in cluster \" + kMeans.clusterInstance(point));\r\n} Python (Scikit-learn) >>> from sklearn import cluster, datasets\r\n>>> iris = datasets.load_iris()\r\n>>> X_iris = iris.data\r\n>>> y_iris = iris.target\r\n\r\n>>> k_means = cluster.KMeans(n_clusters=3)\r\n>>> k_means.fit(X_iris)\r\nKMeans(copy_x=True, init='k-means++', ...\r\n>>> print(k_means.labels_[::10])\r\n[1 1 1 1 1 0 0 0 0 0 2 2 2 2 2]\r\n>>> print(y_iris[::10])\r\n[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2] Ở trong ví dụ Python phía trên, ta sử dụng bộ dữ liệu  Iris  chứa kích thước đài hoa và cánh hoa cho 3 giống hoa Iris khác nhau, chia những dữ liệu này thành 3 cụm, rồi sau đó so sánh với giá trị thực tế của chúng, để kiểm tra độ chính xác của thuật toán. Trong trường hợp này, chúng ta thấy rằng dữ liệu được tách thành 3 cụm (ứng với 3 giống hoa) khác nhau và K-Means đã nhận ra chính xác những phần tử nào cùng nằm chung 1 cụm ( Chú ý rằng Unsupervised Learning là bài toán không có nhãn nên chỉ số k bằng (0, 1, 2) ở trên chỉ là ngẫu nhiên, có tác dụng phân biệt chứ không phải là nhãn đầu ra ). Tuy nhiên, làm cách nào mà ta chọn ra được số cụm (k) thích hợp? Câu hỏi tương tự như vậy thường rất phổ biến trong Học máy. Nếu chúng ta yêu cầu nhiều cụm hơn, dữ liệu sẽ được chia nhỏ ra, và giá trị error (tổng khoảng cách) cũng sẽ nhỏ hơn. Vậy, như thế có phải sẽ là tốt hơn nếu như ta chọn k lớn nhất có thể? Chúng ta có thể chọn k = m (số điểm), như thế mỗi điểm sẽ trở thành tâm của chính nó và mỗi cụm sẽ chỉ có 1 điểm? Điều đó không sai, error sẽ bằng 0, nhưng chúng ta sẽ không thể tìm được mô tả đơn giản cho dữ liệu, và mô hình thu được cũng không thể phủ được những điểm mới thêm vào. Vấn đề này có tên gọi là  overfitting , và tất nhiên chúng ta sẽ không mong gặp phải nó. Một cách để giải quyết vấn đề này là bổ sung thêm hàm phạt (penalty) cho số lượng cụm. Từ đó, mục tiêu của ta lúc này không chỉ còn giảm thiểu error, mà phải cân bằng cả  error + penalty . Giá trị error sẽ tiến dần tới 0 khi chúng ta tăng số lượng cụm, nhưng đồng thời penalty cũng tăng theo. Quá trình tăng số lượng cụm sẽ dừng lại khi mà lượng error giảm đi thấp hơn so với giá trị penalty, và kết quả thu được là kết quả tối ưu. Có một giải pháp sử dụng  Bayesian Information Criterion  (BIC) để tính k có tên gọi là X-Means [Pelleg and Moore, 2000]. Một thứ khác chúng ta cần quan tâm đó là hàm khoảng cách. Hiển nhiên, với những điểm nằm trong không gian, khoảng cách Euclid rõ ràng là hiệu quả nhất, nhưng đôi khi ta cần thêm vài \"mánh khóe\" cho những loại dữ liệu đặc trưng khác nhau, ví dụ như các giá trị rời rạc,... Việc này yêu cầu khá nhiều kiến thức chuyên ngành liên quan tới dữ liệu đó. Hoặc, chúng ta có thể nhờ tới sự trợ giúp của Học máy để huấn luyện ra hàm khoảng cách thích hợp nhất. Nếu bạn có 1 tập các dữ liệu huấn luyện (đã biết trước chúng được phân cụm thế nào qua nhãn của chúng), kĩ thuật  Supervised Learning  (học có giám sát) có thể được ứng dụng để tìm ra hàm khoảng cách thích hợp, rồi áp dụng nó vào trong dữ liệu cần phân cụm. Ngoài ra, có 1 thuật toán phân cụm khác có tên là  Expectation-Maximization  (EM) cũng gần tương tự với 2 giai đoạn được dùng trong K-Means. Nói chính xác thì K-Means có thể coi là 1 phiên bản đơn giản hơn của EM. Tuy nhiên, đừng nhầm lẫn chúng với nhau mặc dù có rất nhiều điểm chung giữa 2 thuật toán này. EM Clustering Như vậy, với K-Means: mỗi điểm sẽ được gán cho 1 nhóm và mỗi nhóm được đại diện bởi 1 tâm. Điều này không quá phức tạp, vì chúng ta chưa gặp phải vấn đề cụm chồng chéo, hoặc những cụm có hình dạng khác hình tròn. Với  EM , ta bây giờ có thể tiến một bước xa hơn nữa và đặc tả mỗi cụm bằng tâm của nó (kì vọng), covariance (hiệp phương sai - qua đó ta có thể biểu diễn được cụm hình elip) và weight (kích thước của cụm). Xác suất mà 1 điểm thuộc về 1 cụm bây giờ được tính bằng xác suất phân phối Gauss đa biến.  Chúng ta sẽ bắt đầu EM bằng cách tính, với mỗi điểm, xác suất mà nó thuộc về từng cụm là bao nhiêu (tất nhiên, các cụm ban đầu cũng được khởi tạo ngẫu nhiên). Đây là bước E-step. Nếu 1 cụm là \"ứng viên\" tốt đối với 1 điểm, nó sẽ có xác suất gần với 1. Tuy nhiên, có xảy ra trường hợp 2 hay nhiều cụm cùng là ứng viên tốt, do đó điểm của chúng ta lúc này sẽ có phân phối xác suất giữa các cụm. Tính chất này của thuật toán được gọi là \"soft clustering\". Bước M-step bây giờ tính toán lại các tham số của mỗi cụm, bằng cách sử dụng kết quả xác suất của các điểm được tính ở bước E-step. Để tính toán tâm mới, covariance mới và weight mới của 1 cụm, mỗi dữ liệu điểm sẽ được đánh trọng số tỉ lệ thuận với xác suất biến cố \"điểm đó thuộc cụm\" (lấy từ E-step). Luân phiên 2 bước này sẽ làm tăng giá trị log-likelihood của hàm xác suất cho tới khi giá trị này hội tụ tới cực đại. Nói thêm, tương tự với K-Means, thuật toán EM chỉ cho ta giá trị cực đại địa phương, vì vậy ta có thể sẽ cần phải thực hiện thuật toán nhiều lần để tìm được mô hình tốt hơn nữa. Nếu ta muốn đưa ra quyết định 1 điểm bất kỳ thuộc cụm nào, đơn giản chỉ cần chọn cụm cho ta giá trị xác suất cao nhất ứng với điểm đó. Và ta cũng có thể hoàn toàn tái tạo lại được 1 mẫu tương tự như dữ liệu ban đầu từ mô hình dựa vào dãy các xác suất thu được. Bài viết gốc:  https://www.toptal.com/machine-learning/clustering-algorithms AlphaGo Vs TensorFlow - Trí thông minh nhân tạo trong tay bạn 01/07/2016 \n                                                Techmaster team\n                                             Blog Home TensorFlow vs Google AI - giấc mơ sâu của Google 04/07/2016 \n                                            Techmaster team\n                                         \n                                    Bởi  Phan Đức Việt Tác giả đang bận code dạo (PHP, Java, C#, Nodejs, React) kiếm tiền mua đất cưới vợ nên chưa viết đoạn mô tả. Techmaster.  Quy định  Về chúng tôi Việc làm Về chúng tôi Giảng viên Cơ sở vật chất Contact Ms Khuê: 090.863.6458 khue@techmaster.vn Mr Cường: 090.220.9011 cuong@techmaster.vn Địa chỉ\n                          Số 78, ngõ 106, Hoàng Quốc Việt, Cầu Giấy, Hà Nội  Giờ mở cửa:  9:00  đến  18:00 Đăng nhập Đăng ký Ghi nhớ tài khoản Đăng nhập Quên mật khẩu? Facebook Google Hiển thị mật khẩu \n                    Đăng ký\n                 \n                    Bằng cách nhấp vào Đăng ký, bạn đã đồng ý với các\n                      Quy định   của chúng tôi.\n                  Hỗ trợ trực tuyến",
          "relevance": "0",
          "title": "Bức tranh tổng quan về thuật toán phân cụm",
          "url": "https://techmaster.vn/posts/33893/thuat-toan-phan-cum"
        },
        {
          "content": "Không nói ra thì ai cũng biết chuyện hôn nhân là chuyện hệ trọng. Vì tính hệ trọng của nó nên đòi hỏi người ta phải suy nghĩ cẩn thận. Nhưng một vấn đề  trong thống kê học và machine learning có tên là 'over-fitting' khuyên bạn không nên suy nghĩ nhiều quá. Trong cuốn sách 'Algorithms to live by' tác giả bàn về ý nghĩa của over-fitting trong việc chọn người bạn đời, thói thần tượng hoá, kĩ nghệ luyện gà đá, tiến hoá và thực phẩm, và ở đây tôi tóm lược vài ý chính như là một giải trí cuối năm cho các bạn.",
          "relevance": "0",
          "title": "'Over-fitting' và ý nghĩa thực tế trong đời sống",
          "url": "http://tmtuanms.byethost11.com/xac-su-t-th-ng-ke/77-over-fitting-va-y-nghia-th-c-t-trong-d-i-s-ng.html"
        },
        {
          "content": "Ông Xuân Hồng Chia sẻ kiến thức và thông tin về Machine learning Menu Skip to content Data Science Big Data Deep learning Exploratory Data Analysis Getting and cleaning data Machine learning Lập trình Python Spark R Weka Kiến thức Toán Statistical Inference Xử lý ngôn ngữ tự nhiên – Natural Language Processing (NLP) Chia sẻ Dự án About Kỹ năng làm việc với Machine Learning Tháng Mười 4, 2015 Tháng Năm 23, 2017 Ông Xuân Hồng Bạn nghĩ gì về bài viết này? Machine learning workflow Feature Engineering Model tuning Overfitting Xét độ lỗi (error) của mô hình dự đoán (hypothesis) h trên: \n_ tập huấn luyện (training data): error_train(h). \n_ quần thể dữ liệu (distribution) D: error_D(h). Mô hình dự đoán h thuộc H quá khớp (overfitting) tập huấn luyện nếu tồn tại một mô hình dự đoán khác là h’ thuộc H sao cho: _ error_train(h) < error_train(h’); _ nhưng có error_D(h) > error_D(h’) Model Ensemble Graph visualization How to Kick Ass in Competitive Machine Learning Facebook Twitter Google LinkedIn Like this: Số lượt thích Đang tải... Liên quan Điều hướng bài viết ←  Statistical hypothesis testing cho dân lập trình Cách xác định bài toán trong Machine Learning  → Nhập bình luận của bạn tại đây... Mời bạn điền thông tin vào ô dưới đây hoặc kích vào một biểu tượng để đăng nhập: Thư điện tử  (bắt buộc) (Địa chỉ của bạn được giấu kín) Tên  (bắt buộc) Trang web  Bạn đang bình luận bằng tài khoản WordPress.com  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Twitter  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Facebook  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Google+  (  Đăng xuất  /  Thay đổi  ) Hủy bỏ Connecting to %s Notify me of new comments via email. Thông báo cho tôi bằng email khi có bài đăng mới. Bạn Không Thể Sống Một Mình Nếu ngày mai tớ trở thành người nổi tiếng. Gặp tớ giữa đường bạn đừng ngại tớ nghe. Cứ nhìn tớ như ngày xưa bạn thấy. Đừng nên nghĩ tớ quá xa vời.  Nếu một ngày tớ là kẻ ăn xin. Thì bố thí cho tớ vài kỷ niệm. Dù rất nhỏ nhưng đừng nên khinh tớ. Vì tớ vẫn là tớ của ngày xưa.  Nếu ngày mai tớ chẳng biết đường về. Hãy chỉ giùm tớ con đường đi đúng đắn. Đừng để tớ sa chân nơi bùn nhớp. Lúc rút chân lên thì đã bẩn mất rồi.  Nếu ngày mai tớ gặp bạn giữa đường. Thì ta sẽ gặp nhau và cười nhé. Đừng nên trách và đừng nên đánh mất. Đôi khi có một số người lướt qua cuộc đời bạn và ngay tức khắc bạn nhận ra rằng sự có mặt của họ ý nghĩa như thế nào. Họ đã dạy bạn những bài học, đã giúp bạn nhận ra giá trị của chính mình hoặc trở thành con người mà bạn từng mơ ước. Có lẽ bạn sẽ không biết được những con người này từ đâu đến ( bạn cùng phòng, người hàng xóm, vị giáo sư, người bạn mất liên lạc từ lâu hay thậm chí là một người hoàn toàn xa lạ ). Nhưng khi bạn thờ ơ với họ, hãy nhớ rằng trong từng khoảnh khắc họ sẽ ảnh hưởng rất sâu sắc đến cuộc đời bạn.  …Cuộc sống không hề làm khó bạn mà chính bạn đã tự làm khó mình bằng cách đặt ra những câu hỏi đại loại như: “Tại sao lại thế này?… Tại sao lại thế kia?…”. Không ai có thể trả lời giúp bạn mà chính bạn phải tự đi tìm lời giải đáp cho mình. Hãy luôn nghĩ về người khác trước khi nghĩ đến bản thân, chắc chắn lúc đó bạn sẽ hài lòng với câu trả lời của chính mình. Và đừng quên luôn đặt câu hỏi: “Mình đã làm được gì?” trước khi tự hỏi: “Mình đã nhận được gì?” nhé! Tôi tin là bạn sẽ thành công!   Thật ra trên đời, con người luôn cần có nhau để giải quyết rất nhiều vấn đề. Nhờ những lúc qua lại giúp đỡ đó mà hoá giải bao muộn phiền, nảy sinh các tình cảm, cùng trải nghiệm biết bao điều thú vị, hiểu biết thêm nhiều tri thức mới. Đây chính là cuộc sống! Vì thế hãy trân trọng những người đang “làm phiền” bên cạnh bạn nhé! Đừng để đến khi mất đi rồi mới nghẹn ngào lại bảo “giá như” … ĐĐ. GS. Thích Phước Tiến \n__(())__ Namo Bụt Sakyamuni Nhận Email khi có bài viết mới Liên hệ Email (bắt buộc) Comment (bắt buộc) Top bài viết Bài viết mới FAQ: Big data cho người mới bắt đầu Tháng Mười 2, 2017 Làm việc với Spark DataFrames – Truy vấn nâng cao (Subqueries, Cube, Rollup) Tháng Chín 15, 2017 Thống kê ứng dụng 3: Các vấn đề trong thống kê Tháng Chín 14, 2017 Thống kê ứng dụng 2: Suy luận thống kê Tháng Chín 12, 2017 Thống kê ứng dụng 1: Quan sát dữ liệu Tháng Chín 7, 2017 AI, Machine Learning, Deep Learning phân biệt như thế nào cho đúng Tháng Chín 4, 2017 Information extraction – Bài toán rút trích thông tin trong văn bản Tháng Tám 28, 2017 Bắt đầu nghiên cứu big data từ đâu và như thế nào Tháng Tám 3, 2017 Data Science – Mỏ vàng của Kỉ nguyên số Tháng Tám 3, 2017 SMA 2017 – Lý thuyết ra quyết định Tháng Sáu 17, 2017 Big Data Chia sẻ Data Science Deep learning Dự án Exploratory Data Analysis Getting and cleaning data Kiến thức Lập trình Machine learning Python R Spark Statistical Inference Toán Weka Xử lý ngôn ngữ tự nhiên - Natural Language Processing (NLP) This slideshow requires JavaScript. Thư viện Tháng Mười 2017  (1) Tháng Chín 2017  (5) Tháng Tám 2017  (3) Tháng Sáu 2017  (3) Tháng Năm 2017  (3) Tháng Ba 2017  (1) Tháng Một 2017  (3) Tháng Mười Hai 2016  (1) Tháng Mười Một 2016  (2) Tháng Mười 2016  (2) Tháng Chín 2016  (1) Tháng Tám 2016  (2) Tháng Bảy 2016  (2) Tháng Năm 2016  (1) Tháng Tư 2016  (1) Tháng Ba 2016  (2) Tháng Hai 2016  (2) Tháng Một 2016  (1) Tháng Mười Hai 2015  (6) Tháng Mười Một 2015  (5) Tháng Mười 2015  (6) Tháng Chín 2015  (11) Tháng Tám 2015  (16) Tháng Bảy 2015  (25) Tháng Sáu 2015  (4) Thống kê 360,709 hits Tháng Mười 2015 H B T N S B C « Th9   Th11 »   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31   Data Science Lập trình Kiến thức Chia sẻ Dự án About Tạo một website miễn phí hoặc 1 blog với WordPress.com. Post to Hủy bỏ %d  bloggers like this:",
          "relevance": "1",
          "title": "Menu",
          "url": "https://ongxuanhong.wordpress.com/2015/10/04/ky-nang-lam-viec-voi-machine-learning/"
        },
        {
          "content": "SEPTENI TECHNOLOGY Developer's Blog The SEPTENI TECHNOLOGY Blog CONNECT WITH US: Categories Agile Android Angular JS Automation Box BDD Business Company Continuous Integration Culture Data Science Design Design Pattern Diary Domain-Driven Design Frontend Developer Functional Programming Kotlin Life skills Machine Learning Marketing Mobile Games None Offshore Process React JS Recruitment Ruby Advanced Ruby Tutorial Box Scala Scrum Security Security Testing SQA TechNote Featured posts Sorry. No data so far. Archives September 2017 August 2017 July 2017 June 2017 May 2017 April 2017 March 2017 February 2017 January 2017 December 2016 November 2016 October 2016 September 2016 August 2016 July 2016 June 2016 May 2016 April 2016 March 2016 February 2016 January 2016 December 2015 November 2015 October 2015 September 2015 August 2015 July 2015 June 2015 May 2015 April 2015 March 2015 February 2015 January 2015 December 2014 November 2014 October 2014 September 2014 August 2014 July 2014 June 2014 May 2014 April 2014 March 2014 February 2014 January 2014 December 2013 November 2013 October 2013 September 2013 August 2013 July 2013 June 2013 May 2013 April 2013 March 2013 \t\tCopyright ©  2017  SEPTENI TECHNOLOGY Developer's Blog  All rights reserved.\r\n\t",
          "relevance": "1",
          "title": "Related posts",
          "url": "http://labs.septeni-technology.jp/technote/ml-17-neural-net-regularization-with-drop-out/"
        },
        {
          "content": "mlcvGru Random thoughts on Machine Learning, Deep Learning and (sometimes) Computer Vision Menu Mathematics Linear Algebra Optimization Probability Statistics Calculus Vision & Learning Computer Science Computer Vision Machine Learning Others Implementation Reading Collections About Search for: Rate this: Share on Facebook (Opens in new window) Click to share on Twitter (Opens in new window) More Click to email (Opens in new window) Click to share on LinkedIn (Opens in new window) Click to share on Google+ (Opens in new window) Like this: Like Loading... Related Post navigation ←  Đạo hàm và vi phân Some good libraries from Microsoft Research  → Dũng says: \n\t\t\t\t\t\t\t\tAugust 27, 2012 at 9:33 am\t\t\t\t\t\t\t Hello anh Vũ, \nCho xin hỏi anh một vấn đề (Nó không liên quan đến bài viết hiện tại của anh). \nAnh có thể giải thích dùng phương pháp Action Matrix để giải hệ phương trình tuyến tính không? \nXin cảm ơn anh. Reply \n\t\t\t\tPingback:  What happens if you over-train a neural net? | IPhVu::iResearch \n\t\t\t\tPingback:  Con đường tơ lụa, hay là hành trình từ perceptron, logistic regression đến neural network | IPhVu::iResearch \n\t\t\t\tPingback:  RBM 3: Contrastive Divergence | mlcvGru \n\t\t\t\tPingback:  Artificial Neural Network – ep.002 | The Prosperous Heart Enter your comment here... Fill in your details below or click an icon to log in: Email  (required) (Address never made public) Name  (required) Website  You are commenting using your WordPress.com account.  (  Log Out  /  Change  )  You are commenting using your Twitter account.  (  Log Out  /  Change  )  You are commenting using your Facebook account.  (  Log Out  /  Change  )  You are commenting using your Google+ account.  (  Log Out  /  Change  ) Cancel Connecting to %s Notify me of new comments via email. Enter your email address to subscribe to this blog and receive notifications of new posts by email. Join 147 other followers Posts by language English Vietnamese Blog Stats 278,983 hits WordPress.com .\n\t\t\t\t\t\t\t\t\t Post to Cancel %d  bloggers like this:",
          "relevance": "1",
          "title": "mlcvGru",
          "url": "https://phvu.net/2012/07/08/gradient-based-learning/"
        },
        {
          "content": "Skip to content \n                  Features\n \n                  Business\n \n                  Explore\n \n                      Marketplace\n \n                  Pricing\n This repository Sign in or Sign up \n    Watch\n   \n    1\n   \n    Star\n   \n      0\n     \n        Fork\n       \n      0\n     khanhptnk / ml-book-vn Code Issues 0 Pull requests 0 \n      Projects\n       0 \n    Insights\n Permalink Branch: master Switch branches/tags Branches Tags \n                master\n               Nothing to show Nothing to show \n        Find file\n       Copy path ml-book-vn / 1_5_rlm.md \n      Fetching contributors…\n     Cannot retrieve contributors at this time Raw Blame History \n      81 lines (41 sloc)\n       \n    13 KB\n   Regularized Loss Minimization Chúng ta sẽ hoàn tất những hiểu biết về overfitting và đưa ra một thuật toán supervised learning hiệu quả hơn ERM để chống lại overfitting. Nhưng trước khi đó ta cùng ôn lại những gì đã học ở phần trước bằng một số câu hỏi ngắn như sau: Q1 : Overfitting là gì? A1 : Là khi model không có khả năng tổng quát từ những gì đã học được: độ sai sót trên training set nhỏ, trên test set to. Q2 : Tại sao overfitting lại có hại? A2 : Vì dữ liệu lúc nào cũng chứa noise. Noise làm cho model tìm được phức tạp quá mức cần thiết. Q3 : Làm sao để biết được model có bị overfitting hay không? A3 : Theo dõi learning curve. Q4 : Làm sao để không bị overfitting? A4 : Nếu bạn đang nói về chuyện làm sao để $$\\mathcal{L} {D {train}}$$ trùng với $$\\mathcal{L}_{\\mathcal{D}}$$ thì câu trả lời là không thể, trừ phi có vô hạn dữ liệu. Đây không phải là một câu hỏi đúng vì overfitting là một khái niệm tương đối, tùy theo \"cảm giác\" của bạn. \"Làm sao để giảm thiểu overfitting?\" mới là câu hỏi đúng! Nguyên nhân gây ra overfitting Như chúng ta đã biết, noise không phải là nguyên nhân trực tiếp gây ra overfitting. Vậy những yếu tố nào gây ra overfitting? Overfitting là sản phẩm của sự cộng hưởng giữa các yếu tố sau: Sử dụng ERM làm objective function . vì objective function và evaluation function có thể rất khác nhau, tối ưu objective function chưa hẳn sẽ tối ưu evaluation function. Giới hạn về dữ liệu : khi có thêm các cặp observation-label, hiển nhiên ta có thêm thông tin về mối quan hệ giữa chúng. Cụ thể hơn, ta thấy rằng, giả sử dùng cùng một loss function khi train và test, $$\\mathcal{L} {D {train}}$$ sẽ hội tụ về $$\\mathcal{L} {\\mathcal{D}}$$ khi số lượng phần tử của $$D {train}$$ tiến đến vô cùng. Khi hai đại lượng này trùng nhau thì overfitting hoàn toàn biến mất. Vì thế, càng có nhiều dữ liệu huấn luyện thì càng ít bị overfitting. Model quá \"mạnh\" : một model quá mạnh là khi nó có khả năng mô phỏng rất nhiều mối quan hệ phức tạp giữa observation và label (cũng tức là mô phỏng được rất nhiều dạng hàm số). Ví dụ nếu $$f_w$$ là một đa thức bậc một, nó có thể mô phỏng tất cả các đa thức bậc một (có dạng $$y = f_w(x) = w_1x + w_2$$). Dù có vô số đa thức bậc một, nhưng mà đây được xem như một model \"yếu\" bởi vì quan hệ tuyến tính được xem như một quan hệ rất đơn giản. Deep neural network được xem là những model mạnh bởi vì chúng mô phỏng được những quan hệ phi tuyến tính. Độ mạnh của model còn phụ thuộc vào cấu trúc và số lượng parameter. Vì bản chất machine learning là ước lượng hàm số, sử dụng một tập model mạnh hơn, thậm chí có khả năng mô phỏng tất cả dạng hàm số tưởng chừng như là một ý hay. Nhưng thực tế đây lại là một ý tưởng này rất tồi. Vì sao? Vì sao dùng model quá mạnh lại không tốt? Giả sử có một cuộc thi trong đó ta yêu cầu mỗi thí sinh phải vẽ được một đường đi qua nhiều nhất các điểm cho trước. Thí sinh tham dự có 2 người: một người là họa sĩ, anh ta rất khéo tay và có thể vẽ tất cả các loại đường cong thẳng; người còn lại là một anh chàng vụng về với cây thước kẻ, anh ta chỉ có thể vẽ đường thẳng. Dĩ nhiên là anh họa sĩ sẽ thắng trong trò chơi này. Nhưng hãy xem xét phản xạ của hai thí sinh trong tình huống sau đây: ta cho đề bài ban đầu là các điểm trên một đường thẳng; sau khi hai người vẽ xong, ta chỉ dịch chuyển một điểm lệch ra khỏi đường thẳng một đoạn nhỏ. Hiển nhiên là ban đầu cả hai người đều vẽ được một đường thẳng đi qua tất cả các điểm. Nhưng sau khi một điểm bị dịch chuyển, anh họa sĩ sẽ vẽ ra một đường hoàn toàn khác với đường thẳng ban đầu để cố đi qua mọi điểm. Ngược lại, anh vụng về thì sẽ vẫn giữ nguyên đáp áp vì đó là đáp án tốt nhất anh có thể vẽ. Điều ta thấy được ở đây đó là anh họa sĩ, vì quá tài hoa, nên anh rất nhạy cảm với những thay đổi nhỏ trong các điểm dữ liệu. Còn anh vụng về, vì năng lực của anh có hạn, nên thường anh sẽ ít bị ảnh hưởng hơn. Nếu như đây không phải là một cuộc thi vẽ qua nhiều điểm mà là một bài toán machine learning, có lẽ anh họa sĩ đã thua rồi. Bởi vì điểm bị dịch chuyển có thể là do tác động của noise để hòng đánh lừa anh. Anh họa sĩ đại diện cho một tập model cực mạnh, có khả năng mô phỏng mọi hàm số. Một tập model mạnh như vậy rất nhạy cảm với  noise  và dễ dàng bị overfitting. Sự kết hợp giữa các yếu tố gây overfitting Các yếu tố gây ra overfitting phải phối hợp với nhau thì mới đủ điều kiện cho nó xuất hiện. Ta xem xét hai tình huống thường gặp sau: Có nhiều dữ liệu : ta có thể vô tư dùng ERM, tập model mạnh mà không lo về overfitting. Đây chính là lý do mà thế giới hân hoan khi Big Data xuất hiện. Làm việc với model yếu : các model thường bị một hội chứng chị em ngược lại với overfitting, gọi là  underfitting . Đây là khi model quá đơn giản so với quan hệ cần tìm. Lúc này, dù có tăng thêm dữ liệu cũng không giúp cho model chính xác thêm. Điều cần làm đó là tăng sức mạnh (tăng số lượng tham số hoặc thay đổi dạng) của model. Mình cũng xin dành ra vài dòng để nói về hiện tượng \"cuồng\" deep learning và áp dụng deep learning lên mọi bài toán. Các model của deep learning là các neural network cực mạnh nên cần rất nhiều dữ liệu để không bị overfitting. Đó là lý do mà dù các model deep learning này không mới, thậm chí là những model đầu tiên của machine learning, nhưng phải chờ đến kỷ nguyên Big Data hiện tại chúng mới phát huy sức mạnh. Nếu không am hiểu về overfitting và áp dụng deep learning vô tội vạ lên những tập dữ liệu chỉ có vài trăm cặp dữ liệu thì thường đạt đượt kết quả không cao. Khi gặp những điều kiện dữ liệu eo hẹp như vậy, nên bắt đầu từ những model đơn giản như linear model trước. Trong machine learning có một định lý nổi tiếng gọi là \"no free lunch\" nói rằng không có một model nào tốt nhất cho tất cả các loại dữ liệu. Vì thế, tùy vào bài toán, vào tính chất và số lượng dữ liệu sẵn có, ta mới xác định được model phù hợp. Regularized loss minimization Trong bài trước, ta đã biết được một phương pháp để giảm thiểu overfitting,  early stopping . Ba yếu tố gây ra overfitting cũng gợi ý cho chúng ta những cách khác để khắc phục vấn đề này. Trong đó, yếu tố thứ hai đưa ra giải pháp đơn giản nhất: tăng kích thước tập huấn luyện. Sau đây, mình sẽ giới thiệu một phương pháp nhằm loại trừ đi yếu tố thứ nhất và thứ ba, được gọi là  regularization . Phổ biến nhất, phương pháp này sẽ  thêm vào ERM objective function một regularizer nhằm hạn chế sức mạnh của model . Giả sử rằng đã lỡ tay chọn một model quá mạnh. Thì không cần phải thay đổi dạng model, ta vẫn có thể hạn chế sức mạnh của nó đi bằng cách giới hạn parameter space (không gian của tham số) của model. Xét hai tập model $$A = { f_w : w \\in X}$$ và $$B = { f_{w'} : w' \\in Y}$$ chỉ khác nhau về parameter space thôi (ký hiệu $$S = {s : c}$$ đọc là \"tập $$S$$ gồm các phần tử $$s$$ sao cho điều kiện $$c$$ thỏa mãn). $$X$$ hoặc $$Y$$ được gọi là không gian tham số của tập model $$A$$ hoặc $$B$$. Trong trường hợp này, nếu $$X \\subset Y$$ (X là tập con của Y) thì rõ ràng tập model $$B$$ biểu diễn được mọi hàm số tập model $$A$$ biểu diễn được, tức là  $$B$$ mạnh hơn $$A$$ . Nếu parameter $$w$$ là một vector số thực có $$d$$ chiều, tập hợp các giá trị $$w$$ có thể nhận, hay còn gọi là parameter space của $$w$$, là tập tất cả các vector có $$d$$ chiều số thực, ký hiệu là $$\\mathbb{R}^d$$. Trong không gian này, mỗi chiều của $$w$$ đều được tự do bay nhảy trong khoảng $$(-\\infty,\\infty)$$. Muốn thu nhỏ lại không gian này, ta cần một cơ chế để thu hẹp miền giá trị của mỗi chiều. Để làm được điều đó, ý tưởng ở đây là định nghĩa một đại lượng để khái quát được \"độ lớn\" của vector $$w$$. Đại lượng này sẽ được dùng làm regularizer, ký hiệu là $$R(w)$$ như ta đã biết, là một hàm số phụ thuộc vào $$w$$. Nó sẽ được gắn thêm vào ERM objective function và được tối thiểu hóa cùng lúc với average loss. Objective function của chúng ta được định nghĩa lại như sau:\n$$\n\\mathcal{L} {D {train}}(f_w) = \\mathcal{L} {D {train}}^{ERM} + \\lambda R(w)\n$$ Tối thiểu hóa objective function này được gọi là quy tắc  regularized loss minimization  (RLM). Chú ý đối với RLM, không nhất thiết là $$\\mathcal{L} {D {train}}^{ERM}$$ phải đạt giá trị tối thiểu để cho objective function trở nên tối thiểu. Nếu một model tối thiểu hóa $$\\mathcal{L} {D {train}}^{ERM}$$ nhưng lại làm cho $$R$$ đạt giá trị lớn thì vẫn có cơ hội để chọn một model khác, dù có $$\\mathcal{L} {D {train}}^{ERM}$$ lớn hơn nhưng lại cho giá trị của $$R$$ nhỏ hơn nhiều. Nói cách khác, ta có thể lựa chọn được một model đơn giản, dù nó không dự đoán hoàn hảo tập huấn luyện. RLM đang đưa model đi gần đến Occam's razor hết mức có thể, chấp nhận hy sinh độ chính xác trên tập huấn luyện để giảm độ phức tạp của model. Hằng số $$\\lambda$$ trong hàm mục tiêu được gọi là  rgularization constant , là một hyperparameter của model. Sự xuất hiện của $$\\lambda$$ trong hàm mục tiêu làm cho vai trò của $$\\mathcal{L} {D {train}}^{ERM}$$ và $$R$$ trở nên  bất đối xứng : nếu ta tăng $$\\mathcal{L} {D {train}}^{ERM}$$ lên $$1$$ đơn vị thì hàm mục tiêu tăng lên $$1$$ đơn vị; trong khi đó nếu tăng $$R$$ lên $$1$$ đơn vị thì hàm mục tiêu tăng lên thêm $$\\lambda$$ đơn vị. Tức là $$1$$ đơn vị của $$\\mathcal{L} {D {train}}^{ERM}$$ có giá trị bằng $$1 / \\lambda$$ đơn vị của $$R$$. Thông thường, ta thường đặt $$\\lambda$$ rất nhỏ, ví dụ $$\\lambda = 10^{-4}$$. Lúc này, $$1$$ đơn vị của $$\\mathcal{L} {D {train}}^{ERM}$$ bằng đến $$10^4$$ đơn vị của $$R$$. Điều này thể hiện rằng ta muốn ưu tiên vào tối thiểu hóa $$\\mathcal{L} {D {train}}^{ERM}$$ hơn là $$R$$. Các regularizer thường gặp $$R(w)$$ thường gặp nhất là  norm của vector . Có rất nhiều loại norm, mình sẽ giới thiệu hai loại norm phổ biến nhất. 1-norm  (L1-norm): $$ R(w) = ||w|| 1 = \\sum {i = 1}^d |w_i|$$ tức là tổng của trị tuyệt đối của các thành phần. 1-norm đặc biệt ở chỗ là, khi đưa vào hàm mục tiêu, nó sẽ thường cho ra model thưa, tức là model có parameter chứa nhiều chiều bằng 0. Model thưa rất có lợi thế trong tính toán và lưu trữ vì ta chỉ cần làm việc trên các chiều khác 0. squared 2-norm  (L2-norm): $$ R(w) = ||w|| 2^2 = \\sum {i = 1}^d w_i^2$$ cũng còn biết đến với cái tên  weight decay , chính là bình phương độ dài của vector $$w$$. Sở dĩ ta phải bình phương là để giúp cho việc tính đạo hàm được dễ hơn khi tối ưu hàm mục tiêu. Lưu ý, đây không thực sự là norm, căn bậc hai của nó mới là norm. Jump to Line Go © 2017  GitHub , Inc. Terms Privacy Security Status Help Contact GitHub API Training Shop Blog About",
          "relevance": "1",
          "title": "Regularized Loss Minimization",
          "url": "https://github.com/khanhptnk/ml-book-vn/blob/master/1_5_rlm.md"
        },
        {
          "content": "artificialbrain.xyz Máy học cho người Việt Home Software Scikit-learn Weka TensorFlow Python  Python - Cài đặt   Python - Tóm tắt   Numpy   Algorithms Clustering K-mean Multiple Regression Logistic Regression Naive Bayes K-nearest neighbor Decision Tree SVM Deep Learning Books  Introduction to Machine Learning with Python  Bắt đầu với Scikit-learn  Thuật toán K- láng giềng (K-nearest Neighbors)  Data Science from Scratch  Programming Collective Intelligence Học Máy Học  Tại sao phải dùng Máy học?  Khi nào nên dùng Máy học? Máy học là gì?  Tại sao cuốn Học Máy Học ra đời?  Bài 1: Hiển thị dữ liệu  Bài 2: Thống kê dữ liệu  Bài 3: Biểu đồ hóa dữ liệu   Bài 4: Chuẩn hóa dữ liệu  Bài 5: SIMPLE LINEAR REGRESSION    Bài 6: MULTIPLE LINEAR REGRESSION  Ngẫu hứng  Hiện tại và tương lai của Trí tuệ Nhân tạo Tổng số lượt xem trang Social Profiles Numpy    Mặc dù chúng tôi rất muốn đề cập tới thuật toán Máy học ngay, tuy nhiên để mọi người hiểu được, chúng ta buộc phải có một vài những kiến ... Bắt đầu với Scikit-learn    What? Scikit-learn là gì?   Scikit-learn (viết tắt là sklearn) là một thư viện mã nguồn mở dành cho học máy - một ngành trong trí tuệ nhâ... Python – Cài đặt    Có thể ngắn gọn rằng: học Máy học rất khó có thể \"cưỡng\" sự hấp dẫn của Python. Đơn giản là vì hiện nay có rất nhiều thư viện c... \nĐược tạo bởi  Blogger .\n Người đóng góp cho blog Nguyen Van-Hau Nguyễn Hoàng Phúc Phan Trọng Đạt Superman Ox Tuân Bùi Lưu trữ Blog \n\n        ▼ \n      \n \n2017\n (20) \n\n        ► \n      \n \ntháng tám 2017\n (4) \n\n        ► \n      \n \ntháng bảy 2017\n (2) \n\n        ▼ \n      \n \ntháng sáu 2017\n (7) BÀI 4: CHUẨN HÓA DỮ LIỆU BÀI 3: BIỂU ĐỒ HÓA DỮ LIỆU BÀI 2: THỐNG KÊ DỮ LIỆU BÀI 1: HIỂN THỊ DỮ LIỆU Một số khái niệm trong Máy học Pandas Numpy \n\n        ► \n      \n \ntháng năm 2017\n (7) Labels libray for machine learning numpy python scikit-learn scipy sklearn Blog Archive ▼  2017 (20) \n\n          ► \n        \n tháng tám (4) \n\n          ► \n        \n tháng bảy (2) ▼  tháng sáu (7) BÀI 4: CHUẨN HÓA DỮ LIỆU BÀI 3: BIỂU ĐỒ HÓA DỮ LIỆU BÀI 2: THỐNG KÊ DỮ LIỆU BÀI 1: HIỂN THỊ DỮ LIỆU Một số khái niệm trong Máy học Pandas Numpy \n\n          ► \n        \n tháng năm (7) Thứ Năm, 8 tháng 6, 2017 Một số khái niệm trong Máy học tháng 6 08, 2017 \n   \n   No comments \nChúng ta đã cùng nhau tìm hiểu và trả lời các câu hỏi: WHY? WHAT? WHEN? Và dần dần chúng ta đang gần tới câu hỏi HOW? Làm thế nào để xây dựng được các chương trình/giải thuật Máy học.  \n      Trong bài này, chúng tôi đề cập tới những thuật ngữ, khái niệm rất hay gặp và quan trọng trong Máy học. Nếu bạn đọc chưa rõ ngay những khái niệm này, xin hãy bỏ qua và đọc lại nó sau. Rất có thể cách diễn giải của chúng tôi chưa thực sự tốt, dù đã rất cố gắng theo cách đơn giản. Data Máy học là chương trình\nmáy tính tự “học” từ dữ liệu. Do vậy việc tìm hiểu kĩ dữ liệu và việc cần thiết.\nTrong khuôn khổ cuốn sách này, dữ liệu giống như một bảng gồm hàng và cột. Đây\nlà một cấu trúc dữ liệu cơ bản trong Máy học. Những dữ liệu khác như hình ảnh,\nvideo, văn bản là những dữ liệu không có cấu trúc sẽ không được xét đến.   2104 3 399900 1600 3 329900 2400 3 369000 1416 2 232000 3000 4 539900 1985 4 299900 1534 3 314900 1427 3 198999 1380 3 212000 1494 3 242500 1940 4 239999 2000 3 347000 Bảng 1. Dữ liệu\nđược trích từ tệp  ex1data1.txt  [1]. Trong\nBảng 1, cột thứ nhất chứa thông tin diện tích ngôi nhà (tính bằng feet 2 \n), cột thứ hai chứa thông tin số phòng ngủ, cột thứ ba chứa thông tin  giá ngôi nhà (tính bằng USD). Bài toán đặt ra\nlà dựa vào thông tin diện tích và phòng ngủ, chúng ta cần đưa ra dự đoán cho\ngiá của ngôi nhà tương ứng. Thể hiện/Dữ kiện \n(instance): Là một hàng trong bảng dữ liệu. Trong ví dụ trên Bảng 2.1 có 12 thể\nhiện/dữ kiện, mỗi thể hiện gồm thông tin của một căn hộ (gồm diện tích, số\nphòng ngủ, và giá tiền). Đặc\ntrưng (feature): là một cột trong bảng dữ liệu. Nó là một thành phần của một đặc\ntrưng của một dữ kiện. Một số đặc trưng là dữ liệu quan sát được và một số là đặc\ntrưng cần được dự đoán. Trong bài toán trên 2 cột đầu (diện tích và số phòng ngủ)\nlà đặc trưng quan sát, cột cuối cùng là đặc trưng cần dự đoán (giá căn hộ). Kiểu dữ liệu \n(Data Type): các đặc trưng đều có dữ liệu, mỗi dữ liệu đều có một kiểu dữ liệu xác\nđịnh. Chúng có thể là kiểu số nguyên, số thực, hay cũng có thể là kiểu rời rạc.\nChúng ta cũng gặp phải kiểu dữ liệu như ngày, tháng, chuỗi kí tự, hay những kiểu\nphức tạp khác; tuy nhiên, chúng sẽ được chuyển sang kiểu nguyên, thực hay rời rạc\nkhi dùng các thuật toán Máy học.  Các tập dữ liệu \n(Datasets): một tập hợp các thể hiện/dữ kiện là một tập dữ liệu. Chúng ta sẽ\ndùng một vài tập dữ liệu cho các mục đích khác nhau. Tập dữ liệu huấn luyện \n(Training dataset): là một tập dữ liệu dùng để huấn luyện cho mô hình của thuật\ntoán Máy học. Nói một cách khác, thuật toán Máy học sẽ học từ tập dữ liệu này. Tập dữ liệu kiểm tra \n(Testing dataset): là một tập dữ liệu dùng để kiểm chứng độ chính xác của mô\nhình của thuật toán Máy học. Tập dữ liệu này không được dùng để huấn luyện mô\nhình.  Có\nthể hiểu một cách trực quan rằng, tập dữ liệu huấn luyện giống như bài tập được\ndùng khi giáo viên ôn tập cho học sinh, sinh viên; còn tập dữ liệu kiểm tra giống\nnhư bài thi. Và bài thi là bài thí sinh chưa bao giờ làm. Một giáo viên giỏi là\nngười lựa chọn được các bài ôn tập có dạng bài hay dạng đề giống như bài thi. Thông\nthường chúng ta cần tách tập dữ liệu mà chúng ta có được ra thành các tập dữ liệu\ncon (dùng để huấn luyện và kiểm tra). Mục tiêu của một\nchương trình Máy học (có giám sát) là xây dựng một hàm f ánh xạ tự dữ kiện quan\nsát (features) tới nhãn (label):  y = f(X) Một số thuật ngữ trong Máy học Máy\nhọc là quá trình xây dựng chương trình tự động “học”. Chúng ta sẽ cũng thảo luận\nqua một số khái niệm học trong Máy học. Qui nạp \n(Induction): Các thuật toán máy học được học thông qua quá trình qui nạp nhờ tập\ndữ liệu huấn luyện (training dataset). Qui nạp là quá trình lập luận đi đến\nKhái quát từ các trường hợp cụ thể. Khả\nnăng dự đoán  (Generalization): Một trong những trọng\ntâm khác của học máy là đạt được tính phổ quát (generalization), nói cách khác\nlà mô hình sau khi được xây dựng bởi thuật toán Máy học dùng để dự đoán hay đưa\nra những quyết định khi gặp dữ liệu mà nó chưa gặp bao giờ (unseen data). Một\nchương trình chỉ hiệu quả với dữ liệu đã gặp nhìn chung không có nhiều tính hữu\ndụng.  Overfitting \n(over-learning): Khi mô hình thể hiện sự chính xác trên tập huấn luyện nhưng lại\nkém chính xác trên tập dữ liệu mới (tập kiểm tra). Cũng có thể hình dung khi mô\nhình quá khớp với tập dữ liệu huấn luyện.  ·         \n Nguyên nhân: Điều này xảy ra khi tập dữ\nliệu huấn luyện của chúng ta có nhiễu (noise), hay mô hình của chúng ta quá phức\ntạp, tức là có quá nhiều tham số so với số dữ liệu quan sát được (thể hiện).\nChính nhiễu đã gây tác động xấu tới quá trình dự đoán của mô hình với dữ liệu mới.\n ·         \n Nhận biết: Khi mô hình cho kết quả độ lệch\nnhỏ nhưng phương sai lớn (low bias nhưng high variance) ·         \n Cách giải quyết: Trình bày trong Bài 8. Underfitting \n(under-learning): Khi một mô hình thực thi không tốt trên cả tập dữ liệu huấn\nluyện và tập dữ liệu kiểm tra. Cũng có thể hình dung khi mô hình không khớp với\ntập dữ liệu huấn luyện. ·         \n Nguyên nhân: Điều này xảy xa khi mô hình\nchúng ta đang xây dựng quá đơn giản so với tập dữ liệu.  ·         \n Nhận biết: Khi mô hình cho kết quả độ lệnh\nlớn nhưng phương sai nhỏ (low variance nhưng high bias). ·         \n Cách giải quyết: Một phương pháp để xử\nlý khi underfitting là hãy thay đổi thuật toán Máy học đang dùng. Good-fit : Khi mà mô hình ở trạng thái tối ưu (sweet\npot) giữa overfitting và underfitting. Đây là đích mà mọi thuật toán Máy học hướng\ntới, nhưng nó rất khó đạt được trong thực tế. Trong quá trình học, lỗi mà mô\nhình gây ra cho tập dữ liệu huấn luyện giảm dần và lỗi trên tập kiểm tra cũng vậy.\nNếu chúng ta huấn luyện quá lâu, lỗi trên tập huấn luyện tiếp tục giảm xuống vì\nmô hình sẽ học vào mức quá chi tiết không thực sự quan trọng (như nhiễu -\nnoise) hướng sang trạng thái overfitting; cùng với thời điểm đó, lỗi trên tập dữ\nliệu kiểm tra sẽ tăng lên và có nghĩa là mức độ Khái quát hóa của mô hình đang\ngiảm xuống. Trạng thái tối ưu (sweet pot) chính là điểm\nngay trước khi lỗi trên tập kiểm tra bắt đầu tăng. Ở trạng thái đó, mô hình\nđang cân bằng trong cả tập dữ liệu huấn luyện và tập dữ liệu mới (kiểm tra). Để\ntìm thấy trạng thái good-fit, chúng ta thường sử dụng hai kĩ thuật để tránh\noverfitting như đã nêu ở trên.  Hình 1 Hình bên trái, ở giữa, và bên phải lần lượt ở trạng thái  underfitting,\ngood-fit, và overfitting [1]. Hình\nbên trái là khi mô hình xây dựng (đường thẳng) đang ở dạng Underfitting, trong\nkhi hình bên phải ở dạng Overfitting, hình ở giữa ở dạng  good-fit  (giữa Underfitting và\nOverfitting). Tham số d chỉ bậc của đa thức, trục tung chỉ giá, và trục hoành\nchỉ diện tích của căn hộ. Sự lựa chọn mô hình: Quá trình thiết lập và huấn\nluyện mô hình như là một quá trình lựa chọn mô hình. Mỗi bước lặp, chúng ta lại\ntạo ra một mô hình mới. Việc lựa chọn thuật toán Máy học cũng là quá trình lựa\nchọn mô hình.  Độ lệch  (Bias): Độ lệch là một số đặc trưng cho giá trị\ntrung bình của các số liệu so với giá trị đúng. Hay nói một cách khác, độ lệch\nlà lỗi từ việc giả định sai của thuật toán học. Mô hình khi có độ lệch sẽ gây\nra những sai số khi khái quát hóa. Một ví dụ đơn giản về độ sai lệch trong cuộc\nsống là khi chúng ta có cái cân luôn nặng hơn 1kg so với trọng lượng chuẩn\ntrong mọi lần đo. Độ lệch ở đây là +1 (đơn vị kg). Bias lớn sẽ dẫn tới\nunderfiting.   Phương\nsai  (Variance): Phương sai là một số đặc trưng cho\nđộ phân tán của các số liệu so với số trung bình của nó. Phương sai  là mức độ nhạy cảm với sự biến động nhỏ trong\ntập dữ liệu được huấn luyện. Một cách để giảm phương sai là chạy nhiều lần trên\ntập dữ liệu với các điều kiện ban đầu khác nhau và tính độ chính xác trung bình\ntrong quá trình thực hiện. Phương sai lớn sẽ gây ra overfitting. Cân bằng giữa Bias-Variance: Việc lựa chọn mô\nhình chính là việc cân bằng giữa Bias-Variance. Mô hình có bias thấp sẽ có\nvariance lớn và sẽ cần huấn luyện nhiều hơn; trong khi mô hình có bias cao sẽ\ncó variance thấp  Hinh 2. Mô tả sự cân bằng Bias-Variance [3].    Trong Hình 2, trục tung chỉ lỗi của mô hình dự\nđoán, trục hoành chỉ số chiều của không gian đầu vào (độ phức tạp của mô hình -\ntỉ lệ với số chiều của các đặc trưng).  Ngoài\nmục tiêu quan trọng của cuốn sách này nhằm giúp người học hiểu và cài đặt được\nnhững thuật toán Máy học quan trọng, cuốn sách cũng sẽ dần dần cung cấp những\nkhái niệm và kiến thức không thể thiếu trong Máy học.  Các kiểu học trong Máy học Máy\nhọc có ba kiểu học chính: học có giám sát (supervised learning), học không giám\nsát (unsupervised learning), và học tăng cường (reinforcement learning). Supervised\nLearning  Khi\nchúng ta có dataset gồm đặc trưng (features) và nhãn (labels). Ví dụ trong Bảng\n2.1, hai cột đầu (diện tích, số phòng ngủ) là features, cột cuối cùng (giá nhà)\nlà labels. Nhiệm vụ là xây dựng một chương trình (có thể coi là một hàm) có khả\nnăng dự đoán label khi cho biết tập các features. Hầu hết những thành tựu của\nMáy học hiện nay đều là học có giám sát. Ứng dụng của học có giám sát hiện nay\nvô cùng nhiều, một số ví dụ điển hình như: ·         \n Bộ lọc thư rác (spam filtering). Google\nphải huấn luyện hàng triệu các email (gồm cả email rác và không rác) để có được\nbộ lọc hiệu quả chúng ta dùng hiện nay. ·         \n Nhận dạng khuôn mặt (face recognition).\nFacebook có chương trình nhận diện mặt khá tốt nhờ nó được huần luyện qua nhiều\nảnh cũ của một người. Khi cho một ảnh mới, nó sẽ dự đoán được người đó. ·         \n Hệ thống gợi ý (recommendation). Hãng\nNetflix đã trao giải thưởng lớn cho nhóm có chương trình gợi ý phim yêu thích mới\ncho người cụ thể khi họ đã cung cấp dữ liệu bộ phim yêu thích. ·         \n Nhận dạng chữ viết tay. Nhờ kĩ thuật mới\ntrong Máy học mà độ chính xác của chương trình đã lên tới 99.7% [13] ·         \n Dự đoán giá cho thị trường chứng khoán,\nthị trường nhà đất, v.v... Học\ncó giám sát có hai loại chính: phân lớp (classification) và hồi qui\n(regression). Trong classification các label có kiểu dữ liệu là rời rạc, trong\nkhi regression có kiểu dữ liệu là liên tục (số thực). Ví dụ cho classification\nlà: ứng dụng lọc thư, nhận dạng khuôn mặt, hệ thống gợi ý xem phim, nhận dạng\nchữ viết tay; trong khi cho regression là dự đoán giá cho thị trường chứng\nkhoán. Với supervised learning, chúng ta có thể hình dung như một hàm số như\nsau: Trong đó  X  là tập đặc trưng (features),  y  là nhãn (label), dạng của hàm  f không biết trước. Nhiệm\nvụ của chúng ta là tìm hiểu và thử nghiệm các thuật toán máy học để tìm ra hàm phù\nhợp nhất. Chú ý rằng, các thuật toán khác nhau sẽ có những giả định khác nhau về\ndạng của hàm trên. Unsupervised\nLearning \nKhi chúng ta chỉ có dataset gồm đặc trưng (features)\nmà không được gán nhãn (labels). Nhiệm vụ của chúng ta là tìm ra sự giống nhau\ngiữa các đối tượng đó. Ví dụ cho học không giám sát: ·         \n Có hai loại âm\nthanh, chúng ta cần tách ra ·         \n Có một tập các\ntin tức, hãy phân loại ra các loại khác nhau (mặc dù chúng ta không biết có bao\nnhiêu loại khác nhau) Reinforcement\nLearning Thuật toán học tăng cường sẽ được áp\ndụng khi tương tác với một môi trường thay đổi nhằm thực hiện một nhiệm vụ nào\nđó (ví dụ như lái xe hay chơi cờ). Các thuật toán học tăng cường cố gắng tìm\nmột chiến lược ánh xạ không gian trạng thái của môi trường tới\ncác hành động mà chương trình nên chọn trong các trạng thái đó để cực đại hóa\nmột khoản thưởng (reward) nào đó về lâu dài . Trong\nMáy học, chương trình học từ cặp ( X /input\n–  y /output), được gọi là có giám sát\nbởi vì nó giám sát quá trình học theo định dạng của kết quả ra ứng với mỗi dữ\nkiện mà thuật toán học. Trong học không giám sát, chúng ta chỉ có  X , chứ không có  y . Học không giám sát nhằm tìm ra một mô hình mà phù hợp với các\nquan sát. Nó khác biệt với  học có giám sát  ở chỗ là đầu ra\nđúng tương ứng cho mỗi đầu vào là không biết trước. Vì\nhầu như tất cả các ứng dụng trong công nghiệp và thương mại hiện nay đều thuộc\nvề học có giám sát do vậy cuốn sách này sẽ chỉ giới thiệu các thuật toán học có\ngiám sát. Tài\nliệu tham khảo:       Andrew Ng. https://www.coursera.org/learn/machine-learning Pedro Domingos.  A few useful things to know\nabout machine learning , Published by ACM\n2012 Article.  Volume 55 Issue 10, October 2012 , p ages 78-87.   http://gerardnico.com/wiki/data_mining/bias_trade-off Trevor Hastie. Robert Tibshirani. Jerome Friedman.  The Elements of Statistical\nLearning: Data Mining, Inference, and Prediction .  Springer; 2nd ed. 2009. Corr. 7th printing 2013 edition,\n745 pages Tom\nMitchell.  Machine Learning . McGraw-Hill Education;\n1 st  edition, March 1997, 432 pages Stephen Marsland.  Machine Learning: An Algorithmic Perspective .  Chapman and Hall/CRC;\n1 st  edition April 2009, 406 pages   http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram Gửi email bài đăng này BlogThis! Chia sẻ lên Twitter Chia sẻ lên Facebook Bài đăng Mới hơn Bài đăng Cũ hơn Trang chủ \n0\nnhận xét:\n        \n Đăng nhận xét \nĐăng ký:\n Đăng Nhận xét (Atom) Labels libray for machine learning numpy python scikit-learn scipy sklearn",
          "relevance": "1",
          "title": "Tổng số lượt xem trang",
          "url": "http://ml4vn.blogspot.com/2017/06/mot-so-khai-niem-trong-may-hoc.html"
        },
        {
          "content": "Navigation Machine Learning Mastery Making developers awesome at machine learning Start Here \r\n     \r\n Blog \r\n     \r\n Books \r\n     \r\n About \r\n     \r\n Contact Want help with algorithms?  Take the FREE Mini-Course . Home Empty Menu Return to Content Overfitting and Underfitting With Machine Learning Algorithms By Jason Brownlee on March 21, 2016 in Machine Learning Algorithms Share on Twitter Tweet Share on Facebook Share Share on LinkedIn Share Share on Google Plus Share The cause of poor performance in machine learning is either overfitting or underfitting the data. In this post, you will discover the concept of generalization in machine learning and the problems of overfitting and underfitting that go along with it. Let’s get started. Overfitting and Underfitting With Machine Learning Algorithms Photo by  Ian Carroll , some rights reserved. Approximate a Target Function in Machine Learning Supervised machine learning is best understood as approximating a target function (f) that maps input variables (X) to an output variable (Y). Y = f(X) This characterization describes the range of classification and prediction problems and the machine algorithms that can be used to address them. An important consideration in learning the target function from the training data is how well the model generalizes to new data. Generalization is important because the data we collect is only a sample, it is incomplete and noisy. Get your FREE Algorithms Mind Map Sample of the handy machine learning algorithms mind map. I've created a handy mind map of 60+ algorithms organized by type. Download it, print it and use it.  Download For Free Also get exclusive access to the machine learning algorithms email mini-course.     Generalization in Machine Learning In machine learning we describe the learning of the target function from training data as inductive learning. Induction refers to learning general concepts from specific examples which is exactly the problem that supervised machine learning problems aim to solve. This is different from deduction that is the other way around and seeks to learn specific concepts from general rules. Generalization refers to how well the concepts learned by a machine learning model apply to specific examples not seen by the model when it was learning. The goal of a good machine learning model is to generalize well from the training data to any data from the problem domain. This allows us to make predictions in the future on data the model has never seen. There is a terminology used in machine learning when we talk about how well a machine learning model learns and generalizes to new data, namely overfitting and underfitting. Overfitting and underfitting are the two biggest causes for poor performance of machine learning algorithms. Statistical Fit In statistics, a fit refers to how well you approximate a target function. This is good terminology to use in machine learning, because supervised machine learning algorithms seek to approximate the unknown underlying mapping function for the output variables given the input variables. Statistics often describe the goodness of fit which refers to measures used to estimate how well the approximation of the function matches the target function. Some of these methods are useful in machine learning (e.g. calculating the residual errors), but some of these techniques assume we know the form of the target function we are approximating, which is not the case in machine learning. If we knew the form of the target function, we would use it directly to make predictions, rather than trying to learn an approximation from samples of noisy training data. Overfitting in Machine Learning Overfitting refers to a model that models the training data too well. Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize. Overfitting is more likely with nonparametric and nonlinear models that have more flexibility when learning a target function. As such, many nonparametric machine learning algorithms also include parameters or techniques to limit and constrain how much detail the model learns. For example, decision trees are a nonparametric machine learning algorithm that is very flexible and is subject to overfitting training data. This problem can be addressed by pruning a tree after it has learned in order to remove some of the detail it has picked up. Underfitting in Machine Learning Underfitting refers to a model that can neither model the training data nor generalize to new data. An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data. Underfitting is often not discussed as it is easy to detect given a good performance metric. The remedy is to move on and try alternate machine learning algorithms. Nevertheless, it does provide a good contrast to the problem of overfitting. A Good Fit in Machine Learning Ideally, you want to select a model at the sweet spot between underfitting and overfitting. This is the goal, but is very difficult to do in practice. To understand this goal, we can look at the performance of a machine learning algorithm over time as it is learning a training data. We can plot both the skill on the training data and the skill on a test dataset we have held back from the training process. Over time, as the algorithm learns, the error for the model on the training data goes down and so does the error on the test dataset. If we train for too long, the performance on the training dataset may continue to decrease because the model is overfitting and learning the irrelevant detail and noise in the training dataset. At the same time the error for the test set starts to rise again as the model’s ability to generalize decreases. The sweet spot is the point just before the error on the test dataset starts to increase where the model has good skill on both the training dataset and the unseen test dataset. You can perform this experiment with your favorite machine learning algorithms. This is often not useful technique in practice, because by choosing the stopping point for training using the skill on the test dataset it means that the testset is no longer “unseen” or a standalone objective measure. Some knowledge (a lot of useful knowledge) about that data has leaked into the training procedure. There are two additional techniques you can use to help find the sweet spot in practice: resampling methods and a validation dataset. How To Limit Overfitting Both overfitting and underfitting can lead to poor model performance. But by far the most common problem in applied machine learning is overfitting. Overfitting is such a problem because the evaluation of machine learning algorithms on training data is different from the evaluation we actually care the most about, namely how well the algorithm performs on unseen data. There are two important techniques that you can use when evaluating machine learning algorithms to limit overfitting: Use a resampling technique to estimate model accuracy. Hold back a validation dataset. The most popular resampling technique is k-fold cross validation. It allows you to train and test your model k-times on different subsets of training data and build up an estimate of the performance of a machine learning model on unseen data. A validation dataset is simply a subset of your training data that you hold back from your machine learning algorithms until the very end of your project. After you have selected and tuned your machine learning algorithms on your training dataset you can evaluate the learned models on the validation dataset to get a final objective idea of how the models might perform on unseen data. Using cross validation is a gold standard in applied machine learning for estimating model accuracy on unseen data. If you have the data, using a validation dataset is also an excellent practice. Further Reading This section lists some recommended resources if you are looking to learn more about generalization, overfitting and underfitting in machine learning. Generalization  on Wikipedia Overfitting  on Wikipedia Inductive Reasoning  on Wikipedia Problem of Induction  on Wikipedia Goodness of Fit  on Wikipedia What is an intuitive explanation of overfitting?  on Quora Summary In this post, you discovered that machine learning is solving problems by the method of induction. You learned that generalization is a description of how well the concepts learned by a model apply to new data. Finally, you learned about the terminology of generalization in machine learning of overfitting and underfitting: Overfitting : Good performance on the training data, poor generliazation to other data. Underfitting : Poor performance on the training data and poor generalization to other data Do you have any questions about overfitting, underfitting or this post? Leave a comment and ask your question and I will do my best to answer it. Frustrated With Machine Learning Math? See How Algorithms Work in Minutes …with just arithmetic and simple examples Discover how in my new Ebook:  Master Machine Learning Algorithms It covers  explanations  and  examples  of  10 top algorithms , like: Linear Regression ,  k-Nearest Neighbors ,  Support Vector Machines  and much more… Finally, Pull Back the Curtain on  Machine Learning Algorithms Skip the Academics. Just Results. Click to learn more . Share on Twitter Tweet Share on Facebook Share Share on LinkedIn Share Share on Google Plus Share About Jason Brownlee \r\n\t\tDr. Jason Brownlee is a husband, proud father, academic researcher, author, professional developer and a machine learning practitioner. He is dedicated to helping developers get started and get good at applied machine learning.\r\n Learn more .\t\t\t\t \r\n\t\t\t\tView all posts by Jason Brownlee  →  Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning Gradient Descent For Machine Learning  25 Responses to  Overfitting and Underfitting With Machine Learning Algorithms Kieron March 21, 2016 at 5:08 am # Loving the website Jason! We are currently looking at building a platform to connect data scientists and the like to companies  http://www.kortx.co.uk Reply Gary August 25, 2016 at 8:03 pm # Hi Jason, \nAre resampling and hold-out (the two options for limiting overfitting) mutually exclusive, or do they often get used together? Would using both render one of the techniques redundant? \nThanks Reply Jason Brownlee August 26, 2016 at 10:32 am # Hi Gary, Typically you want to pick one method to estimate the performance of your algorithm. Generally, k-fold cross validation is the recommended method. Using two such approaches in conjunction does not really make sense, at least to me off the cuff. Reply Bruno August 29, 2016 at 6:27 pm # Thank you Jason for these article, I applied you recipe quite successfully! Nevertheless, I remarked that cross validation does not prevent from overfitting. \nDepending on data and algorithm, it can be very easy to get low error rate using cross validation but overfitting. \nDid use saw that already ? This is the reason why I’m using two steps : \n1. compare means of train set score with test set score to check I do not overfit to much, and adjust algorithm parameters \n2. compute cross validation to get the general performance using previous parameters. Am I wrong while doing this procedure ? \nMany Thanks! Reply Jason Brownlee August 30, 2016 at 8:27 am # I agree Bruno, CV is a technique to reduce overfitting, but must be employed carefully (e.g. no of folds). The human is biased, so you also limit the number of human-in-the-loop iterations, because we will encourage the method to overfit, even with CV. Therefore it is also a good idea to hold back a validation dataset that is only evaluated once for final model selection. You procedure looks fine, consider adding a validation dataset for use after CV. Reply Bruno September 5, 2016 at 6:43 pm # Jason, I can’t figure out how to use the validation set \nDo you use it to check for performance agreement/no overfiting with CV score ? Which score (and error) do you use as model performance : the one computed from the validation set or the one from the CV ? Thank a lot Reply Jason Brownlee September 6, 2016 at 9:46 am # Hi Bruno, I would suggest using the CV to estimate the skill of a model. I would further suggest that the validation dataset can be used as a smoke test. For example, the error should be within 2-3 standard deviations of the mean error estimate of the CV, e.g. “reasonable”. Finally, once you pick a model, you can go ahead and train it on all your training data and start using it to make predictions. I hope that helps. Reply Bruno September 7, 2016 at 5:39 pm # Hi Jason, \n‘got it! \nMany thanks for your clear answers and your time. Reply Jason Brownlee September 8, 2016 at 7:35 am # You’re welcome Bruno. Reply Lijo November 2, 2016 at 7:23 am # Hi Jason, \nI was wondering why cant we use a validation dataset and then find the sweet spot by comparing our model with the training set and validation dataset, what are the disadvantages of using this procedure? Reply Jason Brownlee November 2, 2016 at 9:13 am # Hi Lijo, It’s hard. Your approach may work fine, but on some problems it may lead to overfitting. Experiment and see.  The knowledge of how well the model does on the held out (invisible) validation set is being fed back into model selection, influencing the training process. Reply Wan. November 18, 2016 at 11:43 am # my constant value is around 111.832 , is that called overfitting? I’m doing a logistic regression to predict malware detection with data traffic 5000 records, i did feature selection technique in rapid miner extracting 7 features out of 56 and do the statistical logistic regression in SPSS . three, significant feature selected out of 7, At last, I need to draw threshold graph where cut off is 80% from the probability value. like I said, my constant is high and performance 97%. please advise. Reply Jason Brownlee November 19, 2016 at 8:40 am # Hi Wan, Overfitting refers to learning the training dataset set so well that it costs you performance on new unseen data. That the model cannot generalize as well to new examples. You can evaluate this my evaluating your model on new data, or using resampling techniques like k-fold cross validation to estimate the performance on new data. Does that help? Reply John January 27, 2017 at 9:42 pm # Great explanation, as always. If you feel like correcting a small typo: \n“Underfitting refers to a model that can neither model the training data **not** generalize to new data.” (I’m not a native English speaker but think it should be “nor”). Reply Jason Brownlee January 28, 2017 at 7:39 am # Thanks John, I fixed a few typos including the one you pointed out. Reply Saqib Qamar May 1, 2017 at 5:28 pm # Hi Jason, \nGreat tutorial regarding overfitting… Thanks a lot Reply Jason Brownlee May 2, 2017 at 5:56 am # Thanks Saqib, I’m glad to hear that. Reply hang May 11, 2017 at 5:35 am # Is the solution to a XOR problem a overfit? It cannot be solved with 2 units, and one output? Reply Jason Brownlee May 11, 2017 at 8:35 am # Perhaps underfit – as in under-provisioned to be able to solve it. Or even ill-suited. Reply machine_learner May 25, 2017 at 8:01 pm # Hi Jason, great article! So just to confirm, cross-validation doesn’t actually prevent overfitting, it’s just used to give you an idea of how well your model would perform on unseen data, and then you can modify the parameters to improve its performance? I.E. It’s just an evaluation technique Hope you reply in time as I got an exam on this! Cheers Reply Jason Brownlee June 2, 2017 at 11:43 am # Correct! Reply Yolower July 9, 2017 at 10:21 am # How do you solve underfitting? Reply Jason Brownlee July 9, 2017 at 10:59 am # More data or more training. Reply geekWolf August 29, 2017 at 1:13 am # Hello, In both OF and UF has poor performance on generalization. So, what is the solution to have a rich generalization ? Thank you. Reply Jason Brownlee August 29, 2017 at 5:08 pm # Careful tuning of your model 🙂 There’s no silver bullet. Reply Leave a Reply  Click here to cancel reply. Comment Name  (required) Email (will not be published)  (required) Website Welcome to Machine Learning Mastery Hi, I'm Dr. Jason Brownlee.\r\n \r\nMy goal is to make practitioners like YOU awesome at applied machine learning. Read More Understand Machine Learning Algorithms! \r\nSick of all the advanced math? \r\nNeed step-by-step explainations for top algorithms? \r\nWant worked examples in spreadsheets? Finally Understand Machine Learning Algorithms!  Popular Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras July 21, 2016 Your First Machine Learning Project in Python Step-By-Step June 10, 2016 Develop Your First Neural Network in Python With Keras Step-By-Step May 24, 2016 How to Setup a Python Environment for Machine Learning and Deep Learning with Anaconda March 13, 2017 Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras July 26, 2016 Time Series Forecasting with the Long Short-Term Memory Network in Python April 7, 2017 Multi-Class Classification Tutorial with the Keras Deep Learning Library June 2, 2016 Multivariate Time Series Forecasting with LSTMs in Keras August 14, 2017 Regression Tutorial with the Keras Deep Learning Library in Python June 9, 2016 How to Implement the Backpropagation Algorithm From Scratch In Python November 7, 2016 © 2017 Machine Learning Mastery. All Rights Reserved.  Privacy  | \r\n Contact  |\r\n About",
          "relevance": "1",
          "title": "Overfitting and Underfitting With Machine Learning Algorithms",
          "url": "https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/"
        },
        {
          "content": "\n \n \n \n# 1   \n \n13-07-2015, 17:14\n\n Join Date: 01-2014 \nPosts: 92\n Ai đọc cuốn Pattern recognition and machine learning rồi cho em hỏi chút \n\n\nEm đang đọc cuốn này  , phần 3.4: Bayesian Model Comparison, có đoạn ở cuối trang 164, đầu trang 165 có đoạn  :  \nWe have seen that the Bayesian framework avoids the problem of over-fitting \nand allows models to be compared on the basis of the training data alone. However, a Bayesian approach, like any approach to pattern recognition, needs to make assumptions about the form of the model, and if these are invalid then the results can \nbe misleading. In particular, we see from Figure 3.12 that the model evidence can \nbe sensitive to many aspects of the prior, such as the behaviour in the tails. Indeed, \nthe evidence is not defined if the prior is improper, as can be seen by noting that \nan improper prior has an arbitrary scaling factor (in other words, the normalization \ncoefficient is not defined because the distribution cannot be normalized). If we consider a proper prior and then take a suitable limit in order to obtain an improper prior \n(for example, a Gaussian prior in which we take the limit of infinite variance) then \nthe evidence will go to zero, as can be seen from (3.70) and Figure 3.12. It may, \nhowever, be possible to consider the evidence ratio between two models first and \nthen take a limit to obtain a meaningful answer. \nThím nào thông não cho em được không ạ  , đây không phải bài tập về nhà đâu nên mọi người đừng gạch nhé  \n \n# 2   \n \n13-07-2015, 20:48\n\n Join Date: 01-2014 \nPosts: 92\n Re: Ai đọc cuốn Pattern recognition and machine learning rồi cho em hỏi chút \nUppppppppp\n \n \n# 3   \n \n13-07-2015, 21:30\n\n Join Date: 04-2010 \nPosts: 62\n Re: Ai đọc cuốn Pattern recognition and machine learning rồi cho em hỏi chút \n\"Chúng ta đã thấy rằng khung Bayesian tránh được vấn đề trên vừa vặn \nvà cho phép các mô hình để được so sánh trên cơ sở dữ liệu huấn luyện một mình. Tuy nhiên, một phương pháp Bayesian, giống như bất kỳ phương pháp để nhận dạng mẫu, cần đưa ra giả định về hình thức của mô hình, và nếu đây là không hợp lệ thì kết quả có thể \nthể gây hiểu nhầm. Đặc biệt, chúng ta thấy từ hình 3.12 rằng các bằng chứng mô hình thể \nnhạy cảm với nhiều khía cạnh của trước, chẳng hạn như các hành vi trong những cái đuôi. Thật vậy, \ncác bằng chứng không được định nghĩa nếu trước là không đúng, như có thể thấy bằng cách ghi nhận rằng \nmột không đúng cách trước khi có một yếu tố rộng tùy ý (hay nói cách khác, việc bình thường \nhệ số không được định nghĩa bởi vì phân phối không thể bình thường). Nếu chúng ta xem xét một trước và sau đó có một giới hạn phù hợp thích hợp để có được một không đúng cách trước \n(Ví dụ, một Gaussian trước, trong đó chúng ta lấy giới hạn của phương sai vô hạn) sau đó \nbằng chứng sẽ đi đến số không, như có thể được nhìn thấy từ (3.70) và Hình 3.12. Nó có thể, \nTuy nhiên, có thể xem xét tỷ lệ bằng chứng giữa hai mô hình đầu tiên và \nsau đó có một giới hạn để có được một câu trả lời có ý nghĩa.\" \nGoogle translated  \n \n# 4   \n \n13-07-2015, 21:45\n\n Join Date: 04-2010 \nPosts: 62\n Re: Ai đọc cuốn Pattern recognition and machine learning rồi cho em hỏi chút \nĐùa tí đoạn này chắc nó đang nhắc đến chuyện tránh over-fitting khi train dữ liệu cho Bayes thôi. Mà lần sau thím nên chụp ảnh đoạn đó lên để còn xem hình mới chém được. \nQuyển này nặng quá nên lười down =))\n You  may not  post new threads You  may not  post replies You  may not  post attachments You  may not  edit your posts BB code  is  On Smilies  are  On [IMG]  code is  On HTML code is  Off Forum Rules Forum Jump User Control Panel Private Messages Subscriptions Who's Online Search Forums Forums Home  Đại sảnh     Thông báo     Thắc mắc & Góp ý         Thread post sai mục     Tin tức iNet     Review sản phẩm  Máy tính để bàn     Overclocking & Cooling     Modding     AMD     Intel     Mainboard & Memory     Đồ họa máy tính     Phần cứng chung     Thiết bị ngoại vi & Phụ kiện     Phần mềm         Download         Phát triển Phần mềm     Trường đua  Games     Thảo luận chung         DOTA2         League of Legends     Garena - Liên Quân Mobile     MMO -- Game Online     Pokemon GO     Crossfire Legends     Overwatch     Hearthstone     FPS     Liên Minh Huyền Thoại  Sản phẩm công nghệ     Máy tính xách tay         Tất tần tật thông tin về Dell anh em cần biết     Các sản phẩm Apple     Máy tính chuyên dụng     Thiết bị di động     Đồ điện tử & Thiết bị gia dụng     Multimedia         HDmedia  theNEXTvoz     Mạng gia đình & Doanh nghiệp nhỏ     Tự build máy chạy macOS     Ô tô & Xe máy     miniPC  Giao lưu Doanh nghiệp & Người dùng (theo Alphabet)     DELL - Thông tin sản phẩm và dịch vụ chính hãng     Hoanghamobile.com     SaiBack.com - Cung cấp thiết bị lưu trữ chuyên nghiệp     WD - Thương hiệu ổ cứng tin cây, dịch vụ hỗ trợ tốt nhất     We can do  Khu vui chơi giải trí     Chuyện trò linh tinh™         Superthreads         From f17 with Love         Offline     Các món ăn chơi     Trưng cầu dân ý  Khu thương mại - Mua và Bán     Máy tính để bàn     Máy tính xách tay     Điện thoại di động     Các thiết bị công nghệ khác Contact Us  -\n vozExpress  -\n Archive  -\n Top \n\nSteam Powered by vBulletin® 0.1 pre-alpha Copyright ©2000 - 2017, Jelsoft Enterprises Ltd.\n\n",
          "relevance": "0",
          "title": "Ai đọc cuốn Pattern recognition and machine learning rồi cho em hỏi chút",
          "url": "https://www.vozforums.net/showthread.php?p=79022276"
        },
        {
          "content": "Khanh's little things Official homepage of Khanh Xuan Nguyen About Research Blog Tutorials Programming Machine Learning Tips Contact About Khanh Nguyen Ph.D. Student at UMaryland Recent Posts Deep learning (4): deep learning hoạt động ra sao? Deep learning (3): PyTorch và MNIST Deep learning (2): thách thức Deep learning (1): góc nhìn giáo dục Machine learning for non-differentiable loss functions June 19, 2016 Machine Learning (4): Overfitting (phần 2) Xin chào các bạn, hôm nay chúng ta sẽ hoàn tất những hiểu biết về overfitting và đưa ra một thuật toán supervised learning hiệu quả hơn ERM để chống lại overfitting. Nhưng trước khi đó ta cùng ôn lại những gì đã học ở  bài viết trước  bằng một số câu hỏi ngắn như sau: Q1 : Overfitting là gì? A1 : Là khi model  không  có khả năng tổng quát từ những gì đã học được: độ sai sót trên tập huấn luyện nhỏ, trên tập kiểm tra to. Q2 : Tại sao overfitting lại có hại? A2 : Vì dữ liệu lúc nào cũng chứa noise, mà model bị overfitting rất nhạy cảm với noise. Noise làm cho model tìm được phức tạp quá mức cần thiết. Q3 : Làm sao để biết được model có bị overfitting hay không? A3 : Theo dõi learning curve. Q4 : Làm sao để không bị overfitting? A4 : Đây không phải là một câu hỏi đúng vì overfitting là một khái niệm tương đối, tùy theo “cảm giác” của bạn. Nếu bạn đang nói về chuyện làm sao để $\\mathcal{L}_{D_{train}}$ trùng với $\\mathcal{L}_{\\mathcal{D}}$ thì câu trả lời là không thể, trừ phi có vô hạn dữ liệu. Trong bài viết này, chúng ta sẽ tìm hiểu sâu hơn về nguyên nhân gây ra overfitting. Như chúng ta đã biết, noise không phải là nguyên nhân trực tiếp gây ra overfitting. Vậy  những yếu tố nào gây ra overfitting? Overfitting là sản phẩm của sự cộng hưởng giữa các yếu tố sau: Thứ nhất,  áp dụng ERM . Thứ hai,  giới hạn về dữ liệu : khi có thêm các cặp observation-label, hiển nhiên ta có thêm thông tin về mối quan hệ giữa chúng. Cụ thể hơn, ta thấy rằng $\\mathcal{L}_{D_{train}}$ sẽ hội tụ về $\\mathcal{L}_{\\mathcal{D}}$ khi độ lớn của $D_{train}$ tiến đến vô cực. Khi hai đại lượng này trùng nhau thì overfitting hoàn toàn biến mất (theo định nghĩa). Vì thế, càng có nhiều dữ liệu huấn luyện thì càng ít bị overfitting. Thứ ba,  tập model quá “mạnh” : khi chọn một dạng model $f_w$ và thay đổi $w$, ta được một tập model. Ví dụ nếu $f_w$ là một đa thức bậc một, thì tập model là tập hợp tất cả các đa thức bậc một (có dạng $y = f_w(x) = w_1x + w_2$). Dù có vô số đa thức như vậy, nhưng mà đây được xem như một tập model “yếu” bởi vì nó không biểu diễu được các hàm phi tuyến tính. Vì bản chất machine learning là ước lượng hàm số, sử dụng một tập model mạnh hơn, thậm chí có khả năng mô phỏng tất cả dạng hàm số tưởng chừng như là một ý hay. Nhưng thực tế đây lại là một ý tưởng này rất tồi. Vì sao? Giả sử có một cuộc thi trong đó ta yêu cầu mỗi thí sinh phải vẽ được một đường đi qua nhiều nhất các điểm cho trước. Thí sinh tham dự có 2 người: một người là họa sĩ, anh ta rất khéo tay và có thể vẽ tất cả các loại đường cong thẳng; người còn lại là một anh chàng vụng về với cây thước kẻ, anh ta chỉ có thể vẽ đường thẳng. Dĩ nhiên là anh họa sĩ sẽ thắng trong trò chơi này. Nhưng hãy xem xét tình huống sau đây: ta cho đề bài ban đầu là các điểm trên một đường thẳng; sau khi hai người vẽ xong, ta chỉ dịch chuyển một điểm lệch ra khỏi đường thẳng một ít. Hiển nhiên là ban đầu cả hai người đều vẽ được một đường thẳng đi qua tất cả các điểm. Nhưng sau khi một điểm bị dịch chuyển, anh họa sĩ sẽ vẽ ra một đường hoàn toàn khác với đường thẳng ban đầu, cho dù điểm đó chỉ bị xê dịch chút đỉnh. Ngược lại, anh vụng về thì sẽ vẫn giữ nguyên đáp áp vì đó là đường tốt nhất anh có thể vẽ. Điều ta thấy được ở đây đó là anh họa sĩ, vì quá tài hoa, nên anh rất nhạy cảm với những thay đổi nhỏ trong các điểm dữ liệu. Còn anh vụng về, vì năng lực của anh có hạn, nên thường anh sẽ ít bị ảnh hưởng hơn. Nếu như đây không phải là một cuộc thi vẽ qua nhiều điểm mà là một bài toán machine learning, có lẽ anh họa sĩ đã thua rồi. Bởi vì điểm bị dịch chuyển có thể là do tác động của noise để hòng đánh lừa anh. Anh họa sĩ đại diện cho một tập model cực mạnh, có khả năng mô phỏng mọi hàm số. Một tập model mạnh như vậy rất nhạy cảm với noise và dễ dàng bị overfitting. (Đôi khi giỏi quá cũng không tốt phải không 🙂 ) Lưu ý là yếu tố ở trên phải phối hợp với nhau thì mới đủ điều kiện cho overfitting xuất hiện. Ta xem xét hai tình huống tiêu biểu như sau: 1.  Có nhiều dữ liệu : ta có thể vô tư dùng ERM, tập model mạnh mà không lo về overfitting. Đây chính là lý do mà thế giới hân hoan khi Big Data xuất hiện. 2.  Làm việc với tập model yếu : các model thường ít bị overfitting mà bị một hội chứng chị em ngược lại với nó, gọi là  underfitting . Đây là khi model quá đơn giản so với quan hệ cần tìm. Lúc này, dù có tăng thêm dữ liệu cũng không giúp cho model chính xác thêm. Điều cần làm đó là tăng sức mạnh (tăng số lượng tham số hoặc thay đổi dạng) của tập model. Mình cũng xin dành ra vài dòng để nói về hiện tượng cuồng deep learning và áp dụng deep learning lên mọi bài toán. Các model của deep learning là những model  cực mạnh  nên cần rất nhiều dữ liệu để không bị overfitting. Các model này không mới, thậm chí là những model đầu tiên của machine learning, nhưng phải chờ đến kỷ nguyên Big Data hiện tại chúng mới phát huy sức mạnh. Nếu không am hiểu về overfitting và áp dụng deep learning vô tội vạ lên những tập dữ liệu chỉ có vài trăm cặp dữ liệu thì thường đạt đượt kết quả không cao. Khi gặp những điều kiện dữ liệu eo hẹp như vậy, nên bắt đầu từ những model đơn giản hơn trước.  Trong machine learning có một định lý gọi là “No Free Lunch” nói rằng không có một model nào tốt nhất cho tất cả các loại dữ liệu. Vì thế, tùy vào bài toán và số lượng dữ liệu sẵn có, ta mới xác định được model phù hợp. Khắc phục overfitting Trong bài trước, ta đã biết được một phương pháp để giảm thiểu overfitting,  early stopping . Ba yếu tố gây ra overfitting cũng gợi ý cho chúng ta những cách khác để khắc phục vấn đề này. Trong đó, yếu tố thứ hai đưa ra giải pháp đơn giản nhất: tăng kích thước tập huấn luyện. Sau đây, mình sẽ giới thiệu một phương pháp nhằm loại trừ đi yếu tố thứ nhất và thứ ba, được gọi là phương pháp  bình thường hóa tham số . Thay vì sử dụng ERM, phương pháp này sẽ thay đổi hàm mục tiêu nhằm hạn chế sức mạnh của model. Giả sử rằng đã lỡ chọn một tập model quá mạnh (như mạng neuron). Thì không cần phải thay đổi dạng model, ta vẫn có thể hạn chế sức mạnh của nó đi bằng cách giới hạn  không gian tham số  (parameter space/domain) của model. Xét hai tập model $A = \\{ f_w : w \\in X\\}$ và $B = \\{ f_{w’} : w’ \\in Y\\}$ (ký hiệu $S = \\{s : c\\}$ đọc là “tập $S$ gồm các phần tử $s$ sao cho điều kiện $c$ thỏa mãn). $X$ hoặc $Y$ được gọi là không gian tham số của tập model tương ứng. Trong trường hợp này, nếu $X \\subset Y$ thì rõ ràng tập model $B$ biểu diễn được mọi hàm số tập model $A$ biểu diễn được, tức là $B$ mạnh hơn $A$. Nếu $w$ là một vector số thực có $d$ thành phần, tập hợp các giá trị $w$ có thể nhận, hay không gian tham số của $w$, là $\\mathbb{R}^d$. Trong không gian này, mỗi thành phần của $w$ đều được tự do bay nhảy trong khoảng $(-\\infty,\\infty)$. Muốn thu nhỏ lại không gian này, ta cần một cơ chế để không cho các thành phần của $w$ trở nên quá lớn. Ý tưởng ở đây là định nghĩa một đại lượng để khái quát được “độ lớn” của vector $w$ và cố gắng giảm thiểu nó. Ta ký hiệu đại lượng này là, $R(w)$, là một hàm số phụ thuộc vào $w$. Khi huấn luyện để tìm ra $w$, cùng với việc giảm thiểu $R(w)$, ta cũng nhớ là mình vẫn cần phải giảm thiểu $\\mathcal{L}_{D_{train}}(w)$. Để thể hiện được việc phải giảm thiểu cùng một lúc hai hàm số, ta sẽ giảm thiểu  tổng  của chúng. Cụ thể, ta định nghĩa lại mục tiêu ở bước (1) của supervised learning như sau: $$ w = \\arg\\min_{w’} \\mathcal{L}_{D_{train}}(w’) + \\lambda R(w’)$$ Quy tắc này được gọi là  regularized loss minimization  (RLM), một mở mộng của ERM. Chú ý là đối với hàm mục tiêu của RLM, không nhất thiết là $\\mathcal{L}_{D_{train}}$ phải đạt giá trị tối thiểu để cho tổng $\\mathcal{L}_{D_{train}} + \\lambda R$ trở nên tối thiểu. Nếu một model tối thiểu hóa $\\mathcal{L}_{D_{train}}$ nhưng lại làm cho $R$ đạt giá trị lớn thì vẫn có cơ hội để chọn một model khác, dù có $\\mathcal{L}_{D_{train}}$ lớn hơn nhưng lại cho giá trị của $R$ nhỏ hơn nhiều. Nói cách khác, ta có thể lựa chọn được một model đơn giản, dù nó không dự đoán hoàn hảo tập huấn luyện. Điều này đúng với ý tưởng để giảm thiểu overfitting đã nói đến ở bài trước. Hàm mục tiêu của RLM đang đưa model đi gần đến Occam’s razor hết mức có thể. Ta chấp nhận hy sinh độ chính xác trên tập huấn luyện để giảm độ phức tạp của model, miễn là giảm được hàm mục tiêu tổng. Tuy nhiên, đây là sự đánh đổi hoàn toàn có lợi cho ta. Hằng số $\\lambda$ trong hàm mục tiêu được gọi là  hằng số bình thường hóa , là một hyperparameter của model. Sự xuất hiện của $\\lambda$ trong hàm mục tiêu làm cho vai trò của $\\mathcal{L}_{D_{train}}$ và $R$ trở nên  bất đối xứng : nếu ta tăng $\\mathcal{L}_{D_{train}}$ lên $1$ đơn vị thì hàm mục tiêu tăng lên $1$ đơn vị; trong khi đó nếu tăng $R$ lên $1$ đơn vị thì hàm mục tiêu tăng lên thêm $\\lambda$ đơn vị. Tức là $1$ đơn vị của $\\mathcal{L}_{D_{train}}$ có giá trị bằng $1 / \\lambda$ đơn vị của $R$. Thông thường, ta thường đặt $\\lambda$ rất nhỏ, ví dụ $\\lambda = 10^{-6}$. Lúc này, $1$ đơn vị của $\\mathcal{L}_{D_{train}}$ bằng đến $10^6$ đơn vị của $R$. Điều này thể hiện rằng ta muốn ưu tiên vào tối thiểu hóa $\\mathcal{L}_{D_{train}}$ hơn là $R$. Các hàm bình thường hóa thường gặp $R(w)$ thường gặp nhất là  norm của vector . Có rất nhiều loại norm, mình sẽ giới thiệu hai loại norm phổ biến nhất. 1-norm : $$ R(w) = ||w||_1 = \\sum_{i = 1}^d |w_i|$$ tức là tổng của trị tuyệt đối của các thành phần. 1-norm đặc biệt ở chỗ là, khi đưa vào hàm mục tiêu, nó sẽ thường cho ra  model thưa , tức là một vector $w$ có nhiều thành phần bằng 0. Model thưa rất có lợi thế trong tính toán và lưu trữ vì ta có thể phớt lờ đi các thành phần bằng 0. squared 2-norm : $$ R(w) = ||w||_2^2 = \\sum_{i = 1}^d w_i^2$$ chính là bình phương độ dài của vector $w$. Sở dĩ ta phải bình phương là để giúp cho việc tính đạo hàm được dễ hơn khi tối ưu hàm mục tiêu. Mình sẽ nói kỹ hơn về vấn đề này vào dịp khác. Tập phát triển Đây là yếu tố cuối cùng để hoàn tất quy trình supervised learning hoàn chỉnh. Vấn đề đặt ra là ta không nhất thiết phải đặt giá trị của hằng số bình thường hóa $\\lambda$ giống nhau cho mọi bài toán. Hơn nữa, ngoài $\\lambda$, còn có nhiều hyperparameter khác ta cần phải lựa chọn (như bậc của đa thức). Làm sao để chọn được tập giá trị tối ưu cho các hyperparameter với từng bài toán? Ta có thể làm như sau: ta chọn một tập giá trị của các hyperparameter, rồi áp dụng 2 bước của supervised learning để tìm ra được một model và đo được độ tốt của nó trên tập kiểm tra. Ta tiếp tục lặp lại quá trình này với nhiều tập giá trị hyperparameter khác nhau. Sau nhiều lần thử chọn như vậy, ta chọn tập giá trị nào cho độ sai sót thấp nhất trên tập kiểm tra. Cẩn thận!  Khi dùng tập kiểm tra để xác định hyperparameter, ta đã vi phạm nguyên tắc train-test độc lập đã nêu ở  bài viết này . Nói một cách đơn giản là ta đã sử dụng tập kiểm tra để chọn model. Để khắc phục điều này, ta cần đến một “tập kiểm tra thứ hai”, chỉ chuyên dùng để tinh chỉnh các hyperparameter và  không  dùng để đưa thông báo cuối cùng về độ tốt của model. Ta gọi đấy là  tập phát triển  (development set). Trong bài viết trước, vì chưa nhắc giới thiệu khái niệm tập phát triển nên định nghĩa early stopping của mình cũng đã vi phạm quy tắc train-test độc lập. Vì thời điểm dừng huấn luyện phụ thuộc vào độ sai sót trên tập kiểm tra, mà model cuối cùng nhận được lại phụ thuộc vào thời điểm dừng huấn luyện, suy ra tập kiểm tra đã gián tiếp chỉ định model cuối cùng. Sau khi biết đến tập phát triển, để áp dụng early stopping một cách đúng đắn, thì ta chỉ việc thay learning curve trên tập kiểm tra bằng learning curve trên tập phát triển. Trong nghiên cứu, tỉ lệ train:dev:test thường được dùng đó là  7:1:2 . Và phút giây được mong chờ đã đến, ta đã tìm được một thuật toán hiệu quả cho supervised learning: Sử dụng tập phát triển để tinh chỉnh hyperameter của model: với mỗi tập giá trị của các hyperparameter (bao gồm cả $\\lambda$): Huấn luyện: tìm $w$ để tối thiểu hóa $\\mathcal{L}_{D_{train}}(w) + \\lambda R(w)$. Trong quá trình huấn luyện, theo dõi learning curve để áp dụng early stopping.  Đánh giá trên tập phát triển: thông báo độ tốt trên tập phát triển là $\\mathcal{L}_{D_{dev}}(w)$.  Đánh giá trên tập kiểm tra: với model $w^*$ cho kết quả tốt nhất ở bước 1, thông báo độ tốt cuối cùng trên tập kiểm tra là $\\mathcal{L}_{D_{test}}(w^*)$.   Post Views:  1,724 Share this: Click to share on Twitter (Opens in new window) Click to share on Facebook (Opens in new window) Click to share on Google+ (Opens in new window) Related Comments  comments",
          "relevance": "1",
          "title": "Machine Learning (4): Overfitting (phần 2)",
          "url": "http://khanhxnguyen.com/overfit-2/"
        },
        {
          "content": "Khanh's little things Official homepage of Khanh Xuan Nguyen About Research Blog Tutorials Programming Machine Learning Tips Contact About Khanh Nguyen Ph.D. Student at UMaryland Recent Posts Deep learning (4): deep learning hoạt động ra sao? Deep learning (3): PyTorch và MNIST Deep learning (2): thách thức Deep learning (1): góc nhìn giáo dục Machine learning for non-differentiable loss functions June 11, 2016 Machine Learning 101 (3): Overfitting (phần 1) Xin chào các bạn, chúng ta lại trở lại với machine learning. Trước khi bắt đầu, mình xin thú nhận là trong  bài viết trước , nhằm mục đích đơn giản hóa vấn đề, mình đã phát biểu như sau: Cụ thể hơn, sau khi định nghĩa được hàm mục tiêu, supervised learning có thể được gói gọn trong 2 bước sau: Tìm $f_w$ để tối thiểu hóa $\\mathcal{L}_{D_{train}}(f_w)$. Thông báo độ tốt của $f_w$ là $\\mathcal{L}_{D_{test}}(f_w)$.  Phát biểu này chưa thật tổng quát. Điều mình đã giản lược nằm ở bước 1 của thuật toán. Trước khi biết đó là gì, mình sẽ giới thiệu một cách viết khác ngắn gọn và mang tính toán học hơn của bước 1: \\begin{equation} \nw = \\arg\\min_{w’} \\mathcal{L}_{D_{train}}(w’) \\ \\ \\ \\ (1) \n\\end{equation} Có 2 thay đổi đáng chú ý ở đây: Kí hiệu $\\arg\\min_x f(x)$ có nghĩa là giá trị của $x$ để hàm $f(x)$ đạt được giá trị cực tiểu. Ví dụ,  $\\arg\\min_x x^2 + 1 = 0$ bởi vì $ x^2 + 1$ đạt giá trị cực tiểu (bằng 1) tại $x = 0$. Mình đang giả sử rằng cấu trúc model đã xác định, tức là hai model nếu khác nhau thì chỉ khác nhau về tham số. Khi đó, mình dùng vector tham số $w$ để chỉ model thay cho $f_w$ (nhưng về bản chất, model vẫn là một hàm số nhé). Sử dụng phương trình (1) để tìm ra model được gọi là quy tắc  empirical risk minimization (ERM) . Mình sẽ giải thích vì sao nó được gọi như vậy. Hàm mục tiêu $\\mathcal{L}_D(w)$ còn được gọi là  hàm rủi ro  (risk function) vì nó thể hiện độ sai lệch của một model trên một tập dữ liệu. Chữ “empirical” được thêm vào bởi vì hàm rủi ro này được tính trên một tập dữ liệu hữu hạn. Vì vậy, giá trị của hàm thay đổi tùy theo tập dữ liệu thu thập được. Vậy  empirical risk minimization  tức là tối thiểu hóa rủi ro trên một tập dữ liệu hữu hạn. Đó chính là những điều mình giới thiệu ở bài trước. ERM không phải là cách duy nhất để tìm ra model từ tập huấn luyện. Trong thực tế, nếu ta ngây thơ áp dụng ERM thì sẽ thường không thu được model có độ tốt cao trên tập kiểm tra. Bài viết này giới thiệu những kiến thức cần thiết để ta đưa ra được một thuật toán supervised learning tốt hơn ERM. Mình sẽ nói kỹ về vấn đề lớn nhất thường gặp phải khi sử dụng ERM,  overfitting , và cách khắc phục nó. Overfitting là một trong những khái niệm  quan trọng bậc nhất  trong machine learning. Vì lượng kiến thức trong bài này khá nhiều, để tránh bị ngộp, mình ra chia làm hai phần. Trong phần này chúng ta sẽ định nghĩa overfitting và tìm hiểu tại sao nó lại được gọi là “bóng ma ám lấy machine learning”. Nào chúng ta lên đường… Albert Einstein từng có một câu nói nổi tiếng là: Everything should be made as simple as possible, but no simpler. Nghĩa là “mọi thứ nên được tối giản hóa  hết  mức có thể, nhưng không nên  quá  mức có thể”. Trong machine learning, người ta thường nhắc đến một nguyên tắc gần tương tự gọi là  Occam’s razor : Entities must not be multiplied beyond necessity. ( Wikipedia ) Áp dụng vào machine learning, nguyên tắc này được hiểu là: Trong tất cả các giả thiết có thể giải thích được một hiện tượng, ta nên chọn giả thiết đơn giản nhất.  Hoặc thậm chí đơn giản hơn: Trong tất cả các model “đúng”, chọn model đơn giản nhất.  Lưu ý là ở đây có đến hai điều kiện cần được đảm bảo:  giả thiết phải đơn giản nhất nhưng vẫn phải thích được hiện tượng . Rất dễ để áp dụng Occam’s razor một cách sai lầm. Ta xét bài toán phân loại thư vào hai loại, spam và không spam. Model đơn giản nhất có thể nghĩ ra đó là random một trong hai lựa chọn với mỗi bức thư. Model này dù tối giản nhưng lại vô dụng và vi phạm Occam’s razor vì nó không thể giải thích tính chất spam. Trong một ví dụ khác như sau, Chọn một đa thức bậc cao phức tạp để “giải thích” (đi qua hết) các điểm màu đen cũng vi phạm Occam’s razor bởi vì thực chất ta chỉ cần một đa thức bậc một đơn giản (đường thẳng đỏ) để làm điều đó. Nhưng vì sao nên tuân thủ Occam’s razor? Với mỗi bài toán supervised learning, cho dù có tồn tại một hàm bí ẩn $f$ sao cho mối quan hệ giữa label và observation là $y = f(x)$, thì dữ liệu trong thực tế cũng không bao giờ phản ánh chính xác được mối quan hệ này. Một trong những nguyên nhân gây ra điều này là do sai số trong dụng cụ đo. Dưới đây là một ví dụ minh họa cho thấy thay vì thu được dữ liệu tuyến tính hoàn hảo ($y = ax$) như hình bên trái thì thường tọa độ của các điểm dữ liệu sẽ bị sai lệch như hình bên phải. Với một cặp dữ liệu $(x, y)$, ta có thể mô tả quá trình biến dạng của nó như sau: $$\\tilde{x} = x + \\epsilon_x \\\\ \\tilde{y} = f(\\tilde{x}) + \\epsilon_y$$ Cuối cùng, dữ liệu thật sự ta nhận được để huấn luyện và kiểm tra model là $(\\tilde{x}, \\tilde{y})$, phiên bản lỗi của $(x, y)$. $\\epsilon_x$ và $\\epsilon_y$ được gọi là  noise  của $x$ và $y$. Noise thường được xem là một biến số ngẫu nhiên ( random variable ), thay đổi tùy theo từng cặp $(x, y)$. Sự xuất hiện của noise làm cho mối quan hệ giữa observation và label trở nên phức tạp hơn quan hệ thực sự giữa chúng. Đối với ví dụ ở trên, thì noise đã biến một quan hệ tuyến tính thành một quan hệ phức tạp hơn (đường xanh dương) mà một đa thức bậc một không thể giải thích nổi nữa: Noise không trực tiếp gây ra overfitting nhưng nó lại làm cho overfitting trở nên gây hại. Về bản chất, khi overfitting, model  cố gắng đi qua tất cả các điểm dữ liệu  (hay dự đoán đúng tất cả các observation). Điều này không gì khác chính là tuân theo ERM một cách tuyệt đối. Nếu làm thế, trong quá hình huấn luyện, noise sẽ “lừa” model học một hàm số hoàn toàn sai so với bản chất của dữ liệu. Khi các điểm dữ liệu của tập kiểm tra xuất hiện, vì noise thường không quá lớn, các điểm mới này cũng vẫn thể hiện phần lớn quan hệ tuyến tính và chỉ hơi lệch với đường thẳng đỏ mà thôi. Trong trường hợp này, nếu áp dụng ERM ta sẽ chọn đường xanh dương thay vì đường thẳng đỏ vì đường thẳng xanh dương cho sai số thấp hơn trên tập huấn luyện (các điểm đen). Nhưng một model phức tạp như đường thẳng xanh dương lại cho sai sót rất lớn trên tập kiểm tra (các điểm xanh lá cây nằm rất xa so với đường xanh dương). Vì sai sót trên tập kiểm tra mới là thứ ta quan tâm nên điều này rất tệ. Ngược lại, nếu chọn model đơn giản như hơn đường thẳng đỏ và chấp nhận sai sót một ít trên tập huấn luyện, sai sót trên tập kiểm tra sẽ nhỏ hơn nhiều. Qua ví dụ này, ta thấy là khi áp dụng Occam’s razor vào machine learning, ta không cần phải tuân thủ nó quá chặt chẽ. Sự xuất hiện của noise làm cho hai tiêu chuẩn của Occam’s razor rất khó được bảo toàn: để giải thích được đúng hơn tập huấn luyện vốn chứa noise, ta buộc phải tăng độ phức tạp của model, và ngược lại. Vì thế, điều ta cần làm là cân bằng giữa hai điều kiện, đưa model gần với Occam’s razor nhất có thể: chọn một model đơn giản vừa phải và giải thích được tập huấn luyện tương đối đúng, nhằm đạt được sai số nhỏ trên tập kiểm tra. Thế nào là overfitting?  Qua phân tích ở phần trên, chắc các bạn cũng hiểu nôm na overfitting là khi ta quá cố gắng tối thiểu hóa $\\mathcal{L}_{D_{train}}(w)$ nhưng điều đó lại làm cho $\\mathcal{L}_{D_{test}}(w)$ lớn. Có nhiều định nghĩa về overfitting. Trong phần này, mình sẽ giới thiệu định nghĩa về overfitting mà mình quen thuộc nhất. Nhưng trước hết, ta cần một định nghĩa về  hàm mục tiêu trên một tập dữ liệu vô hạn : $$ \\mathcal{L}_{\\mathcal{D}}(w) = \\mathbb{E}_{(x, y) \\sim \\mathcal{D}} \\left[ L \\left( f_w(x), y \\right)  \\right] =\\sum_{(x, y)} L \\left( f_w(x), y \\right) \\mathcal{D}(x, y) $$ $\\mathcal{D}$ ở đây không phải là một tập dữ liệu mà là một  phân bố xác suất  lên các cặp dữ liệu $(x, y)$, với $\\mathcal{D}(x, y)$ là xác suất xuất hiện của cặp $(x, y)$. Khi nói về một tập dữ liệu “vô hạn”, ý mình đang ám chỉ đến việc liên tục lấy các mẫu $(x, y)$ từ phân bố $\\mathcal{D}$. Với các bạn chưa quen thuộc khái niệm này, mình sẽ minh họa bằng ví dụ đơn giản sau: Giả sử ta có một phân bố xác suất về sấp ngửa của một đồng xu như sau: 60% ngửa và 40% sấp. Muốn lấy một mẫu từ phân bố này, ta ngẫu nhiên chọn một số thực trong đoạn $[0, 1)$, tạm gọi là $r$. Nếu $r \\leq 0.6$, ta chọn mẫu là ngửa, ngược lại ta chọn mẫu là sấp. Lặp lại quá trình này vô hạn lần, ta được một tập dữ liệu vô hạn gồm các mẫu sấp ngửa (ví dụ sấp, ngửa, sấp, sấp, ngửa, …). Ký hiệu $\\mathbb{E}_{x \\sim P} \\left[ f(x) \\right]$ được gọi là  kỳ vọng  của đại lượng $f(x)$ với $x$ được lấy mẫu từ phân bố $P$, có thể hiểu là một  phép tính trung bình cộng trên tập vô hạn . Ta thấy là định nghĩa hàm mục tiêu trên tập vô hạn thật ra không khác gì mấy so với định nghĩa hàm mục tiêu trên tập hữu hạn ở bài trước; ta chỉ thay phép trung bình cộng trên tập hữu hạn thành phép toán tương ứng trên tập vô hạn. Giảm thiểu $\\mathcal{L}_{\\mathcal{D}}(w)$ thực ra mới là mục đích tối thượng của supervised learning; ta chỉ dùng $\\mathcal{L}_{D_{test}}(w)$ để ước lượng $\\mathcal{L}_{\\mathcal{D}}(w)$ vì ta không thể nào có nguồn dữ liệu vô hạn. Đến đây, ta định nghĩa overfitting là khi: Model quá tập trung vào việc đoán đúng hết tất cả các điểm dữ liệu của tập huấn luyện, nhưng việc đó lại làm giảm khả năng dự đoán của nó trên một tập dữ liệu mới khác. Nói cách khác, khi $\\mathcal{L}_{D_{train}}(w)$ nhỏ nhưng $\\mathcal{L}_{\\mathcal{D}}(w) -\\mathcal{L}_{D_{train}}(w)$ lớn (với $D_{train}$ được lấy mẫu từ $\\mathcal{D}$). Chuẩn đoán overfitting Trong định nghĩa trên, thế nào được xem là “lớn” thì tùy thuộc vào từng ứng dụng khác nhau. Hơn nữa, ta cũng không thể nào tính được $\\mathcal{L}_{\\mathcal{D}}(w)$ nên cũng không thể dựa vào định nghĩa đó để chuẩn đoán xem model có bị overfitting hay không. Để làm điều này, overfitting ta cần theo dõi  learning curve , một biểu đồ thể hiện sự biến động của $\\mathcal{L}_{D_{train}}$ và $\\mathcal{L}_{D_{test}}$ trong quá trình huấn luyện. Giả sử quá trình huấn luyện model cần một khoảng thời gian nhất định. Nếu cứ sau một khoảng thời gian, ta ghi lại giá trị của $\\mathcal{L}_{D_{train}}$ và $\\mathcal{L}_{D_{test}}$ và vẽ biểu đồ của chúng theo thời gian, ta được learning curve. Hình ở trên minh hoạ learning curve trên tập kiểm tra và huấn luyện khi xuất hiện overfitting. Có vài điểm đáng chú ý sau: Nếu ta áp dụng một phương pháp tối ưu hàm số hiệu quả, sai sót trên tập huấn luyện  giảm  theo thời gian. Ngược lại, sai sót trên tập kiểm tra  không phải lúc nào cũng giảm . Nếu model bị overfitting, đến một lúc nào đó, sai sót này sẽ bắt đầu tăng trở lại. Thời điểm mà  sai sót trên tập kiểm tra bắt đầu có xu hướng tăng  được xem thời điểm bắt đầu overfitting. Vì sao? Vì sau đó, việc huấn luyện sẽ làm model dự đoán ngày càng tốt hơn trên tập huấn luyện, nhưng lại cho sai sót ngày càng nhiều trên tập kiểm tra. Cách chuẩn đoán này cũng gợi ý cho ta cách làm đơn giản nhất để giảm thiểu overfitting:  dừng huấn luyện ngay tại thời điểm bắt đầu overfitting . Phương pháp này được gọi là  early stopping . Early stopping có tác dụng ngăn không cho khoảng cách giữa $\\mathcal{L}_{D_{train}}$ và $\\mathcal{L}_{D_{test}}$ tăng lên thêm. Để rút ngắn khoảng cách này hơn nữa, ta cần thêm một phương pháp phức tạp hơn mà mình sẽ nói đến ở bài sau. Còn bây giờ, mình muốn kết thúc phần đầu bài viết bằng việc nhấn mạnh lại tầm quan trọng của overfitting: nếu không có overfitting thì machine learning không thể được xem như một ngành nghiên cứu riêng biệt, bởi vì khi đó ta có thể vận dụng hết mọi công cụ tối ưu hàm số của toán học để giảm $\\mathcal{L}_{D_{train}}$ về mức tối đa. Overfitting thể hiện trở ngại khi ta cố gắng bắt máy tính mô phỏng khả năng của con người: làm thế nào mà một người có thể tổng quát được những kiến thức đã học và áp dụng để xử lý tình huống chưa từng gặp, thậm chí là còn sáng tạo ra những thứ chưa hề tồn tại? Post Views:  1,525 Share this: Click to share on Twitter (Opens in new window) Click to share on Facebook (Opens in new window) Click to share on Google+ (Opens in new window) Related Comments  comments",
          "relevance": "1",
          "title": "Machine Learning 101 (3): Overfitting (phần 1)",
          "url": "http://khanhxnguyen.com/machine-learning-101-overfit/"
        },
        {
          "content": "Explore EXPLORE BY INTERESTS Career & Money Business Biography & History Entrepreneurship Leadership & Mentoring Money Management Time Management Personal Growth Happiness Psychology Relationships & Parenting Religion & Spirituality Self-Improvement Politics & Current Affairs Politics Society Science & Tech Science Tech Health & Fitness Fitness Nutrition Sports & Recreation Wellness Lifestyle Arts & Languages Fashion & Beauty Food & Wine Home & Garden Travel Entertainment Celebrity Biography & Memoir Pop Culture Biographies & History Biography & Memoir History Fiction Children’s & YA Classic Literature Contemporary Fiction Historical Fiction LGBTQ Fiction Mystery, Thriller & Crime Romance Science Fiction & Fantasy BROWSE BY CONTENT TYPE Books Audiobooks News & Magazines Sheet Music Search Upload Sign in Join close user settings menu Options Join Sign In Upload 0.0  ( 0 ) Document Actions Download Share or Embed Document Embed Description:  Machine learning co ban View More Machine learning co ban Copyright:  © All Rights Reserved Download  as PDF, TXT or read online from Scribd Flag for inappropriate content Recommended Documents Top Nonfiction On Scribd The Unwinding: An Inner History of the New America by  George Packer Yes Please by  Amy Poehler Sapiens: A Brief History of Humankind by  Yuval Noah Harari Top Fiction On Scribd Extremely Loud and Incredibly Close: A Novel by  Jonathan Safran Foer The Sympathizer: A Novel (Pulitzer Prize for Fiction) by  Viet Thanh Nguyen The Silver Linings Playbook: A Novel by  Matthew Quick \n \n 1.1 1.2 1.3 1.3.1 1.3.2 1.3.3 1.3.4 1.3.5 1.3.6 1.3.7 1.3.8 Table of Contents Lời tựa Machine learning là gì? Supervised Learning Các khái niệm cơ bản Hai góc nhìn về supervised learning Objective function Overfitting Regularized Loss Minimization Tinh chỉnh các  hyperparameter  Thuật toán supervised learning tổng quát Hàm mất mát 1 \n \n Lời tựa Xin chào, mình là  Nguyễn Xuân Khánh, một người đang học và  nghiên cứu về machine learning. Lĩnh vực mình chuyên sâu cho đến bây  giờ là natural language processing (gọi tắt là NLP), tức là làm cho máy tính có  khả năng hiểu được ngôn ngữ của con người. Mình không phải là một chuyên gia nghiên cứu,  kinh nghiệm còn khá non nếu so với  các giáo sư và kỹ sư đầu ngành nhưng mình rất muốn đem bộ môn này về giới thiệu với mọi người, nhất là các bạn còn đang loay hoay tìm câu trả lời cho câu hỏi:  học Tin học sau này có thể làm được gì? Cuốn sách này được viết nhằm đưa ra một trong rất nhiều câu trả lời cho câu hỏi trên. Nội dung chủ yếu giới thiệu về  một lĩnh vực thu  hút rất nhiều sự  chú ý trong vài năm gần  đây , machine learning . Cuốn sách tập trung vào các khái niệm và ứng dụng cơ bản trong lĩnh vực này , nhằm giúp người  đọc có đọc nền tảng học thuật vững  chắc trước khi đi  sâu hơn vào nghiên cứu và sáng chế. Ngày 20/7/2016 Lời tựa 2 \n \n Machine learning là gì? Machine learning gây nên cơn sốt công nghệ trên  toàn thế giới trong vài  năm nay . Trong giới học thuật, mỗi  năm có hàng ngàn bài báo khoa học về  đề tài này. T rong giới công nghiệp, từ các công ty  lớn như Google, Facebook, Microsoft đến các công  ty khởi nghiệp đều đầu tư vào machine learning. Hàng l oạt các ứng dụng sử  dụng machine learning ra đời trên mọi linh vực của cuộc sống, từ khoa học máy tính đến những ngành ít liên quan hơn như vật lý, hóa học, y học, chính trị.   AlphaGo , cỗ máy AI với khả năng tính toán trong một không gian có số lượng phần tử còn nhiều hơn số lượng hạt trong vũ trụ, tối ưu hơn  bất kì đại kì thủ cờ vây nào, là một trong rất nhiều ví dụ hùng hồn cho sự vượt  trội của machine learning so với các phương pháp cổ  điển. Vậy thực chất, machine learning là gì?  Để giới thiệu về machine learning , mình xin dựa vào mối quan hệ  của nó với ba khái niệm sau: 1.  Machine  learnin g  và  trí tuệ nhân tạo  (Artificial Intelligence hay AI) 2.  Machine  learnin g  và  Big Data . 3.  Machine  learnin g  và  dự đoán tương lai . Trí tuệ nhân tạo , AI, một cụm từ vừa  gần gũi vừa xa lạ đối với chúng ta. Gần gũi bởi vì  thế giới đang phát sốt với những công nghệ được dán nhãn AI. Xa lạ bởi vì một A I thực thụ vẫn còn nằm ngoài tầm với của chúng ta. Nói đến AI, hẳn mỗi người sẽ liên tưởng  đến một hình ảnh khác nhau. Các bạn có để ý rằng vài thập niên gần đây có một sự thay đổi về diện mạo của AI trong các bộ  phim quốc tế. Trước đây, các nhà sản xuất phim thường xuyên đưa hình ảnh robot vào phim (như  Terminator  ), nhằm gieo vào đầu người xem suy nghĩ rằng trí tuệ nhân tạo là một  phương thức nhân bản con người  bằng máy móc. Tuy nhiên, trong những bộ phim gần hơn về  đề tài này, ví dụ như  Transcendence  do Johny Depp vào vai chính, ta không thấy hình ảnh của một  con robot nào cả. Thay vào đó  là một bộ não điện toán khổng lồ chỉ huy hàng vạn  con Nanobot, được gọi là  Singularity . Tất nhiên cả  hai hình ảnh đều là hư cấu và giả tưởng, nhưng sự thay đổi như vậy cũng một phần nào phản ánh sự thay đổi ý niệm của con người về AI. AI  bây giờ được xem như vô hình vô dạng, hay nói cách khác có thể mang bất cứ hình dạng nào. Vì nói về AI là nói về một  bộ não , chứ không phải nói về một cơ thể. Trong giới hàn lâm, theo hiểu biết chung, AI là một ngành khoa học được sinh ra với mục đích làm cho máy tính có được trí thông minh. Mục tiêu này vẫn khá mơ hồ vì không phải ai cũng đồng ý với một  định nghĩa thống nhất về trí  thông minh. Thế nên các nhà khoa học phải định nghĩa một số mục tiêu cụ thể hơn, một trong số đó là việc làm cho máy tính lừa Machine learning là gì? 3 Footer Menu About About Scribd Press Our blog Join our team! Contact Us Join today Invite Friends Gifts Legal Terms Privacy Copyright Support Help / FAQ Accessibility Purchase help AdChoices Publishers Social Media Copyright © 2017 Scribd Inc.  . Browse Books . Mobile Site . Site Directory . Site Language:  English 中文 Español العربية Português 日本語 Deutsch Français Turkce Русский язык Tiếng việt Język polski Bahasa indonesia Sign up to vote on this title Useful Not useful Close Dialog Are you sure? This action might not be possible to undo. Are you sure you want to continue? CANCEL OK Close Dialog This title now requires a credit Use one of your book credits to continue reading from where you left off, or restart the preview. Restart preview Loading",
          "relevance": "1",
          "title": "Machine Learning Co Ban",
          "url": "https://www.scribd.com/document/357529046/Machine-Learning-Co-Ban"
        },
        {
          "content": "Blog Khoa Học Máy Tính Tầm nhìn ta thật ngắn mà đã thấy bao thứ để làm — Alan Turing Skip to content Trang chủ Gỡ rối tơ lòng Bản Quyền Nhóm bloggers Tuyển Tập Gõ ký hiệu Toán Đăng nhập Đăng ký «  BEAST: Một phương pháp tấn công HTTPS mới (1) BEAST: Một phương pháp tấn công HTTPS mới (2)  » Trong ngành Học Máy và Thống Kê nói chung có vấn đề đau đầu là  vấn đề overfitting . Nếu dữ liệu dùng để luyện mô hình có nhiễu lớn thì một mô hình “giải thích” dữ liệu hoàn hảo sẽ nhiều khả năng đoán tương lai sai toét. Tôi đã liên hệ vấn đề overfitting với thiên kiến khẳng định trong bài  điểm sách Thiên Nga Đen : chăm chăm đi tìm một “lý thuyết” giải thích lịch sử không phải là khó, tìm mô hình có khả năng dự đoán tương lai mới khó. Rất dễ tìm hàm “fit” giá chứng khoán (Google chẳng hạn) từ hồi nó mới ra thị trường, nhưng hàm đó có đoán được giá ngày mai không? Luyện gà chọi thi đại học cũng tiềm ẩn một vấn đề overfitting. Các sinh viên “fit” vào một mô hình thi cử cứng nhắc chưa chắc đã dễ giáo dục thành các kỹ sư, bác sĩ, luật sư giỏi.  Gần đây ,  Narayana Murthy của Infosys đã gióng hồi chuông cảnh báo ở hội nghị “Pan IIT” summit rằng các kỹ sư tốt nghiệp hệ thống IIT càng lúc càng tệ, do vấn đề luyện gà chọi để “fit” vào hệ thống thi cử. More emphasis has to be given to research at the undergraduate level and examinations should test independent thinking of students rather than their ability to solve problems.  Murthy said in order to produce good research at IITs, the Indian government has to be persuaded to create institutions that fund research projects.  In addition, faculty members should also be evaluated annually on their research performance by an independent committee, Murthy said adding that India must shift from the tenure system for its faculty to a five year contractual appointment system.  «  BEAST: Một phương pháp tấn công HTTPS mới (1) BEAST: Một phương pháp tấn công HTTPS mới (2)  » Nghi Posted 04/10/2011 at 12:46 pm  | Permalink Interesting analogy of overfitting problem! Not sure Narayana Murthy’s ideas are useful though. Why should examinations not test students’ ability to solve problems? Also, the five year contractual system may turn out very bad. If Princeton had adopted such a system, Andrew Wiles would never have produced his 7+ year proof of Fermat’s Last Theorem. Reply NQH Posted 04/10/2011 at 12:58 pm  | Permalink Hi Nghi, I guess his point is “problem solving” is necessary but not sufficient. This is a fair point. I have seen many Chinese students who did very well in the GRE exams but are not very good students in general. I myself took the TOEFL test some 17 years back, got 610 points, and didn’t really know English! Yeah, removing the tenure system is a different beast. Reply Nghi Posted 05/10/2011 at 1:02 pm  | Permalink I think the GRE, like its high school version SAT, is more useful in detecting bad students rather than good students. The exam’s too easy; it’s not a good example of overfitting 🙂 I’m thinking of the college entrance exams in VN and the IMO. Reply Nguyễn Xuân Long Posted 05/10/2011 at 12:09 pm  | Permalink Hi bác Hưng — nice analogy. We live in a time that see more and more specialists and less generalists. This seems true everywhere you look. Is this a good or bad thing? Reply Nam Posted 08/12/2011 at 12:42 am  | Permalink Liên tưởng này của anh Hưng rất thú vị. Trước đến giờ em vẫn nghĩ là phép đo khả năng học một ngành nào đó ở DH bằng kỳ thi vào DH hiện nay nhiều khi không hiệu quả. Em cũng biết về overfitting, nhưng sao ko nghĩ ra liên tưởng này nhỉ? 😀 Nhân chuyện này, em cũng có một liên tưởng. Chúng ta đều biết một trong những vấn đề phải quan tâm khi thực thi tiến trình là context switch. Khi chạy quá nhiều tiến trình một lúc, việc chuyển ngữ cảnh quá nhiều sẽ làm giảm hiệu năng hệ thống một cách rõ rệt. Em tưởng tượng phần lớn GV DH của VN bây giờ đều gặp phải vấn đề như thế: làm quá nhiều việc vặt (để kiếm cơm hoặc ko ra tiền nhưng bị bắt làm) nên dùng nhiều context switches, cuối cùng hiệu quả công việc bị giảm rõ rệt. 😀 Reply Comment You may use these  HTML  tags and attributes <a href=\"\" title=\"\"> <abbr title=\"\"> <acronym title=\"\"> <b> <blockquote cite=\"\"> <cite> <code> <del datetime=\"\"> <em> <i> <q cite=\"\"> <s> <strike> <strong>  Name * Email * Website Phản hồi mới maiyeu  on  Bèo dạt mây trôi T.  on  Mật mã hiện đại (2) nguyễn nguyên ngọc  on  Làm an toàn thông tin thì học gì? Nguyễn thành đạt  on  Bình chọn triết gia vĩ đại nhất Chân trời mới  on  Chuyện kể trên vnexpress và thống kê nghiêm chỉnh Phạm Tuân  on  Gỡ rối tơ lòng Duy  on  Pinkie Pie Trương Thanh Nguyên  on  Tản mạn về cơ hội trong ngành Thống kê (và KHMT) Thanh  on  Sách Bên Thắng Cuộc của Huy Đức bảo thành  on  Học Khoa Học Máy Tính nên đọc sách gì? Nguyễn Hồng Quang  on  Gỡ rối tơ lòng Quyet  on  Siêu đồ thị phi chu trình và đồ thị dây cung trần hưng  on  Thời con nít đã xa. Nguyễn Văn Uy  on  Làm an toàn thông tin thì học gì? doi co luu  on  Mini course ở VIASM Bài mới Đánh nhau bằng toán 19 năm một nốt mới Mini course ở VIASM ISI, tiêu chí thẩm định, và các động lực ngược Tôi đi biểu tình ở Mỹ Lập trình máy kỳ dị Từ đại số đến bitcoin Bài báo có một cái hình thú vị Một bài toán thú vị Trận cờ thế kỷ Chuyên Mục Âm Nhạc  (62)\n Ảnh hưởng của CNTT  (6)\n Ảo giác  (3)\n Bảo mật và mật mã học  (82)\n Biển Đông  (7)\n Blog cầu  (5)\n Bơi  (7)\n C++  (6)\n Các hệ thống máy tính  (9)\n Các hội nghị KHMT  (12)\n Công nghệ phần mềm  (6)\n Cấu trúc dữ liệu  (6)\n Chính sách  (1)\n Chính trị trong ngành  (31)\n Chưa phân loại  (56)\n CNTT các nước và VN  (47)\n Combinatorics  (26)\n Cơ sở dữ liệu  (10)\n Danh ngôn  (13)\n Dành cho du học sinh  (111)\n Games  (2)\n Giáo dục  (88)\n Giới thiệu sách  (38)\n KHMT và Kinh Tế  (1)\n KHMT và luật pháp  (6)\n KHMT và sinh học  (6)\n KHMT và triết học  (3)\n Lập trình  (22)\n Lịch Sử  (2)\n Lịch sử Việt Nam  (3)\n Lý thuyết mã hóa  (6)\n Lý thuyết tính toán  (57)\n Lý thuyết thông tin  (17)\n Logic  (2)\n Mạng máy tính  (36)\n Mỹ quốc  (12)\n Nghiên cứu nghiên kiếc  (50)\n Nhân vật và sự kiện  (133)\n Python  (3)\n Quả đất của ta  (1)\n Siêu Nhiên  (1)\n Thông báo  (31)\n Thần kinh học  (1)\n Thầy bói nói mò  (1)\n Thuật ngữ  chuyên ngành  (8)\n Thuật Toán  (70)\n Thơ  (7)\n Tin tức đó đây  (106)\n Toán Ứng Dụng  (13)\n Toán tối ưu  (5)\n Trang web hay  (31)\n Trí tuệ nhân tạo  (52)\n Vui – Giải Trí  (279)\n Vượt định kiến  (26)\n Xác suất & thống kê  (71)\n Xuất bản  (14)\n Y Học  (2)\n Kho bài Kho bài Select Month  December 2015   December 2014   October 2014   September 2014   May 2014   February 2014   December 2013   October 2013   September 2013   August 2013   June 2013   April 2013   March 2013   February 2013   January 2013   December 2012   November 2012   October 2012   September 2012   August 2012   July 2012   June 2012   May 2012   April 2012   March 2012   February 2012   January 2012   December 2011   November 2011   October 2011   September 2011   August 2011   July 2011   June 2011   May 2011   April 2011   March 2011   February 2011   January 2011   December 2010   November 2010   October 2010   September 2010   August 2010   July 2010   June 2010   May 2010   April 2010   March 2010   February 2010   January 2010   December 2009   November 2009   October 2009   September 2009   August 2009   July 2009   June 2009   May 2009   April 2009   March 2009   February 2009   January 2009   December 2008   November 2008   October 2008   September 2008   August 2008   July 2008   June 2008   May 2008   April 2008   March 2008   February 2008   January 2008   December 2007   November 2007   October 2007   September 2007   August 2007   July 2007   June 2007   May 2007   April 2007   March 2007   February 2007   January 2007   December 2006   November 2006   October 2006   September 2006   August 2006   July 2006   June 2006   May 2006   April 2006   March 2006   February 2006   January 2006   December 2005   November 2005   October 2005   September 2005   August 2005   July 2005   June 2005   May 2005   April 2005   March 2005   February 2005  Báo chí Harvard Magazine MIT Technology Review Tạp Chí Tia Sáng The Chronicle of Higher Education Tuổi Trẻ Bảo mật Blog Bảo Mật Thông Tin Brian Krebs Schneier on Security Security | News.blog | CNET News.com Spam Kings Blog worm blog Blog Việt Bùi Nguyên Cẩm Ly Giáp Văn Dương Hoàng Hoài Minh Huy Đức Lê Hồng Giang Minh Biện Ngô Bảo Châu Nguyễn Ngọc Tư Nguyễn Tiến Dũng Nguyễn Đình Đăng Nhiệt Huyết Phạm Thị Hoài Quỹ Trí Tuệ Việt Nam Talawas Tin Khó Tin Toe Loe Trang Hạ Trần Hữu Dũng Trần Vinh Dự Vũ Hà Văn Vũ Hoàng Linh Đàm Thanh Sơn Đông A Đỗ Quốc Anh Đoàn Kết Chưa phân loại smashingtelly Giáo dục ACM: Education Diễn đàn giáo dục Internet Archive Quỹ Trí Tuệ Việt Nam Kỹ thuật ACM Tech Policy Beautiful Code Boing Boing Cộng đồng mã nguồn mở VN Developing for Developers Diễn đàn tin học Freedom to Tinker GeekPress Gustavo Duarte Luis von Blog Marc Andreessen Matt Might News.com’s Blog Official Google Blog Reinventing the Internet Volatile and Decentralized Khoa học khác Cold War Files fragments of consciousness Musings Overcoming Bias RealClimate ScienceBlogs Susan Polgar Chess Blog Talk.Origins Tangled Bank The Panda’s Thumb The Scientific Activist This Week Finds Kinh tế, luật pháp, xã hội A Tiny Revolution Andrew Sullivan.com Chicago’s Law Faculty Computing Chris Creative Capitalism Crooked Timber Daily Kos Freakonomics Free exchange Furdlog Instapundit Marginal Revolution Social Science Statistics Structured Procrastination The Becker-Posner Blog The Volokh Conspiracy Vietnam Quant. Society Đỗ Quốc Anh Lý thuyết & thuật toán Algorithmic Game Theory Combinatorics and More Comp. Complexity Blog CS Theory Overflow ECCC Ernie’s 3D Pancakes Glob of Thoughts Godel's Lost Letter In Theory Luca Trevisan Machine Learning (Theory) My Biased Coin My slice of pizza Rudy’s Blog Sariel’s Blog Shtetl-Optimized tcs math The Geomblog The Quantum Pontiff Theory Matters Mạng Renesys Não học Mind Hack Neuro Philosophy Toán học Ars Mathematica Diễn đàn toán học Good Math, Bad Math Mathematics and Computation Mathematics Weblog Notices of the AMS Richard Borcherds Terry Tao The n-Category Cafe Tim Gowers Tim Gowers VNMATH Vật Lý Cosmic Variance John Baez’s Weekly Finds Not Even Wrong PhysOrg.com ScienceBlogs The n-Category Cafe Three-Toed Sloth Usenet Physics FAQ Vật Lý Việt Nam Đàm Thanh Sơn         \n\n   \t\t\tBản quyền của  Blog Khoa Học Máy Tính , © 2005- 2017 . Thiết kế với  WordPress . Theme con của theme  Thematic Theme Framework .\n\n\t",
          "relevance": "0",
          "title": "Luyện gà chọi và vấn đề overfitting",
          "url": "http://www.procul.org/blog/2011/10/03/luy%E1%BB%87n-ga-ch%E1%BB%8Di-va-v%E1%BA%A5n-d%E1%BB%81-overfitting/"
        },
        {
          "content": "Overfitting From Wikipedia, the free encyclopedia \n\t\t\t\t\tJump to:\t\t\t\t\t navigation , \t\t\t\t\t search This article  needs additional citations for  verification .  Please help  improve this article  by  adding citations to reliable sources . Unsourced material may be challenged and removed. (August 2017) ( Learn how and when to remove this template message ) \nThe green line represents an overfitted model and the black line represents a regularised model. While the green line best follows the training data, it is too dependent on it and it is likely to have a higher error rate on new unseen data, compared to the black line. \nNoisy (roughly linear) data is fitted to both linear and  polynomial  functions. Although the polynomial function is a perfect fit, the linear version can be expected to generalize better. In other words, if the two functions were used to extrapolate the data beyond the fit data, the linear function would make better predictions. In  statistics  and  machine learning , one of the most common tasks is to fit a \"model\" to a set of  training data , with the goal of making reliable predictions on unseen test data. In  overfitting , a  statistical model  describes  random error  or noise instead of the underlying relationship [ citation needed ] . Overfitting occurs when a model is excessively complex, such as having too many  parameters  relative to the number of observations. A model that has been overfitted has poor  predictive  performance, as it overreacts to minor fluctuations in the training data [ citation needed ] . Underfitting  occurs when a statistical model or machine learning algorithm cannot capture the underlying trend of the data. Underfitting would occur, for example, when fitting a linear model to non-linear data. Such a model would have poor predictive performance. The possibility of overfitting exists because the criterion used for training the model is not the same as the criterion used to judge the efficacy of a model. In particular, a model is typically trained by maximizing its performance on some set of training data. However, its efficacy is determined not by its performance on the training data but by its ability to perform well on unseen data. Overfitting occurs when a model begins to \"memorize\" training data rather than \"learning\" to generalize from a trend. As an extreme example, if the number of parameters is the same as or greater than the number of observations, a simple model or learning process can perfectly predict the training data simply by memorizing the training data in its entirety, but such a model will typically fail drastically when making predictions about new or unseen data, since the simple model has not learned to generalize at all [ citation needed ] . The potential for overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape, and the magnitude of  model error  compared to the expected level of noise or error in the data [ further explanation needed ] [ citation needed ] . Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new data set than on the data set used for fitting. [1]  In particular, the value of the  coefficient of determination  will  shrink  relative to the original training data. In order to avoid overfitting, it is necessary to use additional techniques (e.g.  cross-validation ,  regularization ,  early stopping ,  pruning ,  Bayesian priors  on parameters,  model comparison  or  dropout ), that can indicate when further training is not resulting in better generalization. The basis of some techniques is either (1) to explicitly penalize overly complex models, or (2) to test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter. Contents 1 Machine learning 1.1 Consequences 2 Regression 3 Underfitting 4 See also 5 References 6 Notes 7 External links Machine learning [ edit ] \nOverfitting/overtraining in supervised learning (e.g.,  neural network ). Training error is shown in blue, validation error in red, both as a function of the number of training cycles. If the validation error increases(positive slope) while the training error steadily decreases(negative slope) then a situation of overfitting may have occurred. The best predictive and fitted model would be where the validation error has its global minimum. Usually a learning  algorithm  is trained using some set of \"training data\": exemplary situations for which the desired output is known. The goal is that the algorithm will also perform well on predicting the output when fed \"validation data\" that was not encountered during its training. Overfitting is the use of models or procedures that violate  Occam's razor , for example by including more adjustable parameters than are ultimately optimal, or by using a more complicated approach than is ultimately optimal. For an example where there are too many adjustable parameters, consider a dataset where training data for  y  can be adequately predicted by a linear function of two dependent variables. Such a function requires only three parameters (the intercept and two slopes). Replacing this simple function with a new, more complex quadratic function, or with a new, more complex linear function on more than two dependent variables, carries a risk: Occam's razor implies that any given complex function is  a priori  less probable than any given simple function. If the new, more complicated function is selected instead of the simple function, and if there was not a large enough gain in training-data fit to offset the complexity increase, then the new complex function \"overfits\" the data, and the complex overfitted function will likely perform worse than the simpler function on validation data outside the training dataset, even though the complex function performed as well, or perhaps even better, on the training dataset. [2] When comparing different types of models, complexity cannot be measured solely by counting how many parameters exist in each model; the expressivity of each parameter must be considered as well. For example, it is nontrivial to directly compare the complexity of a neural net (which can track curvilinear relationships) with  m  parameters to a regression model with  n  parameters. [2] Overfitting is especially likely in cases where learning was performed too long or where training examples are rare, causing the learner to adjust to very specific random features of the training data, that have no  causal relation  to the  target function . In this process of overfitting, the performance on the training examples still increases while the performance on unseen data becomes worse. As a simple example, consider a database of retail purchases that includes the item bought, the purchaser, and the date and time of purchase. It's easy to construct a model that will fit the training set perfectly by using the date and time of purchase to predict the other attributes; but this model will not generalize at all to new data, because those past times will never occur again. Generally, a learning algorithm is said to overfit relative to a simpler one if it is more accurate in fitting known data (hindsight) but less accurate in predicting new data (foresight). One can intuitively understand overfitting from the fact that information from all past experience can be divided into two groups: information that is relevant for the future and irrelevant information (\"noise\"). Everything else being equal, the more difficult a criterion is to predict (i.e., the higher its uncertainty), the more noise exists in past information that needs to be ignored. The problem is determining which part to ignore. A learning algorithm that can reduce the chance of fitting noise is called  robust . Consequences [ edit ] The most obvious consequence of overfitting is poor performance on the validation dataset. Other negative consequences include: [2] A function that is overfitted is likely to request more information about each item in the validation dataset than does the optimal function; gathering this additional unneeded data can be expensive or error-prone, especially if each individual piece of information must be gathered by human observation and manual data-entry. A more complex, overfitted function is likely to be less portable than a simple one. At one extreme, a one-variable linear regression is so portable that, if necessary, it could even be done by hand. At the other extreme are models that can be reproduced only by exactly duplicating the original modeler's entire setup, making reuse or scientific reproduction difficult. Regression [ edit ] Outside machine learning, overfitting is also a problem in the broad study of regression, including regression done \"by hand\". In the extreme case, if there are p variables in a  linear regression  with p data points, the fitted line will go exactly through every point. [3]  There are a variety of rules of thumb for the number of observations needed per independent variable, including 10  [4]  and 10-15. [5]  In the process of regression model selection, the mean squared error of the random regression function can be split into random noise, approximation bias, and variance in the estimate of regression function, and bias-variance tradeoff is often used to overcome overfit models. Underfitting [ edit ] Underfitting occurs when a statistical model or machine learning algorithm cannot capture the underlying trend of the data. It occurs when the model or algorithm does not fit the data enough. Underfitting occurs if the model or algorithm shows low variance but high bias (to contrast the opposite, overfitting from high variance and low bias). It is often a result of an excessively simple model. [6] See also [ edit ] Bias–variance tradeoff Curve fitting Data dredging Occam's razor Model selection VC dimension  - measures the complexity of a learning model. Larger VC dimension means larger risk of overfitting. References [ edit ] Leinweber, D. J. (2007). \"Stupid Data Miner Tricks\".  The Journal of Investing .  16 : 15–22.  doi : 10.3905/joi.2007.681820 .   Tetko, I. V.; Livingstone, D. J.; Luik, A. I. (1995).  \"Neural network studies. 1. Comparison of Overfitting and Overtraining\" (PDF) .  J. Chem. Inf. Comput. Sci. 35  (5): 826–833.  doi : 10.1021/ci00027a006 .   Notes [ edit ] ^ Everitt B.S. (2002) Cambridge Dictionary of Statistics, CUP.  ISBN   0-521-81099-X  (entry for \"Shrinkage\") ^  a b c Hawkins, Douglas M. \"The problem of overfitting.\" Journal of chemical information and computer sciences 44.1 (2004): 1-12. ^ Martha K. Smith (2014-06-13).  \"Overfitting\" .  University of Texas at Austin . Retrieved  2016-07-31 .   ^ Draper, Norman R.; Smith, Harry (1998).  Applied regression analysis, 3rd Edition . New York:  Wiley .  ISBN   978-0471170822 .   ^ Jim Frost (2015-09-03).  \"The Danger of Overfitting Regression Models\" . Retrieved  2016-07-31 .   ^ Cai, Eric (2014-03-20).  \"Machine Learning Lesson of the Day – Overfitting and Underfitting\" .  StatBlogs .   External links [ edit ] Overfitting: when accuracy measure goes wrong  - an introductory video tutorial. The Problem of Overfitting Data CSE546: Linear Regression Bias / Variance Tradeoff \n\t\t\t\t\t\tRetrieved from \" https://en.wikipedia.org/w/index.php?title=Overfitting&oldid=800138887 \"\t\t\t\t\t Categories :  Statistical inference Regression analysis Machine learning Hidden categories:  Articles needing additional references from August 2017 All articles needing additional references All articles with unsourced statements Articles with unsourced statements from August 2017 Articles with unsourced statements from September 2017 Wikipedia articles needing clarification from September 2017 Navigation menu Personal tools Not logged in Talk Contributions Create account Log in Namespaces Article Talk Variants Views Read Edit View history More Search Navigation Main page Contents Featured content Current events Random article Donate to Wikipedia Wikipedia store Interaction Help About Wikipedia Community portal Recent changes Contact page Tools What links here Related changes Upload file Special pages Permanent link Page information Wikidata item Cite this page Print/export Create a book Download as PDF Printable version Languages Deutsch Español فارسی Français Italiano 日本語 Polski Português Русский Basa Sunda Українська 中文 Edit links  This page was last edited on 11 September 2017, at 18:38. Text is available under the  Creative Commons Attribution-ShareAlike License ;\nadditional terms may apply.  By using this site, you agree to the  Terms of Use  and  Privacy Policy . Wikipedia® is a registered trademark of the  Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Developers Cookie statement Mobile view",
          "relevance": "1",
          "title": "Overfitting",
          "url": "https://en.wikipedia.org/wiki/Overfitting"
        },
        {
          "content": "Share ? Profiles  ▼ Communities  ▼ Apps  ▼ My Blogs Public Blogs My Updates About this blog  Musing about Analytics, Optimization, Data Science, and Machine Learning  Leverages Python and Mathematical Optimization.  \r\n\r\nI am now publishing my code (esp notebooks) on git hub at: https://github.com/jfpuget/  \r\n\r\nMy Views are my own.\r\n Facebook Twitter Google LinkedIn RSS Links \n            \tMy github repository\n             \n            \t@JFPuget on twitter\n             \n            \tOptimization Community on Deve...\n             \n            \tFree CPLEX Trials\n             \n            \tFree Cloud trial\n             \n            \tFree Software For Academics An...\n             \n            \tSupport Forum on developerWork...\n             \n            \tCPLEX and OPL products\n             \n            \tIBM Decision Optimization Cent...\n             \n            \tLinkedIn profile for jfpuget\n             \n            \tMichael's Trick Operations Res...\n             \n            \tOR in an OB World\n             \n            \tOpen Courses on Operations Res...\n             \n            \tPunk Rock OR\n             Model 1 predictions Model 1 predictions Notify Other People Notify Other People + Notify: Quarantine this entry Provide a reason for quarantining this blog entry (optional): duplicateEntry Mark as Duplicate Find the duplicate idea: Previous Entry Main Next Entry",
          "relevance": "1",
          "title": "The Machine Learning Workflow",
          "url": "https://www.ibm.com/developerworks/community/blogs/jfp/entry/Overfitting_In_Machine_Learning?lang=en"
        },
        {
          "content": "Posts Questions Discussions Announcements  No announcement yet. All Announcements  Sign In/Sign up\n     Nguyen Duc Vinh  Follow Asked Aug 23rd, 3:13 pm Asked Aug 23rd, 3:13 pm •  95  0  1  95  0  1  Report\n         +3 Cách chọn Neural network hyperparameters Machine Learning \n                        clip\n                         share Share a link to question \n                Close\n              95  0  1  Report\n         Mọi người có thể cho mình một số lời khuyên hay phương pháp cụ thể trong việc lựa chọn hyperparameters tốt nhất khi training DNN không. Mình có đọc qua một số tài liệu thì việc này phụ thuộc vào khá nhiều yếu tố cũng như bài toán cụ thể. Tuy nhiên tổng quát nhất thì mình nên bắt đầu từ đâu, các bước cơ bản cũng như những lưu ý khi thực hiện? Mình cảm ơn! Post answer Nguyen Duc Vinh @vinhnguyenict Follow  20\n          2\n          1\n          0\n         \n                        Clip this question\n                     Add a comment 1 ANSWERS Nguyen Ngoc Vinh  Follow Answered Aug 23rd, 4:27 pm Answered Aug 23rd, 4:27 pm Accepted +5 Mình thấy việc lựa chọn hyperparameters cho DNN là công việc khá  nhạt nhẽo  và đòi hỏi khá nhiều công sức. Lý do cũng như bạn nói và hiện tại cũng không có một phương pháp cụ thể nào có thể áp dụng cho tất cả các bài toán (theo mình nghĩ vậy)  😟 Thông thường NN sẽ có khá khá hyperparameters cũng như việc xây dựng NN là khá mềm dẻo (nhiều loại network topology -  cách mà các artificial neurons liên kết với nhau). Ví dụ như khi sử dụng MLP (Multi-Layer Perceptron) có khá nhiều hyperparameter mình cần quan tâm như: số lượng  hidden layers ,  weight initialization technique , số lượng  neurons cho một layer , loại  activation function  (logistic, ReLU, tanh),... Sử dụng Grid Search và Cross-validation có lẽ là phương pháp quen thuộc nhất khi đi tìm hyperparameter set, và nó cũng hoạt động khá hiệu quả trong nhiều bài toán. Tuy nhiên với những bài toán có lượng dữ liệu lớn hoặc có quá nhiều hyperparameters như NN chẳng hạn, phương pháp này khá chậm. Để tìm được hyperparameter set tốt nhất trong hyperparameter space với một khoảng thời gian chấp nhận được không phải là việc đơn giản. Bạn có thể sử dụng Randomized Search hay công cụ  http://oscar.calldesk.ai  (mình chưa thử nó bao giờ, google thấy cái tool này hay hay  😅 ). Số lượng Hidden Layers Trên thực tế NN với 1 hidden layer (shallow net) có thể được sử dụng để mô tả những hàm khá phức tạp nếu có đủ số lượng neurons  😁  Nếu bài toán không quá phức tạp thì mình có thể thử nghiệm với NN với 1 hidden layer trước xem nó ra kết quả tốt không. Sau đó có thể add thêm hidden layer nếu cần thiết. Vậy thì mình cần gì đến DNN  🤔  Trên thực tế DNN có thể cho ra kết quả tốt với số lượng neuron cho mỗi layer giảm đi đáng kể so với   shallow net , từ đó giúp cho việc training nhanh hơn đáng kể. Thử với 1 hoặc 2 hidden layer và kiểm tra kết quả xem đã như mình mong muốn !? Tăng số lượng hidden layer lên cho đến khi overfit tập training. Số lượng Neurons cho một Hidden Layer Đối với input và output layer thì số lượng neurons sẽ phụ thuộc vào loại input và output cần thiết của bài toán. Ví dụ bài toàn cổ điển phân loại digit (MNIST dataset), số lượng input neuron có thể là 784 (28*28) và 10 neuron cho output (do phân loại 10 chữ số từ 0 đến 9). Do vậy mình chỉ cần quan tấm đến số lượng neuron cho hidden layer. Tương tự như số lượng hidden layer, mình có thể tăng số lượng neurons cho các hidden layer cho đến khi overfit tập training. Ngược lại với cách đầu tiên, mình có thể bắt đầu với một DNN với số lượng layer và lượng neurons nhiều hơn cần thiết, sau đó sử dụng một số kĩ thuật như  early stopping ,  regularization (l1, l2, max-norm)  hay  dropout  để tránh model bị overfitting  😵 Activation Function Trong nhiều trường hợp  ReLU  (và các biến thể của nó) có thể là lựa chọn khá tốt cho các hidden layer vì việc tính toán là khá nhanh. Đối với output layer,  softmax  có lẽ là lựa chọn tốt cho các bài toán classification, còn với các bài toán regression thì không cần activation function cho output layer. Mình có tham khảo thêm một số nguồn thì thấy có một configuration sau (performance khá tốt trong nhiều trường hợp) Initialization:  He initialization  (tránh các vấn đề liên quan đến  vanishing/exploding graidents  khi bắt đầu training) Activation function:  ELU  (có performance tốt hơn ReLU trong khá nhiều trường hợp),  Sigmoid  hoặc  Tanh  cho các bài toán classification (các giá trị có thể được đẩy về [0, 1]). Regularization:  Dropout Normalization:  Batch Normalization  (tránh các vấn đề liên quan đến  vanishing/exploding graidents  trong quá trình training) Optimizer:  Nesterov Accelerated Gradient  !? Learning rate schedule: không 😴 😴 😴 Đây là một số suy nghĩ của mình, chắc vẫn còn thiếu nhiều nhưng mong giúp giải quyết câu hỏi này một phần nào đó.  🤝 share Share a link to this answer \n                Close\n             Nguyen Ngoc Vinh @vinhnguyen Follow  2083\n          108\n          1\n          8\n         Nguyen Duc Vinh  @vinhnguyenict  • Aug 24th, 10:33 am Cảm ơn bạn vì câu trả lời khá chi tiết này nhé. Còn nhiều chỗ mình chưa hiểu nhưng có keywords để tìm hiểu rồi. Thanks 🙃🙃🙃 \n        0\n     | share Share a link to this comment \n                Close\n             Nguyen Duc Vinh  @vinhnguyenict  • Aug 28th, 10:14 am @vinhnguyen  Bạn có thể nói rõ hơn cho mình về cách chọn activation function được không? Mình cảm ơn. \n        0\n     | share Share a link to this comment \n                Close\n             Nguyen Ngoc Vinh  @vinhnguyen  • Sep 7th, 1:23 am Mình thấy một vấn đề khá quen thuộc khi training một NN đó là  Vanishing Gradients  - gradient error sẽ nhỏ dần trong quá trình  backpropagation .  Đến một thời điểm nào đó connection weights của các layer thấp hơn sẽ không thay đổi (khi sử dụng Gradient Descent chẳng hạn), quá trình trainning sẽ không tìm được một kết quả tốt. Ngược lại nếu gradient lớn dần trong quá trình  backpropagation  thì sẽ dẫn đến hiện tượng  Exploding Gradients  - cũng không tốt một chút nào. Có hai nguyên nhân chính dẫn đến hiện tượng trên: Sử dụng  saturating activation function  (ví dụ như signmoid function) Kĩ thuật khởi tạo các connection weights (weight initialization) Ví dụ trong trường hợp sigmoid function, khi input lớn thì function sẽ trở nên bão hòa (saturating) tại 0 và 1, đạo hàm rất gần với 0, gradients trong quá trình  backpropagation  sẽ nhỏ dần khi đến các layer thấp hơn. ReLU và các biến thể của nó thường được sử dụng thay cho sigmoid function, do nó không có hiện tượng bão hòa và khá nhanh khi tính toán. R e L U ( z ) = m a x ( 0 , z ) ReLU(z) = max(0, z) \n R e L U ( z ) = m a x ( 0 , z ) Tuy nhiên khi sử dụng ReLU chúng ta gặp phải một hiện tượng đó là  Dying ReLUs , khi một số neuron sẽ chỉ cho ra giá trị là 0 trong quá trình trainning (khi connection weights của neuron được update và weighted sum của các inputs nhỏ hơn 0 -> output = 0 -> gradient = 0). Để giải quyết vấn đề này thì chúng ta sẽ dùng một số biến thể của ReLU. Leaky ReLU Ref:  https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Leaky_ReLUs \nRef:  https://arxiv.org/pdf/1505.00853.pdf L e a k y R e L U ( z ) = m a x ( α z , z ) LeakyReLU(z) = max(\\alpha z, z) \n L e a k y R e L U ( z ) = m a x ( α z , z ) Ở đây alpha có tác dụng điều chỉnh độ dốc của hàm khi z < 0. Thay vì bằng 0 như trong ReLU output sẽ có giá trị thay đổi, tránh được hiện tượng dying ReLUs. RReLU - Randomized Leaky ReLU Tương tự như Leaky ReLU nhưng alpha sẽ được chọn ngẫu nhiên trong một khoảng nào đó khi training và được giữ nguyên trong quá trình testing -> tránh được hiện tượng overfitting. PReLU - Parametric Leaky ReLU Ở biến thể này thì alpha sẽ được \"learn\" trong quá trình trainning thay vì là một hyperparameter. Nó hoạt động khá tốt khi dataset lớn nhưng có thể dẫn đến overfitting nếu dataset quá nhỏ. ELU - Exponential Linear Unit Ref:  https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#ELUs E L U ( z ) = { z if  z > = 0 α [ exp ( z ) − 1 ] otherwise   ELU(z) = \\begin{cases}\n   z &\\text{if } z >= 0  \\\\\n   \\alpha [\\exp(z) - 1] &\\text{otherwise }\n\\end{cases}  E L U ( z ) = { z α [ exp ( z ) − 1 ] ​ if  z > = 0 otherwise  ​ def elu (z, alpha= 1 ) :\n     return  np.where(z< 0 , alpha*(np.exp(z)- 1 ), z)\n ELU có performance khá tốt so với các biến thể trước đó của ReLU, với một số cải tiến như sau: Cho ra giá trị âm khi z < 0, giá trị trung bình của đầu ra sẽ gần với 0 -> tránh được hiện tượng vanishing gradients Gradient sẽ khác 0 khi z < 0 -> tránh được hiện tượng dying ReLUs ELU là một hàm  khá  trơn -> việc sử dụng Gradient Descent sẽ hiệu quả hơn SELU (Scaled Exponential Linear Units) Ref:  https://arxiv.org/pdf/1706.02515.pdf \nRef: SELU + Dropout:  https://github.com/bioinf-jku/SNNs/blob/master/selu.py S E L U ( z ) = λ { z if  z > 0 α [ exp ( z ) − α ] if  z < = 0  SELU(z) = \\lambda \\begin{cases}\n   z &\\text{if } z > 0  \\\\\n   \\alpha [\\exp(z) - \\alpha] &\\text{if } z <= 0\n\\end{cases}  S E L U ( z ) = λ { z α [ exp ( z ) − α ] ​ if  z > 0 if  z < = 0 ​ def elu(z,  alpha = 1 ):\n     return  np.where(z <  0 ,  alpha  * (np. exp (z) -  1 ), z)\n\n\ndef selu(z,\n          scale = 1.0507009873554804934193349852946 ,\n          alpha = 1.6732632423543772848170429916717 ):\n     return scale  * elu(z,  alpha )\n Một loại activation function khá mới (2017) và có performance rất tốt so với các loại activation function khác. Kết luận Thông thường:  ELU > Leaky ReLU (và các biến thể của nó) > ReLU > tanh > logistic (signmoid) \nNếu quan tâm đến thời gian training:  Leaky ReLUs > ELUs \nThông thường giá trị của  alpha  sẽ là  0.01 cho leaky ReLU và 1 cho ELU \nSử dụng RReLU nếu model overfitting tập training, PReLU nếu tập training lớn 🙂 😃 😄 \n        +3\n     | share Share a link to this comment \n                Close\n             Add a comment Facebook Viblo Hot authors Resources Posts Questions Videos Tags Authors Help Terms  RSS Feed Browser extension Atom plugin Feedback FAQ \n                    © 2017  Viblo . All rights reserved.\n                 \n        Viblo - Free service for technical knowledge sharing\n     Feedback \n                        Your feedback will be shared with our product delivery teams, and taken into consideration\n                        for future development.\n                     Name Email Send \n                Sign In\n             \n                Sign Up\n             Join us to find useful information required to improve your skills \n            Forgot your password?\n         Sign In Free  service for technical knowledge  sharing Join us to find useful information required to improve your skills \n                    I agree to  Viblo Terms of Service Sign Up or",
          "relevance": "1",
          "title": "Cách chọn Neural network hyperparameters",
          "url": "https://viblo.asia/q/cach-chon-neural-network-hyperparameters-vEla30dxKkw"
        },
        {
          "content": "Diary Working – IT experiences Sharing diary working Information Technology Menu Home About Contact Facebook Google+ GitHub WordPress.com Intruduce about machine learning\n Sometimes we want to classify some data into such a set of catagory or many other difficult tasks we always can use machine learning to do that. Machine learning can discover not only the mapping from input to output but also the representation itself. This approach is also know as representation learning. When we design features and learning algorithm learning about goal is to separate all factors that explains about our data and we can using it to separating different class of data (in deep learning this problem known to do in hidden layer) By the way, Some of my experiences about features and practical experiences for machine learning algorithms is that: if you are must build or create a data set for machine learning task such : classification, clustering, or also regression, one problem you must keep in mind that if you want to your ML learn and perform well, firstly human can do this clearly, such as when you want to create a data set classification if want ML can learn and do that, you must label you data whenever you can label data clearly in that your model can.  We can’t expect our model can do well when myself can’t do. And the second stick note is that, when ever you know or find out some features or some trick that easily can use to classify or cluster your data, please use it and give it for machine learning,  it can dramatically improve our model. Giải thuật học (learning algorithm) – LA\n Một LA trong học máy là một LA có thể học từ dữ liệu. Nhiệm vụ T (The task T) : học máy cho phép chúng ta có thể giải quyết các những nhiệm vụ khó khăn mà không thể giải quyết được bởi những chương trình cố định. Nhiệm vụ T là các vấn đề mà chúng ta cần giải quyết Đo hiệu năng hoạt động P (The performance Measure P) Trải nghiệm E (The experience E ) Capacity, Overfitting, and Underfitting\n Thách thức lớn nhất trong lĩnh vực học máy đó là những mô hình học được cần hoạt động tốt trên bộ dữ liệu mới, không có trong dữ liệu huấn luyện. Khả năng hoạt động tốt trên dữ liệu mới gọi là   generalization Đặc biệt, khi training, chúng ta sử dụng dữ liệu traning cho mô hình học máy và tính lỗi trên dữ liệu traning, lỗi này gọi là training error. Trong quá trình training chúng ta sẽ đi tối ưu để giảm training error.  Chúng ta mong muốn có một mô hình học mà   generalization error    hay còn gọi là  test error  đạt giá trị nhỏ nhất có thể. Hai yếu tố phản ánh một mô hình học máy học tốt hay không đó là:\n training error nhỏ khác biệt giữa training error và test error nhỏ. Hai vấn đề trên liên quan đến những vẫn đề chính trong machine learning là: underfitting và overfitting Underfitting là mô hình không thể đạt được giá trị training error nhỏ trong tập training Overfiting là vấn đề khi sự khác biệt giữa training error và test error quá lớn Chúng ta có thể điều chỉnh việc underfitting hay overfitting bằng cách thay đổi capacity của mô hình học. Hiểu một cách cơ bản thì capacity của một mô hình là khả năng của mô hình mà nó có thể khớp với lượng đa dạng các functions. Một cách hiệu quả để điều khiển capacity của mô hình là chọn trước hypothesis space. Hypothesis space là một tập các functions mà mô hình học được phép lựa chọn để trở thành lời giải. Share this: Twitter Facebook Google Like this: Like Loading... Enter your comment here... Fill in your details below or click an icon to log in: Email  (required) (Address never made public) Name  (required) Website  You are commenting using your WordPress.com account.  (  Log Out  /  Change  )  You are commenting using your Twitter account.  (  Log Out  /  Change  )  You are commenting using your Facebook account.  (  Log Out  /  Change  )  You are commenting using your Google+ account.  (  Log Out  /  Change  ) Cancel Connecting to %s Notify me of new comments via email. Post navigation Previous Previous post: Diary About Sentiment Problem in Vietnamese Next Next post: Sự khác biệt giữa statistic learning và machine learnig. Khác biệt giữa SVM và Linear Logistic Recent Posts Sự khác biệt giữa statistic learning và machine learnig. Khác biệt giữa SVM và Linear Logistic July 23, 2017 Machine learning( ML) fundamental and Deep learning ( DL) June 20, 2017 Diary About Sentiment Problem in Vietnamese April 11, 2017 Recall Neural network March 29, 2017 Home About Contact Facebook Google+ GitHub WordPress.com Diary Working – IT experiences Create a free website or blog at WordPress.com. %d  bloggers like this:",
          "relevance": "1",
          "title": "Machine learning( ML) fundamental and Deep learning ( DL)",
          "url": "https://itducvu.wordpress.com/2017/06/20/co-ban-ve-hoc-may/"
        },
        {
          "content": "Toggle navigation Onelib domains login  [['share your know'|translate]]  New author  Setting  logout  Onelib   About   Onelib Button   Explore   documents   domains   authors  See us on  G+   Facebook   Twitter ",
          "relevance": "0",
          "title": "Sử dụng trí tuệ nhân tạo của Google, một nông trang dưa chuột tại Nhật Bản tự động hóa thành công",
          "url": "http://www.onelib.org/article/su-dung-tri-tue-nhan-tao-cua-google-mot-nong-trang-dua-chuot-tai-nhat-ban-tu-dong-hoa-thanh-cong"
        },
        {
          "content": "Trang chủ Code Sách hay Xin chào !!! Phân loại Giải tích lồi  (14)\n Lý thuyết học máy  (31)\n Công cụ xác suất  (3)\n Học thống kê  (5)\n Mô hình đồ thị  (1)\n Quy hoạch lồi  (13)\n Quy hoạch nón  (3)\n Quy hoạch phi tuyến  (10)\n Quy hoạch tuyến tính  (18)\n Thuật toán  (6)\n Chia để trị  (3)\n Uncategorized  (2)\n Lưu trữ Tháng Mười Hai 2010  (3) Tháng Mười 2010  (2) Tháng Bảy 2009  (6) Tháng Sáu 2009  (6) Tháng Một 2009  (6) Tháng Tám 2008  (6) Tháng Bảy 2008  (8) Tháng Tư 2008  (7) Tháng Ba 2008  (4) Tháng Hai 2008  (30) Thống kê 170,997 lần xem Bài gần đây  Boost.MPI (3) – Lập trình thuật toán song song của Nocedal & cộng sự Boost.MPI (2) – Bài toán phân lớp tuyến tính Boost.MPI (1) Mô hình học gần đúng (PAC) (2) – Lớp khái niệm vô hạn Mô hình học gần đúng (PAC) (1) – Lớp khái niệm hữu hạn Mô hình đồ thị (Graphical models) (1) – Giới thiệu Chiều VC (VC dimension) (4) – Một số lớp hàm đơn giản Nguyên tắc tối thiểu hóa rủi ro thực nghiệm (4) – Ước lượng kì vọng Rademacher, chiều VC Chiều VC (VC dimension) (3) – Hàm tăng trưởng (growth function) Chiều VC (VC dimension) (2) – Phá vỡ một tập hợp (shatter) Chiều VC (VC dimension) (1) – Khái niệm, lớp khái niệm Nguyên tắc tối thiểu hóa rủi ro thực nghiệm (3) – Kì vọng Rademacher Nguyên tắc tối thiểu hóa rủi ro thực nghiệm (2) – Tập mẫu học giản lược Nguyên tắc tối thiểu hóa rủi ro thực nghiệm (1) – Rủi ro thực nghiệm và rủi ro kì vọng Độ tập trung quanh kì vọng (concentration inequalities) (2) – McDiarmid inequality Lời bình gần đây tqlong  on  Phương pháp đơn hình (Simplex… nga on  Phương pháp đơn hình (Simplex… Học on  Máy phân lớp sử dụng véctơ hỗ… Học on  Máy phân lớp sử dụng vectơ hỗ… tqlong  on  Máy phân lớp sử dụng vectơ hỗ… Trang Code Sách hay Xin chào !!! Blogroll Blog Khoa học máy tính WordPress.com WordPress.org Quản trị Đăng ký Đăng nhập RSS  cho bài viết Dòng thông tin  các phản hồi. WordPress.com Lịch Tháng Mười 2017 H B T N S B C « Th12       1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31   Theo dõi Entries (RSS) Comments (RSS) Blog D.Q. Huy American hero in the Vietnam war Ung thư – nó chẳng quan tâm bạn là ai? (10) Khác biệt về đột biến di truyền giữa bệnh ung thư thực quản người châu Á và phương Tây  –  (nghiên cứu của chúng tôi) Các phương pháp điểm trong \n\t\t\tThêm\t\t Xem nhiều nhất \n\t\t\t\t\t\tBài toán nhân đa thức - Phép biến đổi Fourier nhanh (Fast Fourier Transform) - FFT\t\t\t\t\t \n\t\t\t\t\t\tHàm lồi (1) - Định nghĩa và tính chất cơ bản\t\t\t\t\t \n\t\t\t\t\t\tBất đẳng thức xác suất (1) - Markov, Chebyshev, Chernoff, Jensen inequalities\t\t\t\t\t \n\t\t\t\t\t\tĐịnh lý tổng quát (Master theorem) tính độ phức tạp các thuật toán chia để trị\t\t\t\t\t \n\t\t\t\t\t\tPhương pháp đơn hình (Simplex method) (8) - Thuật toán đơn hình đối ngẫu\t\t\t\t\t \n\t\t\t\t\t\tTập lồi (1) - Định nghĩa và một số ví dụ\t\t\t\t\t \n\t\t\t\t\t\tCác khái niệm trong Học máy (Machine Learning) (1) - Tổng quan\t\t\t\t\t \n\t\t\t\t\t\tBoost.MPI (3) – Lập trình thuật toán song song của Nocedal & cộng sự\t\t\t\t\t \n\t\t\t\t\t\tTập lồi (2) - Tổ hợp lồi, bao lồi, bao affine, chiều, đơn hình, nón lồi, bao nón\t\t\t\t\t \n\t\t\t\t\t\tPhân lớp bằng siêu phẳng (2) - Sử dụng các đại lượng thống kê (linear discriminant analysis)\t\t\t\t\t Từ khóa armijo Artificial Neural Networks bao lồi bao nón bayes biến đổi biến đổi Gauss bài toán đối ngẫu chiều VC concept cực tiểu cực đại cực đại hóa khả năng dạng bảng dạng chuẩn tắc empirical risk minimization farkas generalization error hàm lồi hàm mục tiêu hàm phân lớp hàm tiềm năng Jensen Karush-Kuhn-Tucker khoảng cách đối ngẫu khái niệm KKT Kullback-Leibler Lagrange lý thuyết VC lớp khái niệm ma trận cơ sở maximum likelihood estimation MLE máy phân lớp sử dụng véctơ hỗ trợ Newton nghiệm cơ sở nón lồi perceptron phá vỡ phân cách phương pháp điểm trong phương pháp đơn hình phần trong quy hoạch Quy hoạch lồi Quy hoạch nón Quy hoạch tuyến tính Radon RBF ràng buộc shatter siêu phẳng siêu phẳng đỡ Slater Support Vector Machines suy biến SVM Taylor tính lồi tập lồi tốc độ hội tụ tối thiểu rủi ro thực nghiệm tổ hợp nón VC dimension VC theory xác suất lỗi điều kiện tối ưu điểm cực biên đơn hình đường thẳng đỉnh định lý đảo đối ngẫu độ giãn biên Ta còn có thể tính toán các giá trị xác suất có điều kiện khác, ví dụ\n , đây là xác suất có điều kiện của ngày cuối cùng nếu biết trước ngày đầu tiên, trong đó  (do tính phân phối của phép cộng và phép nhân). Quá trình tính tổng trên như sau:\n Với mọi giá trị của   tính , \nđây là một hàm của  . Với mọi giá trị của   tính tổng . Tiếp tục như vậy đến cuối cùng ta được  . Nguyên tắc tối thiểu hóa rủi ro thực nghiệm (4) – Ước lượng kì vọng Rademacher, chiều VC Posted by Trần Quốc Long trên Tháng Bảy 7, 2009 Bài trước  cho thấy mối quan hệ giữa   và   liên quan đến  kỳ vọng Rademacher  của lớp hàm  . Định lý:  Nếu   và hàm lỗi   là  hàm lỗi 0-1 , , ta có  . Chứng minh:  Theo định nghĩa  (do  )  (do   có cùng phân bố với  ) Nhận xét:  Định lý cho phép liên hệ giữa kì vọng Rademacher của lớp hàm   và kì vọng Rademacher của  . Nếu đặt   là tập tất cả các khả năng phân lớp của lớp hàm   trên tập mẫu học   (Ta đã biết đây chính là   khái niệm  phá vỡ  ( shatter ) đã giới thiệu trong bài này ). Ta có tức là đại lượng này không phụ thuộc vào số lượng hàm trong   mà phụ thuộc vào việc lớp hàm này có thể  phá vỡ  ( shatter )  tập mẫu học   đến mức nào. Để ước lượng   ta có bổ đề sau: Bổ đề  lớp hữu hạn  Massart (Massart’s finite  class lemma ): Cho   là một tập hữu hạn,   là các biến ngẫu nhiên Rademacher, ta có . trong đó  . Chứng minh:  Xét đại lượng  . Với mọi  , ta có  ( bất đẳng thức Jensen , do hàm   là hàm lồi)  (do   là hàm đồng biến)  (do  )  (do  )  (do   độc lập)  (do   là  biến Rademacher )  (do  , tra  Wikipedia  để biết khai triển Taylor của hàm   rồi so sánh với khai triển Taylor của  ) Lấy loga cả hai vế ta được  . Chọn   để tối thiểu hóa vế phải ta được   và   (đpcm). Bổ đề Massart giúp ta ước lượng   qua định lý sau Định lý:  Với lớp hàm phân lớp   mà   và   là  hàm tăng trưởng  (growth function )  của   ta có . Chứng minh:  Ta có  (do   và định nghĩa của  hàm tăng trưởng  (growth function ) ) Hệ quả:  Nếu  chiều VC  của lớp hàm   hữu hạn thì   khi  . Chứng minh:  Thật vậy, nếu chiều VC của lớp hàm   hữu hạn và bằng   thì  ta đã biết  hay  . Nhận xét:  Kết quả này chứng tỏ Nguyên tắc tối thiểu hóa rủi ro thực nghiệm khả thi vì   với xác suất cao khi số mẫu học   do ta đã biết ở  bài trước nghĩa là rủi ro kì vọng của   xấp xỉ rủi ro kì vọng thấp nhất có thể được với xác suất cao. Ước lượng   bị giới hạn bởi hàm tăng trưởng  . Hàm tăng trưởng lại bị giới hạn bởi chiều VC. Vì vậy, chiều VC là một thuộc tính quan trọng cần xét đến khi học trên lớp hàm  .  Ước lượng cận trên  của chiều VC của một lớp hàm hoặc  chỉ ra nó vô hạn  sẽ giúp ta xác định có nên áp dụng nguyên tắc tối thiểu hóa rủi ro thực nghiệm hoặc phải cẩn trọng khi áp dụng nguyên tắc này hay không. Tại sao phải cẩn trọng khi áp dụng  nguyên tắc tối thiểu hóa rủi ro thực nghiệm  (ERM) khi chiều VC vô hạn hoặc rất lớn ? Bởi vì khi một lớp hàm   có chiều VC   vô hạn (hoặc rất lớn)  có nghĩa là với mọi tập mẫu học (và mọi cách phân chia nó), luôn có một hàm phân lớp   thuộc    thỏa mãn cách phân chia này. Nếu ta áp dụng ERM, ta sẽ tìm ra hàm   chứ không phải hàm   mà ta muốn (mặc dù rủi ro thực nghiệm (số lỗi) trên tập mẫu học bằng 0). Đây chính là hiện tượng  học quá (overfitting)  khi sử dụng các lớp hàm có khả năng biểu diễn mạnh (ví dụ:  mạng nơron – neural network ). Posted in  Học thống kê ,  Lý thuyết học máy  | Tagged:  Artificial Neural Networks ,  chiều VC ,  empirical risk minimization ,  ERM ,  finite class lemma ,  growth function ,  hàm tăng trưởng ,  học quá ,  massart ,  mạng nơron ,  overfitting ,  rademacher ,  shatter ,  tối thiểu rủi ro thực nghiệm ,  VC dimension  |     Leave a Comment » Post to Hủy bỏ",
          "relevance": "1",
          "title": "Posts Tagged ‘overfitting’",
          "url": "https://csstudyfun.wordpress.com/tag/overfitting/"
        },
        {
          "content": "Homepage Follow Sign in Get started Homepage Tran Phuong (Ri) Blocked Unblock Follow Following Hope is Everything Oct 12, 2016 Lính mới & Deep Learning Hành trình tiến đến Deep Learning và câu chuyện tìm ra các model ứng dụng tối ưu. Khởi đầu Thật khó có thể hình dung được cách đây 1 vài năm chúng ta có thể nghĩ rằng AI đang trở thành một lĩnh vực nở rộ. Chúng ta đang ở tiệm cận của quy mô dữ liệu và tự động hoá ở mức cực đại và chắc có lẽ AI là thứ mà rất nhiều các nhà lớn, nhà đầu tư, các công ty và nhóm khởi nghiệp đang tìm cách tham gia. Ngày 9 tháng 1, 2007 Gần 10 năm trước, chiến iPhone đầu tiên được giới thiệu bởi Steve Jobs, cuộc cách mạng di dộng diễn ra ngay sau đó với sự bùng nổ đến chóng mặt. Ít ai để ý rằng Steve Jobs đã không muốn cho bất cứ ai phát triển ứng dụng (apps) cho nó, bởi theo ông thì chỉ những gì ông kiểm soát được trong một thể thống nhất mới tạo nên một thiết bị tuyệt vời. Ông đã chần chừ, nhưng không quá lâu và App Store ra đời vào ngày 7 tháng 10, 2008 (một năm rưỡi sau). Và đến giờ App Store đã có hơn 2 triệu apps với tổng hơn 50 tỷ USD doanh thu đo đếm được từ IAP của Apple. Và một câu hỏi lớn đặt ra là đây có phải là đỉnh của sự phát triển mô hình ứng dụng di động hay chưa? Mặc dù có tới 2 triệu apps nhưng thực sự chỉ có một nhóm rất nhỏ các app phổ biến, và ngày càng nhiều nhà lớn chi phối các app quan trọng, hay bản thân Apple cũng như Google đều nắm những app lớn như Video Call, Messaging, Mail, Calendar, Music, Mobile Cloud Storage… Hố đen mới từ Messaging Chúng ta chứng kiến thương vụ đình đám ông chủ Facebook mua lại WhatsApp với giá 19 tỷ USD và biết Facebook trở thành nhà lớn chi phối quá bán thị trường các ứng dụng tin nhắn di động. Bộ đôi WhatsApp Facebook Messenger đã có 1 tỷ người dùng active mỗi tháng, và có hơn 60 tỷ tin nhắn mỗi ngày. Những con số thật đáng kinh ngạc. Hố đen mới được hình thành, nó hút nhiều nguồn lực xoay quanh câu hỏi  “Messaging sẽ biến đổi ra sao và các mô hình kinh doanh trên đó?” Tại hội nghị F8, 4/2016, Facebook công bố Messenger Platform và trình diễn các ứng dụng Chatbots đầu tiên về ecommerce và publishing. Không phải Facebook là nhà đầu tiên giới thiệu nền tảng cho chatbots, trước đó có Telegram, Slack, Wechat…, thế nhưng sự áp đảo có tính chi phối của môi trường Messenger + WhatsApp đã tạo nên một cú hích lớn và thực sự là một hố đen bắt đầu thu hút anh tài vào đây. Khởi đầu cho Chatbots Ý tưởng Chatbots không mới, những năm 80 đã có nhiều ứng dụng mang tính chất trình diễn, một số đề tài khoa học xoay quanh câu hỏi “Liệu chúng ta có thể tạo ra cỗ máy giao tiếp giống người không?” Rất nhiều paper và các nhà khoa học đã cố gắng đi tìm lời giải. Đến nay, việc tạo ra cỗ máy “có khả năng trừu tượng và ý thức được mình đang tồn tại\" dường như bế tắc. Andrew Ng’s view: “Most of the value of deep learning today is in relatively narrow domains where you can get a lot of data. Here’s one example of something it cannot do: have a meaningful conversation. There are demos, and if you cherry-pick the conversation, it looks like it’s having a meaningful conversation, but if you actually try it yourself, it quickly goes off the rails.” Vậy phải chăng chatbots khó chinh phục người dùng? Họ có thể chat với bạn bè mỗi ngày, mọi lúc mọi nơi, còn chatbots có lẽ quá ngô nghê và máy móc để thuyết phục họ quay lại? Machine Learning (ML) vs Deep Learning (DL) Câu trả lời cho thế hệ Chatbots mới nằm ở ML và DL. Một thế hệ mới có khả năng duy trì và phát triển hội thoại (context & developing conversation). Một thế hệ có khả năng được huấn luyện (train) và tìm ra được những giá trị trong khối dữ liệu lớn và mang đến cho từng người dùng. ML được bắt đầu bằng ý tưởng sử dụng các thuật toán thống kê trong đó xác suất tiền nghiệm tác động đến kết quả hậu nghiệm, ví dụ bạn có thể ứng dụng  Suy luận Bayes  để tính xác suất hậu nghiệm. Machine learning is the subfield of computer science that “gives computers the ability to learn without being explicitly programmed” (Arthur Samuel, 1959). Machine Learning có quan hệ rất gần và thường có bao gồm với tính máy dựa trên thống kê ( computational statistics ), vì vậy đại số tuyến tính ( Linear Algebra ) được vận dụng rất nhiều trong ML. Ứng dụng của ML chủ yếu tập trung nhiều vào việc tiên đoán (prediction) dựa vào việc huấn luyện bởi các tập mẫu (datasets), các tập này được các chuyên gia lựa chọn các đặc trưng (features engineering). Nguồn Sklearn Classifier Comparison  http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html Bạn có thể dùng các thuật toán dạng học có giám sát (supervised) cho các bài toán phân loại dữ liệu. Ưu điểm của supervised là độ chính xác cao nếu có tập train tốt, các nhãn được định sẵn. Trong trường hợp tập train chưa gán nhãn trước bạn có thể dùng học không giám sát (unsupervised). Trong ứng dụng thực tế, nếu bạn có một toà soạn hoặc một trang cập nhật nội dung thường xuyên, bạn có thể ứng supervised để train cho tập mẫu theo chủ đề, từ đó có thể tiên đoán cho các bài viết mới, độ chính xác có thể lên tới 97 đến 98%. Độ chính xác của các giải thuật ML này bị chi phối lớn bởi việc định nghĩa các đặc trưng (features). Ví dụ, để đoán xem tên một người là nam hay nữ, bạn có thể dựa vào tên đệm, tên hay thậm chí là chữ cái đầu và cuối. Trong xử lý văn bản thì bước tiền xử lý như tokenization, tính tf-idf, kbest có tác động lớn đến độ chính xác (accuracy). Như vậy rõ ràng ML cổ điển có một giới hạn lớn đó là phụ thuộc rất nhiều features engineering, điều đó cần rất nhiều kinh nghiệm của các chuyên gia ML. Deep Learning và cuộc chơi của dữ liệu siêu lớn Deep Learning là  một thuật toán dựa trên một số ý tưởng từ não bộ tới việc tiếp thu nhiều tầng biểu đạt, cả cụ thể lẫn trừu tượng, qua đó làm rõ nghĩa của các loại dữ liệu.  Deep Learning  được ứng dụng trong nhận diện hình ảnh, nhận diện giọng nói, xử lý ngôn ngữ tự nhiên. (Phát biểu khá trừu tượng, trên Google khi bạn gõ từ khoá “Deep Learning là gì\"). Để diễn giải, bạn có thể xem cách Andrew Ng biểu đạt bằng đồ thị dưới đây: Nguồn  http://cs229.stanford.edu/materials/CS229-DeepLearning.pdf Có thể hiểu Deep Learning là những phương thức có tính trí tuệ nhân tạo mới nhằm đạt được hiệu suất và độ chính xác cao hơn khi tăng khối lượng dữ liệu. Việc xây dựng các mạng neural như  CNN ,  RNN  hay cả các mạng lai ghép nhằm giải quyết bài toán giảm tối đa sự phụ thuộc vào quá trình chọn features nói ở phần trên. Bạn có thể tưởng tượng bây giờ để nhận diện giọng nói hay chữ viết, bạn không cần phải tiền xử lý nhiều và phải định nghĩa các đặc trưng nhận diện, bạn chỉ cần đưa vào gần như là dữ liệu thô (raw input) vào mạng neural để xử lý. Bản thân của các mạng này vẫn sử dụng các thuật toán thống kê, có thể nói đây là một kiểu học vẹt ở quy mô siêu lớn, càng nhiều dữ liệu, độ chính xác càng cao. Trên phương diện kinh doanh, việc hiểu rõ Machine Learning và Deep Learning có ý nghĩa rất lớn trong việc lựa chọn và mức độ ứng dụng thực tế, Chatbots là một ứng dụng điển hình, không phải cứ nói ứng dụng ML/DL/AI là được, bạn cần tỉnh táo và hiểu đầy đủ trước khi dồn nguồn lực cho những thứ có thể vắt kiệt sức bạn trước khi bạn đến đích. Thách thức của Chatbots Chatbots đang là một xu hướng lớn, được các nhà lớn, giới đầu tư và nhiều startups nhảy vào, rất nhiều anh chị em tinh hoa bước vào chung sức giải. Khoảng thời gian gần 1 năm qua, đã có rất nhiều chatbots ra đời, nhưng tựu chung đều ở mức cơ bản, hiểu nôm na là gõ từ khoá a thì làm việc A, bí quá sẽ tìm kiếm theo cụm từ. Một số thì xài thủ thuật để trong hao hao giống người. Tuy nhiên, không thể phủ nhận rằng bot sau ngon hơn bot trước, từng bước nhiều nhà lớn và tổ chức đầu tư nghiêm túc vào lĩnh vực này. Thách thức của Chatbots gồm: Context & Developing Conversation : Được hiểu là làm sao người dùng giao tiếp với máy mà ko máy móc quá, duy trì các cuộc hội thoại đủ nghĩa và hữu ích, không đơn thuần là mệnh lệnh đơn. Ví dụ khi bạn nói 1 câu ngắn “Tớ muốn tìm khách sạn”, bot cần phát triển hội thoại thêm như “Bạn muốn tìm khách sạn ở đâu, hay phòng đơn hay đôi, có wifi hay ăn sáng miễn phí không…. Hội thoại thì đa dạng, không phải là một kịch bản theo thứ tự 123. Hiện nay NLP, NLU là các công nghệ có thể áp dụng nhưng chưa rõ ràng, đòi hỏi việc nghiên cứu thêm. Training:  Nói đến huấn luyện, có lẽ đây là đặc trưng rất quan trọng của loài người, con người nếu được huấn luyện, được dạy và có khả năng học thì sẽ tiến bộ, làm được nhiều việc phức tạp và đạt kết quả cao hơn. Hiện nay với Chatbots có thể được huấn luyện từ dữ liệu doanh nghiệp, từ lịch sử chat của người dùng, và nhiệm vụ tìm cách nối giữa dữ liệu doanh nghiệp với người dùng sao cho đạt hiệu quả kinh doanh tốt nhất: khách hàng quay lại thường xuyên, bán được nhiều hàng hơn… Performance & Scale Out:  Chatbots đòi hỏi xử lý trên từng truy vấn theo ngữ cảnh người dùng nên việc tối ưu truy vấn đòi hỏi một kiến trúc tốt và có khả năng mở rộng được. Hiện nay, chưa có các mô hình, platform, engine đủ rõ ràng để ứng dụng quy mô, các nhà lớn cũng chưa có câu trả lời. Amazon Skills, IBM Watson, Microsoft Bot Framework, Google Actions with API.AI, Facebook Wit.AI và rất nhiều startups khác đưa ra giải pháp, và ngày một tốt hơn. Nếu bạn phát triển bot lúc này, đồng nghĩa bạn sẽ theo hướng nghiên cứu và thí nghiệm hơn là thương mại hoá. Tuy nhiên nếu bạn chờ thì dường như rất khó cho bạn cạnh tranh khi nhiều đơn vị khác đã đi trước bạn. deeplearningbook.org Những gì tớ nói ở trên cũng để đi đến đây, dự án cùng nhau học Deep Learning :) The Deep Learning textbook is a resource intended to help students and practitioners enter the field of machine learning in general and deep learning in particular. The online version of the book is now complete and will remain available online for free. The deep learning textbook can now be pre-ordered on  Amazon . Pre-orders should ship on December 16, 2016. Nội dung Part I: Applied Math and Machine Learning Basics 2 Linear Algebra 3 Probability and Information Theory 4 Numerical Computation 5 Machine Learning Basics Part II: Modern Practical Deep Networks 6 Deep Feedforward Networks 7 Regularization for Deep Learning 8 Optimization for Training Deep Models 9 Convolutional Networks 10 Sequence Modeling: Recurrent and Recursive Nets 11 Practical Methodology 12 Applications Part III: Deep Learning Research 13 Linear Factor Models 14 Autoencoders 15 Representation Learning 16 Structured Probabilistic Models for Deep Learning 17 Monte Carlo Methods 18 Confronting the Partition Function 19 Approximate Inference 20 Deep Generative Models Đây là một cuốn textbook (sách giáo khoa) mà team tớ lựa chọn để anh em cùng học và chia sẻ. Mục tiêu là hiểu rõ về Deep Learning và ứng dụng nó một cách hiệu quả vào thực tế. Mô hình học: Học viên truy cập vào tài liệu deeplearningbook này trên Google Docs, tại đây người học sẽ để lại các note/comment. Nhóm tớ sẽ follow up, đưa ra các điểm chính, các ý tưởng, source code (lưu trên github) cụ thể từ các giải thuật nhắc đến trong textbook này. Học viên có thể mời các chuyên gia về Deep Learning vào thảo luận hoặc trả lời các thắc mắc, nhóm tớ sẽ tích cực mời các anh chị về Deep Learning tham gia. Đây là dự án phi lợi nhuận và tất cả mọi người đều có thể tham gia dựa vào các quy tắc ứng xử do nhóm đặt ra. Khai giảng khoá học vào 23pm 20/10/2016, bây giờ bạn có thể đăng ký tham gia bằng cách request quyền truy cập  tại đây. ❤ Machine Learning Deep Learning AI Deeplearingbook Course A single golf clap? Or a long standing ovation? By clapping more or less, you can signal to us which stories really stand out. 40 16 Blocked Unblock Follow Following Tran Phuong (Ri) Hope is Everything Follow Chappiebot Viết về Machine Learning, Deep Learning và một chút hương vị AI 40 Never miss a story from  Chappiebot , when you sign up for Medium.  Learn more Never miss a story from  Chappiebot Get updates Get updates",
          "relevance": "0",
          "title": "Lính mới & Deep Learning",
          "url": "https://blog.chappiebot.com/l%C3%ADnh-m%E1%BB%9Bi-deep-learning-62e6abf81739"
        },
        {
          "content": "Random Forest và vấn đề phân loại - xếp hạng hồ sơ tín dụng Trong  post trước  chúng ta thấy rằng Random Forest (RF) chính là mô hình mà ngân hàng nên lựa chọn để đạt lợi nhuận tối ưu. Lí thuyết về RF cho vấn đề phân loại các bạn có thể đọc từ các nghiên cứu của Shapire et al. (1998) và Breiman (2001). Bài viết này không có tham vọng trình bày lí thuyết của mô hình phân loại này vì người viết chưa đủ hiểu mà chỉ hướng vào hướng dẫn thực hành cũng như khả năng áp dụng của RF cho bài toán phân loại - xếp hạng tín dụng (Credit Scoring). Bộ số liệu được sử dụng là  GermanCredit  tích hợp cùng gói caret. Biến mục tiêu là Class theo đó những hồ sở tốt có nhãn là “Good” và hồ sơ xấu có nhãn là “Bad”. Mục tiêu của chúng ta là phân loại cho hồ sơ xin vay tín dụng tại một ngân hàng của Đức căn cứ vào 61 biến số khác nhau như thu nhập, tình trạng hôn nhân, mục đích của việc xin vay tín dụng (rất nhiều). Như vậy biến tiên lượng (biến dự báo) bao gồm cả biến định tính lẫn định lượng: rm(list = ls())\r\nlibrary(caret)\r\ndata(\"GermanCredit\")\r\n#  Tạo ra bản sao thứ hai của bộ dữ liệu và xem qua: \r\ndung <- GermanCredit\r\nstr(dung) ## 'data.frame':    1000 obs. of  62 variables:\r\n##  $ Duration                              : int  6 48 12 42 24 36 24 36 12 30 ...\r\n##  $ Amount                                : int  1169 5951 2096 7882 4870 9055 2835 6948 3059 5234 ...\r\n##  $ InstallmentRatePercentage             : int  4 2 2 2 3 2 3 2 2 4 ...\r\n##  $ ResidenceDuration                     : int  4 2 3 4 4 4 4 2 4 2 ...\r\n##  $ Age                                   : int  67 22 49 45 53 35 53 35 61 28 ...\r\n##  $ NumberExistingCredits                 : int  2 1 1 1 2 1 1 1 1 2 ...\r\n##  $ NumberPeopleMaintenance               : int  1 1 2 2 2 2 1 1 1 1 ...\r\n##  $ Telephone                             : num  0 1 1 1 1 0 1 0 1 1 ...\r\n##  $ ForeignWorker                         : num  1 1 1 1 1 1 1 1 1 1 ...\r\n##  $ Class                                 : Factor w/ 2 levels \"Bad\",\"Good\": 2 1 2 2 1 2 2 2 2 1 ...\r\n##  $ CheckingAccountStatus.lt.0            : num  1 0 0 1 1 0 0 0 0 0 ...\r\n##  $ CheckingAccountStatus.0.to.200        : num  0 1 0 0 0 0 0 1 0 1 ...\r\n##  $ CheckingAccountStatus.gt.200          : num  0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ CheckingAccountStatus.none            : num  0 0 1 0 0 1 1 0 1 0 ...\r\n##  $ CreditHistory.NoCredit.AllPaid        : num  0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ CreditHistory.ThisBank.AllPaid        : num  0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ CreditHistory.PaidDuly                : num  0 1 0 1 0 1 1 1 1 0 ...\r\n##  $ CreditHistory.Delay                   : num  0 0 0 0 1 0 0 0 0 0 ...\r\n##  $ CreditHistory.Critical                : num  1 0 1 0 0 0 0 0 0 1 ...\r\n##  $ Purpose.NewCar                        : num  0 0 0 0 1 0 0 0 0 1 ...\r\n##  $ Purpose.UsedCar                       : num  0 0 0 0 0 0 0 1 0 0 ...\r\n##  $ Purpose.Furniture.Equipment           : num  0 0 0 1 0 0 1 0 0 0 ...\r\n##  $ Purpose.Radio.Television              : num  1 1 0 0 0 0 0 0 1 0 ...\r\n##  $ Purpose.DomesticAppliance             : num  0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ Purpose.Repairs                       : num  0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ Purpose.Education                     : num  0 0 1 0 0 1 0 0 0 0 ...\r\n##  $ Purpose.Vacation                      : num  0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ Purpose.Retraining                    : num  0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ Purpose.Business                      : num  0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ Purpose.Other                         : num  0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ SavingsAccountBonds.lt.100            : num  0 1 1 1 1 0 0 1 0 1 ...\r\n##  $ SavingsAccountBonds.100.to.500        : num  0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ SavingsAccountBonds.500.to.1000       : num  0 0 0 0 0 0 1 0 0 0 ...\r\n##  $ SavingsAccountBonds.gt.1000           : num  0 0 0 0 0 0 0 0 1 0 ...\r\n##  $ SavingsAccountBonds.Unknown           : num  1 0 0 0 0 1 0 0 0 0 ...\r\n##  $ EmploymentDuration.lt.1               : num  0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ EmploymentDuration.1.to.4             : num  0 1 0 0 1 1 0 1 0 0 ...\r\n##  $ EmploymentDuration.4.to.7             : num  0 0 1 1 0 0 0 0 1 0 ...\r\n##  $ EmploymentDuration.gt.7               : num  1 0 0 0 0 0 1 0 0 0 ...\r\n##  $ EmploymentDuration.Unemployed         : num  0 0 0 0 0 0 0 0 0 1 ...\r\n##  $ Personal.Male.Divorced.Seperated      : num  0 0 0 0 0 0 0 0 1 0 ...\r\n##  $ Personal.Female.NotSingle             : num  0 1 0 0 0 0 0 0 0 0 ...\r\n##  $ Personal.Male.Single                  : num  1 0 1 1 1 1 1 1 0 0 ...\r\n##  $ Personal.Male.Married.Widowed         : num  0 0 0 0 0 0 0 0 0 1 ...\r\n##  $ Personal.Female.Single                : num  0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ OtherDebtorsGuarantors.None           : num  1 1 1 0 1 1 1 1 1 1 ...\r\n##  $ OtherDebtorsGuarantors.CoApplicant    : num  0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ OtherDebtorsGuarantors.Guarantor      : num  0 0 0 1 0 0 0 0 0 0 ...\r\n##  $ Property.RealEstate                   : num  1 1 1 0 0 0 0 0 1 0 ...\r\n##  $ Property.Insurance                    : num  0 0 0 1 0 0 1 0 0 0 ...\r\n##  $ Property.CarOther                     : num  0 0 0 0 0 0 0 1 0 1 ...\r\n##  $ Property.Unknown                      : num  0 0 0 0 1 1 0 0 0 0 ...\r\n##  $ OtherInstallmentPlans.Bank            : num  0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ OtherInstallmentPlans.Stores          : num  0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ OtherInstallmentPlans.None            : num  1 1 1 1 1 1 1 1 1 1 ...\r\n##  $ Housing.Rent                          : num  0 0 0 0 0 0 0 1 0 0 ...\r\n##  $ Housing.Own                           : num  1 1 1 0 0 0 1 0 1 1 ...\r\n##  $ Housing.ForFree                       : num  0 0 0 1 1 1 0 0 0 0 ...\r\n##  $ Job.UnemployedUnskilled               : num  0 0 0 0 0 0 0 0 0 0 ...\r\n##  $ Job.UnskilledResident                 : num  0 0 1 0 0 1 0 0 1 0 ...\r\n##  $ Job.SkilledEmployee                   : num  1 1 0 1 1 0 1 0 0 0 ...\r\n##  $ Job.Management.SelfEmp.HighlyQualified: num  0 0 0 0 0 0 0 1 0 1 ... dim(dung) ## [1] 1000   62 # Tỉ lệ hai  loại  hồ sơ: \r\ntable(dung$Class) / nrow(dung) ## \r\n##  Bad Good \r\n##  0.3  0.7 Thực hiện thuật toán RF với gói randomForest Có nhiều gói có thể sử dụng để thực hiện RF trong R. Tiện lợi nhất có lẽ là dùng gói caret. Tuy nhiên người mới tiếp cận và học ML nên sử dụng gói  randomForest  với hàm  randomForest()  để hiểu kĩ hơn về mô hình phân loại này. Không giống như, chẳng hạn mô hình phân loại Logistic, RF (và các mô hình học máy Machine Learning nói chung) có hai tham số tinh chỉnh (tên tiếng Anh là Turning Parameter). Đây là những tham số có ảnh hưởng đến phẩm chất của mô hình. Với RF thì hai tham số đó là  mtry  và  ntree . Mặc định thì hàm randomForest() sẽ tự động chọn ntree = 500. Riêng mtry có thể lấy theo công thức là lấy phần nguyên căn bậc 2 của số biến tiên lượng. Ví dụ, căn bậc 2 của 61 là 7.8 và chúng ta sẽ lấy mtry = 8. Tuy vậy, Liaw & Wiener (2002) chỉ ra rằng chỉ cần lấy mtry = 1 đã là đủ tốt cho khá nhiều bộ dữ liệu còn mtry = 2 thì đã là đủ tốt cho hầu hết tình huống. Chính vì lí do này mà nếu không tuyên bố cụ thể, hàm randomForest() sẽ mặc định mtry = 2. Cần lưu ý rằng mtry tăng thì có thể làm cho khả năng phân loại (dự báo) của RF tăng. Với bộ dữ liệu GermanCredit chúng ta thực hiện RF như sau: # Tái lập kết quả bằng lệnh gieo  hạt: \r\nset.seed(1709)\r\n# Thực  hiện RF với toàn bộ số liệu: \r\nlibrary(randomForest)\r\nfit <- randomForest(Class ~ ., \r\n                    data = dung, \r\n                    # Lựa chọn này để lấy  cả thông  tin về \r\n                    # tầm quan trọng của biến số:\r\n                    importance = TRUE, \r\n                    mtry = 2,\r\n                    # Chọn ntree là 1000: \r\n                    ntree = 1000)\r\n\r\n# Xem qua kết quả: \r\nfit ## \r\n## Call:\r\n##  randomForest(formula = Class ~ ., data = dung, importance = TRUE,      mtry = 2, ntree = 1000) \r\n##                Type of random forest: classification\r\n##                      Number of trees: 1000\r\n## No. of variables tried at each split: 2\r\n## \r\n##         OOB estimate of  error rate: 28.6%\r\n## Confusion matrix:\r\n##      Bad Good class.error\r\n## Bad   17  283 0.943333333\r\n## Good   3  697 0.004285714 Đọc một số kết quả chính: OOB estimate of error rate: 28.6%  chính là mức độ sai lệch của mô hình RF. Nó chính là bằng (283 + 3) / 1000. Và do đó mức độ chính xác của mô hình (đối với bộ dữ liệu được thử nghiệm) là 1 - 28.6 = 71.4%. Lưu ý rằng con số 28.6 chính là  tỉ lệ sai sót huấn luyện  và con số này nó chỉ mới đúng cho một mẫu cụ thể. Với mẫu khác thì chưa chắc. Tất nhiên chúng ta kì vọng nó nên thấp hơn 28.6%. Bad 17 283 0.943333333  nghĩa là có 300 hồ sơ xấu thì phân lại đúng có 17 còn xếp nhầm 283 thành tốt (tức sai sót tới 94.33%). Đây là tin không hay. Vì xếp nhầm hồ sơ xấu thành tốt thì hậu quả cực kì tai hại so với xếp nhầm hồ sơ tốt thành xấu. Tất nhiên, con số này chỉ đúng  với một mẫu mà thôi  . Chúng ta kì vọng rằng các con số này sẽ theo chiều hướng khả quan hơn đối với những bộ dữ liệu mới. Có đúng là RF như chúng ta thấy tệ đến mức đó không? Chúng ta sẽ trả lời câu hỏi đó ở các phần sau. Chúng ta có thể hình ảnh hóa mức độ quan trọng của các biến số dự trên tiêu chí Gini. Phân tích mức độ quan trọng của biến số đối với mô hình được sử dụng cho nhiều mục đích, ví dụ như là lựa chọn biến số cho mô hình: # Mức độ quan trọng của biến số căn cứu vào hệ số Gini: \r\nimp_gini <- data.frame(importance(fit, type = 2))\r\n# Thêm cột biến tương ứng với tên biến  số: \r\nimp_gini$Variable <- rownames(imp_gini)\r\n# Tên các biến  này là dài và xấu. Có thể  \r\n# lấy 4 kí tự của chúng làm kí hiệu: \r\n\r\nimp_gini$Var <- abbreviate(imp_gini$Variable, minlength = 4)\r\n# Rồi hình ảnh  hóa mức  độ quan trọng của các biến: \r\nlibrary(ggpubr)\r\nggdotchart(imp_gini, \r\n           rotate = TRUE, \r\n           x = \"Var\", \r\n           y = \"MeanDecreaseGini\", \r\n           sorting = \"desc\",                        \r\n           add = \"segments\",  \r\n           title = \"Variable Importance based on Gini\",\r\n           ylab = FALSE, \r\n           ggtheme = theme_bw()) # Mức độ quan trọng của biến số căn cứu vào Accuracy\r\n# rồi hình  ảnh hóa tương tự như trên nếu  muốn: \r\nimp_acc <- data.frame(importance(fit, type = 1)) Kế tiếp chúng ta sử dụng mô hình cho dự báo. Giả sử chúng ta có 100 hồ sơ xin vay, thì mô hình này sẽ phân loại như sau: # Lấy ra 100 hồ sơ giả định (từ mẫu ban đầu chẳng hạn): \r\nset.seed(1)\r\ntest <- dung[sample(1000, 100), ]\r\n# Dự báo của RF đối với 100 bộ HS này: \r\ndubao <- predict(fit, test)\r\n# So sánh với thực tế: \r\ntable(dubao, test$Class) ##       \r\n## dubao  Bad Good\r\n##   Bad   11    0\r\n##   Good  15   74 # Nếu sử dụng hàm confusionMatrix của gói caret \r\n# thì sẽ thu được nhiều thông tin hơn: \r\nconfusionMatrix(dubao, test$Class, positive = \"Bad\") ## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction Bad Good\r\n##       Bad   11    0\r\n##       Good  15   74\r\n##                                           \r\n##                Accuracy : 0.85            \r\n##                  95% CI : (0.7647, 0.9135)\r\n##     No Information Rate : 0.74            \r\n##     P-Value [Acc > NIR] : 0.0061070       \r\n##                                           \r\n##                   Kappa : 0.5205          \r\n##  Mcnemar's Test P-Value : 0.0003006       \r\n##                                           \r\n##             Sensitivity : 0.4231          \r\n##             Specificity : 1.0000          \r\n##          Pos Pred Value : 1.0000          \r\n##          Neg Pred Value : 0.8315          \r\n##              Prevalence : 0.2600          \r\n##          Detection Rate : 0.1100          \r\n##    Detection Prevalence : 0.1100          \r\n##       Balanced Accuracy : 0.7115          \r\n##                                           \r\n##        'Positive' Class : Bad             \r\n##  OK. Có vẻ tốt. Vì RF dự báo chính xác 100% hồ sơ xấu. Xếp loại sai 15 hồ sơ tốt thành hồ sơ xấu. Rõ ràng những con sô này (dựa trên mẫu kiểm định 100 hồ sơ) là khả quan hơn nhiều. Sử dụng Random Forest xếp hạng tín dụng trong thực tế Các phân tích trên chỉ ra rằng, chúng ta sẽ sai lầm nếu phán xét chất lượng của một mô hình phân loại nếu chỉ dựa vào 1 mẫu thử nghiệm duy nhất (đây là điều thường thấy ở các nghiên cứu ở VN mà người viết có dịp đọc). Để khách quan chúng ta nên, chẳng hạn, xây dựng mô hình với một (hoặc nhiều) bộ dữ liệu huấn luyện rồi sử dụng mô hình có được để kiểm tra trên phần dữ liệu còn lại gọi là dữ liệu kiểm định. R codes dưới đây thực hiện ý tưởng trên. Cụ thể, bộ dữ liệu ban đầu được chia theo tỉ lệ 50-50. Tức 50% dữ liệu gốc (gọi là training) được sử dụng để dựng mô hình RF còn 50% còn lại (gọi là testing) sẽ sử dụng để test lại mô hình theo cách thức sau: lấy 250 quan sát ngẫu nhiên trong số 500 quan sát của testing để đánh giá ngược lại mô hình. lặp lại quá trình này 100 lần và phẩm chất của mô hình được đánh giá qua 100 lần kiểm tra trên 250 mẫu con này. Chú ý rằng không một quan sát nào thuộc mẫu con kích thước 250 được mô tả ở trên có mặt ở training data. Điều này được thực hiện để tránh overfitting. # Lấy ra testing và training như vẫn đảm bảo là\r\n# tỉ lệ giữa hai loại hồ sơ là 7 - 3 trong mẫu \r\n# dữ liệu huấn luyện: \r\n\r\nlibrary(tidyverse)\r\nset.seed(123)\r\ntraining <- dung %>% \r\n  group_by(Class) %>% \r\n  sample_frac(0.5) %>% \r\n  ungroup()\r\n\r\n# Phần  còn lại là dữ liệu kiểm định: \r\ntesting <- dplyr::setdiff(dung, training)\r\n\r\n# Thực hiện RF: \r\nset.seed(321)\r\nfit <- randomForest(Class ~ ., \r\n                    data = training, \r\n                    importance = TRUE, \r\n                    mtry = 2,\r\n                    ntree = 1000)\r\n\r\n\r\n# Sử dụng RF có được để dự báo cho 100 mẫu con: \r\n\r\nket_qua <- data.frame()\r\nfor (i in 1:100) {\r\n  set.seed(i)\r\n  dulieu_test <- testing[sample(500,  250), ]\r\n  dubao <- predict(fit, dulieu_test)\r\n  k <- as.vector(table(dubao, dulieu_test$Class))\r\n  ket_qua <- rbind(ket_qua, k)\r\n  names(ket_qua) <- c(\"BB\", \"GB\", \"BG\", \"GG\")\r\n}\r\n\r\n\r\n# Giả thiết là lợi nhuận 10%, mỗi hồ sơ duyệt vay\r\n# sẽ được vay 1 đô thì lợi nhuận và Accuracy là: \r\nket_qua <- ket_qua %>% \r\n  mutate(Profit = 0.1*GG - BG, \r\n         Accuracy = (GG +  BB) / (BB + GG + BG + GB))\r\n\r\nsummary(ket_qua) ##        BB              GB              BG             GG       \r\n##  Min.   : 3.00   Min.   :58.00   Min.   :0.00   Min.   :163.0  \r\n##  1st Qu.: 5.00   1st Qu.:65.00   1st Qu.:0.00   1st Qu.:171.0  \r\n##  Median : 6.00   Median :69.00   Median :0.00   Median :175.0  \r\n##  Mean   : 5.96   Mean   :68.66   Mean   :0.47   Mean   :174.9  \r\n##  3rd Qu.: 7.00   3rd Qu.:72.00   3rd Qu.:1.00   3rd Qu.:178.0  \r\n##  Max.   :10.00   Max.   :80.00   Max.   :1.00   Max.   :187.0  \r\n##      Profit         Accuracy     \r\n##  Min.   :15.30   Min.   :0.6760  \r\n##  1st Qu.:16.50   1st Qu.:0.7120  \r\n##  Median :17.05   Median :0.7220  \r\n##  Mean   :17.02   Mean   :0.7235  \r\n##  3rd Qu.:17.60   3rd Qu.:0.7360  \r\n##  Max.   :18.60   Max.   :0.7680 # Cũng nên đánh giá mức  độ ổn định của  kết quả\r\n# với đường thẳng đứng màu xanh là lợi nhuận trung bình: \r\nket_qua %>% \r\n  ggplot(aes(Profit)) + \r\n  geom_density(fill = \"red\", alpha = 0.3) + \r\n  geom_vline(xintercept = mean(ket_qua$Profit), \r\n             color = \"blue\", \r\n             size = 1.2) Như vậy, nếu sử dụng RF thì lợi nhuận trung bình sẽ là 16.90 đô còn mức độ chính xác chung của mô hình phân loại là 79.7%. Tuy nhiên Accuracy không phải là thứ được quan tâm hàng đầu trong tình huống này. Một là, mục tiêu của ngân hàng là Profit. Accuracy nên xếp thứ yếu. Hai là, ngân hàng sẵn sàng ưa thích hơn một mô hình phân loại chính xác nhóm hồ sơ xấu thậm chí ngay cả khi mô hình này có tỉ lệ chính xác kém khi phân loại chung (cho cả hai nhóm hồ sơ) hay kém chính xác khi phân loại hồ sơ tốt. Vì, cái giá phải trả khi phân loại nhầm hồ sơ xấu là rất đắt. Đắt hơn nhiều so với việc phân loại nhầm hồ sơ tốt. Tất nhiên ngoài lợi nhuận trung bình chúng ta còn có thể phải (và thường) quan tâm đến các đặc điểm thống kê khác với hàm describe() của gói psych: library(psych)\r\n# Cách 1 cho trình bày: \r\ndescribe(ket_qua) ##          vars   n   mean   sd median trimmed  mad    min    max range\r\n## BB          1 100   5.96 1.72   6.00    5.90 1.48   3.00  10.00  7.00\r\n## GB          2 100  68.66 4.64  69.00   68.72 4.45  58.00  80.00 22.00\r\n## BG          3 100   0.47 0.50   0.00    0.46 0.00   0.00   1.00  1.00\r\n## GG          4 100 174.91 4.75 175.00  174.78 5.19 163.00 187.00 24.00\r\n## Profit      5 100  17.02 0.70  17.05   17.03 0.82  15.30  18.60  3.30\r\n## Accuracy    6 100   0.72 0.02   0.72    0.72 0.02   0.68   0.77  0.09\r\n##           skew kurtosis   se\r\n## BB        0.30    -0.63 0.17\r\n## GB       -0.10    -0.21 0.46\r\n## BG        0.12    -2.01 0.05\r\n## GG        0.21     0.11 0.48\r\n## Profit   -0.09    -0.61 0.07\r\n## Accuracy  0.05    -0.18 0.00 # Cách 2: \r\n\r\nknitr::kable(round(describe(ket_qua), 2)) vars n mean sd median trimmed mad min max range skew kurtosis se BB 1 100 5.96 1.72 6.00 5.90 1.48 3.00 10.00 7.00 0.30 -0.63 0.17 GB 2 100 68.66 4.64 69.00 68.72 4.45 58.00 80.00 22.00 -0.10 -0.21 0.46 BG 3 100 0.47 0.50 0.00 0.46 0.00 0.00 1.00 1.00 0.12 -2.01 0.05 GG 4 100 174.91 4.75 175.00 174.78 5.19 163.00 187.00 24.00 0.21 0.11 0.48 Profit 5 100 17.02 0.70 17.05 17.03 0.82 15.30 18.60 3.30 -0.09 -0.61 0.07 Accuracy 6 100 0.72 0.02 0.72 0.72 0.02 0.68 0.77 0.09 0.05 -0.18 0.00 Kết quả này cho thấy, ví dụ, Profit có độ méo (Skewness) là -0.09. Một con số rất bé về giá trị tuyệt đối. COn số này nếu tiến về 0 thì phân phối sẽ tiến về chuẩn. Hệ số nhọn (Skewness) là - 0.61. Khi so sánh với hệ số nhọn của phân phối chuẩn (có Kurtosis = 0) thì ta có thể thấy phân phối của Profit hơi tù. Một mô hình sẽ nên được ưa thích nếu độ méo gần bằng 0 về giá trị tuyệt đối còn hệ số nhọn càng lớn càng tối. Tất nhiên chúng ta có thể sử dụng kiểm định JB để kiểm tra xem Profit có phải là phân phối chuẩn không: library(fBasics)\r\njarqueberaTest(ket_qua$Profit) ## \r\n## Title:\r\n##  Jarque - Bera Normalality Test\r\n## \r\n## Test Results:\r\n##   STATISTIC:\r\n##     X-squared: 1.454\r\n##   P VALUE:\r\n##     Asymptotic p Value: 0.4834 \r\n## \r\n## Description:\r\n##  Fri Aug 11 22:01:46 2017 by user: win10ls Giá trị p-value = 0.4834 > 5% nên chúng ta có bằng chứng thống kê để cho rằng phân phối của Profit là chuẩn. Có thể tạo ra cả Histogram và Density đồng thời cho Profit: ket_qua %>% ggplot(aes(Profit)) + \r\n  geom_density(alpha = 0.3, fill = \"blue\", color = \"blue\") + \r\n  geom_histogram(aes(y = ..density..), fill = \"red\", alpha = 0.3, color = \"red\") + \r\n  theme_bw() Hoặc cho tất cả các tiêu chí khá về phẩm chất của mô hình: ket_qua %>% \r\n  gather(Indicator, Value) %>% \r\n  ggplot(aes(Value)) + \r\n  geom_density(alpha = 0.3, fill = \"blue\", color = \"blue\") + \r\n  geom_histogram(aes(y = ..density..), fill = \"red\", alpha = 0.4, color = \"red\") + \r\n  theme_bw() + \r\n  facet_wrap(~ Indicator, scale = \"free\") Vài kết luận Post này hướng dẫn việc thực hiện RF và sử dụng RF cho bài toán phân loại - xếp hạng tín dụng. Tất nhiên trong thực tế không làm đơn giản như thế. Ví dụ, vấn đề tìm tham số tinh chỉnh tối ưu cho RF là chưa được nói đến trong bài này. References L. Breiman. Random forests. Machine Learning, 45(1): 5–32, 2001. R. Shapire, Y. Freund, P. Bartlett, and W. Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. Annals of Statistics, 26 (5):1651–1686, 1998. Liaw, A., & Wiener, M. (2002). Classification and regression by randomForest. R news, 2(3), 18-22.",
          "relevance": "0",
          "title": "Machine Learning (Part 1): Random Forest for Credit Scoring",
          "url": "https://rstudio-pubs-static.s3.amazonaws.com/298605_7f4500d1d58e4055aa5920981a8a73e8.html"
        }
      ]
    },
    {
      "query": "Thuật toán phân cụm trong học không giám sát",
      "description": "Tìm hiểu về giải thuật K-means trong Cluster của học không giám sát, bất kì trang web nào trình bày thuật giải, cách cài đặt, ví dụ, demo chương trình, các phương pháp mở rộng đều là két quả phù hợp",
      "sites": [
        {
          "title": "Bức tranh tổng quan về thuật toán phân cụm",
          "relevance": "1",
          "content": " 090.220.9011  \n                                cuong@techmaster.vn Đăng nhập Đăng ký \n                Trang chủ\n             \n                Khóa học\n             \n                Thực tập\n             \n                Lịch khai giảng\n             \n                Blog\n             \n                Việc làm\n             \n                Phỏng vấn\n             \n                Chúng tôi  arrow_drop_down \n                Khách hàng - Đối tác\n             \n                Giảng viên\n             \n                Thiết bị đào tạo\n             \n                Quy định chung\n             \n                Tuyển dụng giảng viên\n             \n                Chơi thể thao\n             \n                Team building - Royal City\n             \n                Video hoạt động\n             \n                Thực tập\n             menu \n                Trang chủ\n             \n                Khóa học\n             \n                Thực tập\n             \n                Lịch khai giảng\n             \n                Blog\n             \n                Việc làm\n             \n                Phỏng vấn\n             \n                Chúng tôi  arrow_drop_down \n                Khách hàng - Đối tác\n             \n                Giảng viên\n             \n                Thiết bị đào tạo\n             \n                Quy định chung\n             \n                Tuyển dụng giảng viên\n             \n                Chơi thể thao\n             \n                Team building - Royal City\n             \n                Video hoạt động\n             \n                Thực tập\n             Trang chủ Blog Bức tranh tổng quan về thuật toán phân cụm close Danh mục bài viết \n                        Agile (13)\n                     \n                        Altassian (4)\n                     \n                        Android (75)\n                     \n                        Arduino (11)\n                     \n                        ASP.net (13)\n                     \n                        C++ (14)\n                     \n                        Cloud (6)\n                     \n                        công việc (142)\n                     \n                        CSS (19)\n                     \n                        Cuộc sống (129)\n                     \n                        Database (19)\n                     \n                        Design (43)\n                     \n                        Điện tử (35)\n                     \n                        Docker (1)\n                     \n                        Education (60)\n                     \n                        Eletronics (4)\n                     \n                        Facebook (2)\n                     \n                        Game (23)\n                     \n                        Git (8)\n                     \n                        Go (2)\n                     \n                        HTML (31)\n                     \n                        iOS (228)\n                     \n                        Java (115)\n                     \n                        JavaScript (59)\n                     \n                        John Vũ (7)\n                     \n                        Kiểm thử (9)\n                     \n                        lập trình (293)\n                     \n                        lập trình Android (32)\n                     \n                        lập trình ios (157)\n                     \n                        lập trình php (31)\n                     \n                        lập trình ứng dụng (93)\n                     \n                        lập trình ứng dụng iphone (42)\n                     \n                        Lập trình web (170)\n                     \n                        Linux (38)\n                     \n                        Mac (9)\n                     \n                        Machine Learning (11)\n                     \n                        Magento (4)\n                     \n                        Microservice (7)\n                     \n                        Microsoft (16)\n                     \n                        News (59)\n                     \n                        Nghề nghiệp (80)\n                     \n                        Node.js (71)\n                     \n                        Odoo (3)\n                     \n                        OpenCV (1)\n                     \n                        PHP (66)\n                     \n                        Project Management (21)\n                     \n                        Python (13)\n                     \n                        Ruby on Rails (54)\n                     \n                        Scala (15)\n                     \n                        SharePoint (6)\n                     \n                        STEM (10)\n                     \n                        Swift (86)\n                     \n                        TechMaster (130)\n                     \n                        Tips and tricks (18)\n                     \n                        Tutor (72)\n                     \n                        Twitter Boostrap (2)\n                     \n                        Uncategorized (26)\n                     \n                        ứng dụng (2)\n                     \n                        Unity3D (13)\n                     \n                        việc làm (126)\n                     \n                        VMware (1)\n                     \n                        WindowsPhone (43)\n                     \n                        WordPress (4)\n                     Bức tranh tổng quan về thuật toán phân cụm 28/05/2016 Bởi  Phan Đức Việt trong\n                                         Machine Learning Nếu bạn đang có ý định trở thành một Data Scientist (nhà khoa học dữ liệu) thì hiện tại đang là 1 thời điểm không hề tồi chút nào. Những con người kể cả khó tính nhất cũng sẽ đổ dồn sự chú ý khi bạn đề cập tới Big Data trong cuộc hội thoại, đám đông sẽ cảm thấy hào hứng khi được nghe bạn chém gió về Trí tuệ nhân tạo cũng như Học máy. Thậm chí những con số do Google cung cấp tại  đây  còn cho thấy: tất cả vẫn chưa có dấu hiệu dừng lại, chúng vẫn tiếp tục phát triển với tốc độ rất nhanh. Càng ngày càng có rất nhiều các giải thuật 'thông minh' đã được phát minh ra để giúp đỡ các nhà khoa học dữ liệu. Tất cả chúng nhìn chung đều có vẻ rất phức tạp, nhưng nếu chúng ta hiểu được và biết cách phối hợp một cách nhuần nhuyễn thì mọi việc sẽ trở nên dễ dàng hơn rất nhiều. Các khóa học về khai phá dữ liệu (Data Mining) hoặc học máy (Machine Learning) vẫn thường mở đầu bằng những ví dụ về phân cụm, lí do đơn giản bởi vì chúng rất thực tế và không quá khó hiểu. Bài toán phân cụm là 1 nhánh ứng dụng chính của lĩnh vực Unsupervised Learning (Học không giám sát), trong đó dữ liệu được mô tả trong bài toán không được dán nhãn (tức là không có đầu ra). Trong trường hợp này, thuật toán sẽ tìm cách phân cụm - chia dữ liệu thành từng nhóm có đặc điểm tương tự nhau, nhưng đồng thời đặc tính giữa các nhóm đó lại phải càng khác biệt càng tốt. Dữ liệu của chúng ta có thể là bất cứ thứ gì, chẳng hạn như dữ liệu về khách hàng: Thuật toán phân cụm sẽ rất hữu ích trong việc đánh giá và chia thành các nhóm người dùng khác nhau, rồi từ đó ta có thể đưa ra những chiến lược marketing phù hợp trên từng nhóm người dùng đó. Tham khảo khóa học  nhập môn Machine Learning  tại TechMaster K-Means Clustering Sau khi dạo qua những màn giới thiệu chung, đa số các khóa học Data Mining sẽ bắt đầu luôn với K-Means: 1 thuật toán tuy đơn giản nhưng lại khá hiệu quả và được sử dụng rộng khắp. Trước khi bắt tay vào làm, chúng ta cần phải xác định sẵn 2 thứ: đó là  hàm khoảng cách  được sử dụng (ví dụ như khoảng cách Euclid) và  số lượng nhóm  mong muốn (ta sẽ kí hiệu trong bài viết này là  k ) Mô phỏng quá trình phân cụm K-Means Thuật toán bắt đầu với việc chọn ra tâm của từng cụm. Chúng ta có thể đơn giản chọn k điểm ngẫu nhiên trong bộ, hoặc sử dụng một số hướng tiếp cận nào khác, nhưng nhìn chung ngẫu nhiên vẫn là cách tốt nhất. Rồi kế tiếp, luân phiên lặp lại 2 giai đoạn sau: Giai đoạn gán : gán từng phần tử trong bộ dữ liệu của chúng ta vào các cụm. Cách thức tiến hành đó là: với mỗi điểm, hãy tính khoảng cách từ điểm đó tới vị trí các tâm, sau cùng: tâm nào gần nhất thì gán vào cụm ứng với cái tâm đó Giai đoạn cập nhật : duyệt từng cụm, cập nhật lại tọa độ của tâm: Như đã biết, sau giai đoạn 1, chúng ta đã thu được k cụm ứng với dãy các điểm được gán cho từng cụm. Tọa độ tâm mới của cụm sẽ bằng trung bình cộng tọa độ các điểm trong cụm Sau càng nhiều vòng lặp, các tâm càng di chuyển chậm dần, và tổng khoảng cách từ mỗi điểm trong cụm tới tâm cụm lại càng nhỏ đi. Quá trình sẽ kết thúc cho tới khi hàm tổng khoảng cách hội tụ (tức là không có sự thay đổi nào xảy ra ở giai đoạn gán nữa). Lúc này tọa độ tâm vẫn sẽ bằng trung bình cộng các điểm hiện tại trong cụm, hay nói cách khác tâm sẽ không còn di chuyển tiếp nữa.  Chú ý thuật toán K-Means chỉ đảm bảo được quá trình này sẽ đưa hàm tổng khoảng cách hội tụ tới điểm cực tiểu địa phương, chứ không chắc chắn đó là giá trị nhỏ nhất của toàn bộ hàm số . Tuy nhiên, điều này là có thể chấp nhận được vì  KHÔNG  phải mô hình nào càng sát với bộ dữ liệu huấn luyện thì cũng sẽ càng tốt. Ta có thể nhận thấy rằng việc lựa chọn tâm lúc khởi điểm cũng có ảnh hưởng tới kết quả cuối cùng thu được, do đó đã nảy sinh rất nhiều ý kiến trái chiều về vấn đề này. Một ý tưởng đơn giản là cho chạy K-Means nhiều lần với mỗi bộ tâm ngẫu nhiên khác nhau, rồi sau đó chọn ra mô hình tốt nhất thông qua việc xét giá trị nhỏ nhất của các hàm tổng khoảng cách ứng với chúng. Một hướng tiếp cận khác trong việc chọn tâm ban đầu đó là chọn những điểm \"xa nhất\". Việc này có thể cho kết quả tốt hơn, tuy nhiên ta sẽ mắc phải vấn đề với những phần tử \"nhiễu\", đó là những phần tử nằm riêng lẻ một mình tách biệt với phần còn lại trong bộ dữ liệu. Do đó chúng sẽ tự lập ra 1 cụm riêng của chính mình. Có một cách giải quyết đã được phát minh để cân bằng đồng thời được cả 2 điều trên, nó có tên gọi là  K-Means++ : trong đó, tâm khởi đầu vẫn được chọn ngẫu nhiên, nhưng là  chọn lần lượt (thay vì đồng loạt)  và kèm theo  xác suất ngẫu nhiên tỉ lệ thuận với khoảng cách tới điểm tâm vừa chọn trước đó . Tức là, các điểm càng nằm phía xa sẽ có khả năng được chọn làm tâm kế tiếp càng lớn. Do đó, nếu có 1 nhóm các điểm, khả năng chỉ 1 điểm từ nhóm đó được chọn làm tâm cũng sẽ cao hơn. K-Means++ cũng đang được chọn sử dụng cho bước khởi tạo của thuật toán K-Mean trong thư viện  Scikit-learn  của Python. Nếu bạn đang lập trình Python, bạn có thể dùng ngay thư viện này. Đối với Java, thư viện  Weka  sẽ là 1 sự lựa chọn đáng để cân nhắc. Java (Weka) // Load some data\r\nInstances data = DataSource.read(\"data.arff\");\r\n\r\n// Create the model\r\nSimpleKMeans kMeans = new SimpleKMeans();\r\n\r\n// We want three clusters\r\nkMeans.setNumClusters(3);\r\n\r\n// Run K-Means\r\nkMeans.buildClusterer(data);\r\n\r\n// Print the centroids\r\nInstances centroids = kMeans.getClusterCentroids();\r\nfor (Instance centroid: centroids) {\r\n  System.out.println(centroid);\r\n}\r\n\r\n// Print cluster membership for each instance\r\nfor (Instance point: data) {\r\n  System.out.println(point + \" is in cluster \" + kMeans.clusterInstance(point));\r\n} Python (Scikit-learn) >>> from sklearn import cluster, datasets\r\n>>> iris = datasets.load_iris()\r\n>>> X_iris = iris.data\r\n>>> y_iris = iris.target\r\n\r\n>>> k_means = cluster.KMeans(n_clusters=3)\r\n>>> k_means.fit(X_iris)\r\nKMeans(copy_x=True, init='k-means++', ...\r\n>>> print(k_means.labels_[::10])\r\n[1 1 1 1 1 0 0 0 0 0 2 2 2 2 2]\r\n>>> print(y_iris[::10])\r\n[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2] Ở trong ví dụ Python phía trên, ta sử dụng bộ dữ liệu  Iris  chứa kích thước đài hoa và cánh hoa cho 3 giống hoa Iris khác nhau, chia những dữ liệu này thành 3 cụm, rồi sau đó so sánh với giá trị thực tế của chúng, để kiểm tra độ chính xác của thuật toán. Trong trường hợp này, chúng ta thấy rằng dữ liệu được tách thành 3 cụm (ứng với 3 giống hoa) khác nhau và K-Means đã nhận ra chính xác những phần tử nào cùng nằm chung 1 cụm ( Chú ý rằng Unsupervised Learning là bài toán không có nhãn nên chỉ số k bằng (0, 1, 2) ở trên chỉ là ngẫu nhiên, có tác dụng phân biệt chứ không phải là nhãn đầu ra ). Tuy nhiên, làm cách nào mà ta chọn ra được số cụm (k) thích hợp? Câu hỏi tương tự như vậy thường rất phổ biến trong Học máy. Nếu chúng ta yêu cầu nhiều cụm hơn, dữ liệu sẽ được chia nhỏ ra, và giá trị error (tổng khoảng cách) cũng sẽ nhỏ hơn. Vậy, như thế có phải sẽ là tốt hơn nếu như ta chọn k lớn nhất có thể? Chúng ta có thể chọn k = m (số điểm), như thế mỗi điểm sẽ trở thành tâm của chính nó và mỗi cụm sẽ chỉ có 1 điểm? Điều đó không sai, error sẽ bằng 0, nhưng chúng ta sẽ không thể tìm được mô tả đơn giản cho dữ liệu, và mô hình thu được cũng không thể phủ được những điểm mới thêm vào. Vấn đề này có tên gọi là  overfitting , và tất nhiên chúng ta sẽ không mong gặp phải nó. Một cách để giải quyết vấn đề này là bổ sung thêm hàm phạt (penalty) cho số lượng cụm. Từ đó, mục tiêu của ta lúc này không chỉ còn giảm thiểu error, mà phải cân bằng cả  error + penalty . Giá trị error sẽ tiến dần tới 0 khi chúng ta tăng số lượng cụm, nhưng đồng thời penalty cũng tăng theo. Quá trình tăng số lượng cụm sẽ dừng lại khi mà lượng error giảm đi thấp hơn so với giá trị penalty, và kết quả thu được là kết quả tối ưu. Có một giải pháp sử dụng  Bayesian Information Criterion  (BIC) để tính k có tên gọi là X-Means [Pelleg and Moore, 2000]. Một thứ khác chúng ta cần quan tâm đó là hàm khoảng cách. Hiển nhiên, với những điểm nằm trong không gian, khoảng cách Euclid rõ ràng là hiệu quả nhất, nhưng đôi khi ta cần thêm vài \"mánh khóe\" cho những loại dữ liệu đặc trưng khác nhau, ví dụ như các giá trị rời rạc,... Việc này yêu cầu khá nhiều kiến thức chuyên ngành liên quan tới dữ liệu đó. Hoặc, chúng ta có thể nhờ tới sự trợ giúp của Học máy để huấn luyện ra hàm khoảng cách thích hợp nhất. Nếu bạn có 1 tập các dữ liệu huấn luyện (đã biết trước chúng được phân cụm thế nào qua nhãn của chúng), kĩ thuật  Supervised Learning  (học có giám sát) có thể được ứng dụng để tìm ra hàm khoảng cách thích hợp, rồi áp dụng nó vào trong dữ liệu cần phân cụm. Ngoài ra, có 1 thuật toán phân cụm khác có tên là  Expectation-Maximization  (EM) cũng gần tương tự với 2 giai đoạn được dùng trong K-Means. Nói chính xác thì K-Means có thể coi là 1 phiên bản đơn giản hơn của EM. Tuy nhiên, đừng nhầm lẫn chúng với nhau mặc dù có rất nhiều điểm chung giữa 2 thuật toán này. EM Clustering Như vậy, với K-Means: mỗi điểm sẽ được gán cho 1 nhóm và mỗi nhóm được đại diện bởi 1 tâm. Điều này không quá phức tạp, vì chúng ta chưa gặp phải vấn đề cụm chồng chéo, hoặc những cụm có hình dạng khác hình tròn. Với  EM , ta bây giờ có thể tiến một bước xa hơn nữa và đặc tả mỗi cụm bằng tâm của nó (kì vọng), covariance (hiệp phương sai - qua đó ta có thể biểu diễn được cụm hình elip) và weight (kích thước của cụm). Xác suất mà 1 điểm thuộc về 1 cụm bây giờ được tính bằng xác suất phân phối Gauss đa biến.  Chúng ta sẽ bắt đầu EM bằng cách tính, với mỗi điểm, xác suất mà nó thuộc về từng cụm là bao nhiêu (tất nhiên, các cụm ban đầu cũng được khởi tạo ngẫu nhiên). Đây là bước E-step. Nếu 1 cụm là \"ứng viên\" tốt đối với 1 điểm, nó sẽ có xác suất gần với 1. Tuy nhiên, có xảy ra trường hợp 2 hay nhiều cụm cùng là ứng viên tốt, do đó điểm của chúng ta lúc này sẽ có phân phối xác suất giữa các cụm. Tính chất này của thuật toán được gọi là \"soft clustering\". Bước M-step bây giờ tính toán lại các tham số của mỗi cụm, bằng cách sử dụng kết quả xác suất của các điểm được tính ở bước E-step. Để tính toán tâm mới, covariance mới và weight mới của 1 cụm, mỗi dữ liệu điểm sẽ được đánh trọng số tỉ lệ thuận với xác suất biến cố \"điểm đó thuộc cụm\" (lấy từ E-step). Luân phiên 2 bước này sẽ làm tăng giá trị log-likelihood của hàm xác suất cho tới khi giá trị này hội tụ tới cực đại. Nói thêm, tương tự với K-Means, thuật toán EM chỉ cho ta giá trị cực đại địa phương, vì vậy ta có thể sẽ cần phải thực hiện thuật toán nhiều lần để tìm được mô hình tốt hơn nữa. Nếu ta muốn đưa ra quyết định 1 điểm bất kỳ thuộc cụm nào, đơn giản chỉ cần chọn cụm cho ta giá trị xác suất cao nhất ứng với điểm đó. Và ta cũng có thể hoàn toàn tái tạo lại được 1 mẫu tương tự như dữ liệu ban đầu từ mô hình dựa vào dãy các xác suất thu được. Bài viết gốc:  https://www.toptal.com/machine-learning/clustering-algorithms AlphaGo Vs TensorFlow - Trí thông minh nhân tạo trong tay bạn 01/07/2016 \n                                                Techmaster team\n                                             Blog Home TensorFlow vs Google AI - giấc mơ sâu của Google 04/07/2016 \n                                            Techmaster team\n                                         \n                                    Bởi  Phan Đức Việt Tác giả đang bận code dạo (PHP, Java, C#, Nodejs, React) kiếm tiền mua đất cưới vợ nên chưa viết đoạn mô tả. Techmaster.  Quy định  Về chúng tôi Việc làm Về chúng tôi Giảng viên Cơ sở vật chất Contact Ms Khuê: 090.863.6458 khue@techmaster.vn Mr Cường: 090.220.9011 cuong@techmaster.vn Địa chỉ\n                          Số 78, ngõ 106, Hoàng Quốc Việt, Cầu Giấy, Hà Nội  Giờ mở cửa:  9:00  đến  18:00 Đăng nhập Đăng ký Ghi nhớ tài khoản Đăng nhập Quên mật khẩu? Facebook Google Hiển thị mật khẩu \n                    Đăng ký\n                 \n                    Bằng cách nhấp vào Đăng ký, bạn đã đồng ý với các\n                      Quy định   của chúng tôi.\n                  Hỗ trợ trực tuyến",
          "url": "https://techmaster.vn/posts/33893/thuat-toan-phan-cum"
        },
        {
          "title": "Cải tiến thuật toán KMean",
          "relevance": "1",
          "content": "Cải tiến thuật toán KMean Về trang chủ Ý tưởng cải tiến Nhược điểm lớn của đa số các thuật toán gom cụm  theo phương pháp phân hoạch thường là phải xác định trước số cụm. Trong thực tế  rất khó để xác định số cụm cần gom là bao nhiêu thì phù hợp. Để có số cụm phù hợp  một chiến thuật hay được sử dụng là gom cụm với nhiều số cụm khác nhau, sau khi  gom xong thì xem xét các cụm bằng cách quan sát cho đến khi nào thấy phù hợp  thì dừng lại. Tuy nhiên việc xem xét, quan sát đó thường mang tính chủ quan nên  cũng khó có kết quả tốt, nhất là trong trường hợp số cụm và số phần tử cần gom  cụm là lớn. Tác giả luận văn đề xuất một cải tiến cho các thuật toán gom cụm, dựa  trên kết quả gom cụm đó sẽ có một số tính toán các hệ số làm cơ sở khuyến nghị  về việc tăng hay giảm số cụm xuất phát từ các nhận xét sau: Khi hệ số trong α của một cụm (là tỷ số giữa khoảng  cách lớn nhất giữa các phần tử trong cụm và khoảng cách trung bình giữa các phần  tử trong cụm) càng lớn hơn 1 thì cụm đó càng méo (không cân đối). Trong một cụm  càng không cân đối thì tính tương đồng giữa các phần tử trong cụm càng không  cao. Theo lý thuyết gom cụm thì những cụm dạng này cần phải tách cụm. Như vậy  khi hệ số α càng lớn sẽ cho ta dấu hiệu cần phải tách cụm.     Hình : Hình dạng cụm  không cân đối trong các cụm dạng này α thường lớn do dmax lớn. \r\n  Hình : Hình dạng cụm  khá cân đối trong các cụm dạng này α thường nhỏ. \r\n  Nếu  kết hợp thêm dmax (khoảng cách lớn nhất giữa các phần tử trong cụm)  là lớn thì rõ ràng cụm đang xét có kích thước khá lớn và các phần tử trong các  cụm dạng này càng có tính tương đồng không cao. Nên đây cũng là một dấu hiệu phụ  cho ta biết khi nào cần phải tách cụm ngoài dấu hiệu chính từ hệ số trong α ở  trên. Dĩ nhiên khái niệm độ lớn của dmax là tùy thuộc vào từng bài  toán cụ thể. \r\n  Gọi αmax  là hệ số trong lớn nhất trong số các hệ số trong α của tất cả các cụm sau khi  gom cụm. \r\n  Để  có một gom cụm tốt, rõ ràng sau khi gom cụm, mà còn tồn tại một cụm nào đó có hệ  số trong α lớn tức là αmax cũng lớn thì cần phải tách cụm đó tức là  phải tăng số cụm cần gom. \r\n  Do  đó α , αmax và dmax sẽ được tính toán để làm các giá trị  định lượng một cách có cơ sở giúp cho việc định hướng khuyến nghị điều chỉnh số  cụm. \r\n  Khi  hệ số ngoài β (là tỷ số giữa khoảng cách nhỏ nhất từ trung tâm cụm này (giả sử  cụm 1) đến các phần tử của cụm khác (giả sử cụm 2) và khoảng cách trung bình từ  trung tâm cụm này đến các phần tử của cụm khác đó) càng tiến gần đến 1 thì cụm  2 có kích thước khá nhỏ và dẹp (β =1 khi cụm 2 chỉ có một phần tử). Đây là một  dấu hiệu cho thấy xu hướng ghép cụm. Như vậy khi hệ số β càng tiến gần đến 1 sẽ  cho ta dấu hiệu càng cần phải ghép cụm.  \r\n  Hình:  Hình dạng hai cụm có xu hướng ghép vào nhau khi β tiến gần đến 1 \r\nNếu  kết hợp thêm ϕmin (khoảng cách nhỏ nhất từ trung tâm cụm này đến cụm  khác) là nhỏ thì rõ ràng cụm 2 nằm sát biên của cụm 1 và kích thước cụm 2 là nhỏ,  dẹp và phân bố sát theo biên của cụm 1 nên nó có xu hướng ghép vào cụm 1 và nếu  được ghép vào thì hầu như vẫn giữ được tính tương đồng trong cụm mới. Nên đây  cũng là một dấu hiệu phụ cho ta biết khi nào cần phải ghép cụm ngoài dấu hiệu  chính từ hệ số trong β ở trên. Dĩ nhiên khái niệm độ lớn hay nhỏ của ϕmin  là tùy thuộc vào từng bài toán cụ thể. Gọi  βmax là hệ số ngoài lớn nhất trong số các hệ số ngoài β sau khi gom  cụm. Để có một gom cụm tốt, rõ ràng sau khi gom cụm,  mà có một hay nhiều cụm nào đó có hệ số ngoài β bằng 1 tức là βmax  cũng bằng 1 thì đó là chỉ dấu cho thấy có khả năng cần phải ghép cụm (càng có  nhiều βmax bằng 1 chỉ dấu càng rõ) tức là phải giảm số cụm cần gom. Do đó β, βmax và ϕmin sẽ  được tính toán để làm các giá trị định lượng một cách có cơ sở giúp cho việc định  hướng khuyến nghị điều chỉnh số cụm. Xuất phát từ những phân tích trên, Chúng tôi đề  xuất thuật toán cải tiến  Chạy  thuật toán K-Means có trọng số lần đầu với số cụm nào đó. Tính  toán các thông số α, dmax, β, ϕmin. Tính  αmax là hệ số trong lớn nhất trong số các hệ số trong α của các cụm. Tính  βmax là hệ số ngoài lớn nhất trong số các hệ số ngoài β. Dựa  trên kết quả tính toán các hệ số khuyến nghị điều chỉnh số cụm α, αmax,  β, βmax đó,  nếu các hệ số này  thể hiện số cụm hiện tại là phù hợp thì dừng lại, ngược lại điều chỉnh lại số cụm  theo khuyến nghị rồi chạy lại thuật toán cho đến khi phù hợp. Các bước của thuật toán K-Means cải tiến như  sau: Đưa vào bảng chứa dữ liệu  cần gom cụm gồm n đối tượng xi, i = 1..n Chọn tham số mờ hóa m  > 1, tham số hội tụ epsilon đủ nhỏ. Chọn véctơ trọng số W có  k thành phần, k là số thuộc tính của xi sao cho:  Đưa vào số cụm cần phân  hoạch c (2 ≤ c < n). Khởi tạo ma trận thành  viên U (c x n) với 0 ≤  μij  ≤ 1 sao cho: ,     j = 1…n Tính trọng tâm Cj của  cụm j (j = 1…c) gồm k thành phần, mỗi thành phần của nó được tính như sau:                              Cjl  =       với  j = 1…c,    l = 1,..k Cập nhật ma trận khoảng  cách D (c x n) theo độ đo khoảng cách đã chọn dji là khoảng cách từ  xi đến Cj dji  =  \r\nVới k là số thuộc tính của xi,    j = 1,..c,      i = 1,…n Cập nhật ma trận thành  viên U Nếu  dji > 0 thì   μji =   \r\nNgược  lại nếu dji = 0 thì xji trùng với trọng tâm Cj  của cụm j, μji = 1. Nếu sự thay đổi của ma trận  U là đủ nhỏ so với bước kế trước thì chuyển đến bước 10. Ngược lại thì lặp lại  từ bước 6. Để xác định  là U thay đổi nhỏ chúng tôi dùng: |  -  | < epsilon.   Với nghĩa   là   tại bước lặp thứ  n. Dựa  trên ma trận U, sắp xếp các đối tượng dữ liệu xi, cùng độ thuộc lớn  nhất của nó vào các cụm theo qui tắc xét  độ thuộc của đối tượng dữ liệu đó với từng cụm, đối tượng dữ liệu  sẽ thuộc vào cụm nào có độ thuộc lớn nhất, nếu có từ hai độ thuộc lớn nhất bằng  nhau trở lên thì chọn một trong số các cụm đó để đưa vào.  Đưa  ra câu hỏi: “Có muốn thực hiện chức năng tính toán các hệ số khuyến nghị điều  chỉnh số cụm số không ?” . Nếu trả lời “Có” thì chuyển đến bước 12, còn trả lời  “Không” thì kết thúc. Tính  hệ số trong α và tìm  αmax qua các bước  tính tuần tự sau: Tính khoảng cách lớn nhất giữa các phần tử trong cụm:  dmax  =      \r\nVới  j = 1,..p,   i = 1,…p,  i # j , p là số phần tử  của cụm, k là số thuộc tính, còn q là số khoảng cách giữa các phần tử  trong cụm. Tính khoảng cách trung  bình giữa các phần tử trong cụm: davr  =         \r\nVới  j = 1,..p,  i = 1,…p,   i # j , p là số phần tử của cụm, k là số thuộc tính, còn q là  số khoảng cách giữa các phần tử trong cụm. Tính hệ số trong α:            α  =  Tính hệ số trong lớn nhất αmax:      αmax   =     với c là số cụm. Tính  hệ số ngoài β của từng  cụm đối với các cụm còn lại  và tìm βmax  qua các bước tính tuần tự sau: Tính khoảng cách nhỏ nhất từ trung tâm cụm j đến các phần tử  trong cụm x:  ϕmin  =    \r\nVới Cj  là trung tâm của cụm j, i = 1,…q,  với q  là số phần tử của cụm x,  k là số thuộc  tính và q cũng chính là số khoảng cách từ trung tâm cụm j đến q phần tử của cụm  x. Tính khoảng cách trung  bình giữa trung tâm cụm j đến các phần tử trong cụm x: ϕavr  =         \r\nVới Cj  là trung tâm của cụm j, i = 1,…q,  với q  là số phần tử của cụm x,  k là số thuộc  tính và q cũng chính là số khoảng cách từ trung tâm cụm j đến q phần tử của cụm  x. Tính hệ số ngoài β:       β   =  Tính hệ số ngoài lớn nhất βmax:      βmax  =     với c là số cụm. Dựa trên kết quả tính toán các hệ số  khuyến nghị điều chỉnh số cụm α, αmax, β, βmax nếu xét thấy  việc gom cụm chưa phù hợp thì quay lại bước 4 điều chỉnh số cụm theo chỉ dấu khuyến nghị, ngược lại thuật  toán kết thúc. Video Demo Cải tiến KMean  Download Demo Hệ điều hành: Windows XP, 7,8  \r\nMicrosoft Frameworks: 4.0 File chạy : Gom_Cum_Hoc_Sinh.exe Liên hệ Email :  vinhvinhit@gmail.com. Trao đổi thông tin cải tiến KMeans.  (Nhận coding thuật toán KMeans cải tiến theo yêu cầu, đề tài, ngôn ngữ C#) \r\nLàm luận văn cải tiến thuật toán KMeans.",
          "url": "http://ungdung.khoa-hnvd.com/Hoc_thuat/Cai_tien_KMeans.html"
        },
        {
          "title": "Kmean một kỹ thuật phân nhóm dữ liệu phổ biến",
          "relevance": "1",
          "content": "Bản tin AGU Phóng sự Ảnh Khoa học với AGU Câu chuyện AGU Góc nhìn Tản mạn Gương mặt SV AGU SV với Câu lạc bộ eNews và Bạn đọc Lướt web cùng SV Tìm kiếm... Tìm kiếm... Trang chủ SV với Câu lạc bộ CLB Tin học Kmean một kỹ thuật phân nhóm dữ liệu phổ biến  Kmean một kỹ thuật phân nhóm dữ liệu phổ biến  In bài này   Gửi Email bài này \r\n\tThuật toán phân nhóm K-Means do MacQueen giới thiệu trong tài liệu “J. Some Methods for Classification and Analysis of Multivariate Observations” năm 1967. K-Means là thuật toán rất quan trọng và được sử dụng phổ biến trong kỹ thuật phân cụm. Tư tưởng chính của thuật toán K-Means là tìm cách phân nhóm các đối tượng (objects) đã cho vào K cụm (K là số các cụm được xác đinh trước, K nguyên dương) sao cho tổng bình phương khoảng cách giữa các đối tượng đến tâm nhóm (centroid ) là nhỏ nhất. Ý tưởng giải thuật \r\n\tVề nguyên lý, có n đối tượng, mỗi đối tượng có m thuộc tính, ta phân chia được các đối tượng thành k nhóm dựa trên các thuộc tính của đối tượng bằng việc áp dụng thuật toán này. Coi mỗi thuộc tính của đối tượng (đối tượng có m thuộc tính) như một toạ độ của không gian m chiều và biểu diễn đối tượng như một điểm của không gian m chiều \r\n\tPhương thức phân loại/nhóm dữ liệu thực hiện dựa trên khoảng cách Euclidean nhỏ nhất giữa đối tượng đến phần tử trung tâm của các nhóm. \r\n\tPhần tử trung tâm của nhóm được xác định bằng giá trị trung bình các phần tử trong nhóm Khoảng cách Euclidean \r\n\tKhoảng cách Euclidean từ đối tượng ai đến phần tử trung tâm nhóm j c j  được tính toán dựa trên công thức: Phần tử trung tâm \r\n\tPhần tử trung tâm của nhóm được xác định bằng giá trị trung bình các phần tử trong nhóm. \r\n\tk phần tử trung tâm (k nhóm) ban đầu được chọn ngẫu nhiên, sau mỗi lần nhóm các đối tượng vào các nhóm, phần tử trung tâm được tính toán lại.  Thuật toán \r\n\tKhởi tạo k phần tử trung tâm một cách ngẫu nhiên (mỗi phần tử trung tâm đại diện cho một nhóm) \r\n\tThực hiện các bước cơ bản sau cho đến khi tất cả các đối tượng được phân loại và không còn còn sự thay đổi của các đối tượng đến các nhóm: \r\n\t1. Chọn ngẫu nhiên K tâm (centroid) cho K cụm (cluster). Mỗi cụm được đại diện bằng các tâm của cụm. \r\n\t2. Tính khoảng cách giữa các đối tượng (objects) đến K tâm (thường dùng khoảng cách Euclidean) \r\n\t3. Nhóm các đối tượng vào nhóm gần nhất \r\n\t4. Xác định lại tâm mới cho các nhóm \r\n\t5. Thực hiện lại bước 2 cho đến khi không có sự thay đổi nhóm nào của các đối tượng Lược đồ mô tả giải thuật Đánh giá giải thuật \r\n\tThuật toán K-means có ưu điểm là dễ dàng cài đặt cho kết quả dễ hiểu, nhưng lại có nhược điểm là phải chỉ ra số lượng cluster và yêu cầu CSDL cần phân nhóm phải xác định được tâm. Thuật toán này không phù hợp với việc khai phá các dữ liệu gồm các cluster có hình dạng không lồi và KMeans hay gặp lỗi với các dữ  liệu có phần tử ngoại lai (outliers). \r\n\tĐối với các tập dữ liệu có số chiều lớn dữ liệu có nhiều phần tử nhiễu như các tập dữ liệu biểu hiện gien thì giải thuật Kmeans thực hiện không đạt hiệu quả cao (Dipti 2015). \r\n\tBên cạnh đó trong một số tập dữ liệu không phải lúc nào mỗi đối tượng cũng chỉ thuộc về 1 cụm, chỉ phù hợp với đường biên giữa các cụm rõ trong trường hợp này cần sử dụng các giải thuật BisClustering \r\n\t  CLB Tin Học  Viết lời bình Họ và tên (*): Email: \n\t\tVui lòng gõ văn bản trong hình vào ô bên dưới\t\t Chọn mã số khác Gửi lời bình Theo dõi \n\t\t\tChưa có lời bình cho bài viết này.\t\t Kể chuyện học tập Mùa xuân tình yêu Cuộc sống muôn màu Ơn Thầy Áo xanh tình nguyện Thư viện của tôi Liên kết nội bộ Đại học An Giang Thư viện Thư điện tử Lượt truy cập Từ ngày 09/09/2016 Hôm nay 92 Hôm qua 112 Tuần này 92 \n\t\t\t\t\tTrở lên trên\t\t\t\t \n\t\t\t\tPhụ trách chung e-News   ThS. Nguyễn Thị Bích Châu  \n\t\t\t\t© 2017 e-News   Số 18 Ung Văn Khiêm, Đông Xuyên, Long Xuyên, An Giang   +84 296 625 6565 nhánh 1605  enews@agu.edu.vn",
          "url": "http://enews.agu.edu.vn/index.php?option=com_content&view=article&id=18545&Itemid=128"
        },
        {
          "title": "Phương pháp K-Means - Cluster không thứ bậc",
          "relevance": "1",
          "content": "Vietlod Kinh tế lượng Home Bài đã đăng Bài sắp đăng Cách đăng ký Xem dạng Blog Tin tức KTL CƠ BẢN Một số thuật ngữ thường dùng trong SEM Một số lưu ý về lệnh collpase Tính toán xác suất trên Stata Xử lý dữ liệu Hồi quy OLS Hồi quy logit Kiểm định thống kê KTL NÂNG CAO Đánh giá tính hợp lý của mô hình cấu trúc Xác định mô hình cấu trúc trong SEM Biến đổi mô hình chưa phù hợp thành phù hợp OLS nâng cao Biến nội sinh Dữ liệu bảng Dữ liệu thời gian Nghiên cứu sự kiện Phân tích nhân tố, SEM Hướng nghiên cứu Các bảng tra Đánh giá ý nghĩa thống kê của các hệ số tải Tính toán xác suất trên Stata Bảng tra hệ số tương quan Pearson Tra dL, dU Bảng tra t Bảng tra F Bảng tra Chi2 ADF – Unit root test Tương quan Pearson ĐỀ THI TRẮC NGHIỆM Kinh tế học Kinh tế lượng Ngành kế toán Ngành quản trị Ngành tài chính Học phần Đại cương Home   |   KTL nâng cao   |   Phương pháp K-Means – Cluster không thứ bậc Phương pháp K-Means – Cluster không thứ bậc Mục   KTL nâng cao 1,321 lượt xem I. GIỚI THIỆU VỀ PHƯƠNG PHÁP K-MEANS Tiếp tục bài viết  phân tích cluster thứ bậc , bài viết này sẽ hướng dẫn thực hành phân tích cluster không thứ bậc theo phương pháp K-Means. Nhắc lại một số nguyên tắc cơ bản của phân tích cluster không thứ bậc theo phương pháp K-Means Ngược lại với phân tích cluster thứ bậc, phương pháp phân tích cluster không thứ bậc (Non Hierarchical Cluster Analysis) không bao gồm quy trình phân nhóm hình cây mà thay vào đó là phân bổ các đối tượng vào số lượng các cluster đã xác định trước. Phân tích cluster không thứ bậc bao gồm 3 phương pháp chủ yếu là:  (i) phương pháp ngưỡng tuần tự (Sequential threshold); (ii) phương pháp ngưỡng song song (Parallel threshold); (iii) phương pháp phân chia tối ưu (Optimizing partitioning) . Các phương pháp này còn được gọi chung là  phương pháp K-Means . Phần nội dung có thu phí bên dưới đã được ẩn. Vui lòng  đăng nhập  hoặc  đăng ký  gói Premium. Trân trọng! Trang: 1  2 3 Factor SPSS 2014-10-12 +Thuyết Nguyễn Thẻ:  Factor SPSS Bài liên quan Đánh giá tính hợp lý của mô hình cấu trúc Xác định mô hình cấu trúc trong SEM Biến đổi mô hình chưa phù hợp thành phù hợp CHUYÊN MỤC Home Bài đã đăng Bài sắp đăng Cách đăng ký Xem dạng Blog Tin tức KTL CƠ BẢN Xử lý dữ liệu Hồi quy OLS Hồi quy logit Kiểm định thống kê KTL NÂNG CAO OLS nâng cao Biến nội sinh Dữ liệu bảng Dữ liệu thời gian Nghiên cứu sự kiện Phân tích nhân tố, SEM Hướng nghiên cứu Các bảng tra Tra dL, dU Bảng tra t Bảng tra F Bảng tra Chi2 ADF – Unit root test Tương quan Pearson ĐỀ THI TRẮC NGHIỆM Kinh tế học Kinh tế lượng Ngành kế toán Ngành quản trị Ngành tài chính Học phần Đại cương ĐĂNG KÝ TÀI KHOẢN Hỗ trợ Nghiên cứu KH \n\t\t\t© Copyright 2014 VietLOD\t\t\t",
          "url": "https://vietlod.com/phuong-phap-k-means-spss"
        },
        {
          "title": "SỬ DỤNG THUẬT TOÁN GOM CỤM MỜ KHAI PHÁ CƠ SỞ DỮ LIỆU ERP TRONG DOANH NGHIỆP DƯỢC PHẨM",
          "relevance": "0",
          "content": "Gửi bài Liên Kết Hướng dẫn Người dùng Bí danh Mật khẩu Ghi nhớ Trang nhất Giới thiệu Đăng nhập Đăng ký Tìm kiếm Số mới ra Số cũ Thông báo Trang nhất  >\n\t Tập 02, Số 2, 2012  >\t Thuân SỬ DỤNG THUẬT TOÁN GOM CỤM MỜ KHAI PHÁ CƠ SỞ DỮ LIỆU ERP TRONG DOANH NGHIỆP DƯỢC PHẨM Nguyễn Đình Thuân, Đoàn Huấn Tóm tắt Thuật toán gom cụm Fuzzy c-means và Fuzzy c-means kết hợp véctor  trọng số được được sử dụng  trong bài báo này nhằm gom cụm dữ  liệu  trong cơ  sở dữ  liệu ERP doanh nghiệp. Mục đích  là  thực nghiệm, so sánh hai  thuật  toán này  trong việc  tìm kiếm các nhóm dữ liệu phù hợp cụ thể ở đây là các nhóm khách hàng có cùng những đặc tính như doanh số, lợi nhuận, số tiền thanh toán, nợ quá hạn... nhằm hỗ trợ cho ban lãnh đạo doanh nghiệp có các quyết định đúng đắn khi đề ra các chính sách  thích ứng cho  từng nhóm khách hàng này, từ đó gia tăng hiệu quả hoạt động kinh doanh cho doanh nghiệp. Từ khóa Gom cụm dữ liệu; gom cụm mờ Toàn văn: PDF Trích dẫn ĐOÀN HUẤN, Các tài liệu, hồ sơ khảo sát, phân tích thiết kế phần mềm ERP, Công ty Cổ phần Giải pháp Phần mềm EnterSoft. 2001-2011. NGUYEN BICH LIEN, DO PHUC. An application of data mining to revenue cycle in ERP and E-commerce environment. Proceedings of the sixth international conference on Information Technology for Education and Research. 2010. TAOYING LI AND YAN CHEN. Fuzzy K-Means Incremental Clustering Based on K-Center and Vector Quantization. Journal of computers, Vol 5, No 11, Nov 2010. ĐỖ PHÚC. Giáo trình Khai thác dữ liệu, Nhà xuất bản Đại học Quốc gia TPHCM - 2009. Refbacks Hiện không có phản hồi",
          "url": "http://jbis.ueh.edu.vn/index.php/TSTHQL/article/view/20"
        },
        {
          "title": "Thuật toán K-Means với bài toán phân cụm dữ liệu",
          "relevance": "1",
          "content": "\r\n\t\t\t                            \r\n        Chào mừng đến với BIS\r\n         Đăng nhập  \r\n         |  Đăng ký \r\n        |  Trợ giúp \r\n\t\t\t\t\t                                    \r\n\t\t\t\t\t\t                                         trong \r\n\t\t\t\t\t\t                                         Data Mining and Business Intelligence... Data Mining and Business Intelligence... (Entire Site) Tìm kiếm BIS  »  Data Mining and Business Intelligence  »  Data Mining and Business Intelligence  »  Thuật toán K-Means với bài toán phân cụm dữ liệu Thuật toán K-Means với bài toán phân cụm dữ liệu Bài cuối 12-23-2015 10:50 AM của  TraMy . 35 trả lời. Trang 1 trong số 2 (36 nội dung) 1  2   Tiếp theo > \r\n\t\t\t\t        \r\n\t\t\t\t                Sắp xếp bài viết:\r\n\t\t\t\t                 Cũ đến mới Mới đến cũ Trước Tiếp theo \r\n\t\t\t\t\t\t\t\t        12-02-2010 11:49 AM    \r\n\t\t\t\t\t\t\t\t     chucnv Tham gia 12-05-2008 Điểm 9,440 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Thuật toán K-Means với bài toán phân cụm dữ liệu Nguyễn Văn Chức –  chuc1803@gmail.com   1.Giới thiệu về kỹ thuật phân cụm trong Khai phá dữ liệu (Clustering Techniques in Data Mining)  Phân cụm là kỹ thuật rất quan trọng trong khai phá dữ liệu, nó thuộc lớp các phương pháp  Unsupervised Learning  trong Machine Learning. Có rất nhiều định nghĩa khác nhau về kỹ thuật này, nhưng về bản chất ta có thể hiểu  phân cụm là các qui trình tìm cách nhóm các đối tượng đã cho vào các cụm (clusters), sao cho các đối tượng trong cùng 1 cụm tương tự (similar) nhau và các đối tượng khác cụm thì không tương tự (Dissimilar) nhau . Mục đích của phân cụm là tìm ra bản chất bên trong các nhóm của dữ liệu. Các thuật toán phân cụm (Clustering Algorithms) đều sinh ra các cụm (clusters). Tuy nhiên, không có tiêu chí nào là được xem là tốt nhất để đánh hiệu của của phân tích phân cụm, điều này phụ thuộc vào mục đích của phân cụm như: data reduction, “natural clusters”, “useful” clusters, outlier detection  Kỹ thuật phân cụm có thể áp dụng trong rất nhiều lĩnh vực như: Marketing: Xác định      các nhóm khách hàng (khách hàng tiềm năng, khách hàng giá trị, phân loại      và dự đoán hành vi khách hàng,…) sử dụng sản phẩm hay dịch vụ của công ty      để giúp công ty có chiến lược kinh doanh hiệu quả hơn; Biology: Phận nhóm động vật và thực vật dựa vào các thuộc      tính của chúng; Libraries:  Theo dõi      độc giả, sách, dự đoán nhu cầu của độc giả…; Insurance, Finance: Phân nhóm các đối tượng sử dụng bảo hiểm      và các dịch vụ tài chính, dự đoán xu hướng (trend) của khách hàng, phát      hiện gian lận tài chính (identifying frauds); WWW:  Phân loại tài      liệu (document classification);  phân loại người dùng web (clustering      weblog);…   Các kỹ thuật phân cụm được phân loại như sau (xem hình) 2. Thuật Toán K-Means K-Means là thuật toán rất quan trọng và được sử dụng phổ biến trong kỹ thuật phân cụm. Tư tưởng chính của thuật toán K-Means là tìm cách phân nhóm các đối tượng (objects) đã cho vào K cụm (K là số các cụm được xác đinh trước, K nguyên dương) sao cho tổng bình phương khoảng cách giữa các đối tượng đến tâm nhóm (centroid ) là nhỏ nhất. Thuật toán K-Means được mô tả như sau    Thuật toán K-Means thực hiện qua các bước chính sau: 1.      Chọn ngẫu nhiên K tâm (centroid) cho K cụm (cluster). Mỗi cụm được đại diện bằng các tâm của cụm. 2.      Tính khoảng cách giữa các đối tượng (objects) đến K tâm (thường dùng khoảng cách Euclidean) 3.      Nhóm các đối tượng vào nhóm gần nhất 4.      Xác định lại tâm mới cho các nhóm 5.      Thực hiện lại bước 2 cho đến khi không có sự thay đổi nhóm nào của các đối tượng   Ví dụ minh họa thuật toán K-Mean: Giả sử ta có 4 loại thuốc A,B,C,D, mỗi loại thuộc được biểu diễn bởi 2 đặc trưng X và Y như sau. Mục đích của ta là nhóm các thuốc đã cho vào 2 nhóm (K=2) dựa vào các đặc trưng của chúng. Bước 1.  Khởi tạo tâm (centroid) cho 2 nhóm. Giả sử ta chọn A là tâm của nhóm thứ nhất (tọa độ tâm nhóm thứ nhất c1(1,1)) và B là tâm của nhóm thứ 2 (tạo độ tâm nhóm thứ hai c2 (2,1)).   Bước 2.  Tính khoảng cách từ các đối tượng đến tâm của các nhóm (Khoảng cách Euclidean) Mỗi cột trong ma trận khoảng cách (D) là một đối tượng (cột thứ nhất tương ứng với đối tượng A, cột thứ 2 tương ứng với đối tượng B,…). Hàng thứ nhất trong ma trận khoảng cách biểu diễn khoảng cách giữa các đối tượng đến tâm của nhóm thứ nhất (c1) và hàng thứ 2 trong ma trận khoảng cách biểu diễn khoảng cách của các đối tượng đến tâm của nhóm thứ 2 (c2). Ví dụ, khoảng cách từ loại thuốc C=(4,3) đến tâm c1(1,1) là 3.61  và đến tâm c2(2,1) là 2.83 được tính như sau:  Bước 3.  Nhóm các đối tượng vào nhóm gần nhất Ta thấy rằng  nhóm 1 sau vòng lặp thứ nhất gồm có 1 đối tượng A và nhóm 2 gồm các đối tượng còn lại B,C,D. Bước 5.  Tính lại tọa độ các tâm cho các nhóm mới dựa vào tọa độ của các đối tượng trong nhóm. Nhóm 1 chỉ có 1 đối tượng A nên tâm nhóm 1 vẫn không đổi, c1(1,1). Tâm nhóm 2 được tính như sau: Bước 6.  Tính lại khoảng cách từ các đối tượng đến tâm mới   Bước 7.  Nhóm các đối tượng vào nhóm Bước 8. Tính lại tâm cho nhóm mới   Bước 8.  Tính lại khoảng cách từ các đối tượng đến tâm mới Bước 9.  Nhóm các đối tượng vào nhóm Ta thấy G 2  = G 1  (Không có sự thay đổi nhóm nào của các đối tượng) nên thuật toán dừng và kết quả phân nhóm như sau:  Thuật toán K-Means có ưu điểm là đơn giản, dễ hiểu và cài đặt. Tuy nhiên, một số hạn chế của K-Means là hiệu quả của thuật toán phụ thuộc vào việc chọn số nhóm K (phải xác định trước) và chi phí cho thực hiện vòng lặp tính toán khoảng cách lớn khi số cụm K và dữ liệu phân cụm lớn. 3. Triển khai ứng dụng phân cụm với phần mềm WeKa Trong ví dụ này, tôi sẽ giới thiệu cách xây dựng một KnowledgeFlow để triển khai kỹ thuật phân cụm dựa trên thuật toán K-Means trên Data Mining Software WeKa. Dữ liệu dùng để phân cụm trong ví dụ này là dữ liệu dùng để phân loại khách hàng của ngân hàng (file dữ liệu  bank.arff ). bank.arff gồm có 11 thuộc tính và 600 khách hàng (instances). Dưới đây là cấu trúc và phân bố dữ liệu của bank.arff Các bạn có thể Down file bank.arff tại đây: Nhiệm vụ của chúng ta là dùng thuật toán K-Means để phân nhóm các khách hàng vào K nhóm (trong ví dụ này K=5) dựa vào sự tương tự (similar) trên11 thuộc tính của họ.  Ta xây dựng một KnowledgeFlow trong WeKa như sau:   Thiết lập các tham số cho thuật toán K-Means như số cụm (trong ví dụ này K=5), Cách tính khoảng cách (trong ví dụ này dùng khoảng cách Euclidean),…     Kết quả phân cụm chi tiết như sau:   PS. The next topic is SOM (Self Organizing Maps) in Clustering Techniques. All comments please send to chucnv@ud.edu.vn. Từ khóa đại diện:  BI ,  Data Mining ,  K-Means ,  Khai phá dữ liệu Điểm chủ đề: 200 \r\n\t\t\t\t\t\t\t\t        12-02-2010 03:12 PM    \r\n\t\t\t\t\t\t\t\t      trả lời  hiengiang Tham gia 11-11-2010 Điểm 430 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Cảm ơn anh Chức rất nhiều. Bài viết ngắn gọn nhưng đầy đủ và dễ hiểu về K-Means. Em rất quan tâm đến việc xây dựng một knowledgeFlow trong weka mà chưa làm được. Anh co thể chỉ giúp em qui trình cụ thể để xây dựng KF trong weka được không? hay anh co tài liệu nào hướng dẫn về KF không cho em xin với. Cảm ơn anh nhiều, Hiền Giang  Điểm chủ đề: 50 \r\n\t\t\t\t\t\t\t\t        12-02-2010 05:05 PM    \r\n\t\t\t\t\t\t\t\t      trả lời  chucnv Tham gia 12-05-2008 Điểm 9,440 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ  Tài liệu về KnowledgeFlow in Weka em có thể download trên website của Weka: http://www.cs.waikato.ac.nz/ml/weka/index.html  Qui trình xây dựng KnowledgeFlow để Mining về cơ bản gồm các bước sau: 1. Nạp dữ liệu (Loading Data): DataSources > Arff Loader 2. Sử dụng Cross Validation để tách dữ liệu thành 2 phần là Training data và Testing Data 3. Sử dụng các thuật toán Data Mining để xây dựng Model (Classification or Clustering algorithms):  4. Đánh giá mô hình (Testing Model):Performance Evaluator  5. Xem kết quả (Visualization) Điểm chủ đề: 20 \r\n\t\t\t\t\t\t\t\t        04-25-2011 01:51 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  nhung.vttn@gmail.com Tham gia 04-24-2011 Điểm 35 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Cảm ơn về bài viết rất bổ ích của anh. Anh có thể cung cấp thêm một số thông tin về thông số seed trong WEKA và Vitualize Tool được không ạ?   Từ khóa đại diện:  BI ,  Data Mining ,  Weka ,  Data Mining Softwares Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        04-25-2011 10:05 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  chucnv Tham gia 12-05-2008 Điểm 9,440 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ \r\n\t\t\t\t\t\t\t\t\t\t    K means sẽ khởi tạo tâm (centroid) của các clusters bằng cách chọn ngẫu nhiên các mẫu (instance) trong traning data và giá trị của tham số seed trong K Means đơn giản chỉ là giá trị dùng để khởi tạo bộ sinh số ngẫu nhiên.\r\n\t\t\t\t\t\t\t\t\t\t    \r\n\t\t\t\t\t\t\t\t\t     Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        06-02-2011 03:49 PM    \r\n\t\t\t\t\t\t\t\t      trả lời  hoangbong Tham gia 06-02-2011 Điểm 55 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ \r\n\t\t\t\t\t\t\t\t\t\t    vay xin hoi anh ky hon mot chut. so seed co the nhan gia trị âm hay bang 0 duoc khong? xin cam on anh\r\n\t\t\t\t\t\t\t\t\t\t    \r\n\t\t\t\t\t\t\t\t\t     Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        06-03-2011 08:50 PM    \r\n\t\t\t\t\t\t\t\t      trả lời  chucnv Tham gia 12-05-2008 Điểm 9,440 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Trong kỹ thuật sinh số ngẫu nhiên (random), thông thường cho phép lặp lại các giá trị do bộ ngẫu nhiên sinh ra. Chẳn hạn như hàm  rand(value1, value2)  sinh ra các số ngẫu nhiên trong khoảng value1 và Value2 và các số này có thể xuất hiện lại nhiều lần. Để bộ sinh số ngẫu nhiên không sinh ra các số trùng nhau ta sử dụng tham số  seed,  tham số seed chính là điểm bắt đầu để sinh số ngẫu nhiên và nó là số tự nhiên. Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        06-06-2011 06:08 PM    \r\n\t\t\t\t\t\t\t\t      trả lời  hoangbong Tham gia 06-02-2011 Điểm 55 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ \r\n\t\t\t\t\t\t\t\t\t\t    vậy xin hỏi sao em cho seed nhận giá trị âm vẫn chạy được. thuật toán k - mean vẫn gom cụm dữ liệu trong trường hợp này26\r\n\t\t\t\t\t\t\t\t\t\t    \r\n\t\t\t\t\t\t\t\t\t     Điểm chủ đề: 20 \r\n\t\t\t\t\t\t\t\t        06-08-2011 08:11 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  huechu Tham gia 06-07-2011 Điểm 90 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Em chào thầy! Cảm ơn thầy về bài \"Thuật toán Kmeans\", nó rất hay và bỗ ích. Sau khi đọc bài viết em có câu hỏi muốn nhờ thầy giúp đỡ là: Trong SQL 2008 nếu dùng Data warehouse làm cơ sỡ dữ liệu( chứa rấ lớn dữ liệu) thì làm sao mình có thể biểu diễn nó trong hệ trục tọa độ giống như ví dụ của thầy (chỉ có 2 thuộc tính trong hệ trục tọa độ 2 chiều: X,Y), Làm sao để xác định được khoảng cách từ đối tượng đến trọng tâm( centroid) Em xin chân thành cảm ơn thầy. Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        06-08-2011 10:32 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  chucnv Tham gia 12-05-2008 Điểm 9,440 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ  Trong ví dụ trên, vì mỗi đối tượng được biểu diễn bởi 2 thuộc tính (2 chiều) nên biểu diễn bằng tọa độ 2 chiều X,Y chỉ để minh họa cho trực quan thôi còn trong thực tế dữ liệu  thường nhiều chiều (mỗi thuộc tính là một chiều) thì việc biểu diễn bằng không gian đa chiều (Multi Dimensional Space) rất phức tạp Nếu p và q là 2 điểm trong không gian n chiều (đối tượng được mô tả bởi n thuộc tính) p  = ( p 1 ,  p 2 ,...,  p n ) và q = ( q 1 ,  q 2 ,...,  q n ). Công thức tính khoảng cách  Euclidean  giữa p và q như sau: Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        06-08-2011 04:37 PM    \r\n\t\t\t\t\t\t\t\t      trả lời  huechu Tham gia 06-07-2011 Điểm 90 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Em cảm ơn thầy về câu trả lời ở bài viết trước,    Nhưng em có một  thắc mắc là: Làm thế nào mình có thể biểu diễn nó trong trong gian nhiều chiều đó. Làm thế nào mình xác định được thuộc tính A là ở tọa độ này, thuộc tính B là ở tọa độ khác....Mình gán tọa độ đó dựa theo cơ sở nào? Điểm chủ đề: 20 \r\n\t\t\t\t\t\t\t\t        06-08-2011 07:38 PM    \r\n\t\t\t\t\t\t\t\t      trả lời  chucnv Tham gia 12-05-2008 Điểm 9,440 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ  Khái niệm về chiều (Dimension) ở đây được hiểu là thuộc tính của đối tượng. Thông thường thi khi số chiều nhiều hơn 3 thì việc biểu diễn trong không gian đa chiều không có ý nghĩa trực quan và rất phức tạp. Một thực tế là số chiều của dữ liệu thường rất lớn (có thể lên đến vài chục thuộc tính). Trong Data Mining Technique, có một số kỹ thuật để hạn chế số chiều của đối tượng (gọi là Feature Selection) nhằm loại bỏ các thuộc tính không ảnh hưởng đến mô hình, cũng như để biểu diễn các đối tượng đa chiều trong không gian 2 hoặc 3 chiều. Tiêu biểu kỹ thuật này là Self Organizing Map - SOM. Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        09-20-2011 09:08 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  quanglamit Tham gia 09-10-2011 Điểm 55 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ \r\n\t\t\t\t\t\t\t\t\t\t    Em đang làm bài tập về gom nhóm dữ liệu nhưng dùng thuật toán AGNES với Single Adjusted Complete Link, Average Link, Mean Link và Centroid Link. Em chưa hiểu rõ những khái niệm này lắm vì trên lớp cô chỉ cho tụi em làm với Single Link và Complete. Anh có thể giải thích cho em những khái niệm về Single Adjusted Complete Link, Average Link, Mean Link và Centroid Link được ko ạ? Em cám ơn anh rất nhiều.\r\n\t\t\t\t\t\t\t\t\t\t    \r\n\t\t\t\t\t\t\t\t\t     Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        09-20-2011 10:16 PM    \r\n\t\t\t\t\t\t\t\t      trả lời  chucnv Tham gia 12-05-2008 Điểm 9,440 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Có rất nhiều cách tính Linkage để chọn tiêu chí phân cụm, trong đó các cách sau đây thường được sử dụng  •  Single Linkage : Khoảng cách giữa 2 clusters được tính là khoảng cách giữa 2 đối tượng gần nhau nhất trong 2 clusters đó (minimum distance). Xem hình dưới •   Complete Linkage : Khoảng cách giữa 2 clusters được tính là khoảng cách giữa 2 đối tượng xa nhau nhất trong 2 clusters đó (maximum distance).  •   Average Group : Khoảng cách giữa 2 clusters được tính là khoảng cách trung bình giữa các đối tượng trong 2 clusters đó (average distance). •  Centroid distance  : Khoảng cách giữa 2 clusters được tính là khoảng cách của 2 tâm của 2 clusters đó (Centroid distance). Điểm chủ đề: 50 \r\n\t\t\t\t\t\t\t\t        09-21-2011 12:27 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  quanglamit Tham gia 09-10-2011 Điểm 55 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ \r\n\t\t\t\t\t\t\t\t\t\t    Em cảm ơn anh nhiều. Vậy còn cái Adjusted Complete Link và Mean Link thì sao anh? Với lại trong Weka em có thấy cái \"Classes to Clusters Evaluation\" nữa, thực tế em chưa hiểu rõ thuật toán này chạy như thế nào? Anh giúp em với\r\n\t\t\t\t\t\t\t\t\t\t    \r\n\t\t\t\t\t\t\t\t\t     Điểm chủ đề: 20 \r\n\t\t\t\t\t\t\t\t        10-13-2011 12:17 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  trungak0902 Tham gia 10-12-2011 Điểm 35 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Cs phương pháp nào để xác định k là tốt nhất không a ? Ví dụ e có 1 bảng dữ liệu và dùng  K-Means để phân cụm. Số cụm được chia nằm trong 1 khoảng nào đó ví dụ 1-10..Làm sao để xác định số cụm k là tốt nhất ? Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        10-13-2011 09:43 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  chucnv Tham gia 12-05-2008 Điểm 9,440 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Vấn đề chọn giá trị K (số cluster) cho giải thuật K-Means Trong giải thuật K-Means, tham số K (số Cluster) phải được xác định trước khi triển khai thuật toán. Việc này này hưởng rất lớn đến kết quả phân cụm của thuật toán. Một khó khăn là hiện nay chưa có giải pháp nào được xem là tốt (về tính khoa học) để chọn tham số này.  Việc chọn tham số K phù hợp với mô hình có thể sử dụng một số phương pháp sau: Thử      model với các giá trị của K, từ đó chọn K cho kết quả phân cụm tốt nhất Sử      dụng ý kiến của chuyên gia. Thường các chuyên gia trong 1 lĩnh vực nào đó      sẽ có cái nhìn (ban đầu) về dữ liệu cần phân cụm và đề xuất giá trị cho      tham số K (có thể có 1 vài giá trị) Sử      dụng kỹ thuật Cross- Validation (CV) n-fold để kiểm định Model từ đó chọn      tham số K (cách này thường được dùng trong các bài báo khao học vì đảm bảo      tính khoa học, giảm sai sót do cảm tính chủ quan). Từ khóa đại diện:  K-Means ,  Clustering model ,  Khai phá dữ liệu Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        10-17-2011 12:07 PM    \r\n\t\t\t\t\t\t\t\t      trả lời  k084061061_1990 Tham gia 10-17-2011 Điểm 20 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ  em đang co một bài tập yêu cầu tìm k tốt nhât,intput la CSDL và k thuoc [1,10] uotput: so k *** .. em chiệu lun..xin anh và các bạn nào co bản demo hướng dẫn mình với..xin chân thành cảm ơn..      Thà là bị nói ngu một lần,còn hơn ngu mãi mãi..!  Điểm chủ đề: 20 \r\n\t\t\t\t\t\t\t\t        10-24-2011 07:17 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  kungfumaster Tham gia 10-23-2011 Điểm 90 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ em chào anh Chức,   em mới tìm hiểu vê DM, search google thì thấy bài viết của anh, rất chi tiết & thú vị.Em  có 1 vài thắc mắc mong anh trả lời cho em được không ạ?   1.   Bước \"Nhóm các đối tượng vào nhóm gần nhất\", cụ thể là  bước 3 , em ko rõ vì sao mình có được ma trận G° như vậy ạ?   2.  Về chọn k centroid lúc đầu thuật toán, cái này mình chọn số k như thế nào cho hợp lý ạ? vì chắc không phải mình chọn bừa 1 số k ngẫu nhiên ( < số lượng objects) được phải không ạ??   Em cảm ơn anh!  Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        10-24-2011 07:51 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  kungfumaster Tham gia 10-23-2011 Điểm 90 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ \r\n\t\t\t\t\t\t\t\t\t\t    Hì hì, đọc đến cuối thì anh đã có trả lời cho thắc mắc thứ 2 :D  Còn thắc mắc đầu tiên em đợi giải thích của anh :D\r\n\t\t\t\t\t\t\t\t\t\t    \r\n\t\t\t\t\t\t\t\t\t     Điểm chủ đề: 20 \r\n\t\t\t\t\t\t\t\t        07-01-2012 05:53 PM    \r\n\t\t\t\t\t\t\t\t      trả lời  truongvutgg Tham gia 07-01-2012 Điểm 40 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Thầy ơi, thầy có bộ giáo trình của môn khai phá dữ liệu cho em xin tham khảo với em cảm ơn thầy : truongvutgg@gmail.com  Điểm chủ đề: 20 \r\n\t\t\t\t\t\t\t\t        10-17-2012 10:48 PM    \r\n\t\t\t\t\t\t\t\t      trả lời  dieptk2 Tham gia 10-17-2012 Điểm 20 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Có bảng Go. Theo mình họ làm như sau: So sánh tâm khoảng cách (Ở đây là c1,c2) từ các Medicine A,B,C,D (Hiểu đơn giản hơn là so sánh hàng trên với hàng dưới). Nếu khoảng cách là nhỏ hơn thì quy ước là 1, và lớn hơn thì là 0. Từ đó ta phân cụm dữ liệu ra  Điểm chủ đề: 20 \r\n\t\t\t\t\t\t\t\t        12-09-2012 12:34 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  cillkimus Tham gia 12-08-2012 Điểm 55 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Chào thầy! Em đã đọc bài viết của thầy và em thấy nó rất hay và dễ hiểu.  Khi em thực hiện gom nhóm trên dữ liệu khác với Kmeans thì em gặp một số khó khăn sau: Chẳng hạn các thuộc tính của em là:   @attribute ID numeric @attribute Animal @attribute I numeric @attribute i numeric @attribute C numeric @attribute c numeric @attribute P numeric @attribute p numeric @attribute M numeric @attribute m numeric 1. Em đang quan tâm không biết có nên bỏ 2 thuộc tính được bôi đậm ở trên để gom nhóm hay không. 2. Khi đã gom nhóm được rồi làm thế nào để mình biết được cặp thuộc tính nào là cặp thuộc tính phân chia nhóm tốt nhất, khi  Visualize cluster assignments ? Em rất mong được sự giúp đỡ của thầy. Kính chúc thầy sức khỏe và thành công!      Điểm chủ đề: 20 \r\n\t\t\t\t\t\t\t\t        03-22-2013 12:25 PM    \r\n\t\t\t\t\t\t\t\t      trả lời  HuongBT Tham gia 03-22-2013 Điểm 35 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ \r\n\t\t\t\t\t\t\t\t\t\t    Cảm ơn anh!\r\n\t\t\t\t\t\t\t\t\t\t    \r\n\t\t\t\t\t\t\t\t\t     Điểm chủ đề: 35 \r\n\t\t\t\t\t\t\t\t        04-01-2013 10:47 AM    \r\n\t\t\t\t\t\t\t\t      trả lời  tutruong Tham gia 03-30-2013 Điểm 255 \r\n\t\t\t\t\t\t\t\t\t        \r\n\t\t\t\t\t\t\t\t\t\t    Re: Thuật toán K-Means với bài toán phân cụm dữ liệu\r\n\t\t\t\t\t\t\t\t\t     Trả lời Liên hệ Cảm ơn bài viết rất rõ ràng và công phu của Anh Chức, em có chỗ này vẫn chưa rõ lắm, mong anh và các bạn giải thích thêm chút: Trong bảng kết quả phân cụm chi tiết:   1) Giá trị hiển thị trong mỗi cluster (mỗi cột) là giá trị trung bình (centroid) của cluster đó phải không vậy?? 2)  Còn dòng  Clustered Instanse:        0      5(8%)       1      7 (12%)       2      11(18%)       3      17(28%)       4      20(33%)  Em vẫn chưa hiểu các giá trị này nói lên ý nghĩa gì ạ, mong được mọi người giải thích chút ạ. Chân thành cảm ơn!  Điểm chủ đề: 35 Trang 1 trong số 2 (36 nội dung) 1  2   Tiếp theo > ©2008-2017 Business Intelligence Solution. All rights reserved.",
          "url": "http://bis.net.vn/forums/p/374/822.aspx"
        },
        {
          "title": "Thuật toán K-Mean trong bài toán Phân cụm dữ liệu",
          "relevance": "1",
          "content": "\nTìm kiếm Blog này\n \nDanh cho sinh viên CNTT\n \nBài viết hay về CNTT, cập nhật thông tin công nghệ, tuyển dụng và chia sẻ kinh nghiệm học tập, Hướng dẫn kiếm tiền online, kiếm tiền trên mạng\n Nhận liên kết Facebook Twitter Pinterest Google+ Email Ứng dụng khác \ntháng 6 26, 2013\n I.  GIỚI THIỆU     Thuật toán K-means clustering do\nMacQueen giới thiệu trong tài liệu “J. Some Methods for Classification and\nAnalysis of Multivariate Observations” năm 1967. K-means Clustering là một\nthuật toán dùng trong các bài toán phân loại/nhóm n đối tượng thành k nhóm dựa\ntrên đặc tính/thuộc tính của đối tượng (k  £ n nguyên, dương). Về nguyên lý, có n đối\ntượng, mỗi đối tượng có m thuộc tính, ta phân chia được các đối tượng thành k\nnhóm dựa trên các thuộc tính của đối tượng bằng việc áp dụng thuật toán này. Coi mỗi thuộc tính của đối tượng (đối\ntượng có m thuộc tính) như một toạ độ của không gian m chiều và biểu diễn đối\ntượng như một điểm của không gian m chiều. a i  =( x i1 , x i2 ,\n... x im )                    (\n 1 ) a i  (i=1..n) - đối tượng thứ i x ij  (i=1..n, j=1..m) - thuộc tính thứ j của\nđối tượng i Phương thức phân loại/nhóm dữ liệu thực hiện dựa\ntrên khoảng cách Euclidean nhỏ nhất giữa đối tượng đến phần tử  trung tâm của các nhóm. Phần\ntử trung tâm của nhóm được xác định bằng giá trị trung bình các phần tử trong\nnhóm. 2.\nKhoảng cách Euclidean.    a i =(x i1 ,\nx i2 ,... x im ) i=1..n - đối tượng thứ i cần phân phân loại c j =(x j1 , x j2 ,... x jm )\nj=1..k - phần tử trung tâm nhóm j Khoảng cách Euclidean từ đối tượng a i  đến\nphần tử trung tâm nhóm j c j  được tính toán dựa trên công thức:      \n(  2 ) d ji  - khoảng cách Euclidean từ a i  đến\nc j x is -  thuộc tính thứ s của đối tượng a i x js -  thuộc tính thứ s của phần tử trung\ntâm c j 3.\nPhần tử trung tâm. k phần tử trung tâm (k\nnhóm) ban đầu được chọn ngẫu nhiên, sau mỗi lần nhóm các đối tượng vào các\nnhóm, phần tử trung tâm được tính toán lại.  Cluster i  = {a 1 , a 2  .... a t } – Nhóm thứ\ni i=1..k,  k số cluster j= 1..m,  m số thuộc tính t - số phần tử hiện có của nhóm thứ i x sj  - thuộc tính thứ j của phần tử s   s=1..t c ij  - toạ độ thứ j của phần tử trung tâm nhóm i;                        (  3 ) II. GIỚI THIỆU VỀ  THUẬT TOÁN K-MEANS. Sơ\nđồ thuật toán: Hình\n3:  Sơ đồ thuật toán K-means clustering T hu ậ t t o á n k - m ea n s b ao g ồ m các bước cơ b ả n s a u : Input:   S ố c ụ m k và c á c trọng t â m c ụ m {m j } k j=1 . Output: Các c ụ m C [ i] (1   ≤    i   ≤\n   k) và hà m tiêu chuẩn E đạt giá trị t ối t hiểu. Begin Bước 1 :\nKhởi tạo Chọn\nk trọng tâm  {m j } k j=1  ban đầu trong không gian Rd (d là số chiều của\ndữ liệu). Việc lựa chọn này có thể là ngẫu nhiên hoặc theo kinh nghiệm. Bư ớ c 2: Tí n h toán k h oảng cách Đối với m ỗi đi ể m X i   (1 ≤  i ≤ n ) , tính t oán k hoảng cá c h của nó t ới m ỗ i trọng t â m m j (1 ≤ j ≤ \n k ). Sau đó t ì m trọng t â m gần nhất đ ố i với mỗi đ i ể m . Bư ớ c 3: Cập nhật l ại\ntrọng t â m Đối với m ỗi 1 ≤ j ≤ k , cập nhật trọng t â m c ụ m m j    bằng cách xác đ ị nh\ntrung bình cộng các v e ctơ đối t ư ợ n g dữ liệu. Điều kiện dừng: Lặp lại các b ư ớc 2 và 3 cho đến khi c ác trọ n g t â m của c ụ m không t h a y\nđổi. End. Thuật toán k-means trên\nđược chứng minh là hội tụ và có độ phức tạp tính toán là:    Trong đó, n là số đối tượng dữ liệu,\nk là số cụm dữ liệu, d là số chiều,  τ  là số vòng lặp, T flop  là thời gian để\nthực hiện một phép tính cơ sở như phép tính nhân, chia,... Như vậy, do k-means\nphân tích phân cụm đơn giản nên có thể áp dụng đối với tập dữ liệu lớn.Tuy\nnhiên, nhược điểm của k-means là chỉ áp dụng với dữ liệu có thuộc tính số và\nkhám phá ra các cụm có dạng hình cầu, k-means còn rất nhạy cảm với nhiễu và các\nphần tử ngoại lai trong dữ liệu. Hơn nữa, chất lượng phân cụm dữ liêuk của thuật\ntoán k-means phụ thuộc nhiều vào các tham số đầu vào như: số cụm k và k trọng\ntâm khởi tạo ban đầu. Trong trường hợp các trọng tâm khởi tạo ban đầu mà quá lệch\nso với các trọng tâm cụm tự nhiên thì kết quả phân cụm của k-means là rất thấp,\nnghĩa là các cụm dữ liệu được khám phá rất lệch so với các cụm trong thực tế.\nTrên thực tế chưa có một giải pháp tối ưu nào để chọn các tham số đầu vào, giải\npháp thường được sử dụng nhất là thử nghiệm với các giá trị đầu vào k khác nhau\nrồi sau đó chọn giải pháp tốt nhất. III.\nCHƯƠNG TRÌNH DEMO. Chương\ntrình gồm các Hàm chính: ·        \n kMeanCluster– Thể hiện một đối tượng ·        \n dist – Đối tượng trung tâm Sub  kMeanCluster (Data() As Variant,\nnumCluster As Integer) Dim\ni As Integer Dim\nj As Integer Dim\nX As Single Dim\nY As Single Dim\nmin As Single Dim\ncluster As Integer Dim\nd As Single Dim sumXY() Dim\nisStillMoving As Boolean isStillMoving\n= True If\ntotalData <= numCluster Then     Data(0, totalData) = totalData                '\ncluster No = total data     Centroid(1, totalData) = Data(1,\ntotalData)   ' X     Centroid(2, totalData) = Data(2,\ntotalData)   ' Y Else                     'calculate minimum distance to\nassign the new data     min = 10 ^ 10                                 'big number     X = Data(1, totalData)     Y = Data(2, totalData)     For i = 1 To numCluster         d = dist(X, Y, Centroid(1, i),\nCentroid(2, i))         If d < min Then             min = d             cluster = i         End If     Next i     Data(0, totalData) = cluster          Do While isStillMoving     ' this loop will surely\nconvergent        calculate new centroids    ReDim sumXY(1 To 3, 1 To numCluster)  '1=X,2=Y,3=count number of data         For i = 1 To totalData             sumXY(1, Data(0, i)) = Data(1, i) +\nsumXY(1, Data(0, i))             sumXY(2, Data(0, i)) = Data(2, i) +\nsumXY(2, Data(0, i))             sumXY(3, Data(0, i)) = 1 + sumXY(3,\nData(0, i))         Next i         For i = 1 To numCluster             Centroid(1, i) = sumXY(1, i) /\nsumXY(3, i)             Centroid(2, i) = sumXY(2, i) /\nsumXY(3, i)         Next i          'assign all data to the new centroids         isStillMoving = False         For i = 1 To totalData             min = 10 ^ 10                                 'big number             X = Data(1, i)             Y = Data(2, i)             For j = 1 To numCluster                 d = dist(X, Y, Centroid(1, j),\nCentroid(2, j))                 If d < min Then                     min = d                     cluster = j                 End If             Next j             If Data(0, i) <> cluster Then                 Data(0, i) = cluster                 isStillMoving = True             End If         Next i     Loop End\nIf End Sub Function  dist (X1 As Single, Y1 As Single, X2 As\nSingle, Y2 As Single) As Single ' calculate Euclidean distance     dist = Sqr((Y2 - Y1) ^ 2 + (X2 - X1) ^ 2) End Function Private\nFunction min2(num1, num2) ' return minimum value between two numbers     If num1 < num2 Then         min2 = num1     Else         min2 = num2     End If End Function  >> DOWNLOAD CODE  TẠI ĐÂY KẾ QUẢ CHƯƠNG TRÌNH  Dữ liệu vào    \nData(1, 1) = 1   \n Data(2, 1) = 1    \n    \nData(1, 2) = 5    \nData(2, 2) = 2    \n    \nData(1, 3) = 8    \nData(2, 3) = 5    \n    \nData(1, 4) = 7    \nData(2, 4) = 3             \nData(1, 5) = 5    \nData(2, 5) = 1 Kết quả chạy\nvới k= 3 Kết quả chạy\nvới k= 2 IV.\nKẾT LUẬN.  Giống\nnhư các thuật toán khác, k- mean cũng có một số hạn chế nhất định: -         \n Việc khởi tạo phần tử trung tâm của nhóm\nban đầu ảnh hưởng đến sự phân chia đối tượng vào nhóm trong trường hợp dữ liệu\nkhông lớn. -         \n Số nhóm k luôn phải được xác định trước. -         \n Không xác định được rõ ràng vùng của\nnhóm, cùng 1 đối tượng, nó có thể được đưa vào nhóm này hoặc nhóm khác khi dung\nlượng dữ liệu thay đổi. -         \n Điều kiện khởi tạo có ảnh hưởng lớn đến\nkết quả. Điều kiện khởi tạo khác nhau có thể cho ra kết quả phân vùng nhóm khác\nnhau.  -         \n Không xác định được mức độ ảnh hưởng của\nthuộc tính đến quá trình tạo nhóm. Như vậy, với dữ liệu nhỏ, thuật toán có thể có những\nhạn chế. Để khắc phục những hạn chế này, nên sử dụng thuật toán kmean trong trường\nhợp dữ liệu lớn.  Về vấn đề hạn chế phân nhóm, có thể dùng phương pháp\nxác định trung tuyến thay vì xác định mean. Một\nquan niệm cho rằng k-means không thể dùng cho dữ liệu có kiểu là định lượng.\nĐiều này không đúng, k-means có thể được dùng giải quyết các bài toán dữ liệu\nđa biến, thậm chí cho các bài toán có nhiều dạng dữ liệu. Chìa khoá cho việc\ngiải bài toán này của k-means là sử dụng ma trận khoảng cách. TÀI LIỆU THAM\nKHẢO 1.      \n Bài\ngiảng của thầy Nguyễn Bá Tường 2.      \n k-means\nclustering  http://en.wikipedia.org/wiki/K-means_clustering 3.      \n How\nthe K-Mean Clustering algorithm works?  http://people.revoledu.com/kardi/tutorial/kMean/Algorithm.htm 4.      \n Kiri\nWagstaff, Claire Cardie; Constrained k-means clustering with Background\nKnowledge  http://www.cse.msu.edu/~cse802/notes/ConstrainedKmeans.pdf \n[TxT] Cơ sở dữ liệu K-Mean Khai phá dữ liệu (datamining) Phân loại dữ liệu Phần mềm Nhận liên kết Facebook Twitter Pinterest Google+ Email Ứng dụng khác \nBài đăng phổ biến từ blog này\n Cây quyết định với bài toán phân loại dữ liệu \ntháng 5 22, 2013\n Khái niệm cây quyết định \nTrong lĩnh vực học máy, cây quyết định là một kiểu mô hình dự báo (predictive model), nghĩa là một ánh xạ từ các quan sát về một sự vật/hiện tượng tới các kết luận về giá trị mục tiêu của sự vật/hiện tượng. Mỗi một nút trong (internal node) tương ứng với một biến; đường nối giữa nó với nút con của nó thể hiện một giá trị cụ thể cho biến đó. Mỗi nút lá đại diện cho giá trị dự đoán của biến mục tiêu, cho trước các giá trị của các biến được biểu diễn bởi đường đi từ nút gốc tới nút lá đó. Kỹ thuật học máy dùng trong cây quyết định được gọi là học bằng cây quyết định, hay chỉ gọi với cái tên ngắn gọn là cây quyết định. Hình minh họa  \nHọc bằng cây quyết định cũng là một phương pháp thông dụng trong khai phá dữ liệu. Khi đó, cây quyết định mô tả một cấu trúc cây, trong đó, các lá đại diện cho các phân loại còn cành đại diện cho các kết hợp của các thuộc tính dẫn tới phân loại đó[1]. Một cây quyết định có thể được học bằng cách chia tập hợp nguồn thành các tập con dựa …\n \nĐọc thêm\n Giới thiệu về ICT và các lĩnh vực ngành nghề liên quan \ntháng 10 13, 2016\n Công nghệ Thông tin - IT (theo wikipedia) \nCông nghệ Thông tin, viết tắt CNTT, (tiếng Anh: Information Technology hay là IT) là một nhánh ngành kỹ thuật sử dụng máy tính và phần mềm máy tính để chuyển đổi, lưu trữ, bảo vệ, xử lý, truyền tải và thu thập thông tin. Ở Việt Nam, khái niệm Công nghệ Thông tin được hiểu và định nghĩa trong nghị quyết Chính phủ 49/CP kí ngày 04/08/1993: \"Công nghệ thông tin là tập hợp các phương pháp khoa học, các phương tiện và công cụ kĩ thuật hiện đại - chủ yếu là kĩ thuật máy tính và viễn thông - nhằm tổ chức khai thác và sử dụng có hiệu quả các nguồn tài nguyên thông tin rất phong phú và tiềm năng trong mọi lĩnh vực hoạt động của con người và xã hội\". Thuật ngữ \"Công nghệ Thông tin\" xuất hiện lần đầu vào năm 1958 trong bài viết xuất bản tại tạp chí Harvard Business Review. Hai tác giả của bài viết, Leavitt và Whisler đã bình luận: \"Công nghệ mới chưa thiết lập một tên riêng. Chúng ta sẽ gọi là công nghệ thông tin (Information Techn…\n \nĐọc thêm\n \nTỰ HỌC CNTT\n Tài liệu dành cho SV CNTT \nĐược tạo bởi Blogger\n \nHình ảnh chủ đề của  Galeries TAILIEUCNTT.ORG \nTài liệu CNTT\n Tự học CNTT \nLưu trữ\n \n2017\n 1 \ntháng bảy\n 1 \n2016\n 7 \ntháng mười\n 4 \ntháng chín\n 1 \ntháng sáu\n 2 \n2015\n 2 \ntháng mười một\n 2 \n2014\n 4 \ntháng năm\n 1 \ntháng tư\n 1 \ntháng ba\n 1 \ntháng hai\n 1 \n2013\n 68 \ntháng mười hai\n 1 \ntháng mười\n 8 \ntháng chín\n 1 \ntháng tám\n 1 \ntháng bảy\n 16 \ntháng sáu\n 26 [Đặt quảng cáo kiếm tiền] Kiếm tiền cực thoải mái ... Thuật toán Cây quyết định ID3 và chương trình mô p... Hướng dẫn sử dụng Google Code Tạo nút BACK TO TOP (TRỞ VỀ ĐẦU TRANG) cho blogspo... Cẩm Nang Việc Làm / Phỏng vấn việc làm / Kỹ năng t... 7 kỹ năng cơ bản để làm việc nhóm hiệu quả CNTT đang được coi là phương thức phát triển mới Diễn đàn cấp cao về CNTT-TT năm 2013: 'CNTT là con... Tính toán lưới - tổng quan và ứng dụng Thuật toán K-Mean trong bài toán Phân cụm dữ liệu HƯỚNG DẪN CÁCH TẠO BẢN GHOST VÀ GHOST WINXP (NTFS ... 10 lời khuyên cho ''dân IT'' khi được phỏng vấn tu... Hướng dẫn viết CV hoàn hảo cho sinh viên mới ra tr... \"Hàng độc\": Kính hiển vi dành riêng cho smartphone... Hướng dẫn tạo Group trong gmail Tổng quản về Sematic Web - Web3.0 [Web ngữ nghĩa] Quy chế đào tạo theo hình thức TÍN CHỈ Chuyển đổi file PDF sang Word miễn phí bằng PDFtoW... Cách chuyển Font chữ từ TCVN3 sang Unicode [sử dụn... Thiết lập SkyDrive của Windows thành thư mục lưu t... Hướng dẫn tạo Blogspot Cùng với Google Drive lưu giữ mọi thứ, chia sẻ mọi... Vẽ tranh tuyệt đẹp bằng phần mềm Micrsoft Office E... Những điều chưa tiết lộ về công việc của một lập t... Vẽ tranh bằng ý nghĩ Samsung được cấp bản quyền thiết kế tablet với màn... \ntháng năm\n 15 Hiển thị thêm Ẩn bớt \nDanh mục bài viết\n Agent ANDROID bài viết hay Blogger' Công cụ lập trình Công cụ phát triển website Công nghệ điện toán đám mây Công nghệ mới Cơ sở dữ liệu Cứu vớt dữ liệu đã bị xóa hoặc format Dịch vụ lưu trữ miễn phí định hướng nghề nghiệp Góc học tập Google code Hệ điều hành Android Hỗ trợ - hướng dẫn IoT K-Mean Khai phá dữ liệu (datamining) Kiếm tiền bằng hình thức rút gọn URL - ADF.LY Kiếm tiền online Kiếm tiền với Linkbucks Kinh nghiệm học tập Kỹ năng tìm việc làm Kỹ năng trả lời phỏng vấn Lập trình viên Mạng máy tính Mobile Ngăn chặn USB truy cập vào máy tính Phần cứng Phân loại dữ liệu Phần mềm Phần mềm diệt Virus miễn phí Phần mềm hỗ trợ download Phần mềm tăng tốc copy dữ liệu Quản trị hosting với CuteFtp miễn phí Thông tin tuyển dụng Thủ thuật - Tiện ích Thuật toán ID3 Tin tức ICT Tính toán lưới Tối ưu hóa ổ cứng Trình duyệt web Tuyển sinh Tự giới thiệu Văn bản Việc làm - Tuyển dụng Hiển thị thêm Ẩn bớt \nBáo cáo Lạm dụng\n",
          "url": "http://ict-tdu.blogspot.com/2013/06/thuat-toan-k-mean-trong-bai-toan-phan.html"
        },
        {
          "title": "Numerical Example of  K-Means Clustering",
          "relevance": "1",
          "content": "Menu Home page Research List Publication Research Collaboration Guide Tutorial Free Online Tutorials Analytic Hierarchy Process Decision Tree Gaussian Mixture Model and EM Algorithm Hierarchical Clustering K Means Clustering K Nearest Neighbor Market Basket Analysis Neural Network M/M/s Queuing Spreadsheet  Q-Learning Support Vector Machines Software Interactive Programs Micro-PedSim Service Blog Jewels of Morning Dew About Resume Frequently Asked Questions Contact Donate Link to Us Terms of Use Numerical Example of  K-Means Clustering \r\n     Object\r\n     \r\n     attribute 1 (X): weight index\r\n     \r\n     attribute 2 (Y): pH\r\n     \r\n     Medicine A\r\n     \r\n     1\r\n     \r\n     1\r\n     \r\n     Medicine B\r\n     \r\n     2\r\n     \r\n     1\r\n     \r\n     Medicine C\r\n     \r\n     4\r\n     \r\n     3\r\n     \r\n     Medicine D\r\n     \r\n     5\r\n     \r\n     4\r\n     \r\n     Object\r\n     \r\n     Feature 1 (X): weight index\r\n     \r\n     Feature 2 (Y): pH\r\n     \r\n     Group (result)\r\n     \r\n     Medicine A\r\n     \r\n     1\r\n     \r\n     1\r\n     \r\n     1\r\n     \r\n     Medicine B\r\n     \r\n     2\r\n     \r\n     1\r\n     \r\n     1\r\n     \r\n     Medicine C\r\n     \r\n     4\r\n     \r\n     3\r\n     \r\n     2\r\n     \r\n     Medicine D\r\n     \r\n     5\r\n     \r\n     4\r\n     \r\n     2\r\n     \r\n    \tCopyright © 2017 Kardi Teknomo\r\n     Revoledu Design",
          "url": "http://people.revoledu.com/kardi/tutorial/kMean/NumericalExample.htm"
        },
        {
          "title": "K-means clustering",
          "relevance": "1",
          "content": "What is K-means clustering? K-means algorithm Deciding the number of clusters Initializing the position of the clusters Good example Bad examples \nInitialization method:  Forgy Random partition \nnumber of clusters =  Reset Iterate 1. Initialize clusters 2. Assign data points to closer cluster 3. Calculate center of each cluster In this scatter plot you have several two-dimensional data points, clustered at 4 distinct positions. You can choose the initialization method and the number of clusters used in the k-means algorithm. The button 'Reset' resets the algorithm and generates a new dataset. The button 'Iterate' runs one step of the algorithm, which becomes bolded in the text below the button. More often than not, you see that the algorithm converges to the best solution. However, if you try enough times, there are some initializations of the clusters that lead to a \"bad\" local minimum. If you choose the wrong number of clusters, you can see the drastic effects on the result of the algorithm. \nInitialization method:  Forgy Random partition \nnumber of clusters =  Reset Iterate 1. Initialize clusters 2. Assign data points to closer cluster 3. Calculate center of each cluster Although the clusters have the same scatter (in fact, the same shape), they are not spherical. As before, you can choose the initialization method and the number of clusters used in the k-means algorithm. The button 'Reset' resets the algorithm and generates a new dataset. The button 'Iterate' runs one step of the algorithm, which becomes bolded in the text below the button. The k-means algorithm never assigns correctly the tips of the shapes because the spherical assumption fails. \nInitialization method:  Forgy Random partition \nnumber of clusters =  Reset Iterate 1. Initialize clusters 2. Assign data points to closer cluster 3. Calculate center of each cluster Although the clusters have spherical shapes, they have different scatters. As before, you can choose the initialization method and the number of clusters used in the k-means algorithm. The button 'Reset' resets the algorithm and generates a new dataset. The button 'Iterate' runs one step of the algorithm, which becomes bolded in the text below the button. The data points that clearly belong to the large cluster but are closer to the small clusters are misclassified.",
          "url": "http://www.onmyphd.com/?p=k-means.clustering"
        },
        {
          "title": "Một số vấn đề về phân cụm dữ liệu",
          "relevance": "1",
          "content": " Tải về Đưọc cung cấp bởi  Xemtailieu.com ×  Tải về bản đầy đủ Đóng",
          "url": "https://text.xemtailieu.com/tai-lieu/mot-so-van-de-ve-phan-cum-du-lieu-155804.html"
        },
        {
          "title": "Menu",
          "relevance": "1",
          "content": "Ông Xuân Hồng Chia sẻ kiến thức và thông tin về Machine learning Menu Skip to content Data Science Big Data Deep learning Exploratory Data Analysis Getting and cleaning data Machine learning Lập trình Python Spark R Weka Kiến thức Toán Statistical Inference Xử lý ngôn ngữ tự nhiên – Natural Language Processing (NLP) Chia sẻ Dự án About Exploratory Data Analysis: K Means Clustering Tháng Bảy 18, 2015 Tháng Mười Hai 3, 2015 Ông Xuân Hồng %(count) bình luận K Means Clustering Random points Initial centroids Finish iteration 1 Update centroids 1 Update clusters Finish example kmeans kmeans 1 kmeans 2 kmeans 3 Facebook Twitter Google LinkedIn Like this: Số lượt thích Đang tải... Liên quan Điều hướng bài viết ←  Exploratory Data Analysis: Hierarchical Clustering Exploratory Data Analysis: Thiết bị đồ họa trong R  → pham yen nói: \n\t\t\t\t\t\t\t\tTháng Bảy 30, 2017 lúc 2:03 sáng\t\t\t\t\t\t\t rat cam on bai viet cua ban Số lượt thích Số lượt thích Phản hồi Nhập bình luận của bạn tại đây... Mời bạn điền thông tin vào ô dưới đây hoặc kích vào một biểu tượng để đăng nhập: Thư điện tử  (bắt buộc) (Địa chỉ của bạn được giấu kín) Tên  (bắt buộc) Trang web  Bạn đang bình luận bằng tài khoản WordPress.com  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Twitter  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Facebook  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Google+  (  Đăng xuất  /  Thay đổi  ) Hủy bỏ Connecting to %s Notify me of new comments via email. Thông báo cho tôi bằng email khi có bài đăng mới. Bạn Không Thể Sống Một Mình Nếu ngày mai tớ trở thành người nổi tiếng. Gặp tớ giữa đường bạn đừng ngại tớ nghe. Cứ nhìn tớ như ngày xưa bạn thấy. Đừng nên nghĩ tớ quá xa vời.  Nếu một ngày tớ là kẻ ăn xin. Thì bố thí cho tớ vài kỷ niệm. Dù rất nhỏ nhưng đừng nên khinh tớ. Vì tớ vẫn là tớ của ngày xưa.  Nếu ngày mai tớ chẳng biết đường về. Hãy chỉ giùm tớ con đường đi đúng đắn. Đừng để tớ sa chân nơi bùn nhớp. Lúc rút chân lên thì đã bẩn mất rồi.  Nếu ngày mai tớ gặp bạn giữa đường. Thì ta sẽ gặp nhau và cười nhé. Đừng nên trách và đừng nên đánh mất. Đôi khi có một số người lướt qua cuộc đời bạn và ngay tức khắc bạn nhận ra rằng sự có mặt của họ ý nghĩa như thế nào. Họ đã dạy bạn những bài học, đã giúp bạn nhận ra giá trị của chính mình hoặc trở thành con người mà bạn từng mơ ước. Có lẽ bạn sẽ không biết được những con người này từ đâu đến ( bạn cùng phòng, người hàng xóm, vị giáo sư, người bạn mất liên lạc từ lâu hay thậm chí là một người hoàn toàn xa lạ ). Nhưng khi bạn thờ ơ với họ, hãy nhớ rằng trong từng khoảnh khắc họ sẽ ảnh hưởng rất sâu sắc đến cuộc đời bạn.  …Cuộc sống không hề làm khó bạn mà chính bạn đã tự làm khó mình bằng cách đặt ra những câu hỏi đại loại như: “Tại sao lại thế này?… Tại sao lại thế kia?…”. Không ai có thể trả lời giúp bạn mà chính bạn phải tự đi tìm lời giải đáp cho mình. Hãy luôn nghĩ về người khác trước khi nghĩ đến bản thân, chắc chắn lúc đó bạn sẽ hài lòng với câu trả lời của chính mình. Và đừng quên luôn đặt câu hỏi: “Mình đã làm được gì?” trước khi tự hỏi: “Mình đã nhận được gì?” nhé! Tôi tin là bạn sẽ thành công!   Thật ra trên đời, con người luôn cần có nhau để giải quyết rất nhiều vấn đề. Nhờ những lúc qua lại giúp đỡ đó mà hoá giải bao muộn phiền, nảy sinh các tình cảm, cùng trải nghiệm biết bao điều thú vị, hiểu biết thêm nhiều tri thức mới. Đây chính là cuộc sống! Vì thế hãy trân trọng những người đang “làm phiền” bên cạnh bạn nhé! Đừng để đến khi mất đi rồi mới nghẹn ngào lại bảo “giá như” … ĐĐ. GS. Thích Phước Tiến \n__(())__ Namo Bụt Sakyamuni Nhận Email khi có bài viết mới Liên hệ Email (bắt buộc) Comment (bắt buộc) Top bài viết Bài viết mới FAQ: Big data cho người mới bắt đầu Tháng Mười 2, 2017 Làm việc với Spark DataFrames – Truy vấn nâng cao (Subqueries, Cube, Rollup) Tháng Chín 15, 2017 Thống kê ứng dụng 3: Các vấn đề trong thống kê Tháng Chín 14, 2017 Thống kê ứng dụng 2: Suy luận thống kê Tháng Chín 12, 2017 Thống kê ứng dụng 1: Quan sát dữ liệu Tháng Chín 7, 2017 AI, Machine Learning, Deep Learning phân biệt như thế nào cho đúng Tháng Chín 4, 2017 Information extraction – Bài toán rút trích thông tin trong văn bản Tháng Tám 28, 2017 Bắt đầu nghiên cứu big data từ đâu và như thế nào Tháng Tám 3, 2017 Data Science – Mỏ vàng của Kỉ nguyên số Tháng Tám 3, 2017 SMA 2017 – Lý thuyết ra quyết định Tháng Sáu 17, 2017 Big Data Chia sẻ Data Science Deep learning Dự án Exploratory Data Analysis Getting and cleaning data Kiến thức Lập trình Machine learning Python R Spark Statistical Inference Toán Weka Xử lý ngôn ngữ tự nhiên - Natural Language Processing (NLP) This slideshow requires JavaScript. Thư viện Tháng Mười 2017  (1) Tháng Chín 2017  (5) Tháng Tám 2017  (3) Tháng Sáu 2017  (3) Tháng Năm 2017  (3) Tháng Ba 2017  (1) Tháng Một 2017  (3) Tháng Mười Hai 2016  (1) Tháng Mười Một 2016  (2) Tháng Mười 2016  (2) Tháng Chín 2016  (1) Tháng Tám 2016  (2) Tháng Bảy 2016  (2) Tháng Năm 2016  (1) Tháng Tư 2016  (1) Tháng Ba 2016  (2) Tháng Hai 2016  (2) Tháng Một 2016  (1) Tháng Mười Hai 2015  (6) Tháng Mười Một 2015  (5) Tháng Mười 2015  (6) Tháng Chín 2015  (11) Tháng Tám 2015  (16) Tháng Bảy 2015  (25) Tháng Sáu 2015  (4) Thống kê 360,709 hits Tháng Bảy 2015 H B T N S B C « Th6   Th8 »   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31   Data Science Lập trình Kiến thức Chia sẻ Dự án About Tạo một website miễn phí hoặc 1 blog với WordPress.com. Post to Hủy bỏ %d  bloggers like this:",
          "url": "https://ongxuanhong.wordpress.com/2015/07/18/exploratory-data-analysis-k-means-clustering/"
        },
        {
          "title": "Possibly the simplest way to explain K-Means algorithm",
          "relevance": "1",
          "content": "Sectors Banking / Finance Retail / eCom Travel / Hospitality Privacy / Security Marketing Telecommunication Media / FMCG Crime / Law Sports Health / Pharma Education Human Resource Tech & Tools Analytics Data Science Business Intelligence Digital Personalization Machine Learning Artificial Intelligence Visualisation Hadoop Data Mining SQL NoSQL Events Resources Video Books e-Books My Favourites Write For Us Big Data - Made Simple Login Privacy / Security \r\n    How to tell if a web site is protecting your data Education \r\n    8 ways machine learning will improve education Analytics \r\n    Top 30 big data tools for data analysis Artificial Intelligence \r\n    Could AI help stop fake news in the near future? Analytics \r\n    7 reasons non-analysts should understand data modelling Business Intelligence \r\n    Top Business Intelligence (BI) tools in the market Visualization \r\n    Big data meets design: Visual communication in big data Data Mining \r\n    Defining your data quality problems Hadoop \r\n    Is Spark better than Hadoop Map Reduce? Artificial Intelligence \r\n    For AI to change business, it needs to be fueled with quality data Visualization \r\n    9 golden rules of data visualization [Infographic] Data Science \r\n    Python vs R for data analysis: An infographic for beginners Machine Learning Manu Jeevan Follow Possibly the simplest way to explain K-Means algorithm 07th Aug `17, 11:00 AM in  Machine Learning Clustering is a technique for finding similarity groups in a data, called clusters. It attempts to group individuals… Comments Share Favorite Manu Jeevan Contributor Follow Aditya Modak Thanks .. easy to understand ! MORE FROM BIG DATA MADE SIMPLE \r\n    Here’s How ‘Big Data’ Can Really Hurt The Poor A White House review of how the government and private sector use large sets of data has found… In  Government \r\n    Nostradamus on Big Data: 5 predictions It’s almost 450 years since Nostradamus wrote his best-seller, ‘Les Propheties, and gave everyone sleepless nights ever since…. In  Analytics \r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t, by  Srikant Sastri on Sep 18 \r\n    Baidu hires former Google artificial intelligence chief China’s top search engine Baidu Inc has hired Google Inc’s former artificial intelligence (AI) chief Andrew Ng to… In  Artificial Intelligence \r\n    Dealing with Unbalanced Class, SVM, Random Forest and Decision Tree in Python So far I have talked about decision trees and ensembles. But I hope, I have made you understand the logic behind these… In  Machine Learning \r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t, by  Manu Jeevan on Jan 29 \r\n    Top 10 tools market researchers should explore Nowadays there is a great variety of different useful tools for market researchers that are free and open… In  Marketing \r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t, by  Karolina Jones on Aug 23 \r\n    Shopping cart analysis with R – Multi-layer pie chart In this post, we will review a very interesting type of visualization – the Multi-layer Pie Chart – and use… In  Data Science \r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t, by  Sergey Bryl' on Jul 01 \r\n    7 things you must know about Big Data before adoption Takeaway: There is absolutely no doubt that the right use of big data helps businesses become more profitable…. In  Analytics \r\n    Applications of Predictive Analytics in various industries They say that those who do not study history are doomed to repeat it. In no form of… In  Analytics \r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t, by  Mohammad Farooq on Oct 31 \r\n    10 useful ways to visualize your data(with examples) Many people spend their day sifting through data, combining multiple data sources, and finally getting data ready for… In  Visualization \r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t, by  Elana Roth on Jun 24 \r\n    Ten handy python libraries for (aspiring) data scientists Data science has gathered a lot of steam in the past few years, and most companies now acknowledge… In  Data Science \r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t, by  Srinath Achanta on Sep 20 \r\n    How to write data analysis reports. Lesson 1—Know Your Content. In every data analysis, putting the analysis and the results into a comprehensible report is the final, and… In  Analytics \r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t, by  Charlie Kufs on Mar 03 \r\n    Disaster Recovery and the Big Data Application When I ask database administrators how they implemented disaster recovery in their big data environments there are two… In  Sectors OUR PARTNERS About us Newsletter Archives RSS feed Contributors Contact Us Subscribe to our Newsletter Subscribe Powered by  \r\n            \tCopyright © 2017  Crayon Data . All rights reserved. \r\n             \r\n               Remember Me\r\n             SIGN UP,  IT’S FREE! Password \r\n\t\t\t\tBy signing up you are accepting  BDMS terms and conditions BACK TO LOGIN Forgot your Password? Enter your Email Address to receive a verification code to change password Subscribe to our newsletter Every week you’ll get latest updates from the world of Big Data, \r\nYou may also get exclusive invite to some prestigious events around the world. Subscribe Settings \r\n\t\t\tSwitch OFF Notifications\r\n\t\t\ton off\r\n\t\t\t READ NEXT \r\n                    How do you explain Machine Learning and Data Mining to a layman? Pararth Shah \r\n                    15 best books to learn Probability Baiju NT \r\n                    Key concepts and terms in Multivariate Statistical Methods Baiju NT",
          "url": "http://bigdata-madesimple.com/possibly-the-simplest-way-to-explain-k-means-algorithm/"
        },
        {
          "title": "Post navigation",
          "relevance": "1",
          "content": "\n\t\t\t\t\tphamdungblog\t\t\t\t Đây là blog chia sẽ về lĩnh vực tài chính và thống kê Menu Home About Contact Thuật toán K-means Posted by  dungphamvan on July 21, 2016 Share this: Twitter Facebook Google Like this: Like Loading... Related Post navigation Previous Post Thuật toán cây quyết định làm việc như thế nào ? Next Post Một số nội dung cơ bản của xác suất Enter your comment here... Fill in your details below or click an icon to log in: Email  (required) (Address never made public) Name  (required) Website  You are commenting using your WordPress.com account.  (  Log Out  /  Change  )  You are commenting using your Twitter account.  (  Log Out  /  Change  )  You are commenting using your Facebook account.  (  Log Out  /  Change  )  You are commenting using your Google+ account.  (  Log Out  /  Change  ) Cancel Connecting to %s Notify me of new comments via email. Recent Posts Ảnh hưởng của khấu hao lên báo cáo tài chính August 18, 2016 Chuỗi bài về phân tích báo cáo tài chính August 18, 2016 Financial Statement Flow August 12, 2016 Hồi quy logistic trong R August 9, 2016 Hướng dẫn phân tích thành phần chính trong R (Principal Component analysis) July 28, 2016 proportion distribution(không biết dịch sao luôn) July 28, 2016 Phân phối nhị thức (binominal distribution) July 28, 2016 Phân phối chuẩn hóa July 28, 2016 Search for: Text Widget This is a text widget. The Text Widget allows you to add text or HTML to your sidebar. You can use a text widget to display text, links, images, HTML, or a combination of these. Edit them in the Widget section of the  Customizer . Archives August 2016 July 2016 Facebook LinkedIn Twitter Instagram A WordPress.com Website .\n\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t Post to Cancel %d  bloggers like this:",
          "url": "https://phamdungblog.wordpress.com/2016/07/21/thuat-toan-k-means/"
        },
        {
          "title": "Subscribe to our newsletter",
          "relevance": "1",
          "content": "Platform Overview Features Technology Documentation Solutions Overview Customer Success Solutions By Role Solutions By Industry Playbooks Lifetime Value Churn NLP Resources Content White Papers Webinars Videos Case Studies Articles Platform Tools Playbooks Trends Grunion Skater Education Overview Tutorials Fundamentals Company Overview Contact Blog Events Newsroom Partners Privacy Policy Platform Overview Features Technology Documentation Solutions Overview Customer Success Solutions By Role Solutions By Industry Playbooks Lifetime Value Churn NLP Resources Content White Papers Webinars Videos Case Studies Articles Platform Tools Playbooks Trends Grunion Skater Education Overview Tutorials Fundamentals Company Overview Contact Blog Events Newsroom Partners Introduction to K-means Clustering Andrea Trevino  | 12.06.16  \n Prerequisites Experience with the specific topic: Novice Professional experience: No industry experience Knowledge of machine learning is not required, but the reader should be familiar with basic data analysis (e.g., descriptive analysis) and the programming language Python. To follow along, download the sample dataset  here . Introduction to  K -means Clustering K -means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable  K . The algorithm works iteratively to assign each data point to one of  K  groups based on the features that are provided. Data points are clustered based on feature similarity. The results of the  K -means clustering algorithm are: 1. The centroids of the  K  clusters, which can be used to label new data 2. Labels for the training data (each data point is assigned to a single cluster) Rather than defining groups before looking at the data, clustering allows you to find and analyze the groups that have formed organically. The \"Choosing K\" section below describes how the number of groups can be determined.   Each centroid of a cluster is a collection of feature values which define the resulting groups. Examining the centroid feature weights can be used to qualitatively interpret what kind of group each cluster represents.   This introduction to the  K -means clustering algorithm covers: Common business cases where  K -means is used The steps involved in running the algorithm A Python example using delivery fleet data Business Uses The  K -means clustering algorithm is used to find groups which have not been explicitly labeled in the data. This can be used to confirm business assumptions about what types of groups exist or to identify unknown groups in complex data sets. Once the algorithm has been run and the groups are defined, any new data can be easily assigned to the correct group. This is a versatile algorithm that can be used for any type of grouping. Some examples of use cases are: Behavioral segmentation:\n Segment by purchase history Segment by activities on application, website, or platform Define personas based on interests Create profiles based on activity monitoring Inventory categorization:\n Group inventory by sales activity Group inventory by manufacturing metrics Sorting sensor measurements:\n Detect activity types in motion sensors Group images Separate audio Identify groups in health monitoring Detecting bots or anomalies:\n Separate valid activity groups from bots Group valid activity to clean up outlier detection In addition, monitoring if a tracked data point switches between groups over time can be used to detect meaningful changes in the data.  Algorithm The  Κ -means clustering algorithm uses iterative refinement to produce a final result. The algorithm inputs are the number of clusters  Κ  and the data set. The data set is a collection of features for each data point. The algorithms starts with initial estimates for the  Κ  centroids, which can either be randomly generated or randomly selected from the data set. The algorithm then iterates between two steps: 1. Data assigment step: Each centroid defines one of the clusters. In this step, each data point is assigned to its nearest centroid, based on the squared Euclidean distance. More formally, if  c i  is the collection of centroids in set  C , then each data point  x  is assigned to a cluster based on where  dist (  ·  )  is the standard ( L 2 ) Euclidean distance. Let the set of data point assignments for each  i th  cluster centroid be  S i . 2. Centroid update step: In this step, the centroids are recomputed. This is done by taking the mean of all data points assigned to that centroid's cluster. The algorithm iterates between steps one and two until a stopping criteria is met (i.e., no data points change clusters, the sum of the distances is minimized, or some maximum number of iterations is reached). This algorithm is guaranteed to converge to a result. The result may be a local optimum (i.e. not necessarily the best possible outcome), meaning that assessing more than one run of the algorithm with randomized starting centroids may give a better outcome. Choosing  K The algorithm described above finds the clusters and data set labels for a particular pre-chosen  K . To find the number of clusters in the data, the user needs to run the  K -means clustering algorithm for a range of  K  values and compare the results. In general, there is no method for determining exact value of  K , but an accurate estimate can be obtained using the following techniques. One of the metrics that is commonly used to compare results across different values of  K  is the mean distance between data points and their cluster centroid. Since increasing the number of clusters will always reduce the distance to data points, increasing  K  will  always  decrease this metric, to the extreme of reaching zero when  K  is the same as the number of data points. Thus, this metric cannot be used as the sole target. Instead, mean distance to the centroid as a function of  K  is plotted and the \"elbow point,\" where the rate of decrease sharply shifts, can be used to roughly determine  K . A number of other techniques exist for validating  K , including cross-validation, information criteria, the information theoretic jump method, the silhouette method, and the G-means algorithm. In addition, monitoring the distribution of data points across groups provides insight into how the algorithm is splitting the data for each  K . Example: Applying K-Means Clustering to Delivery Fleet Data As an example, we'll show how the  K -means algorithm works with a  sample dataset of delivery fleet driver data . For the sake of simplicity, we'll only be looking at two driver features: mean distance driven per day and the mean percentage of time a driver was >5 mph over the speed limit. In general, this algorithm can be used for any number of features, so long as the number of data samples is much greater than the number of features. Step 1: Clean and Transform Your Data For this example, we've already cleaned and completed some simple data transformations. A sample of the data as a  pandas DataFrame  is shown below. The chart below shows the dataset for 4,000 drivers, with the distance feature on the x-axis and speeding feature on the y-axis. Step 2: Choose K and Run the Algorithm Start by choosing  K =2.  For this example, use the Python packages  scikit-learn  and  NumPy  for computations as shown below: import numpy as np\nfrom sklearn.cluster import KMeans\n\n### For the purposes of this example, we store feature data from our \n### dataframe `df`, in the `f1` and `f2` arrays. We combine this into \n### a feature matrix `X` before entering it into the algorithm.\nf1 = df['Distance_Feature'].values \nf2 = df['Speeding_Feature'].values\n\nX=np.matrix(zip(f1,f2))\nkmeans = KMeans(n_clusters=2).fit(X) The cluster labels are returned in  kmeans.labels_ .  Step 3: Review the Results The chart below shows the results. Visually, you can see that the  K -means algorithm splits the two groups based on the distance feature. Each cluster centroid is marked with a star. Group 1 Centroid = (50, 5.2) Group 2 Centroid = (180.3, 10.5) Using domain knowledge of the dataset, we can infer that Group 1 is urban drivers and Group 2 is rural drivers. Step 4: Iterate Over Several Values of K Test how the results look for  K =4. To do this, all you need to change is the target number of clusters in the  KMeans()  function. kmeans = KMeans(n_clusters=4).fit(X) The chart below shows the resulting clusters. We see that four distinct groups have been identified by the algorithm; now speeding drivers have been separated from those who follow speed limits, in addition to the rural vs. urban divide. The threshold for speeding is lower with the urban driver group than for the rural drivers, likely due to urban drivers spending more time in intersections and stop-and-go traffic. Additional Notes and Alternatives Feature Engineering Feature engineering is the process of using domain knowledge to choose which data metrics to input as features into a machine learning algorithm. Feature engineering plays a key role in  K -means clustering; using meaningful features that capture the variability of the data is essential for the algorithm to find all of the naturally-occurring groups.   Categorical data (i.e., category labels such as gender, country, browser type) needs to be encoded or separated in a way that can still work with the algorithm.   Feature transformations, particularly to represent rates rather than measurements, can help to normalize the data. For example, in the delivery fleet example above, if total distance driven had been used rather than mean distance per day, then drivers would have been grouped by how long they had been driving for the company rather than rural vs. urban.   Alternatives A number of alternative clustering algorithms exist including DBScan, spectral clustering, and modeling with Gaussian mixtures. A dimensionality reduction technique, such as principal component analysis, can be used to separate groups of patterns in data.   One possible outcome is that there are no organic clusters in the data; instead, all of the data fall along the continuous feature ranges within one single group. In this case, you may need to revisit the data features to see if different measurements need to be included or a feature transformation would better represent the variability in the data. In addition, you may want to impose categories or labels based on domain knowledge and modify your analysis approach. For more information on K-means clustering, visit the  scikit learn site .  Want to keep learning? Download our  new study from Forrester  about the tools and practices keeping companies on the forefront of data science. Data Science Subscribe to our newsletter   Sign up today to receive the latest DataScience content in your inbox. Platform Features Technology Documentation Solutions Customer Success By Industry By Role Playbooks Resources Blog Education Education Tutorials Fundamentals Company Contact Events Newsroom   SLA Partners Connect © 2017 DataScience.com All Rights Reserved  · Privacy Policy · Terms of Use",
          "url": "https://www.datascience.com/blog/k-means-clustering"
        },
        {
          "title": "Machine learning : K-means Clustering",
          "relevance": "1",
          "content": "Posts Questions Discussions Announcements  No announcement yet. All Announcements  Sign In/Sign up\n     +4 NguyenDuong  Follow Published Jun 30th, 10:09 pm Machine learning : K-means Clustering Machine Learning  Jun 30th, 10:09 pm\n          126  1  0  Report\n     Trong bài trước, chúng ta học thuật toán  Hồi qui tuyến tính Linear Regression . Đây là thuật toán đơn giản nhất trong Supervised learning. Bài viết này chúng ta chuyển sang học về một thuật toán cơ bản trong Unsupervised learning - thuật toán  K-means clustering (phân nhóm K-means) . Đây là là một thuật toán khá gần gũi với tôi vì trong quá trình làm nghiên cứu ở đại học, tôi đã làm khá sâu về graph và đường đi ngắn nhất với bài toàn tìm  k Nearest Neighbor . Hiểu về K-Means Clustering Trước hết chúng ta sẽ tìm hiểu về thuật toán  K-means clustering  trước bằng ví dụ : Bài toán kích thước áo T-shirt Giả sử có một công ty định ra mắt một mẫu sản phẩn mới T-shirt vào thị trường. Tất nhiên họ sẽ phải sản xuất rất nhiều size để phù hợp với sự đa dạng của thị trường người dùng. Với định hướng đo, công ty đã tiến hành khảo sát dữ liệu  chiều cao  và  cân nặng  của người dùng, và vẽ nó thành 1 đồ thị như sau : Công ty này ko thể đủ nguồn lực để có thể sản xuất áo với tất cả mọi size. Thực tế này trong kinh doanh bạn cũng dễ dàng hiểu được. THay vì đó, họ sẽ chia số lượng người dùng thành các size như là Small, Medium, Large và sản xuất chỉ 3 mẫu như thế. 3 mẫu này là đủ khớp với tất cả mọi người và thị trường. Ở đây việc phân chia các người dùng vào 3 nhóm trên sẽ được xử lý bằng kĩ thuật phân nhóm K-means. Thuật toán này sẽ cho ta 3 size áo tối ưu nhất - thoã mãn tất cả mọi người. Tất nhiên, nếu như ko thể tìm được 3 size áo vừa vặn thoả mãn mọi người trong nhóm, công ty sẽ chia nhỏ nhóm thêm thành nhiều nhóm khác, có thể là 5, có thể là nhiều hơn nữa .... :) Bạn nhìn hình sẽ rõ nhé Làm thế nào mà chia nhỏ được thế Thuật toán này sẽ là một vòng lặp. Để đơn giản mình sẽ mô tả từng step của thuật toán và môt chút hình minh hoạ. Hãy xem nhóm dữ liệu như đồ thị dưới ( để dễ hình dung thì bạn hãy xem đây như là đồ thị về chiều cao - cân nặng của nhóm người dùng ). Chúng ta sẽ nhóm các dữ liệu thành 2 nhóm. STEP 1 :   Thuật toán này chọn ngẫu nhiên 2  trọng tâm . C1 và C2 ( thỉnh thoảng, bất kì 2 cặp điểm nào cũng sẽ được xem như là 2 cặp trọng tâm ). STEP 2 :   Tính toán khoảng cách từ mỗi điểm đến 2 trọng tâm đó. Nếu một dữ liệu thử nghiệm là gần với C1 hơn thì nó sẽ được gán nhãn '0'. Nếu đó là gần với C2 hơn, nó sẽ được dán nhãn là '1' (nếu bạn dùng nhiều trọng tâm hơn thì bạn sẽ có nhiều label hơn ví dụ như '2', '3' v.v... ). Trong hình, chúng ta sẽ tô màu cho các nhãn 0 là đỏ và xanh cho các nhãn 1. Chúng ta có hình như dưới : STEP 3 :  Tiếp theo chúng ta tính toán trung bình của tất cả các điểm màu xanh và đỏ riêng biệt. Kết quả này sẽ cho phép quyết định được  trọng tâm mới . C1 và C2 sẽ được tái thiết lập vị trí mới với các điểm dữ liệu mới ( chú ý, các hình ảnh dưới ko phải là giá trị thật đây chỉ là ví dụ thôi nhé ). Tiếp, chúng ta lại lặp lại bước 2. Chúng ta có kết quả như sau : Bây giờ, bước 2 và bước 2 sẽ được lặp đi lặp lại cho đến khi trọng tâm được hội tụ về 1 điểm cố định. Hoặc đơn giản là nó sẽ dừng lại ở trong một phạm vi - thoả mãn một chuẩn mà chúng ta đã đưa ra từ trước. Ví du như là đạt đến số lần vòng lặp tối đa đã định nghĩa trước, hoặc là đạt được một chuẩn chính xác nào đó .v...  Các điểm này sẽ thoả mãn tổng khoảng cách giữa dữ liệu test và trọng tâm tương ứng là nhỏ nhất.   Hoặc cũng có thể là tổng khoảng cách giữa C1 tới điểm đỏ và C2 tới điểm xanh là nhỏ nhất. Cuối cùng chúng ta có trọng tâm như sau : K-Means Clustering sử dụng trong OpenCV Mục tiêu của phần nay là sẽ giúp bạn sử dụng công cụ OpenCV - cụ thể là cv2.kmeans() để thực hiện thuật toán k-means. Tìm hiểu về Parameters samples  : Mẫu thì nên là kiểu dữ liệu của  np.float32 , và mỗi đối tượng cần được đặt trọng một cột duy nhất. nclusters (K) : Số nhóm yêu cầu tại thời điểm cuối cùng criteria  : Đây là tiêu chí chấm dứt vòng lặp. Khi tiêu chí này được thỏa mãn, thuật toán sẽ dừng vòng lặp. Trên thực tế, nó phải là một nhóm 3 thông số (type, max_iter, epsilon): 3.a - loại tiêu chí chấm dứt: Nó có 3 lá cờ như sau: \ncv2.TERM_CRITERIA_EPS - ngăn sự lặp lại thuật toán nếu đạt được độ chính xác nhất định - epsilon đạt được. \ncv2.TERM_CRITERIA_MAX_ITER - dừng thuật toán sau khi số lượng nhất định được lặp đi lặp lại \ncv2.TERM_CRITERIA_EPS max_iter + cv2.TERM_CRITERIA_MAX_ITER - ngăn chặn sự lặp đi lặp lại khi một trong các điều kiện trên nó được đáp ứng 3.b - max_iter - Một số nguyên xác định số lượng tối đa vòng lặp. 3.c - Độ chính xác - epsilon attempts : Cờ để xác định số lần các thuật toán được thực hiện bằng cách sử dụng việc đánh nhãn khởi tạo khác nhau. flags  : Cờ này được sử dụng để xác định trung tầm ban đầu được chọn như thế nào. Về cơ bản có 2 cờ được sử dụng là : cv2.KMEANS_PP_CENTERS và cv2.KMEANS_RANDOM_CENTERS. Output parameters compactness  : Đây là tổng của bình phương khoảng cách từ mỗi điểm đến trọng tâm tương tự của họ. labels  : Đây là mảng các label trong đó mỗi phần tử của mảng được đánh dấu '0', '1' ..... centers  : Đây là mảng trọng tâm của các nhóm. Bây giờ chúng ta sẽ thấy làm thế nào để áp dụng thuật toán K-Means với ví dụ. Dữ liệu với chỉ một biến Hãy xem xét, bạn có một bộ dữ liệu chỉ với một tính năng, tức là một chiều. Ví dụ như bài toán áo T-shirt ở trên nhưng chúng ta chỉ sử dụng mỗi chiều cao của người dùng để quyết định size áo . Chúng ta sẽ bắt đầu bằng cách tạo ra dữ liệu và vẽ nó trên đồ thị trong Matplotlib : import numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\n\nx = np.random.randint(25,100,25)\ny = np.random.randint(175,255,25)\nz = np.hstack((x,y))\nz = z.reshape((50,1))\nz = np.float32(z)\nplt.hist(z,256,[0,256]),plt.show()\n Chúng ta có  z  là một mảng có kích thước 50, và các giá trị khác nhau từ 0 đến 255. Cho  z  thành một vector cột.  Sau đó, chúng ta tạo dữ liệu của loại np.float32. Chúng ta sẽ có hình ảnh sau: Bây giờ chúng ta áp dụng thuật toán K-Means. Trước đó chúng ta cần xác định các tiêu chí. Tiêu chuẩn của tôi là, bất cứ khi nào 10 lần lặp của thuật toán được chạy, hoặc đạt được độ chính xác của epsilon = 1.0, thuật toán sẽ dừng và trả về kết quả : #  Define criteria = (  type , max_iter = 10 , epsilon = 1.0 ) \ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n \n#  Set flags (Just to avoid line  break in  the code) \nflags = cv2.KMEANS_RANDOM_CENTERS\n \n#  Apply KMeans \ncompactness,labels,centers = cv2.kmeans(z,2,None,criteria,10,flags)\n Việc này sẽ cho ta các giá trị về   compactness ,  label  và trọng tâm. Trong trường hợp này, ta có các trung tâm là 60 và 207. Nhãn sẽ có cùng kích thước với dữ liệu test, ở đây mỗi dữ liệu sẽ được gắn nhãn là '0', '1', '2' v.v... tùy thuộc vào trọng tâm của chúng. Bây giờ ta chia dữ liệu vào các nhóm khác nhau tùy thuộc vào nhãn của chúng. A  = z[labels== 0 ]\n B  = z[labels== 1 ]\n Bây giờ chúng ta sẽ vẽ A màu đỏ và B màu xanh và trọng tâm của chúng bằng màu vàng. # Now plot  'A' in  red,  'B' in  blue,  'centers' in  yellow\nplt.hist(A, 256 ,[ 0 , 256 ], color  =  'r' )\nplt.hist(B, 256 ,[ 0 , 256 ], color  =  'b' )\nplt.hist(centers, 32 ,[ 0 , 256 ], color  =  'y' )\nplt.show()\n Chúng ta sẽ có kết quả như sau : Dữ liệu với chỉ nhiều biến Trong ví dụ ở trên, chúng ta chỉ có chiều cao để quyêt định bài toán. Bây giơ, hãy cân nhắc luôn cả cân nặng nhé . Bài toán sẽ trở thành phân nhóm tập các chiều cao - cân nặng. Chú ý rằng ở ví dụ trước, chúng ta lưu dữ liệu vào một vector đơn. Mỗi biến sẽ được sắp xếp trong một cột, mỗi cột sẽ tương ứng với một giá trị test được đẩy vào. Trong trường hợp này, giả sử chúng ta có tập dữ liệu 50x2, tức là sẽ có cặp chiều cao - cân nặng của 50 người. Cột đầu tiên sẽ tương ứng với chiều cao của 50 người và cột thứ 2 là cân nặng của họ. Dòng dầu tiên tương ứng với 2 yếu tố ( chiều cao - cân nặng ) của người thứ nhất. v.v... tương tự như vậy cho đến dòng số 50. Giải thích có vẻ hơi dài dòng nhưng nếu bạn nào còn nhớ kiến thức toán về ma trận hẳn sẽ định hình được ngay ý niệm này trong đầu : Tiếp, chúng ta sẽ code như sau : import numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\n\nX = np.random.randint(25,50,(25,2))\nY = np.random.randint(60,85,(25,2))\nZ = np.vstack((X,Y))\n\n # convert to np.float32 \nZ = np.float32(Z)\n\n # define criteria and apply kmeans() \ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\nret,label,center=cv2.kmeans(Z,2,None,criteria,10,cv2.KMEANS_RANDOM_CENTERS)\n\n # Now separate the data, Note the flatten() \nA = Z[label.ravel()==0]\nB = Z[label.ravel()==1]\n\n # Plot the data plt.scatter(A[:,0],A[:,1]) plt.scatter(B[:,0],B[:,1],c = 'r') plt.scatter(center[:,0],center[:,1],s = 80,c = 'y', marker = 's') \nplt.xlabel('Height'),plt.ylabel('Weight')\nplt.show()\n Kết quả chúng ta có được : Color Quantization Color Quantization là một process giảm số lượng màu trong một hình ảnh. Lý do để giảm chất lượng ảnh là để giảm thiểu dung lượng nhớ và tải server or đơn giản là đôi khi trên một số thiết bị có thể có giới hạn sao cho nó có thể sản xuất chỉ số lượng màu giới hạn. Trong những trường hợp đó, lượng tử hóa màu được thực hiện. Ở đây chúng ta sử dụng k-means clustering để \"trung bình hoá màu\". Hẳn là bạn đã nhận ra sự liên quan rồi nhỉ. Màu sẽ có 3 biến là R, G, B. Vì vậy, chúng ta cần định hình lại image vào một mảng có kích thước Mx3 (M là số pixel trong images). Sau khi phân nhóm, chúng ta thay giá trị trọng tâm  ( R, G, B ) cho tất cả các điểm ảnh sao cho hình ảnh mới sẽ chỉ có một số lượng màu xác định. Tiếp tục, chúng ta lại reshape nó trở lại dạng hình ảnh bình thường : Code sẽ như sau : import numpy as np\nimport cv2\n\nimg = cv2.imread('home.jpg')\nZ = img.reshape((-1,3))\n\n # convert to np.float32 \nZ = np.float32(Z)\n\n # define criteria, number of clusters(K) and apply kmeans() \ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\nK = 8\nret,label,center=cv2.kmeans(Z,K,None,criteria,10,cv2.KMEANS_RANDOM_CENTERS)\n\n # Now convert back into uint8, and make original image \ncenter = np.uint8(center)\nres = center[label.flatten()]\nres2 = res.reshape((img.shape))\n\ncv2.imshow('res2',res2)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n Kết quả với K = 8 : Tham khảo https://pythonprogramming.net/flat-clustering-machine-learning-python-scikit-learn/ http://blog.galvanize.com/introduction-k-means-cluster-analysis/ https://www.dezyre.com/data-science-in-r-programming-tutorial/k-means-clustering-techniques-tutorial http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_ml/py_kmeans/py_kmeans_opencv/py_kmeans_opencv.html http://machinelearningcoban.com/2017/01/01/kmeans/ NguyenDuong @duongichi Follow  509\n          25\n          39\n         \n                            Clip this post\n                         Have problems with  Machine Learning ?  Ask on Viblo » Comments  No comments yet.\n         +4 • • • Facebook Viblo Hot authors Resources Posts Questions Videos Tags Authors Help Terms  RSS Feed Browser extension Atom plugin Feedback FAQ \n                    © 2017  Viblo . All rights reserved.\n                 \n        Viblo - Free service for technical knowledge sharing\n     Feedback \n                        Your feedback will be shared with our product delivery teams, and taken into consideration\n                        for future development.\n                     Name Email Send \n                Sign In\n             \n                Sign Up\n             Join us to find useful information required to improve your skills \n            Forgot your password?\n         Sign In Free  service for technical knowledge  sharing Join us to find useful information required to improve your skills \n                    I agree to  Viblo Terms of Service Sign Up or",
          "url": "https://viblo.asia/p/machine-learning-k-means-clustering-Az45bNRo5xY"
        },
        {
          "title": "10 thuật toán học máy mà các kỹ sư cần biết",
          "relevance": "1",
          "content": "Posts Questions Discussions Announcements  No announcement yet. All Announcements  Sign In/Sign up\n     +1 Vương Hưng  Follow Published Aug 1st, 12:26 am 10 thuật toán học máy mà các kỹ sư cần biết Machine Learning  Aug 1st, 12:26 am\n          643  3  0  Report\n     \nKhông còn nghi ngờ gì nữa, lĩnh vực học máy / trí tuệ nhân tạo (AI) đã ngày càng trở nên phổ biến hơn trong vài năm qua. Một nhánh nhỏ của nó là Big Data đang là xu hướng hot nhất trong ngành công nghệ cao hiện nay, học máy trở nên rất mạnh mẽ để đưa ra các dự đoán hoặc gợi ý được tính dựa trên số lượng lớn dữ liệu. Một số ví dụ phổ biến nhất về học máy là các thuật toán của Netflix để đưa ra các gợi ý về phim dựa trên những bộ phim mà bạn đã xem trong quá khứ hoặc các thuật toán của Amazon đề xuất các sách dựa trên sách mà bạn đã mua trước đây. Những hệ thống gợi ý đó (Recommended System) sẽ giúp ích khá nhiều cho những người dùng trong việc đưa ra những lựa chọn của mình. Ngoài ra, AI còn có những khả năng như nhận dạng biển số xe tự động, giúp sửa lỗi chính tả, tạo các con robot có khả năng giao tiếp với con người,...Còn nhiều nhiều những khả năng mà AI có thể làm được. AI đang phát triển và sẽ còn phát triển mạnh trong tương lai. Machine Learning được chia thành 3 nhánh chính: supervised learning (học có giám sát), unsupervised learning (học không có giám sát), và reinforcement learning (học tăng cường). Học có giám sát được dùng trong trường hợp một thuộc tính (nhãn) có sẵn cho một tập dữ liệu nhất định (tập huấn luyện), nhưng thiếu và cần được dự đoán cho các trường hợp khác. Học không có giám sát thì ngược lại, nó được sử dụng trong trường hợp khám phá các mối quan hệ tiềm ẩn trong một tập dữ liệu không được gán nhãn (các mục không được chỉ định trước). Học tăng cường thì nằm giữa 2 loại trên - có một số hình thức phản hồi có sẵn cho mỗi bước tiên đoán hoặc hành động, nhưng không có nhãn chính xác hoặc thông báo lỗi Dưới đây là 10 thuật toán rơi vào 2 loại đầu tiên, hi vọng vẫn đủ để bạn quan tâm: Học có giám sát 1. Cây quyết định (Decision Trees) Cây quyết định là công cụ hỗ trợ quyết định sử dụng biểu đồ dạng cây hoặc mô hình của các quyết định và kết quả có thể xảy ra của chúng, bao gồm kết quả sự kiện ngẫu nhiên, chi phí tài nguyên và lợi ích. Dưới đây là một ví dụ điển hình của cây quyết định: Cây quyết định này cho ta gợi ý về việc có đi đá bóng hay không. Ví dụ, quang cảnh có  nắng , độ ẩm  trung bình  thì tôi sẽ đi đá bóng. Ngược lại, nếu trời  mưa , gió  mạnh  thì tôi sẽ không đi đá bóng nữa. Cây quyết định tuy là mô hình khá cũ, khá đơn giản những vẫn còn được ứng dụng khá nhiều và hiệu quả. Đứng dưới góc nhìn thực tế, cây quyết định là một danh sách tối thiểu các câu hỏi dạng yes/no mà người ta phải hỏi, để đánh giá xác suất đưa ra quyết định đúng đắn. 2. Phân loại Bayes (Naïve Bayes Classification) Phân loại Bayes là một nhóm các phân loại xác suất đơn giản dựa trên việc áp dụng định lý Bayes với các giả định độc lập (naïve) giữa các đặc tính. Trong đó: P(A|B)  là xác suất có điều kiện A khi biết B, P(A)  là xác suất giả thuyết A (tri thức có được về giải thuyết A trước khi có dữ liệu B), P(B|A)  là xác suất có điều kiện B khi biết giả thuyết A, P(B)  là xác suất của dữ liệu quan sát B không quan tâm đến bất kỳ giả thuyết A nào. Thuật toán này được áp dụng trong một số bài toán như: Đánh dấu một email là spam hay không. Phân loại bài viết tin tức thuộc lĩnh vực công nghệ, chính trị hay thể thao. Kiểm tra một đoạn văn bản mang cảm xúc tích cực hay tiêu cực. Sử dụng cho các phần mềm nhận diện khuôn mặt. \n... 3. Hồi quy tuyến tính (Ordinary Least Squares Regression) Nếu bạn biết thống kê, bạn có thể đã nghe nói về hồi quy tuyến tính trước đây.  Bình phương nhỏ nhất  là một phương pháp để thực hiện hồi quy tuyến tính. Bạn có thể suy nghĩ về hồi quy tuyến tính như là nhiệm vụ kẻ một đường thẳng đi qua một tập các điểm. Có rất nhiều chiến lược có thể thực hiện được, và chiến lược \"bình phương nhỏ nhất\" sẽ như thế này - Bạn có thể vẽ một đường thẳng, và sau đó với mỗi điểm dữ liệu, đo khoảng cách thẳng đứng giữa điểm và đường thẳng. Đường phù hợp nhất sẽ là đường mà các khoảng cách này càng nhỏ càng tốt. Một số ví dụ là người ta có thể sử dụng mô hình này để dự đoán giá cả (nhà đất, chứng khoán), điểm số,... 4. Hồi quy logistic (Logistic Regression) Hồi quy logistic là một cách thống kê mạnh mẽ để mô hình hóa một kết quả nhị thức với một hoặc nhiều biến giải thích. Nó đo lường mối quan hệ giữa biến phụ thuộc phân loại và một hoặc nhiều biến độc lập bằng cách ước tính xác suất sử dụng một hàm logistic, là sự phân bố tích lũy logistic. Thuật toán này được sử dụng trong một số trường hợp: Điểm tín dụng ( quyết định có cho khách hàng vay vốn hay không) Đo mức độ thành công của chiến dịch marketing Dự đoán doanh thu của một sản phẩm nhất định Dự đoán động đất \n.... 5. Support Vector Machines (SVM) SVM là phương pháp phân loại nhị phân. Cho một tập các điểm thuộc 2 loại trong môi trường N chiều, SVM cố gắng tìm ra N-1 mặt phẳng để phân tách các điểm đó thành 2 nhóm. \nVí dụ, cho một tập các điểm thuộc 2 loại như hình bên dưới, SVM sẽ tìm ra một đường thẳng nhằm phân cách các điểm đó thành 2 nhóm sao cho khoảng cách giữa đường thẳng và các điểm xa nhất có thể. Xét về quy mô, một số vấn đề lớn nhất đã được giải quyết bằng cách sử dụng SVM (với việc thực hiện sửa đổi phù hợp) ví dụ như hiển thị quảng cáo, phát hiện giới tính dựa trên hình ảnh, phân loại hình ảnh có quy mô lớn ... 6. Kết hợp các phương pháp (Ensemble Methods) Phương pháp này dựa rên sự kết hợp của một vài phương pháp kể trên để dự đoán kết quả, sau đó sẽ đưa ra kết quả cuối cùng dựa vào trọng số của từng phương pháp Vậy phương pháp này hoạt động như thế nào và tại sao nó lại ưu việt hơn các mô hình cá nhân? Trung bình sai số (bias): một số phương pháp hoạt động tốt và cho sai số nhỏ, ngược lại cũng có một số phương pháp cho sai số lớn. Trung bình ta được một sai số chấp nhận được, có thể nhỏ hơn sai số khi sử dụng duy nhất một phương pháp. Giảm độ phụ thuộc vào tập dữ liệu (variance): ý kiến tổng hợp của một loạt các mô hình sẽ ít nhiễu hơn là ý kiến đơn lẻ của một mô hình. Trong lĩnh vực tài chính, đây được gọi là đa dạn hóa - một - một danh mục hỗn hợp của nhiều cổ phiếu sẽ ít biến động hơn so với chỉ một trong số các cổ phiếu riêng lẻ. Giảm over-fit: over-fit là hiện tượng khi mô hình hoạt động rất tốt với dữ liệu training, nhưng rất kém đối với dữ liệu test. Việc kết hợp nhiều mô hình cùng lúc giúp giảm vấn đề này. Học không có giám sát 7. Thuật toán gom cụm (Clustering Algorithms) Gom cụm là nhiệm vụ nhóm một tập hợp các đối tượng sao cho các đối tượng trong cùng một nhóm (cluster) giống nhau hơn so với các đối tượng trong các nhóm khác. Gom cụm có nhiều phương pháp khác nhau, sau đây là một vài trong số đó: Gom cụm dựa vào tâm điểm (Centroid-based algorithms) Gom cụm dựa vào tính kết nối (Connectivity-based algorithms) Gom cụm dựa vào mật độ (Density-based algorithms) Gom cụm dựa vào xác suất (Probabilistic) Gom cụm dựa trên giảm chiều dữ liệu (Dimensionality Reduction) Gom cụm dựa trên mạng nơ-ron/deep leanring (Neural networks / Deep Learning) 8.Phân tích thành phần chính (Principal Component Analysis - PCA) PCA là một thuật toán thống kê sử dụng phép biến đổi trực giao để biến đổi một tập hợp dữ liệu từ một không gian nhiều chiều sang một không gian mới ít chiều hơn (2 hoặc 3 chiều) nhằm tối ưu hóa việc thể hiện sự biến thiên của dữ liệu. Phép biến đổi tạo ra những ưu điểm sau đối với dữ liệu: Giảm số chiều của không gian chứa dữ liệu khi nó có số chiều lớn, không thể thể hiện trong không gian 2 hay 3 chiều. Xây dựng những trục tọa độ mới, thay vì giữ lại các trục của không gian cũ, nhưng lại có khả năng biểu diễn dữ liệu tốt tương đương, và đảm bảo độ biến thiên của dữ liệu trên mỗi chiều mới. Tạo điều kiện để các liên kết tiềm ẩn của dữ liệu có thể được khám phá trong không gian mới, mà nếu đặt trong không gian cũ thì khó phát hiện vì những liên kết này không thể hiện rõ. Đảm bảo các trục tọa độ trong không gian mới luôn trực giao đôi một với nhau, mặc dù trong không gian ban đầu các trục có thể không trực giao. Một số ứng dụng của PCA bao gồm nén, đơn giản hóa dữ liệu để dễ dàng học tập, hình dung. Lưu ý rằng kiến thức miền là rất quan trọng trong khi lựa chọn có nên tiếp tục với PCA hay không. Nó không phù hợp trong trường hợp dữ liệu bị nhiễu (tất cả các thành phàn của PCA đều có độ biến thiên khá cao) 9. Singular Value Decomposition Trong đại số tuyến tính, SVD là một thừa số của ma trận phức tạp thực sự. Đối với  một ma trận m*n đã xác định M, tồn tại một sự phân rã sao cho M = UΣV, trong đó U và V là các ma trận đơn nhất và Σ là một ma trận chéo. PCA thực ra là một ứng dụng đơn giản của SVD. Trong khoa học máy tính, các thuật toán nhận dạng khuôn mặt đầu tiên được sử dụng PCA và SVD để biểu diễn khuôn mặt như là một sự kết hợp tuyến tính của \"eigenfaces\", làm giảm kích thước, và sau đó kết hợp khuôn mặt với các tính chất thông qua các phương pháp đơn giản. Mặc dù các phương pháp hiện đại phức tạp hơn nhiều, nhiều người vẫn còn phụ thuộc vào các kỹ thuật tương tự. 10. Phân tích thành phần độc lập (Independent Component Analysis) ICA là một kỹ thuật thống kê nhằm tìm ra các yếu tố ẩn nằm dưới các bộ biến ngẫu nhiên, các phép đo hoặc tín hiệu. ICA định nghĩa một mô hình phát sinh cho dữ liệu đa biến quan sát được, thường được đưa ra như một cơ sở dữ liệu lớn các mẫu. Trong mô hình, các biến số dữ liệu giả định là hỗn hợp tuyến tính của một số biến tiềm ẩn chưa biết, và hệ thống hỗn hợp cũng không rõ. Các biến tiềm ẩn được giả định không gaussian và độc lập với nhau, và chúng được gọi là các thành phần độc lập của dữ liệu được quan sát. ICA có liên quan đến PCA, nhưng nó là một kỹ thuật mạnh hơn nhiều, có khả năng tìm ra các yếu tố bên dưới của các nguồn trong khi những phương pháp cổ điển thất bại hoàn toàn. Ứng dụng của nó bao gồm hình ảnh kỹ thuật số, cơ sở dữ liệu tài liệu, chỉ số kinh tế và đo lường tâm lý. \nKết thúc bài viết ở đây, hi vọng bạn đọc đã có những cái nhìn tổng quan về các thuật toán phổ biến trong AI. Nếu cảm thấy thích thú, hãy đào sâu hơn về chúng để có thể tạo ra những ứng dụng có \"trí tuệ nhân tạo\" phục vụ cho mọi người. Nguồn http://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html Vương Hưng @vuong.xuan.hung Follow  485\n          16\n          26\n         \n                            Clip this post\n                         Have problems with  Machine Learning ?  Ask on Viblo » Comments  No comments yet.\n         +1 • • • Facebook Viblo Hot authors Resources Posts Questions Videos Tags Authors Help Terms  RSS Feed Browser extension Atom plugin Feedback FAQ \n                    © 2017  Viblo . All rights reserved.\n                 \n        Viblo - Free service for technical knowledge sharing\n     Feedback \n                        Your feedback will be shared with our product delivery teams, and taken into consideration\n                        for future development.\n                     Name Email Send \n                Sign In\n             \n                Sign Up\n             Join us to find useful information required to improve your skills \n            Forgot your password?\n         Sign In Free  service for technical knowledge  sharing Join us to find useful information required to improve your skills \n                    I agree to  Viblo Terms of Service Sign Up or",
          "url": "https://viblo.asia/p/10-thuat-toan-hoc-may-ma-cac-ky-su-can-biet-RQqKLn8ml7z"
        },
        {
          "title": "Top Ad unit 728 × 90",
          "relevance": "0",
          "content": "Home Q&A Sitemap \nIEEV\n Nghiên cứu là để sẻ chia Top Ad unit 728 × 90 Latest news \nrecent\n Home Data Mining Thuật Toán Triển Khai K-Mean và Ứng Dụng \nK-Mean và Ứng Dụng\n Bioz Nguyen 3:27:00 PM Data Mining ,\n Thuật Toán ,\n Triển Khai [Nguồn từ: Internet] \n1.  Giới thiệu \nThuật toán phân hoạch (clustering) thực hiện việc phân nhóm dữ liệu hay chia một tập dữ liệu lớn thành những tập nhỏ bao gồm các phần tử có sự tương đồng nào đó. K-Means là một thuật toán phân hoạch thuộc nhóm các thuật toán máy học không cần sự định hướng trước (unsupervised machine learning) được biết đến khá rộng rãi. Thuật toán này không cần quá trình huấn luyện để học, ghi nhớ các mô hình tri thức tách bạch với quá trình chạy. Điều này cũng đồng nghĩa với việc, K-means cho ra kết quả cuối cùng không thể đoán trước và rất khác nhau dù cho cùng điều kiện đầu vào. Quá trình học, cập nhật và chạy diễn ra đồng thời. K-means được sử dụng khi bạn có một tập hợp gồm các điểm dữ liệu (data point) không được dán nhãn ( dữ liệu chưa được phân loại theo nhóm hay danh mục ). Mục tiêu của thuật toán là tìm ra K nhóm của tập dữ liệu này với K cho trước. Thuật toán xử lý dựa trên thao tác lặp (iterative) để gán mỗi điểm dữ liệu vào 1 trong K nhóm dựa vào thuộc tính (feature) đã cho của từng điểm dữ liệu. Các điểm dữ liệu được phân nhóm (cluster) theo tiêu chí tương đồng về các thuộc tính của chúng. Kết quả của thuật toán K-means là: \n- K điểm trung tâm (centroid) của K nhóm (cluster), K điểm này có thể được dùng để tiếp tục gắn nhãn cho dữ liệu mới khi chúng được thêm vào tập dữ liệu ban đầu. \n- Gắn nhãn cho tập điểm dữ liệu (mỗi điểm dữ liệu được gán cho một nhóm xác định ứng với một điểm trung tâm) \nBỏ qua việc định nghĩa các nhóm nhãn dữ liệu trước khi thực thi thuật toán, việc phân cụm cho phép bạn tìm và phân tích các nhóm hình thành một cách có sắp xếp. Việc chọn số nhóm K là thao tác quan trọng cần quan tâm. \n2.  Ứng dụng \nThuật toán K-means clustering thường được sử dụng để tìm ra các nhóm mà không được gắn nhãn một cách rõ ràng trong tập dữ liệu. Điều này thường có ý nghĩa trong việc xác nhận tính đúng của các giả thiết về những kiểu nhóm đang tồn tại hay chỉ ra những nhóm chưa biết trong một tập dữ liệu phức tạp. Một khi thuật toán đã chạy và các nhóm đã được định nghĩa, bất kỳ dữ liệu mới nào đều có thể dễ dàng được gắn vào nhóm thích hợp. \nĐây là một thuật toán rất phổ biến, được ứng dụng với bất kỳ kiểu phân hoạch, nhóm nào. Một vài ví dụ như sau: \n- Một công ty chuyển vận muốn mở chuỗi các trung tâm giao nhận hàng trong một thành phố. Trong tình huống đó họ cần đối mặt với các vấn đề như sau: \n   + Họ cần phải phân tích để biết khu vực mà có nhiều đơn đặt hàng thường xuyên. \n   + Họ cần biết bao nhiêu trung tâm nên được mở để có thể đảm bảo giao nhận hiệu quả trong một khu vực. \n   + Họ cần tìm ra vị trí thích hợp để mở trung tâm trong các khu vực nhằm đảm bảo tối ưu khoảng cách giữa trung tâm và khách hàng của họ. [Nguồn từ: Internet] \n- Phân tích thông tin tội phạm có liên quan tới nghiện ma túy ở Việt Nam. Nguồn dữ liệu bao gồm các loại hình phạm tội do nhiều loại thuốc khác nhau gây ra, bao gồm Heroin, Cocaine cho tới các loại gây nghiện trong toa bác sĩ, đặc biệt là với trẻ vị thành niên. Tỉ lệ phạm tội do lạm dụng thuốc có thể giảm nhờ việc xây dựng các trung tâm cai nghiện tại chổ trong những khu vực chịu tác động lớn bởi loại hình tội phạm này. Với nguồn dữ liệu được cho, các mục tiêu khác nhau có thể được định ra. Ví dụ như: \n   + Phân loại tội phạm dựa trên việc lạm dụng dược chất để tìm ra các nguyên nhân chính. \n   + Phân loại tội phạm dựa trên nhóm tuổi. \n   + Phân tích dữ liệu để xác định hình thức trung tâm cai nghiện cần xây dựng \n   + Tìm ra số lượng trung tâm cai nghiện cần xây dựng để đạt hiệu quả trong việc giãm tỉ lệ tội phạm do nghiện thuốc. \nNgoài ra, việc theo dõi nếu một điểm dữ liệu được giám sát chuyển đổi qua lại giữa các nhóm theo thời gian có thể đem lại thông tin hữu ích trong thay đổi của tập dữ liệu. \n3.  Thuật toán Công thức biểu diễn thuật toán \nThuật toán K-Means hoạt động dựa vào các bước xử lý theo trình tự: \n-  Bước 1 : Cho dữ liệu đầu vào là một tập S gồm M điểm (mỗi điểm là một phần tử có N thuộc tính (feature), hay có thể gọi là một vector N chiều), và giá trị K (số cluster). \n-  Bước 2 :  với mỗi cluster trong K cluster ta cần có một điểm trung tâm (cluster centroid hay Mean), vì vậy ta cần khởi tạo giá trị cho K điểm ngẫu nhiên, mỗi điểm cùng có N thuộc tính và giá trị nằm trong khoảng biên giá trị tương ứng với thuộc tính đó của tập dữ liệu được cho. Đôi khi người ta chỉ đơn giản là chọn ngẫu nhiên K điểm trung tâm từ trong số các điểm dữ liệu thuộc tập được cho. \n-  Bước  3: Lần lượt kiểm tra và nhóm mỗi điểm trong tập S vào một trong K nhóm mà nó gần nhất (dựa vào sự tương đồng, hay khoảng cách) với điểm trung tâm của nhóm đó. Khoảng cách Euclid \nCó rất nhiều dạng khoảng cách hay độ tương đồng ( tham khảo thêm ) có thể được sử dụng, trong đó phổ biến nhất có lẽ là khoảng cách Euclid. \n-  Bước 4 : Cập nhật giá trị của các điểm trung tâm bằng giá trị trung bình của tập điểm trong mỗi nhóm. \n-  Bước 5 : lặp lại bước 3 và 4 cho đến khi giá trị điểm trung tâm không còn thay đổi, khi đó thuật toán kết thúc. \n4.  Lựa chọn thay thế \nCó nhiều thuật toán tương tự có thể được sử dụng để thay thế cho K-mean clustering như DBScan, spectral clustering, và mô hình phức hợp Gaussian (Gaussian mixture), hay một kỹ thuật giảm chiều dữ liệu như PCA (principal component analysis) có thể được dùng để phân nhóm các mẫu (pattern) dữ liệu. \nBinh Nguyen - Bioz K-Mean và Ứng Dụng  \n        Reviewed by  Bioz Nguyen \n        on \n         \n3:27:00 PM\n  \n        Rating:  5 Tags : Data Mining Thuật Toán Triển Khai Tweet Share Share Share Share Bioz Nguyen About  Sweetheme  Number of Entries :  35 We are Developers Team do our best to create beautiful work for our clients. Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s. \nTriển Khai\n Next  You are viewing Most Recent Post \nSubscribe to:\n Post Comments (Atom) Bioz Nguyen Đừng nói nữa hãy làm đi! Social Counter pinterest [120] Followers instagram [502] Followers dribbble [10k] Followers youtube [4.5k] Followers rss [860] Followers gplus [250] Followers twitter [1k] Followers facebook [634] Followers FIND US ON FACEBOOK \nfbbox/https://www.facebook.com/iloveieev\n Blog Archive Blog Archive 05/21 - 05/28 (1) 05/14 - 05/21 (1) 03/26 - 04/02 (1) 01/08 - 01/15 (1) 05/22 - 05/29 (1) 05/15 - 05/22 (1) 05/01 - 05/08 (1) 04/24 - 05/01 (1) 03/20 - 03/27 (2) 03/13 - 03/20 (1) 01/17 - 01/24 (1) 08/23 - 08/30 (1) 08/02 - 08/09 (1) 07/19 - 07/26 (5) 07/12 - 07/19 (1) 06/07 - 06/14 (1) 04/19 - 04/26 (1) 03/22 - 03/29 (1) 12/14 - 12/21 (1) 02/16 - 02/23 (1) 12/29 - 01/05 (1) 11/24 - 12/01 (2) 08/11 - 08/18 (1) 07/28 - 08/04 (1) 06/30 - 07/07 (1) 06/16 - 06/23 (1) 05/19 - 05/26 (1) 04/28 - 05/05 (1) 04/07 - 04/14 (1) 03/31 - 04/07 (1) 12/23 - 12/30 (1) 11/25 - 12/02 (1) 10/28 - 11/04 (1) 09/02 - 09/09 (1) 07/15 - 07/22 (1) 03/25 - 04/01 (1) 03/18 - 03/25 (2) 02/05 - 02/12 (4) 01/15 - 01/22 (2) 11/27 - 12/04 (1) 10/30 - 11/06 (3) 10/02 - 10/09 (1) 09/25 - 10/02 (1) 09/18 - 09/25 (1) 06/26 - 07/03 (1) 06/19 - 06/26 (1) 06/05 - 06/12 (1) 05/08 - 05/15 (1) 05/01 - 05/08 (1) 04/10 - 04/17 (2) 03/27 - 04/03 (1) 03/20 - 03/27 (1) 03/13 - 03/20 (2) 02/27 - 03/06 (1) 02/20 - 02/27 (1) 01/30 - 02/06 (1) 01/23 - 01/30 (5) 01/16 - 01/23 (1) 12/19 - 12/26 (1) 12/05 - 12/12 (2) 11/28 - 12/05 (1) 11/21 - 11/28 (1) 10/24 - 10/31 (1) 10/10 - 10/17 (1) 10/03 - 10/10 (1) 09/12 - 09/19 (1) 05/02 - 05/09 (1) 03/28 - 04/04 (2) 03/21 - 03/28 (4) 03/07 - 03/14 (2) 01/24 - 01/31 (1) 01/17 - 01/24 (2) 12/27 - 01/03 (1) 12/13 - 12/20 (1) 12/06 - 12/13 (1) 11/29 - 12/06 (1) 11/01 - 11/08 (4) 09/13 - 09/20 (2) 09/06 - 09/13 (4) 07/12 - 07/19 (1) 07/05 - 07/12 (1) 06/28 - 07/05 (1) 06/21 - 06/28 (2) 06/14 - 06/21 (6) 06/07 - 06/14 (4) 05/24 - 05/31 (2) 05/17 - 05/24 (18) 01/18 - 01/25 (1) Recents \nrecentposts\n Comments \nrecentcomments\n Popular Ma trận (Matrix), Định thức (Determinant), ma trận liên hợp (Adjugate Matrix)  - Ma trận  (matrix) có thể được hiểu đơn giản là một tập hợp các số được sắp xếp trong một bảng (table), hay lưới (grid). Chính từ khái niệ... Standard Deviation (Độ lệch chuẩn)  Độ lệch chuẩn (Standard Deviation) là một khái niệm khá thông dụng trong quá trình khảo sát ảnh (Image Statistics). Một trong những tham số... Phân biệt Mean, Median, Mode, và Range     Trong cuộc sống cũng như học tập có rất nhiều các kiến thức dù rất dễ mà không hiểu tại sao học hoài cứ quên và đọc mãi vẫn cứ lộn, nhầm... Min Filter, Max Filter, Min - Max Filter và Mid-Point Filter  Trong bài viết này Bioz sẽ giới thiệu một loạt các bộ lọc đơn giản có liên quan tới giá trị cực đại ( Max ) và giá trị cực tiểu ( Min ). Cũ... Kiểm tra số nguyên tố (Primality test) - lời giải đầu tiên  Kiểm tra một số có phải số nguyên tố hay không là một bài toán khá quan trọng trong khoa học máy tính. Vì số nguyên tố được sử dụng rất rộn... Follow by Email Tags Chuyên Đề Computer Vision Data Mining Dữ Liệu Test Hệ Thống Nhúng Kiến Thức Nền Lập Trình Mã Hóa Mã Nguồn Mở Ngôn Ngữ C Ngôn Ngữ PHP Nhiếp Ảnh OpenCV Phân Vùng Raspberry Pi Sắp Xếp Tài Liệu Thuật Toán Tìm Kiếm Triển Khai Xác Suất Xử Lý Âm thanh Xử Lý Ảnh Ý Tưởng RANDOM POSTS \nrandomposts\n Home Sitemap  Contact us  All Rights Reserved by  IEEV  © 2009 - 2016 Powered By  Blogger , Designed by  Sweetheme Contact Form \nName\n \nEmail\n * \nMessage\n * \nPowered by  Blogger .\n",
          "url": "http://www.ieev.org/2017/05/k-mean-va-ung-dung.html"
        },
        {
          "title": "PHƯƠNG PHÁP NÉN ẢNH SỬ DỤNG MẠNG NƠRON NHÂN TẠO VÀ K-MEANS.",
          "relevance": "0",
          "content": "CÁC SỐ ĐÃ ĐĂNG Năm 2017 Số 9(118).2017-Quyển 1 Số 8(117).2017 Số 7(116).2017 Số 6(115).2017 Số 5(114).2017-Quyển 1 Số 5(114).2017-Quyển 2 Số 4(113).2017 Số 3(112).2017-Quyển 2 Số 3(112).2017-Quyển 1 Số 2(111).2017-Quyển 1 Số 1(110).2017 Năm 2016 12(109).2016 11(108).2016 Quyển 2 11(108).2016 Quyển 1 Số 10(107).2016 Số 9(106).2016 Số 8(105).2016 Số 7(104).2016 Số 6(103).2016 Số 5(102).2016 Số 4(101).2016 Số 3(100).2016 Số 2(99).2016 Số 1(98).2016 Năm 2015 Số 12(97).2015, Quyển 2 Số 12(97).2015, Quyển 1 Số 11(96).2015, Quyển 2 Số 11(96).2015, Quyển 1 Số 10(95).2015 Số 9(94).2015 Số 8(93).2015 Số 7(92).2015 Số 6(91).2015 Số 5(90).2015 Số 4(89).2015 Số 3(88).2015 Số 1(86).2015 Số 2(87).2015 Năm 2014 Số 12(85).2014, Quyển 2 Số 12(85).2014, Quyển 1 Số 11(84).2014, Quyển 2 Số 11(84).2014, Quyển 1 Số 10(83).2014 Số 9(82).2014 Số 8(81).2014 Số 7(80).2014 Số 6(79).2014, Quyển 2 Số 6(79).2014, Quyển 1 Số 5(78).2014 Số 4(77).2014 Số 3(76).2014 Số 2(75).2014 Số 1(74).2014, Quyển 2 Số 1(74).2014, Quyển 1 Năm 2013 Số 12(73).2013 Quyển 2 Số 12(73).2013 Quyển 1 Số 11(72).2013 Số 10(71).2013 Số 9(70).2013 Số 8(69).2013 Số 7(68).2013 Số 6(67).2013 Số 5(66).2013 Số 4(65).2013 Số 3(64).2013 Số 2(63).2013, quyển 2 Số 2(63).2013, quyển 1 Số 1(62).2013 Năm 2012 Số 12(61), quyển 3 Số 12(61), quyển 2 Số 12(61), quyển 1 Số 11(60).quyển 2 Số 11(60).quyển 1 Số 10(59) Số 9(58), quyển 3 Số 9(58), quyển 2 Số 9(58), quyển 1 Số 8(57), quyển 3 Số 8(57), quyển 2 Số 8(57), quyển 1 Số 7(56) Số 6(55) Số 5(54) Số 4(53) Số 3(52) Số 2(51) Số 1(50) Năm 2011 Số 42 Số 43 Số 44 Số 45 - Quyển 1 Số 45 - Quyển 2 Số 46 Số 47 - Quyển 1 Số 47 - Quyển 2 Số 48 Số 49 Số 45 - Quyển 3 Năm 2010 Số 41 Số 40 - Quyển 3 Số 40 - Quyển 2 Số 40 - Quyển 1 Số 39 - Quyển 2 Số 39 - Quyển 1 Số 38 Số 37 Số 36 Năm 2009 Số 30 Số 31 Số 32 Số 33 Số 34 Số 35 Năm 2008 Số 24 Số 25 Số 26 Số 27 Số 28 Số 29 Năm 2007 Số 18 Số 19 Số 20 Số 21 Số 22 Số 23 Năm 2006 Số 13 Số 14 Số 15+16 Số 17 Năm 2005 Số 9 Số 10 Số 11 Số 12 Năm 2004 Số 5 Số 6 Số 7 Số 8 Năm 2003 Số 1 Số 2 Số 3 Số 4 PHƯƠNG PHÁP NÉN ẢNH SỬ DỤNG MẠNG NƠRON NHÂN TẠO VÀ K-MEANS. IMAGE COMPRESSION BASED ON ARTIFICIAL NEURAL NETWORKS AND K-MEANS   Tác giả: Võ Văn Nhật,  Phạm Minh Tuấn*   Tóm tắt bằng tiếng Việt: Mạng nơron nhân tạo là một phương pháp hiệu quả trong việc nén ảnh. Mạng nơron nhân tạo có khả năng xấp xỉ không gian màu của một bức ảnh bằng một không gian nhỏ hơn so với không gian của bức ảnh ban đầu. Nếu ảnh đầu vào có các dạng màu sắc gần giống nhau tại các vị trí khác nhau trên cùng một bức ảnh thì việc xấp xỉ sẽ dễ dàng. Tuy nhiên, ảnh đầu vào có rất nhiều dạng màu sắc khác nhau thì việc xấp xỉ sẽ trở nên khó khăn. Báo cáo này đề xuất phương pháp nén ảnh sử dụng mạng neural kết hợp với phương pháp phân nhóm k-means nhằm hạn chế sự mất mát thông tin màu sắc của bức ảnh trong quá trình nén. Trước tiên, phương pháp đề xuất chia bức ảnh thành nhiều block khác nhau. Sau đó phân nhóm các block này sử dụng k-means. Mỗi nhóm block sẽ được thông qua một mạng nơron khác nhau để xây dựng không gian xấp xỉ. Kết quả thực nghiệm trên các ảnh thực cho thấy phương pháp đề xuất tốt hơn so với phương pháp trước đó. \r\n                            \r\n                           Từ khóa: mạng nơron nhân tạo; phân nhóm; k-mean; nén ảnh; ảnh số.   Tóm tắt bằng tiếng Anh: Using artificial neural network is an effective method in image compression. Artificial neural networks have the ability to approximate the color space of an image by a smaller space than from the original image. The approximation will be easy if the input image has many similarities in color at different locations. However, input image has many different types of colors, the approximation becomes difficult. This paper proposes the image compression method using a neural network method combined with k-means to minimize the loss of color information of the image in the compression process. First, the proposed method split image into diferent blocks. Then cluster these blocks using k-means. Finally, this paper builds an approximation space using the neural networks for all groups of blocks. Experimental results on real images show that the proposed method is better than the conventional method. \r\n                            \r\n                           Key words: neural networks; clustering; k-mean; compress; image   CÁC BÀI BÁO TRONG SỐ 1(74).2014, QUYỂN 2 TT Tiêu đề Số lần Trang 1 THUẬT TOÁN TÌM LUỒNG CỰC ĐẠI TRÊN MẠNG GIAO THÔNG MỞ RỘNG Tác giả:    Trần Quốc Chiến ,  Trần Ngọc Việt ,  Nguyễn Đình Lầu 921 1 2 ỨNG DỤNG KỸ THUẬT CÂY QUYẾT ĐỊNH TRONG KHAI PHÁ DỮ LIỆU XÂY DỰNG HỆ THỐNG TƯ VẤN CHỌN NGÀNH TUYỂN SINH ĐẠI HỌC. Tác giả:    Nguyễn Văn Chức* 1535 5 3 CẢI TIẾN THỰC THI ĐỘT BIẾN TRONG KIỂM THỬ ĐỘT BIẾN CHO CÁC MÔ HÌNH SIMULINK SỬ DỤNG TÍNH TOÁN SONG SONG Tác giả:    Lê Thị Mỹ Hạnh , Khuất Thanh Tùng,  Nguyễn Thanh Bình 27341 9 4 NGHIÊN CỨU KIẾN TRÚC VÀ XÂY DỰNG HỆ THỐNG CHỨNG THỰC TẬP TRUNG CHO ĐẠI HỌC ĐÀ NẴNG Tác giả:    Mai Trần Trung Hiếu* ,  Trịnh Công Duy ,  Hồ Phan Hiếu 802 14 5 THIẾT KÊ TIỀN MÃ HOÁ TUYẾN TÍNH CHO KÊNH TRUYỀN TWO-WAY RELAY Tác giả:    Nguyễn Lê Hùng ,  Nguyễn Duy Nhật Viễn ,  Tăng Tấn Chiến ,  974 17 6 NHẬN DẠNG DẤU THANH VÀ MŨ TRONG KÝ TỰ TIẾNG VIỆT VIẾT TAY Tác giả:    Huỳnh Hữu Hưng ,  Nguyễn Trọng Nguyên 26548 21 7 MÔ PHỎNG LƯU LƯỢNG DÒNG CHẢY HÀNG THÁNG VỚI MÔ HÌNH FGAR(1) VÀ MÔ HÌNH MGAR(1) Tác giả:    Nguyễn Văn Hưng ,  Ngô Thị Thanh Trang 873 25 8 GIẢI PHÁP TẠO GIÁO TRÌNH ĐIỆN TỬ BẰNG VĂN PHẠM PHI NGỮ CẢNH Tác giả:    Nguyễn Thị Minh Hỷ 705 30 9 ỨNG DỤNG GIS XÂY DỰNG HỆ THỐNG HỖ TRỢ CẢNH BÁO NGUY CƠ TRƯỢT LỞ ĐẤT TỈNH QUẢNG NGÃI  Tác giả:    Nguyễn Tấn Khôi , Bùi Đức Thọ 814 33 10 GIẢI PHÁP XÂY DỰNG KHO NGỮ LIỆU ĐA NGỮ VIỆT-ÊĐÊ GÁN NHÃN THEO NGỮ CẢNH  Tác giả:    Hoàng Thị Mỹ Lệ ,  Phan Huy Khánh 882 38 11 HỆ THỐNG ĐA CHỨC NĂNG HỖ TRỢ NGƯỜI KHUYẾT TẬT Tác giả:   Trần Quang Nam, Dương Nguyễn Khánh Nam, Nguyễn Văn Tây,  Phạm Văn Tuấn 845 42 12 MỘT CÁCH TIẾP CẬN TÌM TẬP PHỔ BIẾN DỰA TRÊN GIÀN TRONG KHAI PHÁ LUẬT KẾT HỢP Tác giả:   Lương Văn Nghĩa   ; Lê Văn Sơn   ;  Huỳnh Triệu Vỹ   791 47 13 PHƯƠNG PHÁP NÉN ẢNH SỬ DỤNG MẠNG NƠRON NHÂN TẠO VÀ K-MEANS. Tác giả:   Võ Văn Nhật,  Phạm Minh Tuấn* 866 50 14 MỘT PHƯƠNG PHÁP MÔ HÌNH HÓA KIẾN TRÚC CHO CÁC ĐỐI TƯỢNG ĐƯỢC GIÁM SÁT TRONG HỆ PHÂN TÁN Tác giả:    Trần Nguyễn Hồng Phúc ,  Lê Văn Sơn 877 55 15 PHÂN MẢNH DỮ LIỆU TRONG THIẾT KẾ CƠ SỞ DỮ LIỆU PHÂN TÁN DỰA VÀO KỸ THUẬT PHÂN CỤM HƯỚNG TRI THỨC  Tác giả:    Lê Văn Sơn ,  Lương Văn Nghĩa 762 59 16 SO SÁNH PHƯƠNG PHÁP NHẬN DẠNG HÀNH ĐỘNG CON NGƯỜI TRONG ĐOẠN VIDEO QUAY BẰNG MỘT CAMERA DÙNG DTW VÀ HMM Tác giả:    Hoàng Lê Uyên Thục ,  Phạm Văn Tuấn , Shian, Ru Ke 985 64 17 MỘT THUẬT TOÁN TÌM TẬP THƯỜNG XUYÊN TRÊN CƠ SỞ DỮ LIỆU GIAO TÁC CÓ TRỌNG SỐ  Tác giả:   Nguyễn Hữu Trọng; Lê Đức An; Trần Xuân Việt; Nguyễn Anh Hào 887 69 18 MÔ HÌNH HỆ THỐNG ĐA TÁC TỬ ĐỂ MÔ PHỎNG GIAO THÔNG ĐÔ THỊ Tác giả:    Nguyễn Thanh Tuấn ,  Hoàng Thị Thanh Hà ,  Lê Quang Vũ 921 74 19 ỨNG DỤNG JQUERY NÂNG CAO KHẢ NĂNG TƯƠNG TÁC VỚI NGƯỜI DÙNG CHO HỆ THỐNG ĐIỀU HÀNH TÁC NGHIỆP ĐẠI HỌC ĐÀ NẴNG Tác giả:    Phạm Anh Tuấn* , Trần Nguyễn Việt Hùng 688 79 20 NHẬN DẠNG CHUYỂN ĐỘNG QUAY DỰA TRÊN MÔ HÌNH MARKOV ẨN VÀ CONFORMAL GEOMETRIC ALGEBRA Tác giả:   Nguyễn Năng Hùng Vân,  Phạm Minh Tuấn , Tachibana Kanta 902 84 \t\t\t\t\t© Đại học Đà Nẵng \t    Tạp chí Khoa học và Công nghệ - Đại học Đà Nẵng Giấy phép hoạt động báo chí số 510/GP-BVHTT, do Bộ Văn hoá - Thông tin cấp ngày 25/11/2002; ISSN số 1859-1531 Địa chỉ: 41 Lê Duẩn Thành phố Đà Nẵng  Điện\tthoại: (0511) 3 817 788; Email:\ttapchikhcn@ud.edu.vn Xây dựng và phát triển bởi Đại học Đà Nẵng; email: udn.it@ud.edu.vn  \t ",
          "url": "http://tapchikhcn.udn.vn/view.aspx?idbb=Phuong_phap_nen_anh_su_dung_mang_Noron_nhan_tao_va_k-means.-6880&Nam=2014&id=105"
        },
        {
          "title": "Trang",
          "relevance": "0",
          "content": "Trang About Menu Skip to content Home Netwoking CCNA CCNP CCIE SYSTEM Linux Windows Programming C++ Download Ebooks Softwares Resources Tips About Marketing, Biology, Libraries, Insurance, Finance, … Share this: Twitter Facebook Google Like this: Số lượt thích Đang tải... Liên quan Nhập bình luận của bạn tại đây... Mời bạn điền thông tin vào ô dưới đây hoặc kích vào một biểu tượng để đăng nhập: Thư điện tử  (bắt buộc) (Địa chỉ của bạn được giấu kín) Tên  (bắt buộc) Trang web  Bạn đang bình luận bằng tài khoản WordPress.com  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Twitter  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Facebook  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Google+  (  Đăng xuất  /  Thay đổi  ) Hủy bỏ Connecting to %s Notify me of new comments via email. Thông báo cho tôi bằng email khi có bài đăng mới. Điều hướng bài viết ←  Download c7200-adventerprisek9-mz.152-4.S1 Free Download SecureCRT 8.0 Full Newest 2016  → Bài viết mới Free Download SecureCRT 8.0 Full Newest 2016 Cài đặt thuật toán K-Means Clustering (GUI C++) Download c7200-adventerprisek9-mz.152-4.S1 How to connect GNS3 1.4.6 to Internet using WiFi on Windows 10 How to fix: Something went wrong on Office 2016/365 Categories C++  (3)\n Download  (1)\n Ebooks  (3)\n Linux  (7)\n Resources  (1)\n SYSTEM  (2)\n Tips  (3)\n Uncategorized  (8)\n Blog tại WordPress.com. Post to Hủy bỏ %d  bloggers like this:",
          "url": "https://7just.wordpress.com/2016/06/04/cai-dat-thuat-toan-k-means-clustering-gui-c/"
        },
        {
          "title": "k-means clustering algorithm",
          "relevance": "1",
          "content": "Data Clustering Algorithms Search this site \"As knowledge increases,  wonder deepens\" Quick Links Introduction k-means clustering algorithm Fuzzy c-means clustering algorithm Hierarchical clustering algorithm Gaussian(EM) clustering algorithm Quality Threshold (QT) clustering algorithm MST based clustering algorithm Density based clustering algorithm kernel k-means clustering algorithm Clustering Algorithm Applications FAQ References My Concise CV     click here Reach Me  Email:                            click here Facebook:                    click here My Blog               click here Interesting Links Sixth Sense:   Part 1    Part 2    Part 3    Next Generation Mobile Phones TED Conferences Fun Gallery Baby Dance Video       Puzzle Game Miscellaneous https://www.kaggle.com/ Interesting article to read k-means clustering algorithm k-means is  one of  the simplest unsupervised   learning    algorithms  that  solve  the well  known clustering problem. The  procedure follows a simple and  easy  way  to classify a given data set  through a  certain number of  clusters (assume k clusters) fixed apriori. The  main  idea  is to  define k centers, one for each cluster. These centers  should  be placed in a cunning   way  because of    different    location    causes different  result. So, the better  choice    is  to place them  as  much as possible  far away from each other. The  next  step is to take each point belonging    to a  given data set and associate it to the nearest center. When no point  is  pending,  the first step is completed and an early group age  is done. At this point we need to re-calculate k new centroids as barycenter of  the clusters resulting from the previous step. After we have these k new centroids, a new binding has to be done  between  the same data set points  and  the nearest new center. A loop has been generated. As a result of  this loop we  may  notice that the k centers change their location step by step until no more changes  are done or  in    other words centers do not move any more. Finally, this  algorithm  aims at  minimizing  an objective function know as squared error function given by:                                                                               where ,                                  ‘||x i  - v j ||’  is the Euclidean distance between  x i  and  v j.                                    ‘c i ’  is the number of data points in  i th  cluster.                                    ‘c’  is the number of cluster centers. Algorithmic steps for k-means clustering  Let    X = {x 1 ,x 2 ,x 3 ,……..,x n } be the set of data points and V = {v 1 ,v 2 ,…….,v c } be the set of centers. 1) Randomly select  ‘c’  cluster centers. 2) Calculate the distance between each data point and cluster centers. 3) Assign the data point to the cluster center whose distance from the cluster center is minimum of all the cluster centers.. 4) Recalculate the new cluster center using:    where,  ‘c i ’  represents the number of data points in  i th  cluster. 5) Recalculate the distance between each data point and new obtained cluster centers. 6) If no data point was reassigned then stop, otherwise repeat from step 3).   Advantages 1) Fast, robust and easier to understand. 2) Relatively\nefficient :  O ( tknd ), where  n  is # objects,  k  is # clusters, d is # dimension of each object, and  t    is #\niterations. Normally,  k ,  t, d  <<  n. 3)  Gives best result when data set are distinct or well separated from each other. Fig I : Showing the res ult of k-means for  'N'  = 60 and  'c'  = 3 Note:  For more detailed figure for k-means algorithm please refer to  k-means figure  sub page.  Disadvantages 1) The learning algorithm   requires apriori specification of the number of  cluster centers. 2) The use of  Exclusive Assignment - If    there are two highly overlapping data then k-means will not be able to resolve          that there are two clusters. 3)  The learning algorithm is not invariant to non-linear transformations i.e. with different representation of  data we get     different results (data represented in form of cartesian co-ordinates and polar co-ordinates will give different results). 4)  Euclidean  distance measures can unequally weight underlying factors. 5) The learning algorithm\nprovides the local optima of the squared error function.  6) Randomly choosing of the\ncluster center cannot lead us to the fruitful result. Pl. refer  Fig . 7)  Applicable only when  mean  is defined i.e. fails for categorical data. 8)  Unable to handle noisy data and  outliers . 9) Algorithm fails for\nnon-linear data set. Fig II : Showing the non-linear data set where  k-means algorithm fails References 1) An Efficient k-means Clustering Algorithm: Analysis and Implementation by Tapas Kanungo, David M. Mount,     Nathan S. Netanyahu, Christine D. Piatko, Ruth Silverman and Angela Y. Wu. 2) Research issues on K-means Algorithm: An Experimental Trial Using Matlab by Joaquin Perez Ortega, Ma. Del      Rocio Boone Rojas and Maria J. Somodevilla Garcia. 3) The k-means algorithm - Notes by Tan, Steinbach, Kumar Ghosh. 4)  http://home.dei.polimi.it/matteucc/Clustering/tutorial_html/kmeans.html 5) k-means clustering by ke chen. \n            Subpages  (2): k-means algorithm figure k-means initial cluster center selection Č Updating... Ċ Ref-1_k-means.pdf (1930k) Azad Naik,  May 11, 2010, 1:51 AM v.1 ď Ċ Ref-2_k-means.pdf (764k) Azad Naik,  May 11, 2010, 1:52 AM v.1 ď Ċ Ref-3_k-means.pdf (361k) Azad Naik,  May 11, 2010, 1:53 AM v.1 ď ć Ref-5_k-means.ppt (498k) Azad Naik,  May 14, 2010, 1:21 AM v.1 ď ć Ref-6_k-means.ppt (784k) Azad Naik,  May 16, 2011, 6:19 AM v.1 ď Comments Sign in | Recent Site Activity | Report Abuse | Print Page | Powered By Google Sites",
          "url": "https://sites.google.com/site/dataclusteringalgorithms/k-means-clustering-algorithm"
        },
        {
          "title": "Statistical Clustering.k-Means .*;",
          "relevance": "1",
          "content": "Contact (); Links (); Home (); AI (); Robotics (); Notes (); About (); Clustering k-Means LVQ Statistical Clustering.k-Means .*; View Java code View Python code k-Means: Step-By-Step Example As a simple illustration of a k-means algorithm, consider the following data set consisting of the scores of two variables on each of seven individuals: Subject A B 1 1.0 1.0 2 1.5 2.0 3 3.0 4.0 4 5.0 7.0 5 3.5 5.0 6 4.5 5.0 7 3.5 4.5   This data set is to be grouped into two clusters.  As a first step in finding a sensible initial partition, let the A & B values of the two individuals furthest apart (using the Euclidean distance measure), define the initial cluster means, giving:   \n\t\t\t\tIndividual Mean \n\t\t\t\tVector (centroid) Group 1 1 (1.0, 1.0) Group 2 4 (5.0, 7.0)   The remaining individuals are now examined in sequence and allocated to the cluster to which they are closest, in terms of Euclidean distance to the cluster mean. The mean vector is recalculated each time a new member is added. This leads to the following series of steps:   Cluster 1 Cluster 2 Step \n\t\t\t\tIndividual Mean \n\t\t\t\tVector (centroid) \n\t\t\t\tIndividual Mean \n\t\t\t\tVector (centroid) 1 1 (1.0, 1.0) 4 (5.0, 7.0) 2 1, 2 (1.2, 1.5) 4 (5.0, 7.0) 3 1, 2, 3 (1.8, 2.3) 4 (5.0, 7.0) 4 1, 2, 3 (1.8, 2.3) 4, 5 (4.2, 6.0) 5 1, 2, 3 (1.8, 2.3) 4, 5, 6 (4.3, 5.7) 6 1, 2, 3 (1.8, 2.3) 4, 5, 6, 7 (4.1, 5.4)   Now the initial partition has changed, and the two clusters at this stage having the following characteristics:   \n\t\t\t\tIndividual Mean \n\t\t\t\tVector (centroid) Cluster 1 1, 2, 3 (1.8, 2.3) Cluster 2 4, 5, 6, 7 (4.1, 5.4)   But we cannot yet be sure that each individual has been assigned to the right cluster.  So, we compare each individual’s distance to its own cluster mean and to  that of the opposite cluster. And we find: \n\t\t\t\tIndividual Distance \n\t\t\t\tto mean (centroid) of Cluster 1 Distance \n\t\t\t\tto mean (centroid) of Cluster 2 1 1.5 5.4 2 0.4 4.3 3 2.1 1.8 4 5.7 1.8 5 3.2 0.7 6 3.8 0.6 7 2.8 1.1   Only individual 3 is nearer to the mean of the opposite cluster (Cluster 2) than its own (Cluster 1).  In other words, each individual's distance to its own cluster mean should be smaller that the distance to the other cluster's mean (which is not the case with individual 3).  Thus, individual 3 is relocated to Cluster 2 resulting in the new partition:   \n\t\t\t\tIndividual Mean \n\t\t\t\tVector (centroid) Cluster 1 1, 2 (1.3, 1.5) Cluster 2 3, 4, 5, 6, 7 (3.9, 5.1)   The iterative relocation would now continue from this new partition until no more relocations occur.  However, in this example each individual is now nearer its own cluster mean than that of the other cluster and the iteration stops, choosing the latest partitioning as the final cluster solution. Also, it is possible that the k-means algorithm won't find a final solution.  In this case it would be a good idea to consider stopping the algorithm after a pre-chosen maximum of iterations.   News Links: 2017 (0) October (0) September (0) August (0) July (0) June (0) May (0) April (0) March (0) February (0) January (0) 2016 (0) 2015 (0) 2014 (0) 2013 (32) 2012 (242) 2011 (217) 2010 (185) 2009 (20) \n\t\t\tSearch News Links:\n\t\t\t public void footer() { About  |  Contact  | Privacy Policy |  Terms of Service  |  Site Map Copyright© 2009-2012 John McCullock. All Rights Reserved. }",
          "url": "http://mnemstudio.org/clustering-k-means-example-1.htm"
        },
        {
          "title": "K Means",
          "relevance": "1",
          "content": "Stanford CS221 Schedule Policies Handouts  Office Hours Submitting Python Tutorial Markov Decisions Regrades Practice Midterms Midterm Solutions K Means Big Picture Homeworks  \n\t\t\t\t\t\t\t\tProgramming:\n\t\t\t\t\t\t\t Pacman Driverless Car Visual Cortex \n\t\t\t\t\t\t\t\tProblem Sets:\n\t\t\t\t\t\t\t Search Pset Variable Pset Learning Pset \n\t\t\t\t\t\t\t\tProjects:\n\t\t\t\t\t\t\t Final Project AI Stories  Self Driving Car Machine Translation Deep Blue Watson \n\t\t\t\tK Means\n\t\t\t \n\t\t\t\tWritten by Chris Piech. Based on a handout by Andrew Ng.\n\t\t\t \n\t\t\t\t\t\tSay you are given a data set where each observed example has a set of features, but has  no  labels. Labels are an essential ingredient to a supervised\n\t\t\t\t\t\talgorithm like Support Vector Machines, which learns a hypothesis function to predict labels given features. So we can't run supervised learning.\n\t\t\t\t\t\tWhat can we do?\n\t\t\t\t\t \n\t\t\t\t\t\tOne of the most straightforward tasks we can\n\t\t\t\t\t\tperform on a data set without labels is to find groups of data in our dataset which are similar to one another -- what we call clusters.\n\t\t\t\t\t \n\t\t\t\t\t\tK-Means is one of the most popular \"clustering\" algorithms. K-means stores $k$ centroids that it uses to define clusters. A point is considered to be in a\n\t\t\t\t\t\tparticular cluster if it is closer to that cluster's centroid than any other centroid.\n\n\t\t\t\t\t \n\t\t\t\t\t\tK-Means finds the best centroids by alternating between (1) assigning data points to clusters based on the current centroids\n\t\t\t\t\t\t(2) chosing centroids (points which are the center of a cluster) based on the current assignment of data points to clusters.\n\n\t\t\t\t\t \n\t\t\t\t\t\t\tFigure 1: K-means algorithm. Training examples are shown as dots, and\n\t\t\t\t\t\t\tcluster centroids are shown as crosses. (a) Original dataset. (b) Random initial cluster centroids. (c-f) Illustration of running two iterations of k-means. In each\n\t\t\t\t\t\t\titeration, we assign each training example to the closest cluster centroid\n\t\t\t\t\t\t\t(shown by \"painting\" the training examples the same color as the cluster\n\t\t\t\t\t\t\tcentroid to which is assigned); then we move each cluster centroid to the\n\t\t\t\t\t\t\tmean of the points assigned to it. Images courtesy of Michael Jordan.\n\t\t\t\t\t\t Visual Cortex: \n\t\t\t\t\tK-Means is the first algorithm you must implement for the Visual Cortex assignment.\n\t\t\t\t \n\t\t\t\t\t\tIn the clustering problem, we are given a training set ${x^{(1)}, ... , x^{(m)}}$, and\n\t\t\t\t\t\twant to group the data into a few cohesive \"clusters.\" Here, we are given feature vectors for each data point\n\t\t\t\t\t\t$x^{(i)} \\in \\mathbb{R}^n$\n\t\t\t\t\t\tas usual; but no labels $y^{(i)}$ (making this an unsupervised learning\n\t\t\t\t\t\tproblem). Our goal is to predict $k$ centroids  and  a label $c^{(i)}$ for each datapoint.\n\t\t\t\t\t\tThe k-means clustering algorithm is as follows:\n\t\t\t\t\t Euclidean Distance: \n\t\t\t\t\t The notation $\\lVert x - y \\lVert$ means  euclidean distance  between vectors $x$ and $y$.\n\t\t\t\t Here is pseudo-python code which runs k-means on a dataset. It is a short algorithm made longer by verbose commenting.\n # Function: K Means\n# -------------\n# K-Means is an algorithm that takes in a dataset and a constant\n# k and returns k centroids (which define clusters of data in the\n# dataset which are similar to one another). def  kmeans(dataSet, k):\n\t\n     # Initialize centroids randomly \n    numFeatures = dataSet.getNumFeatures()\n    centroids = getRandomCentroids(numFeatures, k)\n    \n     # Initialize book keeping vars. \n    iterations = 0\n    oldCentroids = None\n    \n     # Run the main k-means algorithm while not  shouldStop(oldCentroids, centroids, iterations):\n         # Save old centroids for convergence test. Book keeping. \n        oldCentroids = centroids\n        iterations += 1\n        \n         # Assign labels to each datapoint based on centroids \n        labels = getLabels(dataSet, centroids)\n        \n         # Assign centroids based on datapoint labels \n        centroids = getCentroids(dataSet, labels, k)\n        \n     # We can get the labels too by calling getLabels(dataSet, centroids) return  centroids\n # Function: Should Stop\n# -------------\n# Returns True or False if k-means is done. K-means terminates either\n# because it has run a maximum number of iterations OR the centroids\n# stop changing. def  shouldStop(oldCentroids, centroids, iterations):\n    if iterations > MAX_ITERATIONS: return True\n     return  oldCentroids == centroids\n # Function: Get Labels\n# -------------\n# Returns a label for each piece of data in the dataset.  def  getLabels(dataSet, centroids):\n     # For each element in the dataset, chose the closest centroid. \n    # Make that centroid the element's label. # Function: Get Centroids\n# -------------\n# Returns k random centroids, each of dimension n. def  getCentroids(dataSet, labels, k):\n     # Each centroid is the geometric mean of the points that\n    # have that centroid's label. Important: If a centroid is empty (no points have\n    # that centroid's label) you should randomly re-initialize it. \n\t\t\t\t\t\tImportant note: You might be tempted to calculate the distance between two points manually,\n\t\t\t\t\t\tby looping over values. This will work, but it will lead to a slow k-means! And a slow k-means\n\t\t\t\t\t\twill mean that you have to wait longer to test and debug your solution. \n\t\t\t\t\t Let's define three vectors: x = np.array([1, 2, 3, 4, 5]\ny = np.array([8, 8, 8, 8, 8])\nz = np.ones((5, 9)) To calculate the distance between x and y we can use:\n np.sqrt(sum((x - y) ** 2)) To calculate the distance between all the length 5 vectors in z and x we can use:\n\t np.sqrt(((z-x)**2).sum(axis=0)) Numpy: \n\t\t\t\t\tK-Means is much faster if you write the update functions using operations on numpy arrays, instead of manually looping over the arrays and updating the values yourself.\n\t\t\t\t \n\t\t\t\t\t\tK-Means is really just the EM (Expectation Maximization) algorithm applied to a\n\t\t\t\t\t\tparticular naive bayes model.\n\t\t\t\t\t \n\t\t\t\t\t\tTo demonstrate this remarkable claim, consider the classic naive bayes model with a class variable which can take on discrete values\n\t\t\t\t\t\t(with domain size $k$) and a set of feature variables, each of which can take on a continuous value (see figure 2). \n\t\t\t\t\t\t\n\t\t\t\t\t\tThe conditional probability distributions for $P(f_i = x | C= c)$ is going to be slightly different\n\t\t\t\t\t\tthan usual. Instead of storing this conditional probability as a table, we are going to store it\n\t\t\t\t\t\tas a single  normal  (gaussian) distribution,\n\t\t\t\t\t\twith it's own mean and a standard deviation of 1. Specifically, this means that: $P(f_i = x | C= c) \\sim \\mathcal{N}(\\mu_{c,i}, 1)$\n\t\t\t\t\t Learning the values of $\\mu_{c, i}$ given a dataset with assigned values to the features but not the class variables is the\n\t\t\t\t\t\tprovably identical to running k-means on that dataset. \n\t\t\t\t\t\t\tFigure 2: The K-Means algorithm is the EM algorithm applied to this Bayes Net.\n\t\t\t\t\t\t \n\t\t\t\t\t\tIf we know that this is the strcuture of our bayes net, but we don't know any of the conditional\n\t\t\t\t\t\tprobability distributions then we have to run Parameter Learning before we can run Inference.\n\t\t\t\t\t \n\t\t\t\t\t\tIn the dataset we are given, all the feature variables are observed (for each data point) but\n\t\t\t\t\t\tthe class variable is hidden. Since we are running Parameter Learning on a bayes net where\n\t\t\t\t\t\tsome variables are unobserved, we should use EM.\n\t\t\t\t\t \n\t\t\t\t\t\tLets review EM. In EM, you randomly initialize your model parameters, then you alternate between (E) assigning values to hidden variables, based on parameters\n\t\t\t\t\t\tand (M) computing parameters based on fully observed data.\n\t\t\t\t\t E-Step : Coming up with values to hidden variables, based on parameters. If you work out the math of chosing the best values for the class variable based on the \n\t\t\t\t\t\tfeatures of a given piece of data in your data set, it comes out to \"for each data-point, chose the centroid that it is closest to, by euclidean distance, and assign that\n\t\t\t\t\t\tcentroid's label.\" The proof of this is within your grasp! See lecture.\n\t\t\t\t\t M-Step : Coming up with parameters, based on full assignments. If you work out the math of chosing the best parameter values based on the \n\t\t\t\t\t\tfeatures of a given piece of data in your data set, it comes out to \"take the mean of all the data-points that were labeled as c.\" \n\t\t\t\t\t \n\t\t\t\t\t\tSo what? Well this gives you an idea of the qualities of k-means. Like EM, it is provably going to find a local optimum. Like EM, it is not necessarily going to \n\t\t\t\t\t\tfind a global optimum. It turns out those random initial values do matter.\n\t\t\t\t\t \n\t\t\t\t\t\tFigure 1 shows k-means with a 2-dimensional feature vector (each point has two dimensions, an x and a y). In your applications, will probably be working\n\t\t\t\t\t\twith data that has a lot of features. In fact each data-point may be hundreds of dimensions. We can visualize clusters in up to 3 dimensions (see figure 3) but beyond that \n\t\t\t\t\t\tyou have to rely on a more mathematical understanding. \n\t\t\t\t\t \n\t\t\t\t\t\t\tFigure 3: KMeans in other dimensions. (left) K-means in 2d. (right) K-means in 3d. You have to imagine k-means in 4d.\n\t\t\t\t\t\t \n\t\t© Stanford 2013 | Designed by  Chris . Inspired by  Niels  and  Percy .\n\t Fall 2012",
          "url": "http://stanford.edu/~cpiech/cs221/handouts/kmeans.html"
        },
        {
          "title": "k-means clustering",
          "relevance": "1",
          "content": "k -means clustering From Wikipedia, the free encyclopedia \n\t\t\t\t\tJump to:\t\t\t\t\t navigation , \t\t\t\t\t search Machine learning  and data mining Problems Classification Clustering Regression Anomaly detection Association rules Reinforcement learning Structured prediction Feature engineering Feature learning Online learning Semi-supervised learning Unsupervised learning Learning to rank Grammar induction Supervised learning ( classification  •  regression ) Decision trees Ensembles  ( Bagging ,  Boosting ,  Random forest ) k -NN Linear regression Naive Bayes Neural networks Logistic regression Perceptron Relevance vector machine (RVM) Support vector machine (SVM) Clustering BIRCH Hierarchical k -means Expectation–maximization (EM) DBSCAN OPTICS Mean-shift Dimensionality reduction Factor analysis CCA ICA LDA NMF PCA t-SNE Structured prediction Graphical models  ( Bayes net ,  CRF ,  HMM ) Anomaly detection k -NN Local outlier factor Neural nets Autoencoder Deep learning Multilayer perceptron RNN Restricted Boltzmann machine SOM Convolutional neural network Reinforcement learning Q-learning SARSA Temporal difference (TD) Theory Bias-variance dilemma Computational learning theory Empirical risk minimization Occam learning PAC learning Statistical learning VC theory Machine-learning venues NIPS ICML ML JMLR ArXiv:cs.LG Related articles List of datasets for machine-learning research Outline of machine learning Machine learning portal v t e k -means clustering  is a method of  vector quantization , originally from  signal processing , that is popular for  cluster analysis  in  data mining .  k -means clustering aims to  partition n  observations into  k  clusters in which each observation belongs to the  cluster  with the nearest  mean , serving as a  prototype  of the cluster. This results in a partitioning of the data space into  Voronoi cells . The problem is computationally difficult ( NP-hard ); however, there are efficient  heuristic algorithms  that are commonly employed and converge quickly to a  local optimum . These are usually similar to the  expectation-maximization algorithm  for  mixtures  of  Gaussian distributions  via an iterative refinement approach employed by both algorithms. Additionally, they both use cluster centers to model the data; however,  k -means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes. The algorithm has a loose relationship to the  k -nearest neighbor classifier , a popular  machine learning  technique for classification that is often confused with  k -means because of the  k  in the name. One can apply the 1-nearest neighbor classifier on the cluster centers obtained by  k -means to classify new data into the existing clusters. This is known as  nearest centroid classifier  or  Rocchio algorithm . Contents 1 Description 2 History: 3 Algorithms 3.1 Standard algorithm 3.1.1 Initialization methods 3.2 Complexity 3.3 Variations 4 Discussion 5 Applications 5.1 Vector quantization 5.2 Cluster analysis 5.3 Feature learning 6 Relation to other statistical machine learning algorithms 6.1 Gaussian mixture model 6.2 Principal component analysis 6.3 Mean shift clustering 6.4 Independent component analysis 6.5 Bilateral filtering 7 Similar problems 8 Software implementations 8.1 Free Software/Open Source 8.2 Proprietary 9 See also 10 References Description [ edit ] Given a set of observations ( x 1 ,  x 2 , …,  x n ), where each observation is a  d -dimensional real vector,  k -means clustering aims to partition the  n  observations into  k  (≤  n ) sets  S  = { S 1 ,  S 2 , …,  S k } so as to minimize the within-cluster sum of squares (WCSS) (i.e.  variance ). Formally, the objective is to find: a r g m i n S ∑ i = 1 k ∑ x ∈ S i ∥ x − μ i ∥ 2 = a r g m i n S ∑ i = 1 k | S i | Var ⁡ S i {\\displaystyle {\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}\\sum _{\\mathbf {x} \\in S_{i}}\\left\\|\\mathbf {x} -{\\boldsymbol {\\mu }}_{i}\\right\\|^{2}={\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}|S_{i}|\\operatorname {Var} S_{i}} where  μ i  is the mean of points in  S i . This is equivalent to minimizing the pairwise squared deviations of points in the same cluster: a r g m i n S ∑ i = 1 k 1 2 | S i | ∑ x , y ∈ S i ∥ x − y ∥ 2 {\\displaystyle {\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}\\,{\\frac {1}{2|S_{i}|}}\\,\\sum _{\\mathbf {x} ,\\mathbf {y} \\in S_{i}}\\left\\|\\mathbf {x} -\\mathbf {y} \\right\\|^{2}} The Equivalence can be deduced from identity  ∑ x ∈ S i ∥ x − μ i ∥ 2 = ∑ x ≠ y ∈ S i ( x − μ i ) ( μ i − y ) {\\displaystyle \\sum _{\\mathbf {x} \\in S_{i}}\\left\\|\\mathbf {x} -{\\boldsymbol {\\mu }}_{i}\\right\\|^{2}=\\sum _{\\mathbf {x} \\neq \\mathbf {y} \\in S_{i}}(\\mathbf {x} -{\\boldsymbol {\\mu }}_{i})({\\boldsymbol {\\mu }}_{i}-\\mathbf {y} )} . Because the total variance is constant, this is also equivalent to maximizing the squared deviations between points in  different  clusters (between-cluster sum of squares, BCSS). [1] History: [ edit ] The term \" k -means\" was first used by James MacQueen in 1967, [2]  though the idea goes back to  Hugo Steinhaus  in 1957. [3]  The standard algorithm was first proposed by Stuart Lloyd in 1957 as a technique for  pulse-code modulation , though it wasn't published outside of  Bell Labs  until 1982. [4]  In 1965, E. W. Forgy published essentially the same method, which is why it is sometimes referred to as Lloyd-Forgy. [5] Algorithms [ edit ] Standard algorithm [ edit ] \nConvergence of k-means The most common algorithm uses an iterative refinement technique. Due to its ubiquity it is often called the  k -means algorithm ; it is also referred to as  Lloyd's algorithm , particularly in the computer science community. Given an initial set of  k  means  m 1 (1) ,…, m k (1)  (see below), the algorithm proceeds by alternating between two steps: [6] Assignment step : Assign each observation to the cluster whose mean has the least squared  Euclidean distance , this is intuitively the \"nearest\" mean. [7]  (Mathematically, this means partitioning the observations according to the  Voronoi diagram  generated by the means).\n S i ( t ) = { x p : ∥ x p − m i ( t ) ∥ 2 ≤ ∥ x p − m j ( t ) ∥ 2   ∀ j , 1 ≤ j ≤ k } , {\\displaystyle S_{i}^{(t)}={\\big \\{}x_{p}:{\\big \\|}x_{p}-m_{i}^{(t)}{\\big \\|}^{2}\\leq {\\big \\|}x_{p}-m_{j}^{(t)}{\\big \\|}^{2}\\ \\forall j,1\\leq j\\leq k{\\big \\}},} where each  x p {\\displaystyle x_{p}}  is assigned to exactly one  S ( t ) {\\displaystyle S^{(t)}} , even if it could be assigned to two or more of them. Update step : Calculate the new means to be the  centroids  of the observations in the new clusters.\n m i ( t + 1 ) = 1 | S i ( t ) | ∑ x j ∈ S i ( t ) x j {\\displaystyle m_{i}^{(t+1)}={\\frac {1}{|S_{i}^{(t)}|}}\\sum _{x_{j}\\in S_{i}^{(t)}}x_{j}} The algorithm has converged when the assignments no longer change. There is no guarantee that the optimum is found using this algorithm. [8] The algorithm is often presented as assigning objects to the nearest cluster by distance. Using a different distance function other than (squared) Euclidean distance may stop the algorithm from converging. [ citation needed ]  Various modifications of k-means such as spherical k-means and  k-medoids  have been proposed to allow using other distance measures. Initialization methods [ edit ] Commonly used initialization methods are Forgy and Random Partition. [9]  The Forgy method randomly chooses  k  observations from the data set and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the cluster's randomly assigned points. The Forgy method tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. According to Hamerly et al., [9]  the Random Partition method is generally preferable for algorithms such as the  k -harmonic means and fuzzy  k -means. For expectation maximization and standard  k -means algorithms, the Forgy method of initialization is preferable. A comprehensive study by Celebi et al., [10]  however, found that popular initialization methods such as Forgy, Random Partition, and Maximin often perform poorly, whereas the approach by Bradley and Fayyad [11]  performs \"consistently\" in \"the best group\" and  K-means++  performs \"generally well\". Demonstration of the standard algorithm 1.  k  initial \"means\" (in this case  k =3) are randomly generated within the data domain (shown in color). 2.  k  clusters are created by associating every observation with the nearest mean. The partitions here represent the  Voronoi diagram  generated by the means. 3. The  centroid  of each of the  k  clusters becomes the new mean. 4. Steps 2 and 3 are repeated until convergence has been reached. As it is a heuristic algorithm, there is no guarantee that it will converge to the global optimum, and the result may depend on the initial clusters. As the algorithm is usually very fast, it is common to run it multiple times with different starting conditions. However, in the worst case,  k -means can be very slow to converge: in particular it has been shown that there exist certain point sets, even in 2 dimensions, on which  k -means takes exponential time, that is  2 Ω( n ) , to converge. [12]  These point sets do not seem to arise in practice: this is corroborated by the fact that the  smoothed  running time of  k -means is polynomial. [13] The \"assignment\" step is also referred to as  expectation step , the \"update step\" as  maximization step , making this algorithm a variant of the  generalized expectation-maximization algorithm . Complexity [ edit ] Regarding computational complexity, finding the optimal solution to the  k -means clustering problem for observations in  d  dimensions is: NP-hard  in general Euclidean space  d  even for 2 clusters [14] [15] [16] [17] NP-hard  for a general number of clusters  k  even in the plane [18] If  k  and  d  (the dimension) are fixed, the problem can be exactly solved in time  O ( n d k + 1 ) {\\displaystyle O(n^{dk+1})} , where  n  is the number of entities to be clustered [19] Thus, a variety of  heuristic algorithms  such as Lloyd's algorithm given above are generally used. The running time of Lloyd's algorithm is naively  O ( n k d i ) {\\displaystyle O(nkdi)} , [20]  where  n  is the number of  d -dimensional vectors,  k  the number of clusters and  i  the number of iterations needed until convergence. On data that does have a clustering structure, the number of iterations until convergence is often small, and results only improve slightly after the first dozen iterations. Lloyd's algorithm is therefore often considered to be of \"linear\" complexity in practice, although it is in the  worst case  superpolynomial. [21] Following are some of the recent insights into this algorithm complexity behavior. In the worst-case, Lloyd's algorithm needs  i = 2 Ω ( n ) {\\displaystyle i=2^{\\Omega ({\\sqrt {n}})}}  iterations, so that the worst-case complexity of Lloyd's algorithm is  superpolynomial . [21] Lloyd's  k -means algorithm has polynomial smoothed running time. It is shown that [13]  for arbitrary set of  n  points in  [ 0 , 1 ] d {\\displaystyle [0,1]^{d}} , if each point is independently perturbed by a normal distribution with mean  0  and variance  σ 2 {\\displaystyle \\sigma ^{2}} , then the expected running time of  k -means algorithm is bounded by  O ( n 34 k 34 d 8 log 4 ⁡ ( n ) / σ 6 ) {\\displaystyle O(n^{34}k^{34}d^{8}\\log ^{4}(n)/\\sigma ^{6})} , which is a polynomial in  n ,  k ,  d  and  1 / σ {\\displaystyle 1/\\sigma } . Better bounds are proven for simple cases. For example, [22]  showed that the running time of  k -means algorithm is bounded by  O ( d n 4 M 2 ) {\\displaystyle O(dn^{4}M^{2})}  for  n  points in an  integer lattice { 1 , … , M } d {\\displaystyle \\{1,\\dots ,M\\}^{d}} . Lloyd's algorithm is the standard approach for this problem, However, it spends a lot of processing time computing the distances between each of the k cluster centers and the n data points. Since points usually stay in the same clusters after a few iterations, much of this work is unnecessary, making the naive implementation very inefficient. Some implementations use the triangle inequality in order to create bounds and accelerate Lloyd's algorithm. [23] [24] [25] Variations [ edit ] Jenks natural breaks optimization :  k -means applied to univariate data k-medians clustering  uses the median in each dimension instead of the mean, and this way minimizes  L 1 {\\displaystyle L_{1}}  norm ( Taxicab geometry ). k-medoids  (also: Partitioning Around Medoids, PAM) uses the medoid instead of the mean, and this way minimizes the sum of distances for  arbitrary  distance functions. Fuzzy C-Means Clustering  is a soft version of K-means, where each data point has a fuzzy degree of belonging to each cluster. Gaussian mixture  models trained with  expectation-maximization algorithm  (EM algorithm) maintains probabilistic assignments to clusters, instead of deterministic assignments, and multivariate Gaussian distributions instead of means. k-means++  chooses initial centers in a way that gives a provable upper bound on the WCSS objective. The filtering algorithm uses  kd-trees  to speed up each k-means step. [26] Some methods attempt to speed up each k-means step using the  triangle inequality . [23] [24] [25] [27] Escape local optima by swapping points between clusters. [28] The  Spherical k-means  clustering algorithm is suitable for textual data. [29] Hierarchical variants such as Bisecting k-means, [30] X-means clustering [31]  and G-means clustering [32] repeatedly split clusters to build a hierarchy , and can also try to automatically determine the optimal number of clusters in a dataset. Internal cluster evaluation  measures such as  cluster silhouette  can be helpful at  determining the number of clusters . Minkowski weighted k-means  automatically calculates cluster specific feature weights, supporting the intuitive idea that a feature may have different degrees of relevance at different features. [33]  These weights can also be used to re-scale a given data set, increasing the likelihood of a cluster validity index to be optimized at the expected number of clusters. [34] Mini-batch K-means: K-means variation using \"mini batch\" samples for data sets that do not fit into memory. [35] Discussion [ edit ] \nA typical example of the k-means convergence to a local minimum. In this example, the result of k-means clustering (the right figure) contradicts the obvious cluster structure of the data set. The small circles are the data points, the four ray stars are the centroids (means). The initial configuration is on the left figure. The algorithm converges after five iterations presented on the figures, from the left to the right. The illustration was prepared with the Mirkes Java applet. [36] k -means clustering result for the  Iris flower data set  and actual species visualized using  ELKI . Cluster means are marked using larger, semi-transparent symbols. k -means clustering and  EM clustering  on an artificial dataset (\"mouse\"). The tendency of  k -means to produce equal-sized clusters leads to bad results, while EM benefits from the Gaussian distribution present in the data set\n This section  may be  confusing or unclear  to readers . In particular, K-means is by itself EM method. Is it Gaussian mixture?.  Please help us  clarify the section . There might be a discussion about this on  the talk page . (February 2016) ( Learn how and when to remove this template message ) Three key features of  k -means which make it efficient are often regarded as its biggest drawbacks: Euclidean distance  is used as a  metric  and  variance  is used as a measure of cluster scatter. The number of clusters  k  is an input parameter: an inappropriate choice of  k  may yield poor results. That is why, when performing k-means, it is important to run diagnostic checks for  determining the number of clusters in the data set . Convergence to a local minimum may produce counterintuitive (\"wrong\") results (see example in Fig.). A key limitation of  k -means is its cluster model. The concept is based on spherical clusters that are separable in a way so that the mean value converges towards the cluster center. The clusters are expected to be of similar size, so that the assignment to the nearest cluster center is the correct assignment. When for example applying  k -means with a value of  k = 3 {\\displaystyle k=3}  onto the well-known  Iris flower data set , the result often fails to separate the three  Iris  species contained in the data set. With  k = 2 {\\displaystyle k=2} , the two visible clusters (one containing two species) will be discovered, whereas with  k = 3 {\\displaystyle k=3}  one of the two clusters will be split into two even parts. In fact,  k = 2 {\\displaystyle k=2}  is more appropriate for this data set, despite the data set containing 3  classes . As with any other clustering algorithm, the  k -means result relies on the data set to satisfy the assumptions made by the clustering algorithms. It works well on some data sets, while failing on others. The result of  k -means can also be seen as the  Voronoi cells  of the cluster means. Since data is split halfway between cluster means, this can lead to suboptimal splits as can be seen in the \"mouse\" example. The Gaussian models used by the  Expectation-maximization algorithm  (which can be seen as a generalization of  k -means) are more flexible here by having both variances and covariances. The EM result is thus able to accommodate clusters of variable size much better than  k -means as well as correlated clusters (not in this example). Applications [ edit ] k -means clustering is rather easy to implement and apply even on large data sets, particularly when using heuristics such as  Lloyd's algorithm  . It has been successfully used in various topics, including  market segmentation ,  computer vision ,  geostatistics , [37] astronomy  and  agriculture . It often is used as a preprocessing step for other algorithms, for example to find a starting configuration. Vector quantization [ edit ] Main article:  Vector quantization Two-channel (for illustration purposes -- red and green only) color image. \nVector quantization of colors present in the image above into Voronoi cells using  k -means. k -means originates from signal processing, and still finds use in this domain. For example, in  computer graphics ,  color quantization  is the task of reducing the  color palette  of an image to a fixed number of colors  k . The  k -means algorithm can easily be used for this task and produces competitive results. A use case for this approach is  image segmentation . Other uses of vector quantization include  non-random sampling , as  k -means can easily be used to choose  k  different but prototypical objects from a large data set for further analysis. Cluster analysis [ edit ] Main article:  Cluster analysis In cluster analysis, the  k -means algorithm can be used to partition the input data set into  k  partitions (clusters). However, the pure  k -means algorithm is not very flexible, and as such is of limited use (except for when vector quantization as above is actually the desired use case!). In particular, the parameter  k  is known to be hard to choose (as discussed above) when not given by external constraints. Another limitation of the algorithm is that it cannot be used with arbitrary distance functions or on non-numerical data. For these use cases, many other algorithms have been developed since. Feature learning [ edit ] k -means clustering has been used as a  feature learning  (or  dictionary learning ) step, in either ( semi- ) supervised learning  or  unsupervised learning . [38]  The basic approach is first to train a  k -means clustering representation, using the input training data (which need not be labelled). Then, to project any input datum into the new feature space, we have a choice of \"encoding\" functions, but we can use for example the thresholded matrix-product of the datum with the centroid locations, the distance from the datum to each centroid, or simply an indicator function for the nearest centroid, [38] [39]  or some smooth transformation of the distance. [40]  Alternatively, by transforming the sample-cluster distance through a  Gaussian RBF , one effectively obtains the hidden layer of a  radial basis function network . [41] This use of  k -means has been successfully combined with simple,  linear classifiers  for semi-supervised learning in  NLP  (specifically for  named entity recognition ) [42]  and in  computer vision . On an object recognition task, it was found to exhibit comparable performance with more sophisticated feature learning approaches such as  autoencoders  and  restricted Boltzmann machines . [40]  However, it generally requires more data than the sophisticated methods, for equivalent performance, because each data point only contributes to one \"feature\" rather than multiple. [38] Relation to other statistical machine learning algorithms [ edit ] Gaussian mixture model [ edit ] k -means clustering, and its associated  expectation-maximization algorithm , is a special case of a  Gaussian mixture model , specifically, the limit of taking all covariances as diagonal, equal, and small. It is often easy to generalize a  k -means problem into a Gaussian mixture model. [43]  Another generalization of the k-means algorithm is the  K-SVD  algorithm, which estimates data points as a sparse linear combination of \"codebook vectors\". K-means corresponds to the special case of using a single codebook vector, with a weight of 1. [44] Principal component analysis [ edit ] It was proven  [45] [46]  that the relaxed solution of  k -means clustering, specified by the cluster indicators, is given by  principal component analysis  (PCA), and the PCA subspace spanned by the principal directions is identical to the cluster centroid subspace. The intuition is that k-means describe spherically shaped (ball-like) clusters. If the data has 2 clusters, the line connecting the two centroids is the best 1-dimensional projection direction, which is also the first PCA direction. Cutting the line at the center of mass separates the clusters (this is the continuous relaxation of the discrete cluster indicator). If the data have three clusters, the 2-dimensional plane spanned by three cluster centroids is the best 2-D projection. This plane is also defined by the first two PCA dimensions. Well-separated clusters are effectively modeled by ball-shaped clusters and thus discovered by K-means. Non-ball-shaped clusters are hard to separate when they are close. For example, two half-moon shaped clusters intertwined in space do not separate well when projected onto PCA subspace. But k-means should not be expected to do well on this data. However, PCA's being a useful relaxation of k-means clustering was not a new result, [47]  and it is straightforward to produce counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions. [48] Mean shift clustering [ edit ] Basic  mean shift  clustering algorithms maintain a set of data points the same size as the input data set. Initially, this set is copied from the input set. Then this set is iteratively replaced by the mean of those points in the set that are within a given distance of that point. By contrast,  k -means restricts this updated set to  k  points usually much less than the number of points in the input data set, and replaces each point in this set by the mean of all points in the  input set  that are closer to that point than any other (e.g. within the Voronoi partition of each updating point). A mean shift algorithm that is similar then to  k -means, called  likelihood mean shift , replaces the set of points undergoing replacement by the mean of all points in the input set that are within a given distance of the changing set. [49]  One of the advantages of mean shift over  k -means is that there is no need to choose the number of clusters, because mean shift is likely to find only a few clusters if indeed only a small number exist. However, mean shift can be much slower than  k -means, and still requires selection of a bandwidth parameter. Mean shift has soft variants much as  k -means does. Independent component analysis [ edit ] It has been shown in  [50]  that under sparsity assumptions and when input data is pre-processed with the  whitening transformation k -means produces the solution to the linear  independent component analysis  (ICA) task. This aids in explaining the successful application of  k -means to  feature learning . Bilateral filtering [ edit ] k -means implicitly assumes that the ordering of the input data set does not matter. The  bilateral filter  is similar to K-means and  mean shift  in that it maintains a set of data points that are iteratively replaced by means. However, the bilateral filter restricts the calculation of the (kernel weighted) mean to include only points that are close in the ordering of the input data. [49]  This makes it applicable to problems such as image denoising, where the spatial arrangement of pixels in an image is of critical importance. Similar problems [ edit ] The set of squared error minimizing cluster functions also includes the  k -medoids  algorithm, an approach which forces the center point of each cluster to be one of the actual points, i.e., it uses  medoids  in place of  centroids . Software implementations [ edit ] Different implementations of the same algorithm were found to exhibit enormous performance differences, with the fastest on a test data set finishing in 10 seconds, the slowest taking 25988 seconds. [1]  The differences can be attributed to implementation quality, language and compiler differences, and the use of indexes for acceleration. Free Software/Open Source [ edit ] the following implementations are available under  Free/Open Source Software  licenses, with publicly available source code. Accord.NET  contains C# implementations for  k -means,  k -means++ and  k -modes. CrimeStat  implements two spatial  k -means algorithms, one of which allows the user to define the starting locations. ELKI  contains  k -means (with Lloyd and MacQueen iteration, along with different initializations such as  k -means++ initialization) and various more advanced clustering algorithms. Julia  contains a  k -means implementation in the JuliaStats Clustering package. KNIME  contains nodes for  k -means and  k -medoids. Mahout  contains a  MapReduce  based  k -means. MLPACK  contains a C++ implementation of  k -means. Octave  contains  k -means. OpenCV  contains a  k -means implementation. PSPP  contains  k -means, The QUICK CLUSTER command performs k-means clustering on the dataset. R  contains three  k -means variations. SciPy  and  scikit-learn  contain multiple  k -means implementations. Spark  MLlib implements a distributed  k -means algorithm. Torch  contains an  unsup  package that provides  k -means clustering. Weka  contains  k -means and  x -means. Proprietary [ edit ] The following implementations are available under  proprietary  license terms, and may not have publicly available source code. Ayasdi MATLAB Mathematica RapidMiner SAP HANA SAS SPSS Stata See also [ edit ] K-means++ Centroidal Voronoi tessellation k q-flats Linde–Buzo–Gray algorithm Self-organizing map Head/tail Breaks References [ edit ] ^  a b Kriegel, Hans-Peter ; Schubert, Erich;  Zimek, Arthur  (2016). \"The (black) art of runtime evaluation: Are we comparing algorithms or implementations?\".  Knowledge and Information Systems .  52 : 341–378.  ISSN   0219-1377 .  doi : 10.1007/s10115-016-1004-2 .   ^ MacQueen, J. B. (1967).  Some Methods for classification and Analysis of Multivariate Observations . Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability.  1 . University of California Press. pp. 281–297.  MR   0214227 .  Zbl   0214.46201 . Retrieved  2009-04-07 .   ^ Steinhaus, H.  (1957). \"Sur la division des corps matériels en parties\".  Bull. Acad. Polon. Sci.  (in French).  4  (12): 801–804.  MR   0090073 .  Zbl   0079.16403 .   ^ Lloyd, S. P. (1957). \"Least square quantization in PCM\".  Bell Telephone Laboratories Paper .    Published in journal much later:  Lloyd., S. P. (1982).  \"Least squares quantization in PCM\" (PDF) .  IEEE Transactions on Information Theory .  28  (2): 129–137.  doi : 10.1109/TIT.1982.1056489 . Retrieved  2009-04-15 .   ^ E.W. Forgy (1965). \"Cluster analysis of multivariate data: efficiency versus interpretability of classifications\".  Biometrics .  21 : 768–769.  JSTOR   2528559 .   ^ MacKay, David  (2003).  \"Chapter 20. An Example Inference Task: Clustering\" (PDF) .  Information Theory, Inference and Learning Algorithms . Cambridge University Press. pp. 284–292.  ISBN   0-521-64298-1 .  MR   2012999 .   ^ Since the square root is a monotone function, this also is the minimum Euclidean distance assignment. ^ Hartigan, J. A.; Wong, M. A. (1979). \"Algorithm AS 136: A K-Means Clustering Algorithm\".  Journal of the Royal Statistical Society. Series C (Applied Statistics) .  28  (1): 100–108.  JSTOR   2346830 .  doi : 10.2307/2346830 .   ^  a b Hamerly, G.; Elkan, C. (2002).  \"Alternatives to the k-means algorithm that find better clusterings\" (PDF) .  Proceedings of the eleventh international conference on Information and knowledge management (CIKM) .   ^ Celebi, M. E., Kingravi, H. A., and Vela, P. A. (2013).  \"A comparative study of efficient initialization methods for the k-means clustering algorithm\" .  Expert Systems with Applications .  40  (1): 200–210.  arXiv : 1209.1960   .  doi : 10.1016/j.eswa.2012.07.021 .   CS1 maint: Multiple names: authors list ( link ) ^ Bradley, Paul S.;  Fayyad, Usama M.  (1998). \"Refining Initial Points for K-Means Clustering\".  Proceedings of the Fifteenth International Conference on Machine Learning .   ^ Vattani., A. (2011).  \"k-means requires exponentially many iterations even in the plane\" (PDF) .  Discrete and Computational Geometry .  45  (4): 596–616.  doi : 10.1007/s00454-011-9340-1 .   ^  a b Arthur, D.; Manthey, B.; Roeglin, H. (2009). \"k-means has polynomial smoothed complexity\".  Proceedings of the 50th Symposium on Foundations of Computer Science (FOCS) .   ^ Garey, M.; Johnson, D.; Witsenhausen, H. (1982-03-01).  \"The complexity of the generalized Lloyd - Max problem (Corresp.)\" .  IEEE Transactions on Information Theory .  28  (2): 255–256.  ISSN   0018-9448 .  doi : 10.1109/TIT.1982.1056488 .   ^ Kleinberg, Jon; Papadimitriou, Christos; Raghavan, Prabhakar (1998-12-01).  \"A Microeconomic View of Data Mining\" .  Data Mining and Knowledge Discovery .  2  (4): 311–324.  ISSN   1384-5810 .  doi : 10.1023/A:1009726428407 .   ^ Aloise, D.; Deshpande, A.; Hansen, P.; Popat, P. (2009). \"NP-hardness of Euclidean sum-of-squares clustering\".  Machine Learning .  75  (2): 245–249.  doi : 10.1007/s10994-009-5103-0 .   ^ Dasgupta, S.; Freund, Y. (July 2009). \"Random Projection Trees for Vector Quantization\".  Information Theory, IEEE Transactions on .  55  (7): 3229–3242.  arXiv : 0805.1390   .  doi : 10.1109/TIT.2009.2021326 .   ^ Mahajan, M.; Nimbhorkar, P.; Varadarajan, K. (2009). \"The Planar k-Means Problem is NP-Hard\".  Lecture Notes in Computer Science . Lecture Notes in Computer Science.  5431 : 274–285.  ISBN   978-3-642-00201-4 .  doi : 10.1007/978-3-642-00202-1_24 .   ^ Inaba, M.; Katoh, N.; Imai, H. (1994).  Applications of weighted Voronoi diagrams and randomization to variance-based  k -clustering .  Proceedings of 10th ACM Symposium on Computational Geometry . pp. 332–339.  doi : 10.1145/177424.178042 .   ^ D., Manning, Christopher (2008).  Introduction to information retrieval . Raghavan, Prabhakar., Schütze, Hinrich. New York: Cambridge University Press.  ISBN   0521865719 .  OCLC   190786122 .   ^  a b Arthur, David; Vassilvitskii, Sergei (2006-01-01).  \"How Slow is the K-means Method?\" .  Proceedings of the Twenty-second Annual Symposium on Computational Geometry . SCG '06. New York, NY, USA: ACM: 144–153.  ISBN   1595933409 .  doi : 10.1145/1137856.1137880 .   ^ Arthur; Abhishek Bhowmick (2009).  A theoretical analysis of Lloyd's algorithm for k-means clustering (PDF)  (Thesis).   ^  a b Phillips, Steven J. (2002-01-04). Mount, David M.; Stein, Clifford, eds.  Acceleration of K-Means and Related Clustering Algorithms . Lecture Notes in Computer Science. Springer Berlin Heidelberg. pp. 166–177.  ISBN   978-3-540-43977-6 .  doi : 10.1007/3-540-45643-0_13 .   ^  a b Elkan, C. (2003).  \"Using the triangle inequality to accelerate k-means\" (PDF) .  Proceedings of the Twentieth International Conference on Machine Learning (ICML) .   ^  a b Hamerly, Greg.  \"Making k-means even faster\" .  citeseerx.ist.psu.edu . Retrieved  2015-12-10 .   ^ Kanungo, T.;  Mount, D. M. ;  Netanyahu, N. S. ; Piatko, C. D.; Silverman, R.; Wu, A. Y. (2002).  \"An efficient k-means clustering algorithm: Analysis and implementation\" (PDF) .  IEEE Trans. Pattern Analysis and Machine Intelligence .  24  (7): 881–892.  doi : 10.1109/TPAMI.2002.1017616 . Retrieved  2009-04-24 .   CS1 maint: Multiple names: authors list ( link ) ^ Drake, Jonathan (2012).  \"Accelerated k-means with adaptive distance bounds\" (PDF) .  The 5th NIPS Workshop on Optimization for Machine Learning, OPT2012 .   ^ Hartigan, J. A.; Wong, M. A. (1979). \"Algorithm AS 136: A K-Means Clustering Algorithm\".  Journal of the Royal Statistical Society, Series C .  28  (1): 100–108.  JSTOR   2346830 .   ^ Dhillon, I. S.; Modha, D. M. (2001). \"Concept decompositions for large sparse text data using clustering\".  Machine Learning .  42  (1): 143–175.  doi : 10.1023/a:1007612920971 .   ^ Steinbach, M., Karypis, G., & Kumar, V. (2000, August). A comparison of document clustering techniques. In KDD workshop on text mining (Vol. 400, No. 1, pp. 525-526). ^ Pelleg, D., & Moore, A. W. (2000, June). X-means: Extending K-means with Efficient Estimation of the Number of Clusters. In ICML (Vol. 1). ^ Hamerly, G., & Elkan, C. (2004). Learning the k in k-means. Advances in neural information processing systems, 16, 281. ^ Amorim, R.C.; Mirkin, B. (2012). \"Minkowski Metric, Feature Weighting and Anomalous Cluster Initialisation in K-Means Clustering\".  Pattern Recognition .  45  (3): 1061–1075.  doi : 10.1016/j.patcog.2011.08.012 .   ^ Amorim, R.C.; Hennig, C. (2015). \"Recovering the number of clusters in data sets with noise features using feature rescaling factors\".  Information Sciences .  324 : 126–145.  doi : 10.1016/j.ins.2015.06.039 .   ^ Sculley, David (2010).  \"Web-scale k-means clustering\" .  Proceedings of the 19th international conference on World wide web . ACM. pp. 1177–1178 . Retrieved  2016-12-21 .   ^ Mirkes, E.M.  \"K-means and K-medoids applet.\" . Retrieved  2 January  2016 .   ^ Honarkhah, M; Caers, J (2010). \"Stochastic Simulation of Patterns Using Distance-Based Pattern Modeling\".  Mathematical Geosciences .  42  (5): 487–517.  doi : 10.1007/s11004-010-9276-7 .   ^  a b c Coates, Adam; Ng, Andrew Y. (2012).  \"Learning feature representations with k-means\" (PDF) . In G. Montavon, G. B. Orr, K.-R. Müller.  Neural Networks: Tricks of the Trade . Springer.   CS1 maint: Uses editors parameter ( link ) ^ Csurka, Gabriella; Dance, Christopher C.; Fan, Lixin; Willamowski, Jutta; Bray, Cédric (2004).  Visual categorization with bags of keypoints (PDF) . ECCV Workshop on Statistical Learning in Computer Vision.   ^  a b Coates, Adam; Lee, Honglak; Ng, Andrew Y. (2011).  An analysis of single-layer networks in unsupervised feature learning (PDF) . International Conference on Artificial Intelligence and Statistics (AISTATS). Archived from  the original (PDF)  on 2013-05-10.   ^ Schwenker, Friedhelm; Kestler, Hans A.; Palm, Günther (2001). \"Three learning phases for radial-basis-function networks\".  Neural Networks .  14  (4–5): 439–458.  CiteSeerX   10.1.1.109.312   .  doi : 10.1016/s0893-6080(01)00027-2 .   ^ Lin, Dekang; Wu, Xiaoyun (2009).  Phrase clustering for discriminative learning (PDF) . Annual Meeting of the  ACL  and IJCNLP. pp. 1030–1038.   ^ Press, WH; Teukolsky, SA; Vetterling, WT; Flannery, BP (2007).  \"Section 16.1. Gaussian Mixture Models and k-Means Clustering\" .  Numerical Recipes: The Art of Scientific Computing  (3rd ed.). New York: Cambridge University Press.  ISBN   978-0-521-88068-8 .   ^ Aharon, Michal; Elad, Michael; Bruckstein, Alfred (2006).  \"K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation\" (PDF) .  IEEE Transactions on Signal Processing .  54  (11): 4311.  Bibcode : 2006ITSP...54.4311A .  doi : 10.1109/TSP.2006.881199 . Archived from  the original (PDF)  on 2013-06-20.   ^ H. Zha, C. Ding, M. Gu, X. He and H.D. Simon (Dec 2001).  \"Spectral Relaxation for K-means Clustering\" (PDF) .  Neural Information Processing Systems vol.14 (NIPS 2001) . Vancouver, Canada: 1057–1064.   CS1 maint: Uses authors parameter ( link ) ^ Chris Ding and Xiaofeng He (July 2004).  \"K-means Clustering via Principal Component Analysis\" (PDF) .  Proc. of Int'l Conf. Machine Learning (ICML 2004) : 225–232.   CS1 maint: Uses authors parameter ( link ) ^ Drineas, P.; A. Frieze; R. Kannan; S. Vempala; V. Vinay (2004).  \"Clustering large graphs via the singular value decomposition\" (PDF) .  Machine learning .  56 : 9–33.  doi : 10.1023/b:mach.0000033113.59016.96 . Retrieved  2012-08-02 .   ^ Cohen, M.; S. Elder; C. Musco; C. Musco; M. Persu (2014). \"Dimensionality reduction for k-means clustering and low rank approximation (Appendix B)\".  arXiv : 1410.6801    [ cs.DS ].   ^  a b Little, M.A.; Jones, N.S. (2011).  \"Generalized Methods and Solvers for Piecewise Constant Signals: Part I\" (PDF) .  Proceedings of the Royal Society A .  467  (2135): 3088–3114.  Bibcode : 2011RSPSA.467.3088L .  doi : 10.1098/rspa.2010.0671 .   ^ Alon Vinnikov and Shai Shalev-Shwartz (2014).  \"K-means Recovers ICA Filters when Independent Components are Sparse\" (PDF) .  Proc. of Int'l Conf. Machine Learning (ICML 2014) .   CS1 maint: Uses authors parameter ( link ) \n\t\t\t\t\t\tRetrieved from \" https://en.wikipedia.org/w/index.php?title=K-means_clustering&oldid=805198778 \"\t\t\t\t\t Categories :  Cluster analysis algorithms Hidden categories:  CS1 French-language sources (fr) CS1 maint: Multiple names: authors list CS1 maint: Uses editors parameter CS1 maint: Uses authors parameter All articles with unsourced statements Articles with unsourced statements from March 2014 Wikipedia articles needing clarification from February 2016 All Wikipedia articles needing clarification Navigation menu Personal tools Not logged in Talk Contributions Create account Log in Namespaces Article Talk Variants Views Read Edit View history More Search Navigation Main page Contents Featured content Current events Random article Donate to Wikipedia Wikipedia store Interaction Help About Wikipedia Community portal Recent changes Contact page Tools What links here Related changes Upload file Special pages Permanent link Page information Wikidata item Cite this page Print/export Create a book Download as PDF Printable version Languages العربية Català Čeština Deutsch Ελληνικά Español فارسی Français 한국어 Bahasa Indonesia Italiano עברית Lietuvių 日本語 Norsk Polski Português Русский Српски / srpski ไทย Türkçe Українська اردو 中文 Edit links  This page was last edited on 13 October 2017, at 18:48. Text is available under the  Creative Commons Attribution-ShareAlike License ;\nadditional terms may apply.  By using this site, you agree to the  Terms of Use  and  Privacy Policy . Wikipedia® is a registered trademark of the  Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Developers Cookie statement Mobile view",
          "url": "https://en.wikipedia.org/wiki/K-means_clustering"
        },
        {
          "title": "Understanding K-means Clustering with Examples",
          "relevance": "1",
          "content": " Select Category\n     Select Category Big Data Analytics Big Data NoSQL Blockchain Business Intelligence Cloud Computing Finance Frameworks IBM Bluemix Marketing Mobile Development Operations Programming Project Management Success Story Systems & Architecture Systems Engineering Testing  BROWSE COURSES  My Courses All Courses Logout My Courses All Courses Log Out X Log In Sign Up   Please enter a valid emailid \n                                show\n                             Forgot Password? Start Learning resend ? \n                                show\n                             reset password \n                        * Can't find the email? Check your Spam folder or Promotions tab for an email from no-reply@edureka.co. Send us an email at  support@edureka.co  or call us at +91 88808 62004\n                       Please provide a valid input show \n                            By Signing up you agree to our  T&C  and  Privacy Policy Create an account   Please provide a valid input     \n                            WELCOME  It seems we don't have your phone number. Please share as it helps us to provide better customer service. Thank you! Continue All Course Home    Blogs \n                              \n                         Big Data Analytics  \n                     Blogs Videos Interview Questions \n            Understanding K-means Clustering with Examples          Recommended by  118 users                             edureka Jul 25, 2014 Add to  Bookmark × Close Email this post \n                                    Please enter a valid input.\n                                 \n                                    Please enter a valid email id or comma separated email id's.\n                                 Submit The post has been successfully mailed. Email  this Post  33.9K   \n             7 A Hospital Care chain wants to open a series of Emergency-Care wards within a region. We assume that the hospital knows the location of all the maximum accident prone areas in the region. They have to decide the number of the Emergency Units to be opened and the location of these Emergency Units, so that all the accident prone areas are covered in the vicinity of these Emergency Units. The challenge is to decide the location of these Emergency Units so that the whole region is covered. Here is when K-means Clustering comes to rescue! Before getting to K-means Clustering, let us first understand what Clustering is. A cluster refers to a small group of objects. Clustering is grouping those objects into clusters. In order to learn clustering, it is important to understand the scenarios that leads to cluster different objects. Let us identify few of them. What is Clustering? Clustering is dividing data points into homogeneous classes or clusters: Points in the same group are as similar as possible Points in different group are as dissimilar as possible When a collection of objects is given, we put objects into group based on similarity. Application of Clustering: Clustering is used in almost all the fields. You can infer some ideas from Example 1 to come up with lot of clustering applications that you would have come across. Listed here are few more applications, which would add to what you have learnt. Clustering helps marketers improve their customer base and work on the target areas. It helps group people (according to different criteria’s such as willingness, purchasing power etc.) based on their similarity in many ways related to the product under consideration. Clustering helps in identification of groups of houses on the basis of their value, type and geographical locations. Clustering is used to study earth-quake. Based on the areas hit by an earthquake in a region, clustering can help analyse the next probable location where earthquake can occur. Clustering Algorithms: A Clustering Algorithm tries to analyse natural groups of data on the basis of some similarity. It locates the centroid of the group of data points. To carry out effective clustering, the algorithm evaluates the distance between each point from the centroid of the cluster. The goal of clustering is to determine the intrinsic grouping in a set of unlabelled data. What is K-means Clustering? K-means (Macqueen, 1967) is one of the simplest unsupervised learning algorithms that solve the well-known clustering problem. K-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. K-means Clustering – Example 1: A pizza chain wants to open its delivery centres across a city. What do you think would be the possible challenges? They need to analyse the areas from where the pizza is being ordered frequently. They need to understand as to how many pizza stores has to be opened to cover delivery in the area. They need to figure out the locations for the pizza stores within all these areas in order to keep the distance between the store and delivery points minimum. Resolving these challenges includes a lot of analysis and mathematics. We would now learn about how clustering can provide a meaningful and easy method of sorting out such real life challenges. Before that let’s see what clustering is. K-means Clustering Method: If k is given, the K-means algorithm can be executed in the following steps: Partition of objects into k non-empty subsets Identifying the cluster centroids (mean point) of the current partition. Assigning each point to a specific cluster Compute the distances from each point and allot points to the cluster where the distance from the centroid is minimum. After re-allotting the points, find the centroid of the new cluster formed. The step by step process: Now, let’s consider the problem in Example 1 and see how we can help the pizza chain to come up with centres based on K-means algorithm. Similarly, for opening Hospital Care Wards: K-means Clustering will group these locations of maximum prone areas into clusters and define a cluster center for each cluster, which will be the locations where the Emergency Units will open. These Clusters centers are the centroids of each cluster and are at a minimum distance from all the points of a particular cluster, henceforth, the Emergency Units will be at minimum distance from all the accident prone areas within a cluster. Here is another example for you, try and come up with the solution based on your understanding of K-means clustering. K-means Clustering – Example 2: Let’s consider the data on drug-related crimes in Canada. The data consists of crimes due to various drugs that include, Heroin, Cocaine to prescription drugs, especially by underage people. The crimes resulted due to these substance abuse can be brought down by starting de-addiction centres in areas most afflicted by this kind of crime. With the available data, different objectives can be set. They are: Classify the crimes based on the abuse substance to detect prominent cause. Classify the crimes based on age groups. Analyze the data to determine what kinds of de-addiction centre is required. Find out how many de-addiction centres need to be setup to reduce drug related crime rate. The K-means algorithm can be used to determine any of the above scenarios by analyzing the available data. Following the K-means Clustering method used in the previous example, we can start off with a given k, following by the execution of the K-means algorithm. Mathematical Formulation for K-means Algorithm: D= { x 1 ,x 2 ,…,x i ,…,x m } à data set of m records x i = (x i1 ,x i2 ,…,x in ) à each record is an n-dimensional vector Finding Cluster Centers that Minimize Distortion: Solution can be found by setting the partial derivative of Distortion w.r.t. each cluster center to zero. For any k clusters, the value of k should be such that even if we increase the value of k from after several levels of clustering the distortion remains constant. The achieved point is called the “Elbow”. This is the ideal value of k, for the clusters created. Related Post:  Application of Clustering in Data Science Using real-time examples.  About edureka ( 252 Posts )  Share on  Related Posts Creating, Validating and Pruning Decision Tree in R  51.3K Cluster Analysis Steps in Business Analytics with R  1.7K 3D Graphs in R Commander  1.7K Statistical Modeling in Business Analytics with R  1.3K  Comments   7 Comments  krupa jain what is the difference between plain and iterative mapreduce? bob ama “If k is given, the K-means algorithm can be executed in the following steps” but you don’t say where “k” in ‘if k is given’ comes from. yogesh indani but k is the number of clusters how can u say in data set rahul thanks EdurekaSupport You are welcome, Rahul!! Please check out other posts as well.  sumit nice one Related Blogs Creating, Validating and Pruning Decision Tree in R Cluster Analysis Steps in Business Analytics with R 3D Graphs in R Commander Statistical Modeling in Business Analytics with R Subscribe to get free Newsletter  Please provide a valid Email Id   Thank you, your subscription is successful  You have already subscribed. SUBSCRIBE BROWSE COURSES © 2014 Brain4ce Education Solutions Pvt. Ltd. Edureka About us News & Media Contact us Careers Blog Reviews Terms & conditions Privacy policy Work with us  Become  an Instructor Hire from Edureka Follow us on  Learn on the GO! © 2014 Brain4ce Education Solutions Pvt. Ltd. All rights Reserved. \"PMP®\",\"PMI®\", \"PMI-ACP®\" and \"PMBOK®\" are registered marks of the Project Management Institute, Inc.  \n                                            MongoDB®, Mongo and the leaf logo are the registered trademarks of MongoDB, Inc. \n                Call us on \n                 1-800-275-9730  (Toll Free) +91 88808 62004 24 X 7 Customer Support\n                 X  1-800-275-9730   (Toll Free)       +91 88808 62004",
          "url": "https://www.edureka.co/blog/k-means-clustering/"
        },
        {
          "title": "A Tutorial on Clustering Algorithms",
          "relevance": "1",
          "content": "A \r\n  Tutorial on Clustering Algorithms Introduction  \r\n  | K-means |  Fuzzy C-means  |  Hierarchical  \r\n  |  Mixture of Gaussians  |  Links K-Means \r\n  Clustering The \r\n  Algorithm \r\n  K-means ( MacQueen, 1967 ) is one of the simplest unsupervised \r\n  learning algorithms that solve the well known clustering problem. The procedure \r\n  follows a simple and easy way to classify a given data set through a certain \r\n  number of clusters (assume k clusters) fixed a priori. The main idea is to define \r\n  k centroids, one for each cluster. These centroids shoud be placed in a cunning \r\n  way because of different location causes different result. So, the better choice \r\n  is to place them as much as possible far away from each other. The next step \r\n  is to take each point belonging to a given data set and associate it to the \r\n  nearest centroid. When no point is pending, the first step is completed and \r\n  an early groupage is done. At this point we need to re-calculate k new centroids \r\n  as barycenters of the clusters resulting from the previous step. After we have \r\n  these k new centroids, a new binding has to be done between the same data set \r\n  points and the nearest new centroid. A loop has been generated. As a result \r\n  of this loop we may notice that the k centroids change their location step by \r\n  step until no more changes are done. In other words centroids do not move any \r\n  more. \r\n  Finally, this algorithm aims at minimizing an  objective function , in \r\n  this case a squared error function. The objective function  , where   \r\n  is a chosen distance measure between a data point   \r\n  and the cluster centre  , is \r\n  an indicator of the distance of the  n  data points from their respective \r\n  cluster centres. The algorithm is \r\n  composed of the following steps:  Place K points into \r\n          the space represented by the objects that are being clustered. These \r\n          points represent initial group centroids. Assign each object \r\n          to the group that has the closest centroid. When all objects have \r\n          been assigned, recalculate the positions of the K centroids. Repeat Steps 2 and \r\n          3 until the centroids no longer move. This produces a separation of \r\n          the objects into groups from which the metric to be minimized can be \r\n          calculated. Although it can \r\n  be proved that the procedure will always terminate, the k-means algorithm does \r\n  not necessarily find the most optimal configuration, corresponding to the global \r\n  objective function minimum. The algorithm is also significantly sensitive to \r\n  the initial randomly selected cluster centres. The k-means algorithm can be \r\n  run multiple times to reduce this effect. K -means \r\n  is a simple algorithm that has been adapted to many problem domains. As we are \r\n  going to see, it is a good candidate for extension to work with fuzzy feature \r\n  vectors.  An \r\n  example Suppose that we have n sample feature \r\n  vectors  x 1 ,  x 2 , ..., \r\n   x n  all from the same class, and we know that they \r\n  fall into k compact clusters, k < n. Let  m i  be \r\n  the mean of the vectors in cluster i. If the clusters are well separated, we \r\n  can use a minimum-distance classifier to separate them. That is, we can say \r\n  that  x  is in cluster i if ||  x  -  m i  \r\n  || is the minimum of all the k distances. This suggests the following procedure \r\n  for finding the k means: Make initial \r\n      guesses for the means  m 1 ,  m 2 , \r\n      ...,  m k Until there \r\n      are no changes in any mean Use the \r\n          estimated means to classify the samples into clusters  For i from 1 to k Replace  m i  \r\n            with the mean of all of the samples for cluster i end_for end_until \r\n       Here is an example \r\n  showing how the means  m 1  and  m 2  \r\n  move into the centers of two clusters.  Remarks \r\n  This is a simple version of the k-means procedure. It can be viewed as a greedy \r\n  algorithm for partitioning the n samples into k clusters so as to minimize the \r\n  sum of the squared distances to the cluster centers. It does have some weaknesses: The way to initialize \r\n    the means was not specified. One popular way to start is to randomly choose \r\n    k of the samples. The results produced depend on \r\n    the initial values for the means, and it frequently happens that suboptimal \r\n    partitions are found. The standard solution is to try a number of different \r\n    starting points. It can happen that the set of \r\n    samples closest to  m i  is empty, so that  m i  \r\n    cannot be updated. This is an annoyance that must be handled in an implementation, \r\n    but that we shall ignore. The results depend on the metric \r\n    used to measure ||  x  -  m i  ||. \r\n    A popular solution is to normalize each variable by its standard deviation, \r\n    though this is not always desirable. The results depend on the value \r\n    of k. This last problem \r\n  is particularly troublesome, since we often have no way of knowing how many \r\n  clusters exist. In the example shown above, the same algorithm applied to the \r\n  same data produces the following 3-means clustering. Is it better or worse than \r\n  the 2-means clustering ?  Unfortunately there \r\n  is no general theoretical solution to find the optimal number of clusters for \r\n  any given data set. A simple approach is to compare the results of multiple \r\n  runs with different k classes and choose the best one according to a given criterion \r\n  (for instance the Schwarz Criterion - see  Moore's slides ), \r\n  but we need to be careful because increasing k results in smaller error function \r\n  values by definition, but also an increasing risk of overfitting. Bibliography J. B. \r\n      MacQueen (1967): \"Some Methods for classification and Analysis of Multivariate \r\n      Observations,  Proceedings of 5-th Berkeley Symposium on Mathematical \r\n      Statistics and Probability\" , Berkeley, University of California \r\n      Press, 1:281-297 Andrew \r\n      Moore: “K-means and Hierarchical Clustering - Tutorial Slides” http://www-2.cs.cmu.edu/~awm/tutorials/kmeans.html Brian T. Luke: “K-Means \r\n      Clustering” http://fconyx.ncifcrf.gov/~lukeb/kmeans.html Tariq Rashid: “Clustering” http://www.cs.bris.ac.uk/home/tr1690/documentation/fuzzy_clustering_initial_report/node11.html Hans-Joachim Mucha and Hizir \r\n      Sofyan: “Nonhierarchical Clustering” http://www.quantlet.com/mdstat/scripts/xag/html/xaghtmlframe149.ht K-means \r\n  interactive demo Previous page  |  Next \r\n  page",
          "url": "https://home.deib.polimi.it/matteucc/Clustering/tutorial_html/kmeans.html"
        },
        {
          "title": "kmeans",
          "relevance": "1",
          "content": "Toggle Main Navigation Log In Products Solutions Academia Support Community Events Contact Us How to Buy Contact Us How to Buy Log In Products Solutions Academia Support Community Events Documentation Toggle navigation Trial Software Product Updates Documentation Home Statistics and Machine Learning Toolbox Examples Functions and Other Reference Release Notes PDF Documentation Cluster Analysis k-Means and k-Medoids\nClustering Statistics and Machine Learning Toolbox Functions kmeans On this page Syntax Description Examples Train a  k -Means Clustering Algorithm Partition Data into Two Clusters Cluster Data Using Parallel Computing Input Arguments X k Name-Value Pair Arguments Display Distance EmptyAction MaxIter OnlinePhase Options Replicates Start Output Arguments idx C sumd D More About k -Means Clustering k -means++ Algorithm Algorithms References Extended Capabilities See Also This is machine translation  \n    \t  Translated by  \n         Mouseover text to see original. Click the button below to return to the English verison of the page.\n      \n       \n    \t \tNote: This page has been translated by  MathWorks .  Please click here \n            To view all translated materals including this page, select Japan from the country navigator on the bottom of this page.   Back to English  × Translate This Page Select Language Bulgarian Catalan Chinese Simplified Chinese Traditional Czech Danish Dutch English Estonian Finnish French German Greek Haitian Creole Hindi Hmong Daw Hungarian Indonesian Italian Japanese Korean Latvian Lithuanian Malay Maltese Norwegian Polish Portuguese Romanian Russian Slovak Slovenian Spanish Swedish Thai Turkish Ukrainian Vietnamese Welsh MathWorks Machine Translation The automated translation of this page is provided by a general purpose third party translator tool. MathWorks does not warrant, and disclaims all liability for, the accuracy, suitability, or fitness for purpose of the translation. Translate kmeans k -means clustering collapse all in page Syntax idx = kmeans(X,k) idx = kmeans(X,k,Name,Value) [idx,C]\n= kmeans( ___ ) [idx,C,sumd]\n= kmeans( ___ ) [idx,C,sumd,D]\n= kmeans( ___ ) Description example idx  = kmeans( X , k )  performs  k -means\nclustering  to partition the observations of the  n -by- p  data\nmatrix  X  into  k  clusters, and\nreturns an  n -by-1 vector ( idx )\ncontaining cluster indices of each observation. Rows of  X  correspond\nto points and columns correspond to variables. By default,  kmeans  uses the squared Euclidean\ndistance measure and the  k -means++\nalgorithm   for cluster center initialization. example idx  = kmeans( X , k , Name,Value )  returns\nthe cluster indices with additional options specified by one or more  Name,Value  pair\narguments. For example, specify the cosine distance, the number of times\nto repeat the clustering using new initial values, or to use parallel\ncomputing. example [ idx , C ]\n= kmeans( ___ )  returns the  k  cluster\ncentroid locations in the  k -by- p  matrix  C . example [ idx , C , sumd ]\n= kmeans( ___ )  returns the within-cluster sums\nof point-to-centroid distances in the  k -by-1 vector  sumd . example [ idx , C , sumd , D ]\n= kmeans( ___ )  returns distances from each point\nto every centroid in the  n -by- k  matrix  D . Examples collapse all Train a  k -Means Clustering Algorithm Open Script Cluster data using  k -means clustering, then plot the cluster regions. Load Fisher's iris data set.  Use the petal lengths and widths as predictors. load  fisheriris \nX = meas(:,3:4);\n\nfigure;\nplot(X(:,1),X(:,2), 'k*' , 'MarkerSize' ,5);\ntitle  'Fisher''s Iris Data' ;\nxlabel  'Petal Lengths (cm)' ;\nylabel  'Petal Widths (cm)' ;\n The larger cluster seems to be split into a lower variance region and a higher variance region.  This might indicate that the larger cluster is two, overlapping clusters. Cluster the data.  Specify  k  = 3 clusters. rng(1);  % For reproducibility \n[idx,C] = kmeans(X,3);\n kmeans  uses the  k -means++ algorithm for centroid initialization and squared Euclidean distance by default.  It is good practice to search for lower, local minima by setting the  'Replicates'  name-value pair argument. idx  is a vector of predicted cluster indices corrresponding to the observations in  X .   C  is a 3-by-2 matrix containing the final centroid locations. Use  kmeans  to compute the distance from each centroid to points on a grid. To do this, pass the centroids ( C ) and points on a grid to  kmeans , and implement one iteration of the algorithm. x1 = min(X(:,1)):0.01:max(X(:,1));\nx2 = min(X(:,2)):0.01:max(X(:,2));\n[x1G,x2G] = meshgrid(x1,x2);\nXGrid = [x1G(:),x2G(:)];  % Defines a fine grid on the plot \n\nidx2Region = kmeans(XGrid,3, 'MaxIter' ,1, 'Start' ,C);\n     % Assigns each node in the grid to the closest centroid Warning: Failed to converge in 1 iterations. \n kmeans  displays a warning stating that the algorithm did not converge, which you should expect since the software only implemented one iteration. Plot the cluster regions. figure;\ngscatter(XGrid(:,1),XGrid(:,2),idx2Region, ... \n    [0,0.75,0.75;0.75,0,0.75;0.75,0.75,0], '..' );\nhold  on ;\nplot(X(:,1),X(:,2), 'k*' , 'MarkerSize' ,5);\ntitle  'Fisher''s Iris Data' ;\nxlabel  'Petal Lengths (cm)' ;\nylabel  'Petal Widths (cm)' ;\nlegend( 'Region 1' , 'Region 2' , 'Region 3' , 'Data' , 'Location' , 'SouthEast' );\nhold  off ;\n Partition Data into Two Clusters Open Script Randomly generate the sample data. rng  default ;  % For reproducibility \nX = [randn(100,2)*0.75+ones(100,2);\n    randn(100,2)*0.5-ones(100,2)];\n\nfigure;\nplot(X(:,1),X(:,2), '.' );\ntitle  'Randomly Generated Data' ;\n There appears to be two clusters in the data. Partition the data into two clusters, and choose the best arrangement out of five initializations. Display the final output. opts = statset( 'Display' , 'final' );\n[idx,C] = kmeans(X,2, 'Distance' , 'cityblock' , ... 'Replicates' ,5, 'Options' ,opts);\n Replicate 1, 3 iterations, total sum of distances = 201.533.\nReplicate 2, 5 iterations, total sum of distances = 201.533.\nReplicate 3, 3 iterations, total sum of distances = 201.533.\nReplicate 4, 3 iterations, total sum of distances = 201.533.\nReplicate 5, 2 iterations, total sum of distances = 201.533.\nBest total sum of distances = 201.533\n By default, the software initializes the replicates separately using  k -means++. Plot the clusters and the cluster centroids. figure;\nplot(X(idx==1,1),X(idx==1,2), 'r.' , 'MarkerSize' ,12)\nhold  on \nplot(X(idx==2,1),X(idx==2,2), 'b.' , 'MarkerSize' ,12)\nplot(C(:,1),C(:,2), 'kx' , ... 'MarkerSize' ,15, 'LineWidth' ,3)\nlegend( 'Cluster 1' , 'Cluster 2' , 'Centroids' , ... 'Location' , 'NW' )\ntitle  'Cluster Assignments and Centroids' \nhold  off You can determine how well separated the clusters are by passing  idx  to  silhouette . Cluster Data Using Parallel Computing Clustering large data sets might take time,\nparticularly if you use online updates (set by default). If you have\na Parallel\n            Computing Toolbox™ license and you invoke a pool of workers,\nthen  kmeans  runs each clustering task (or replicate)\nin parallel. Therefore, if  Replicates  > 1, then\nthe parallel computing decreases time to convergence. Randomly generate a large data set from a Gaussian mixture\nmodel. Mu = bsxfun(@times,ones(20,30),(1:20)');  % Gaussian mixture mean \nrn30 = randn(30,30);\nSigma = rn30'*rn30;  % Symmetric and positive-definite covariance \nMdl = gmdistribution(Mu,Sigma);\n\nrng(1);  % For reproducibility \nX = random(Mdl,10000);\n Mdl  is a 30-dimensional  gmdistribution  model\nwith 20 components.  X  is a  10000 -by- 30  matrix\nof data generated from  Mdl . Invoke a parallel pool of workers. Specify options for\nparallel computing. pool = parpool;                       % Invokes workers \nstream = RandStream( 'mlfg6331_64' );   % Random number stream \noptions = statset( 'UseParallel' ,1, 'UseSubstreams' ,1, ... 'Streams' ,stream); Starting parallel pool (parpool) using the 'local' profile ... connected to 4 workers. The input argument  'mlfg6331_64'  of  RandStream  specifies\nto use the multiplicative lagged Fibonacci generator algorithm.  options  is\na structure array containing fields that specify options for controlling\nestimation. The Command Window indicates that four workers are available.\nThe number of workers might vary on your system. Cluster the data using  k -means clustering.\nSpecify that there are  k  = 20 clusters in the data\nand increase the number of iterations. Typically, the objective function\ncontains local minima. Specify 10 replicates to help find a lower,\nlocal minimum. tic;  % Start stopwatch timer \n[idx,C,sumd,D] = kmeans(X,20, 'Options' ,options, 'MaxIter' ,10000, ... 'Display' , 'final' , 'Replicates' ,10);\ntoc  % Terminate stopwatch timer Replicate 7, 44 iterations, total sum of distances = 7.55218e+06.\nReplicate 4, 95 iterations, total sum of distances = 7.53848e+06.\nReplicate 2, 104 iterations, total sum of distances = 7.54232e+06.\nReplicate 6, 80 iterations, total sum of distances = 7.54237e+06.\nReplicate 8, 111 iterations, total sum of distances = 7.54445e+06.\nReplicate 1, 52 iterations, total sum of distances = 7.55817e+06.\nReplicate 5, 70 iterations, total sum of distances = 7.55278e+06.\nReplicate 3, 94 iterations, total sum of distances = 7.54858e+06.\nReplicate 10, 56 iterations, total sum of distances = 7.54547e+06.\nReplicate 9, 83 iterations, total sum of distances = 7.53701e+06.\nBest total sum of distances = 7.53701e+06\nElapsed time is 3.239232 seconds. The Command Window displays the number of iterations and the\nterminal objective function value for each replicate. The output arguments\ncontain the results of replicate  9  because it has\nthe lowest total sum of distances. Input Arguments collapse all X  —  Data numeric matrix Data, specified as a numeric matrix. The rows of  X  correspond\nto observations, and the columns correspond to variables. If  X  is a numeric vector, then  kmeans  treats\nit as an  n -by-1 data matrix, regardless of its\norientation. Data Types:  single  |  double k  —  Number of clusters positive integer Number of clusters in the data, specified as a positive integer. Data Types:  single  |  double Name-Value Pair Arguments Specify optional\n      comma-separated pairs of  Name,Value  arguments.  Name  is\n      the argument name and  Value  is the corresponding value.\n         Name  must appear inside single quotes ( ' ' ). You can\n      specify several name and value pair arguments in any order as\n         Name1,Value1,...,NameN,ValueN . Example:  'Distance','cosine','Replicates',10,'Options',statset('UseParallel',1)  specifies\nthe cosine distance,  10  replicate clusters at different\nstarting values, and to use parallel computing. collapse all 'Display'  —  Level of output to display 'off'  (default) |  'final'  |  'iter' Level of output to display in the Command Window, specified\nas the comma-separated pair consisting of  'Display'  and\none of the following options: 'final'  — Displays results\nof the final iteration 'iter'  — Displays results\nof each iteration 'off'  — Displays nothing Example:  'Display','final' Data Types:  char 'Distance'  —  Distance measure 'sqeuclidean'  (default) |  'cityblock'  |  'cosine'  |  'correlation'  |  'hamming' Distance measure, in  p -dimensional space,\nused for minimization, specified as the comma-separated pair consisting\nof  'Distance'  and  'sqeuclidean' ,  'cityblock' ,  'cosine' ,  'correlation' ,\nor  'hamming' . kmeans  computes centroid clusters differently\nfor the different, supported distance measures. This table summarizes\nthe available distance measures. In the formulae,  x  is\nan observation (that is, a row of  X ) and  c  is\na centroid (a row vector). Distance Measure Description Formula 'sqeuclidean' Squared Euclidean distance (default). Each centroid is\nthe mean of the points in that cluster. d ( x , c ) = ( x − c ) ( x − c ) ′ 'cityblock' Sum of absolute differences, i.e., the  L 1\ndistance. Each centroid is the component-wise median of the points\nin that cluster. d ( x , c ) = ∑ j = 1 p | x j − c j | 'cosine' One minus the cosine of the included angle between points\n(treated as vectors). Each centroid is the mean of the points in that\ncluster, after normalizing those points to unit Euclidean length. d ( x , c ) = 1 − x c ′ ( x x ′ ) ( c c ′ ) 'correlation' One minus the sample correlation between points (treated\nas sequences of values). Each centroid is the component-wise mean\nof the points in that cluster, after centering and normalizing those\npoints to zero mean and unit standard deviation. d ( x , c ) = 1 − ( x − x ¯ → ) ( c − c ¯ → ) ′ ( x − x ¯ → ) ( x − x ¯ → ) ′ ( c − c ¯ → ) ( c − c ¯ → ) ′ , where \n x ¯ → = 1 p ( ∑ j = 1 p x j ) 1 → p c ¯ → = 1 p ( ∑ j = 1 p c j ) 1 → p 1 → p  is a row vector\nof  p  ones. 'hamming' This measure is only suitable for binary data. It\nis the proportion of bits that differ. Each centroid is the component-wise\nmedian of points in that cluster. d ( x , y ) = 1 p ∑ j = 1 p I { x j ≠ y j } , where  I  is the indicator function. Example:  'Distance','cityblock' Data Types:  char 'EmptyAction'  —  Action to take if cluster loses all member observations 'singleton'  (default) |  'error'  |  'drop' Action to take if a cluster loses all its member observations,\nspecified as the comma-separated pair consisting of  'EmptyAction'  and\none of the following options. Value Description 'error' Treat an empty cluster as an error. 'drop' Remove any clusters that become empty.  kmeans  sets\nthe corresponding return values in  C  and  D  to  NaN . 'singleton' Create a new cluster consisting of the one point furthest\nfrom its centroid (default). Example:  'EmptyAction','error' Data Types:  char 'MaxIter'  —  Maximum number of iterations 100  (default) |  positive integer Maximum number of iterations, specified as the comma-separated\npair consisting of  'MaxIter'  and a positive integer. Example:  'MaxIter',1000 Data Types:  double  |  single 'OnlinePhase'  —  Online update flag 'off'  (default) |  'on' Online update flag, specified as the comma-separated pair consisting\nof  'OnlinePhase'  and  'off'  or  'on' . If  OnlinePhase  is  on ,\nthen  kmeans  performs an online update phase in\naddition to a batch update phase. The online phase can be time consuming\nfor large data sets, but guarantees a solution that is a local minimum\nof the distance criterion. In other words, the software finds a partition\nof the data in which moving any single point to a different cluster\nincreases the total sum of distances. Example:  'OnlinePhase','on' Data Types:  char 'Options'  —  Options for controlling iterative algorithm for minimizing fitting criteria []  (default) |  structure array returned by  statset Options for controlling the iterative algorithm for minimizing\nthe fitting criteria, specified as the comma-separated pair consisting\nof  'Options'  and a structure array returned by  statset . These options require Parallel\nComputing Toolbox™. This table summarizes the available options. Option Description 'Streams' A  RandStream  object\nor cell array of such objects. If you do not specify  Streams ,  kmeans  uses\nthe default stream or streams. If you specify  Streams ,\nuse a single object except when: You have an open parallel pool UseParallel  is  true . UseSubstreams  is  false . In that case, use a cell array the same size\nas the parallel pool. If a parallel pool is not open, then  Streams  must\nsupply a single random number stream. 'UseParallel' If  true ,  Replicates  >\n1, and if a parallel pool of workers from the Parallel\nComputing Toolbox is open, then the software implements  k -means\non each replicate in parallel.  If the Parallel Computing Toolbox™ is\nnot installed, or a parallel pool of workers is not open, computation\noccurs in serial mode. Default is  default , meaning\nserial computation. 'UseSubstreams' Set to  true  to compute in parallel in a\nreproducible fashion. Default is  false . To compute\nreproducibly, set  Streams  to a type allowing substreams:  'mlfg6331_64'  or  'mrg32k3a' . \nTo ensure more predictable\nresults, use  parpool  and explicitly\ncreate a parallel pool before invoking  kmeans  and\nsetting  'Options',statset('UseParallel',1) . Example:  'Options',statset('UseParallel',1) Data Types:  struct 'Replicates'  —  Number of times to repeat clustering using new initial cluster centroid positions 1  (default) |  positive integer Number of times to repeat clustering using new initial cluster\ncentroid positions, specified as the comma-separated pair consisting\nof  'Replicates'  and an integer.  kmeans  returns\nthe solution with the lowest  sumd . You can set  'Replicates'  implicitly by supplying\na 3-D array as the value for the  'Start'  name-value\npair argument. Example:  'Replicates',5 Data Types:  double  |  single 'Start'  —  Method for choosing initial cluster centroid positions 'plus'  (default) |  'cluster'  |  'sample'  |  'uniform'  |  numeric matrix  |  numeric array Method for choosing initial cluster centroid positions (or  seeds ),\nspecified as the comma-separated pair consisting of  'Start'  and  'cluster' ,  'plus' ,  'sample' ,  'uniform' ,\na numeric matrix, or a numeric array, . This table summarizes the\navailable options for choosing seeds. Value Description 'cluster' Perform a preliminary clustering phase on a random 10% subsample\nof  X . This preliminary phase is itself initialized\nusing  'sample' . 'plus'  (default) Select  k  seeds by implementing the  k -means++\nalgorithm   for cluster center initialization. 'sample' Select  k  observations from  X  at\nrandom. 'uniform' Select  k  points uniformly at random from\nthe range of  X . Not valid with the Hamming distance. numeric matrix k -by- p  matrix of centroid\nstarting locations. The rows of  Start  correspond\nto seeds. The software infers  k  from the first\ndimension of  Start , so you can pass in  []  for  k . numeric array k -by- p - r  array\nof centroid starting locations. The rows of each page correspond to\nseeds. The third dimension invokes replication of the clustering routine.\nPage  j  contains the set of seeds for replicate  j .\nThe software infers the number of replicates (specified by the  'Replicates'  name-value\npair argument) from the size of the third dimension. Example:  'Start','sample' Data Types:  char  |  double  |  single Note The software treats  NaN s as missing data,\nand removes any row of  X  containing at least one  NaN .\nRemoving rows of  X  reduces the sample size. Output Arguments collapse all idx  — Cluster indices numeric column vector Cluster indices, returned as a numeric column vector.  idx  has\nas many rows as  X , and each row indicates the cluster\nassignment of the corresponding observation. C  — Cluster centroid locations numeric matrix Cluster centroid locations, returned as a numeric matrix.  C  is\na  k -by- p  matrix, where row  j  is\nthe centroid of cluster  j . sumd  — Within-cluster sums of point-to-centroid distances numeric column vector Within-cluster sums of point-to-centroid distances, returned\nas a numeric column vector.  sumd  is a  k -by-1\nvector, where element  j  is the sum of point-to-centroid\ndistances within cluster  j . D  — Distances from each point to every centroid numeric matrix Distances from each point to every centroid, returned as a numeric\nmatrix.  D  is an  n -by- k  matrix,\nwhere element ( j , m ) is the distance\nfrom observation  j  to centroid  m . More About collapse all k -Means Clustering k-means clustering ,\nor Lloyd’s algorithm  [2] , is an iterative, data-partitioning algorithm that\nassigns  n  observations to exactly one of  k  clusters\ndefined by centroids, where  k  is chosen before\nthe algorithm starts.  The algorithm proceeds as follows: Choose  k  initial cluster centers\n( centroid ). For example, choose  k  observations\nat random (by using  'Start','sample' ) or use the  k -means\n++ algorithm  for cluster center initialization (the default). Compute point-to-cluster-centroid distances of all\nobservations to each centroid. There are two ways to proceed (specified by  OnlinePhase ): Batch update — Assign each observation to the\ncluster with the closest centroid. Online update — Individually assign observations\nto a different centroid if the reassignment decreases the sum of the\nwithin-cluster, sum-of-squares point-to-cluster-centroid distances. \nFor more details, see  Algorithms . Compute the average of the observations in each cluster\nto obtain  k  new centroid locations. Repeat steps 2 through 4 until cluster assignments\ndo not change, or the maximum number of iterations is reached. k -means++ Algorithm The  k -means++\nalgorithm  uses an heuristic to find centroid seeds for  k -means\nclustering. According to Arthur and Vassilvitskii  [1] ,  k -means++\nimproves the running time of Lloyd’s algorithm, and the quality\nof the final solution. The  k -means++ algorithm chooses seeds as\nfollows, assuming the number of clusters is  k .  Select an observation uniformly at random from the\ndata set,  X . The chosen observation is the first\ncentroid, and is denoted  c 1 . Compute distances from each observation to  c 1 .\nDenote the distance between  c j  and\nthe observation  m  as  d ( x m , c j ) . Select the next centroid,  c 2  at\nrandom from  X  with probability  d 2 ( x m , c 1 ) ∑ j = 1 n d 2 ( x j , c 1 ) . To choose center  j : Compute the distances from each observation to each\ncentroid, and assign each observation to its closest centroid. For  m  = 1,..., n  and  p  =\n1,..., j  – 1, select centroid  j  at\nrandom from  X  with probability d 2 ( x m , c p ) ∑ { h ; x h ∈ C p } d 2 ( x h , c p ) , where  C p  is\nthe set of all observations closest to centroid  c p  and  x m  belongs\nto  C p . That is, select each subsequent center with a probability proportional\nto the distance from itself to the closest center that you already\nchose. Repeat step 4 until  k  centroids\nare chosen. Arthur and Vassilvitskii  [1]  demonstrate,\nusing a simulation study for several cluster orientations, that  k -means++\nachieves faster convergence to a lower sum of within-cluster, sum-of-squares\npoint-to-cluster-centroid distances than Lloyd’s algorithm. Algorithms kmeans  uses a two-phase iterative\nalgorithm to minimize the sum of point-to-centroid distances, summed\nover all  k  clusters. This first phase uses  batch updates , where each iteration\nconsists of reassigning points to their nearest cluster centroid,\nall at once, followed by recalculation of cluster centroids. This\nphase occasionally does not converge to solution that is a local minimum.\nThat is, a partition of the data where moving any single point to\na different cluster increases the total sum of distances. This is\nmore likely for small data sets. The batch phase is fast, but potentially\nonly approximates a solution as a starting point for the second phase.\n  This second phase uses  online updates ,\nwhere points are individually reassigned if doing so reduces the sum\nof distances, and cluster centroids are recomputed after each reassignment.\nEach iteration during this phase consists of one pass though all the\npoints. This phase converges to a local minimum, although there might\nbe other local minima with lower total sum of distances. In general,\nfinding the global minimum is solved by an exhaustive choice of starting\npoints, but using several replicates with random starting points typically\nresults in a solution that is a global minimum. If  Replicates  =  r  >\n1 and  Start  is  plus  (the default),\nthen the software selects  r  possibly different\nsets of seeds according to the  k -means++\nalgorithm . If you enable the  UseParallel  option\nin  Options  and  Replicates  >\n1, then each worker selects seeds and clusters in parallel. References [1] Arthur, David, and Sergi Vassilvitskii.\n“K-means++: The Advantages of Careful Seeding.”  SODA\n‘07: Proceedings of the Eighteenth Annual ACM-SIAM Symposium\non Discrete Algorithms . 2007, pp. 1027–1035. [2] Lloyd, Stuart P. “Least Squares Quantization\nin PCM.”  IEEE Transactions on Information Theory .\nVol. 28, 1982, pp. 129–137. [3] Seber, G. A. F.  Multivariate\nObservations . Hoboken, NJ: John Wiley & Sons, Inc.,\n1984. [4] Spath, H.  Cluster Dissection\nand Analysis: Theory, FORTRAN Programs, Examples . Translated\nby J. Goldschmidt. New York: Halsted Press, 1985. Extended Capabilities Tall Arrays Calculate with arrays that have more rows than fit in memory.  This function supports tall arrays for out-of-memory\ndata with some limitations. Only random sample initialization is supported. Supported syntaxes: idx = kmeans(X,k)  performs classic\nk-means clustering. [idx,C] = kmeans(X,k)  also returns\nthe  k  cluster centroid locations. [idx,C,sumd] = kmeans(X,k)  additionally\nreturns the  k  within-cluster sums of point-to-centroid\ndistances. [___] = kmeans(___,Name,Value)  specifies\nadditional name-value pair options using any of the other syntaxes.\nValid options are: 'Start'  — Method used to\nchoose the initial cluster centroid positions. Value can be:  'plus'  (default) — Select  k  observations\nfrom  X  using a variant of the kmeans++ algorithm\nadapted for tall data. 'sample'  — Select  k  observations\nfrom  X  at random. Numeric matrix — A k-by-p matrix to explicitly\nspecify starting locations. 'Options'  — An options structure\ncreated using the  statset  function. For tall\narrays,  kmeans  uses the fields listed here and\nignores all other fields in the options structure: 'Display'  — Level of display.\nChoices are  'iter'  (default),  'off' ,\nand  'final' . 'MaxIter'  — Maximum number\nof iterations. Default is  100 . 'TolFun'  — Convergency tolerance\nfor the within-cluster sums of point-to-centroid distances. Default\nis  1e-4 . This option field only works with tall\narrays. For more information, see  Tall Arrays  (MATLAB). C/C++ Code Generation Generate C and C++ code using MATLAB® Coder™. Usage notes and limitations: If the  Start  method uses random\nselections, the initial centroid cluster positions might not match MATLAB ® . If the number of rows in  X  is fixed,\ncode generation does not remove rows of  X  that\ncontain a  NaN . The cluster centroid locations in  C  can\nhave a different order than in MATLAB. In this case, the cluster\nindices in  idx  have corresponding differences. If you provide  Display , its value\nmust be  'off' . If you provide  Streams , it must\nbe empty and  UseSubstreams  must be  false . When you set the  UseParallel  option to  true :   Some computations can execute in parallel even when\n                                         Replicates  is  1 . For\n                                    large data sets, when  Replicates  is\n                                         1 , consider setting the\n                                         UseParallel  option to\n                                         true . kmeans  uses  parfor  to create\n                                    loops that run in parallel on supported shared-memory multicore\n                                    platforms. Loops that run in parallel can be faster than loops\n                                    that run on a single thread. If your compiler does not support\n                                    the Open Multiprocessing (OpenMP) application interface or you\n                                    disable OpenMP library,  MATLAB\n                                        Coder™  treats\n                                    the  parfor -loops as\n                                         for -loops. To find supported compilers,\n                                    see  Supported Compilers .  See Also clusterdata  |  gmdistribution  |  linkage  |  parpool  |  silhouette  |  statset Topics Create Clusters and Determine Separation Determine the Correct Number of Clusters Avoid Local Minima Introduction to k-Means Clustering Introduced before R2006a × MATLAB Command You clicked a link that corresponds to this MATLAB command:  Run the command by entering it in the MATLAB Command Window.\n        Web browsers do not support MATLAB commands. Close × Select Your Country Choose your country to get translated content where available and see local events and offers. Based on your location, we recommend that you select:  . You can also select a location from the following list: Americas Canada  (English) United States  (English) Europe Belgium  (English) Denmark  (English) Deutschland  (Deutsch) España  (Español) Finland  (English) France  (Français) Ireland  (English) Italia  (Italiano) Luxembourg  (English) Netherlands  (English) Norway  (English) Österreich  (Deutsch) Portugal  (English) Sweden  (English) Switzerland\n                     Deutsch English Français United Kingdom  (English) Asia Pacific Australia  (English) India  (English) New Zealand  (English) 中国  (简体中文) 日本  (日本語) 한국  (한국어) See all countries Trial Software Product Updates Statistics and Machine Learning Toolbox Documentation Examples Functions and Other Reference Release Notes PDF Documentation Other Documentation MATLAB Symbolic Math Toolbox Neural Network Toolbox Bioinformatics Toolbox Curve Fitting Toolbox Documentation Home Support MATLAB Answers Installation Help Bug Reports Product Requirements Software Downloads Free eBook: Machine Learning with MATLAB Download now Explore Products MATLAB Simulink Student Software Hardware Support File Exchange Try or Buy Downloads Trial Software Contact Sales Pricing and Licensing Learn to Use Documentation Tutorials Examples  Videos and Webinars Training Get Support Installation Help Answers  Consulting Application Status License Center About  MathWorks Careers Newsroom Social Mission About  MathWorks MathWorks Accelerating the pace of engineering and science MathWorks  is the leading developer of mathematical computing software for engineers and scientists. Discover... United States\n Patents Trademarks Privacy Policy Preventing Piracy © 1994-2017 The MathWorks, Inc. Join the conversation",
          "url": "https://www.mathworks.com/help/stats/kmeans.html?requestedDomain=www.mathworks.com"
        },
        {
          "title": "K-Means Clustering",
          "relevance": "1",
          "content": "This site uses cookies for analytics, personalized content and ads. By continuing to browse this site, you agree to this use. Learn more SALES: 1-800-867-1380\n MY ACCOUNT PORTAL Sign in Features Features Infrastructure Web Mobile Dev & Test Media Integration Big Data Big Compute Data Management Identity & Access Management Storage, Backup & Recovery Discover What is Azure Enterprise IT Application Hosting Azure vs. Amazon Web Services Azure in China Services COMPUTE Virtual Machines Web Sites Mobile Services Cloud Services DATA SERVICES Storage SQL Database HDInsight Cache Backup Recovery Manager APP SERVICES Media Services Services Bus Notification Hubs Scheduler BizTalk Services Visual Studio Online Active Directory Multi-Factor Authentication Automation CDN NETWORK SERVICES ExpressRoute Virtual Network Traffic Manager Case Studies Pricing Overview Pricing Details COMPUTE Virtual Machines Web Sites Mobile Services Cloud Services DATA SERVICES Storage SQL Database HDInsight Cache Backup Site Recovery APP SERVICES Media Services Service Bus Notification Hubs Scheduler Automation BizTalk Services Visual Studio Online Active Directory Multi-Factor Authentication CDN NETWORK SERVICES ExpressRoute Virtual Network Traffic Manager Data Transfers Calculator Purchase Options Member Offers MSDN BizSpark Startups Microsoft Partner Network Regions Support Plans FAQ Documentation Downloads Add-ons Community Blog Service Updates Events Partners Education Newsletter Support Support Options Support Plans Forums Service Dashboard Trust Center Overview Security Privacy Compliance Legal FAQ FREE TRIAL Machine Learning Modules Initialize Model Clustering Clustering K-Means Clustering K-Means Clustering K-Means Clustering K-Means Clustering TOC Collapse the table of content Expand the table of content This documentation is archived and is not being maintained. This documentation is archived and is not being maintained. K-Means Clustering   Published: March 2, 2015 Updated: August 20, 2017 Configures and initializes a K-means clustering model Category:  Machine Learning / Initialize Model / Clustering Module Overview This article describes how to use the  K-Means Clustering  module in Azure Machine Learning Studio to create an untrained K-means clustering model. K-means is one of the simplest and the best known  unsupervised  learning algorithms, and can be  used for a variety of machine learning tasks, such as  detecting abnormal data , clustering of text documents, and analysis of a dataset prior to using other classification or regression methods. To create a clustering model, you add this module to your experiment, connect a dataset, and set parameters such as the number of clusters you expect, the distance metric to use in creating the clusters, and so forth. After you have configured the module hyperparameters, connect the untrained model to the  Train Clustering Model  or the  Sweep Clustering  modules to train the model on the input data that you provide.  Because the K-means algorithm is an unsupervised learning method, a label column is optional. If your data includes a label, you can use the label values to guide selection of the clusters and optimize the model. If your data has no label, the algorithm will create clusters representing possible categories based solely on the data.   Tip  \nIf your training data has labels, consider using one of the supervised  classification  methods provided in Azure Machine Learning. For example, you might compare the results of clustering to the results when using one of the multiclass decision tree algorithms. Understanding K-Means Clustering In general, clustering uses iterative techniques to group cases in a dataset into clusters that contain similar characteristics. These groupings are useful for exploring data, identifying anomalies in the data, and eventually for making predictions. Clustering models can also help you identify relationships in a dataset that you might not logically derive by browsing or simple observation. For these reasons, clustering is often used in the early phases of machine learning tasks, to explore the data and discover  unexpected correlations. When you configure a clustering model using the k-means method, you must specify a target number  k  indicating the number of  centroids  you want in the model. The centroid is a point that is representative of each cluster. The K-means algorithm assigns each incoming data point to one of the clusters by minimizing the within-cluster sum of squares. When processing the training data, the K-means algorithm begins with an initial set of randomnly chosen centroids, which serve as starting points for each cluster, and applies Lloyd's algorithm to iteratively refine the locations of the centroids. The K-means algorithm stops building and refining clusters when it meets one or more of these conditions: The centroids stabilize, meaning that cluster assignments for individual points no longer change and the algorithm has converged on a solution. The algorithm completed running the specified number of iterations. After completing the training phase, you use the  Assign Data to Clusters  module to assign new cases to one of the clusters that was found by the k-means algorithm. Cluster assignment is performed by computing the distance between the new case and the centroid of each cluster. Each new case is assigned to the cluster with the nearest centroid. How to Configure K-Means Clustering Add the  K-Means Clustering  module to your experiment. Specify how you want the model to be trained, by setting the  Create trainer mode  option. Single Parameter . If you know the exact parameters you want to use in the clustering model, you can provide a specific set of values as arguments. Parameter Range . If you are not sure of the best parameters, you can find the optimal parameters by specifying multiple values and using the  Sweep Clustering  module to find the optimal configuration. The trainer iterates over multiple combinations of the settings you provided and determine the combination of values that produces the optimal clustering results. For  Number of Centroids , type the number of clusters you want the algorithm to begin with. The model is not guaranteed to produce exactly this number of clusters. The algorithn starts with this number of data points and iterates to find the optimal configuration, as described in the  Technical Notes  section. If you are performing a parameter sweep, the name of the property changes to  Range for Number of Centroids . You can use the  Range Builder  to specify a range, or you can type a series of numbers representing different numbers of clusters to create when initializing each model. The properties  Initialization  or  Initialization for sweep  are used to specify the algorithm that is used to define the initial cluster configuration. First N .    Some initial number of data points are chosen from the data set and used as the initial means. Also called the  Forgy method . Random .    The algorithm randomly places a data point in a cluster and then computes the initial mean to be the centroid of the cluster's randomly assigned points. Also called the  random partition  method. K-Means++ .   This is the default method for initializing clusters. The  K-means ++  algorithm was proposed in 2007 by David Arthur and Sergei Vassilvitskii to avoid poor clustering by the standard k-means algorithm.  K-means ++  improves upon standard K-means by using a different method for choosing the initial cluster centers. K-Means++Fast .   A variant of the  K-means ++  algorithm that was optimized for faster clustering. Evenly .   Centroids are located equidistant from each other in the d-Dimensional space of n data points. Use label column .   The values in the label column are used to guide the selection of centroids. For  Random number seed , optionally type a value to use as the seed for the cluster initialization. This value can have a significant effect on cluster selection. If you use a parameter sweep, you can specify that multiple initial seeds be created, to look for the best initial seed value. For  Number of seeds to sweep , type the total number of random seed values to use as starting points. For  Metric , choose the function to use for measuring the distance between cluster vectors, or between new data points and the randomly chosen centroid. Azure Machine Learning supports the following cluster distance metrics: Euclidean .   The Euclidean distance is commonly used as a measure of cluster scatter for K-means clustering. This metric is preferred because it minimizes the mean distance between points and the centroids. Cosine .                     The cosine function is used to measure cluster similarity. Cosine similarity is useful in cases where you do not care about the length of a vector, only its angle. For  Iterations , type the number of times the algorithm should iterate over the training data before finalizing the selection of centroids. You can adjust this parameter to balance accuracy vs. training time. For  Assign label mode , choose an option that specifies how a label column, if present in the dataset, should be handled. Because K-means clustering is an unsupervised machine learning method, labels are optional. However, if your dataset already has a label column, you can use those values to guide selection of the clusters, or you can specify that the values be ignored. Ignore label column . The values in the label column are ignored and are not used in building the model. Fill missing values . The label column values are used as features to help build the clusters. If any rows are missing a label, the value is imputed by using other features. Overwrite from closest to center . The label column values are replaced with predicted label values, using the label of the point that is closest to the current centroid. Train the model. If you set  Create trainer mode  to  Single Parameter , add a tagged dataset and train the model by using the  Train Clustering Model  module. If you set  Create trainer mode  to  Parameter Range , add a tagged dataset and train the model using  Sweep Clustering . You can use the model trained using those parameters, or you can make a note of the parameter settings to use when configuring a learner. Results After you have finished configuring and training the model, you have a model that you can use to generate scores. However, there are multiple ways to train the model, and multiple ways to view and use the results: To capture a snapshot of the model in your workspace If you used the  Train Clustering Model  module Right-click the  Train Clustering Model  module. Select  Trained model  and then click  Save as Trained Model . If you used the  Sweep Clustering  module to train the model Right-click the  Sweep Clustering  module. Select  Best Trained model  and then click  Save as Trained Model . The saved model will represent the training data at the time you saved the model. If you later update the training data used in the experiment, it will not update the saved model. To see a visual representation of the clusters in the model If you used the  Train Clustering Model  module Right-click the module, and select  Results dataset . Select  Visualize . If you used the  Sweep Clustering  module Add an instance of the  Assign Data to Clusters  module and generate scores using the  Best Trained model . Right-click the  Assign Data to Clusters  module, select  Results dataset , and select  Visualize . The chart is generated by using  Principal Component Analysis , which is a technique in data science for compressing the feature space of a model. The chart shows some set of features, compressed into two dimensions, that best characterize the difference between the clusters. By visually reviewing the general size of the feature space for each cluster and how much the clusters overlap, you can get an idea of how well your model might perform. For example, the following PCA charts represent the results from two models trained using the same data: the first was configured to output two clusters, and the second was configured to output three clusters. From these charts, you can see that increasing the number of clusters did not necessarily improve separation of the classes.   Tip  \nUse the  Sweep Clustering  module to choose the optimal set of hyperparameters, including the random seed and number of starting centroids. To see the list of data points and the clusters they belong to There are two options for viewing the dataset with results, depending on how you trained the model: If you used the  Sweep Clustering  module to train the model Use the checkbox in the  Sweep Clustering  module to specify whether you want to see the input data together with the results, or see just the results. When training is complete, right-click the module, and select  Results dataset  (output number 2) Click  Visualize . If you used the  Train Clustering Model  module Add the  Assign Data to Clusters  module and connect the trained model to the left-hand input. Connect a dataset to the right-hand input. Add the  Convert to Dataset  module to your experiment and connect it to the output of  Assign Data to Clusters . Use the checkbox in the  Assign Data to Clusters  module to specify whether you want to see the input data together with the results, or see just the results. Run the experiment, or run just the  Convert to Dataset  module. Right-click  Convert to Dataset , select  Results dataset , and click  Visualize . The output contains the input data columns first, if you included them, and the following columns for each row of input data: Assignment : The assignment is a value between 1 and  n , where  n  is the total number of clusters in the model. Each row of data can be assigned to only one cluster. DistancesToClusterCenter no.n :  This value measures the distance from the current data point to the centroid for the cluster. A separate column in output for each cluster in the trained model. The values for cluster distance are based on the distance metric you selected in the option,  Metric for measuring cluster result . Even if you perform a parameter sweep on the clustering model, only one metric can be applied during the sweep. If you change the metric, you might get different distance values. To visualize intra-cluster distances In the dataset of results from the previous section, click the column of distances for each cluster. Studio displays a histogram that visualizes the distribution of distances for points within the cluster. For example, the following histograms show the distribution of cluster distances from the same experiment, using four different metrics. All other settings for the parameter sweep were the same. Changing the metric resulted in a different number of clusters in one model. In general, you should choose a metric that maximizes the distance between data points in different classes, and minimizes distances within a class. You can use the precomputed means and other values in the  Statistics  pane to guide you in this decision.   Tip  \nYou can extract means and other values used in visualizations by using the  PowerShell module for Azure Machine Learning . Or use the  Execute R Script  module to compute a custom distance matrix. Tips for Generating the Best Clustering Model It is known that the  seeding  process used during clustering can significantly affect the model. Seeding means the initial placement of points into potental centroids. For example, if the dataset contains many outliers, and an outlier is chosen to seed the clusters, no other data points would fit well with that cluster and the cluster could be a singleton: that is, a cluster with only one point. There are various ways to avoid this problem: Use a parameter sweep to change the number of centroids and try multiple seed values. Create multiple models, varying the metric or iterating more. Use a method such as PCA to find variables that have a detrimental effect on clustering. See the  Find similar companies  sample for a demonstration of this technique. In general, with clustering models, it is possible that any given configuration will result in a locally optimized set of clusters. In other words, the set of clusters returned by the model suits only the current data points, and is not generalizable to other data. If you used a different initial configuration, the K-means method might find a different, perhaps superior, configuration.   Important  \nWe recommend that you always experiment with the parameters, create multiple models, and compare the resulting models. Examples For examples of how K-means clustering is used in Azure Machine Learning, see these experiments in the  Model Gallery : The  Group iris data  sample compares the results of  K-Means Clustering  and  Multiclass Logistic Regression  for classification, The  Color Quantization sample  builds multiple K-means models with different parameters to find the optimum image compression. The  Clustering: Similar Companies  sample uses K-means with different numbers of centroids to find groups of similar companies in the S&P500. Technical Notes Given a specific number of clusters ( K ) to find for a set of  D -dimensional data points with  N  data points, the K-means algorithm builds the clusters as follows: The module initializes a  K -by- D  array with the final centroids that define the  K  clusters found. By default, the module assigns the first  K  data points in order to the  K  clusters. Starting with an initial set of  K  centroids, the method uses Lloyd's algorithm to iteratively refine the locations of the centroids. The algorithm terminates when the centroids stabilize or when a specified number of iterations are completed. A similarity metric (by default, Euclidean distance) is used to assign each data point to the cluster that has the closest centroid.   Warning  If you pass a parameter range to  Train Clustering Model , it will use only the first value in the parameter range list. If you pass a single set of parameter values to the  Sweep Clustering  module, when it expects a range of settings for each parameter, it ignores the values and using the default values for the learner. If you select the  Parameter Range  option and enter a single value for any parameter, that single value you specified will be used throughout the sweep, even if other parameters change across a range of values. Module Parameters Name Range Type Default Description Number of Centroids >=2 Integer 2 Number of Centroids Metric List (subset) Metric Euclidean Selected metric Initialization List Centroid initialization method K-Means++ Initialization algorithm Iterations >=1 Integer 100 Number of iterations Outputs Name Type Description Untrained model ICluster interface Untrained K-Means clustering model Exceptions For a list of all exceptions, see  Machine Learning Module Error Codes . Exception Description Error 0003 Exception occurs if one or more of inputs are null or empty. See Also Clustering Assign Data to Clusters Train Clustering Model Sweep Clustering Show: Inherited\n         Protected\n         Print  Export ( 0 )\n             Print  Share\n             IN THIS ARTICLE Is this page helpful? Yes No Additional feedback? 1500  characters remaining Submit Skip this Thank you! We appreciate your feedback. Go Social Facebook Twitter Rss Newsletter Microsoft Azure Features Services Regions Case Studies Pricing Calculator Documentation Downloads Marketplace Microsoft Azure in China Community Blogs Forums Events Support Forums Service Dashboard Support Account Subscriptions Profile Preview Features Management Portal Trust Center Security Privacy Compliance Hello from Seattle\n English English Dansk Deutsch Español Français Italiano Nederlands Polski Português Svenska Türkçe Pусский 日本語 한국어 简体中文 繁體中文 \n                          Contact Us Trademarks Privacy & Cookies Feedback © 2017 Microsoft © 2017 Microsoft",
          "url": "https://msdn.microsoft.com/en-us/library/azure/dn905944.aspx"
        },
        {
          "title": "Bài toán phân nhóm giả định",
          "relevance": "0",
          "content": "Điều hướng Đăng ký Đăng nhập Tìm kiếm  Tìm kiếm\n Diễn đàn eBook Github Top Trending DevDocs Bitcoin Tags RaoXYZ CongtyAZ \n\t\t\t\tYour browser does not seem to support JavaScript. As a result, your viewing experience will be diminished, and you may not be able to execute some actions.\n\t\t\t \n\t\t\t\tPlease download a browser that supports JavaScript, or enable it if it's disabled (i.e. NoScript).\n\t\t\t Ứng dụng thuật toán phân cụm K-means trong nhận diện chữ số viết tay Chủ đề này đã bị xóa. Chỉ ban quản trị mới xem được. V vinhnd được chỉnh sửa lần cuối bởi  Bài này tôi sẽ giới thiệu một trong những thuật toán cơ bản nhất trong  Unsupervised learning  - thuật toán K-means clustering (phân cụm K-means). Trong thuật toán K-means clustering, chúng ta không biết nhãn (label) của từng điểm dữ liệu. Mục đích là làm thể nào để phân dữ liệu thành các cụm (cluster) khác nhau sao cho dữ liệu trong cùng một cụm có tính chất giống nhau. Ý tưởng đơn giản nhất về cluster (cụm) là tập hợp các điểm ở gần nhau trong một không gian nào đó (không gian này có thể có rất nhiều chiều trong trường hợp thông tin về một điểm dữ liệu là rất lớn). Hình bên dưới là một ví dụ về 3 cụm dữ liệu (từ giờ viết gọn là cluster). Giả sử mỗi cluster có một điểm đại diện (center) màu vàng. Và những điểm xung quanh mỗi center thuộc vào cùng nhóm với center đó. Một cách đơn giản nhất, xét một điểm bất kỳ, ta xét xem điểm đó gần với center nào nhất thì nó thuộc về cùng nhóm với center đó. Tới đây, chúng ta có một bài toán thú vị: Trên một vùng biển hình vuông lớn có ba đảo hình vuông, tam giác, và tròn màu vàng như hình trên. Một điểm trên biển được gọi là thuộc lãnh hải của một đảo nếu nó nằm gần đảo này hơn so với hai đảo kia . Hãy xác định ranh giới lãnh hải của các đảo. Tóm tắt thuật toán: **Đầu vào: ** Dữ liệu XX và số lượng cluster cần tìm KK. Đầu ra:  Các center MM và label vector cho từng điểm dữ liệu YY. Chọn KK điểm bất kỳ làm các center ban đầu. Phân mỗi điểm dữ liệu vào cluster có center gần nó nhất. Nếu việc gán dữ liệu vào từng cluster ở bước 2 không thay đổi so với vòng lặp trước nó thì ta dừng thuật toán. Cập nhật center cho từng cluster bằng cách lấy trung bình cộng của tất các các điểm dữ liệu đã được gán vào cluster đó sau bước 2. Quay lại bước 2. Chúng ta có thể đảm bảo rằng thuật toán sẽ dừng lại sau một số hữu hạn vòng lặp. Thật vậy, vì hàm mất mát là một số dương và sau mỗi bước 2 hoặc 3, giá trị của hàm mất mát bị giảm đi. Theo kiến thức về dãy số trong chương trình cấp 3: nếu một dãy số giảm và bị chặn dưới thì nó hội tụ! Hơn nữa, số lượng cách phân nhóm cho toàn bộ dữ liệu là hữu hạn nên đến một lúc nào đó, hàm mất mát sẽ không thể thay đổi, và chúng ta có thể dừng thuật toán tại đây. Chúng ta sẽ có một vài thảo luận về thuật toán này, về những hạn chế và một số phương pháp khắc phục. Nhưng trước hết, hãy xem nó thể hiện như thế nào trong một ví dụ cụ thể dưới đây. Phân nhóm chữ số viết tay Bộ cơ sở dữ liệu MNIST Bộ cơ sở dữ liệu MNIST là bộ cơ sở dữ liệu lớn nhất về chữ số viết tay và được sử dụng trong hầu hết các thuật toán nhận dạng hình ảnh (Image Classification). MNIST bao gồm hai tập con: tập dữ liệu huấn luyện (training set) có tổng cộng 60k ví dụ khác nhau về chữ số viết tay từ 0 đên 9, tập dữ liệu kiểm tra (test set) có 10k ví dụ khác nhau. Tất cả đều đã được gán nhãn. Hình dưới đây là ví dụ về một số hình ảnh được trích ra từ MNIST. Mỗi bức ảnh là một ảnh đen trắng (có 1 channel), có kích thước 28x28 pixel (tổng cộng 784 pixels). Mỗi pixel mang một giá trị là một số tự nhiên từ 0 đến 255. Các pixel màu đen có giá trị bằng 0, các pixel càng trắng thì có giá trị càng cao (nhưng không quá 255). Dưới đây là một ví dụ về chữ số 7 và giá trị các pixel của nó. (Vì mục đích hiển thị ma trận pixel ở bên phải, tôi đã resize bức ảnh về 14x14) Bài toán phân nhóm giả định Bài toán:  Giả sử rằng chúng ta không biết nhãn của các chữ số này, chúng ta muốn phân nhóm các bức ảnh gần giống nhau về một nhóm. Lại thêm một giả sử nữa là chúng ta mới chỉ biết tới thuật toán phân nhóm  K-means clustering  gần đây, chúng ta sẽ giải quyết bài toán này thế nào? Trước khi áp dụng thuật toán  K-means clustering , chúng ta cần coi mỗi bức ảnh là một điểm dữ liệu. Và vì mỗi điểm dữ liệu là 1 vector (hàng hoặc cột) chứ không phải ma trận như số 7 ở trên, chúng ta phải làm thêm một bước đơn giản trung gian gọi là vectorization (vector hóa). Nghĩa là, để có được 1 vector, ta có thể tách các hàng của ma trận pixel ra, sau đó đặt chúng cạnh nhau, và chúng ta được một vector hàng rất dài biểu diễn 1 bức ảnh chữ số. Chú ý: Cách làm này chỉ là cách đơn giản nhất để mô tả dữ liệu ảnh bằng 1 vector. Trên thực tế, người ta áp dụng rất nhiều kỹ thuật khác nhau để có thể tạo ra các vector đặc trưng (feature vector) giúp các thuật toán có được kết quả tốt hơn. Làm việc trên Python Trước tiên các bạn vào trang chủ của MNIST để  download  bộ cơ sở dữ liệu này. Trong bài này chúng ta chỉ dùng bộ dữ liệu tét với 10k ảnh và không cần label nên các bạn chỉ cần download file  t10k-images-idx3-ubyte.gz Trước tiên chúng ta cần khai báo một số thư viện: numpy  cho các phép toán liên quan đến ma trận.  mnist  để đọc dữ liệu từ MNIST.  matplotlib  để hiển thị hình vẽ.  sklearn  chính là  scikit-learn  mà chúng ta đã làm quen trong các bài trước.  Python\n# %reset\nimport numpy as np \nfrom mnist import MNIST\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n Để hiện thị nhiều bức ảnh các chữ số cùng một lúc, tôi có dùng thêm hàm số display_network.py. Thực hiện thuật toán K-means clustering trên toàn bộ 10k chữ số. \nfrom display_network import *\n\nmndata = MNIST('../MNIST/') # path to your MNIST folder \nmndata.load_testing()\nX = mndata.test_images\n\nkmeans = KMeans(n_clusters=K).fit(X)\npred_label = kmeans.predict(X)\n Đến đây, sau khi đã tìm được các center và phân nhóm dữ liệu vào từng cluster, tôi muốn hiển thị xem center trông như thế nào và các bức ảnh được phân vào mỗi cluster có giống nhau hay không. Dưới đây là kết quả khi tôi chọn ngẫu nhiên 20 bức ảnh từ mỗi cluster. \nÁp dụng K-means clustering vào tập test set của bộ cơ sở dữ liệu MNIST với K = 10 cluster. Cột 1: centers của các cluster. Các cột còn lại: Mỗi hàng là 20 điểm dữ liệu ngẫu nhiên được chọn ra từ mỗi cluster. Mỗi hàng tương ứng với một cluster, cột đầu tiên có nền xanh bên trái là centers tìm được của các clusters (màu đỏ hơn là các pixel có giá trị cao hơn). Chúng ta thấy rằng các center đều hoặc là giống với một chữ số nào đó, hoặc là kết hợp của hai/ba chữ số nào đó. Ví dụ: center của nhóm thứ 4 là sự kết hợp của các số 4, 7, 9; của hàng thứ 7 là kết hợp của chữ số 7, 8 và 9. Tuy nhiên, các bức ảnh lấy ra ngẫu nhiên từ mỗi nhóm trông không thực sự giống nhau. Lý do có thể là những bức ảnh này ở xa các center của mỗi nhóm (mặc dù center đó đã là gần nhất). Như vậy thuật toán K-means clustering làm việc không thực sự tốt trong trường hợp này. (Thật may là vì thế nên chúng ta vẫn còn nhiều thứ để học nữa). Chúng ta vẫn có thể khai thác một số thông tin hữu ích sau khi thực hiện thuật toán này. Bây giờ, thay vì chọn ngẫu nhiên các bức ảnh trong mỗi cluster, tôi chọn 20 bức ảnh gần center của mỗi cluster nhất, vì càng gần center thì độ tin cậy càng cao. Hãy xem hình dưới đây: \nÁp dụng K-means clustering vào tập test set của bộ cơ sở dữ liệu MNIST với K = 10 cluster. Cột 1: centers của các cluster. Các cột còn lại: Mỗi hàng là 20 điểm dữ liệu gần center nhất của mỗi cluster. Bạn có thể thấy dữ liệu trong mỗi hàng khá giống nhau và giống với center ở cột đầu tiên bên trái. Có một vài quan sát thú vị có thể rút ra từ đây: Có hai kiểu viết chữ số 1, một thẳng, một chéo. Và K-means clustering nghĩ rằng đó là hai chữ số khác nhau. Điều này là dễ hiểu vì K-means clustering là thuật toán Unsupervised learning. Nếu có sự can thiệp của con người, chúng ta có thể nhóm hai clusters này vào làm một. Hàng số 9, chữ số 4 và 9 được phân vào cùng 1 cluster. Sự thật là hai chữ số này cũng khá giống nhau. Điều tương tự xảy ra đối với hàng số 7 với các chữ số 7, 8, 9 được xếp vào 1 cluster. Với các cluster này, chúng ta có thể tiếp tục áp dụng K-means clustering để phân nhỏ cluster đó ra. Trong clustering có một kỹ thuật thường được sử dụng là Hierarchical clustering (clustering phân tầng ). Có hai loại Hierachical clustering: Agglomerative tức “đi từ dưới lên”. Ban đầu coi mỗi điểm dữ liệu thuộc 1 cluster khác nhau, sau đó các cặp cluster gần giống nhau được gộp lại làm một cluster lớn hơn. Lặp lại quá trình này đến khi nhận được kết quả chấp nhận được. Divisive tức “đi từ trên xuống”. Ban đầu coi tất cả các điểm dữ liệu thuộc cùng một cluster, sau đó chia nhỏ mỗi cluster bằng một thuật toán clustering nào đó. Cảm ơn các bạn đã đọc bài viết! \nNguồn: Viblo Trả lời Trích dẫn 1 Reply Trả lời cuối cùng  machine-learning 41 Tải thêm các bài gửi khác 1 Số bài viết 633 Lượt xem Trả lời Trả lời dưới dạng chủ đề Hãy đăng nhập để trả lời  \n 1 / 1 × Có vẻ như bạn đã mất kết nối tới LaptrinhX, vui lòng đợi một lúc để chúng tôi thử kết nối lại.",
          "url": "http://laptrinhx.com/topic/11218/ung-dung-thuat-toan-phan-cum-k-means-trong-nhan-dien-chu-so-viet-tay"
        },
        {
          "title": "Nâng cao chất lượng gom cụm kết quả tìm kiếm video sử dụng kết hợp đặc trưng âm thanh, đặc trưng thị giác và thông tin văn bản",
          "relevance": "0",
          "content": "For full functionality of ResearchGate it is necessary to enable JavaScript.\n            Here are the  \n                instructions how to enable JavaScript in your web browser . See all › 15 References Download citation Share Facebook  Twitter  Google+  LinkedIn  Reddit  Download full-text PDF Nâng cao chất lượng gom cụm kết quả tìm kiếm video sử dụng kết hợp đặc trưng âm thanh, đặc trưng thị giác và thông tin văn bản Conference Paper  ·  December 2015   with   66 Reads Conference:  Hội Thảo Quốc Gia 2015 về Điện Tử, Truyền Thông và Công Nghệ Thông Tin (ECIT 2015), At Tp Hồ Chí Minh, Việt Nam Cite this publication Phuc Quang Nguyen University of Information Technology, VNU-HCM Nguyễn Thị Anh Thư +  1 Ngô Đức Thành Tu-Anh Nguyen Hoang 2.11 University of Information Technology, VNU-HCM, Vietnam Show   more authors Abstract Bài báo này đề xuất phương pháp sử dụng kết hợp đặc trưng âm thanh, đặc trưng thị giác và thông tin văn bản để nâng cao chất lượng gom cụm kết quả tìm kiếm video. Mặc dù hướng tiếp cận kết hợp đa đặc trưng đã được giới thiệu trong các lớp bài toán như tìm kiếm video (video retrieval), phân lớp video (video classification) nhưng đóng góp chính của bài báo này là phân tích ưu điểm của từng loại đặc trưng cụ thể làm cơ sở cho việc kết hợp đa đặc trưng và là công trình đầu tiên sử dụng kết hợp đặc trưng âm thanh, đặc trưng thị giác và thông tin văn bản đi kèm video để giải quyết bài toán gom cụm kết quả tìm kiếm video. Các thí nghiệm được tiến hành trên kết quả tìm kiếm video của YouTube với phương pháp kết hợp đề xuất cho kết quả tốt hơn so với việc chỉ áp dụng từng loại đặc trưng riêng lẻ trong quá trình gom cụm video. Discover the world's research 13+ million  members 100+ million  publications 700k+  research projects Join for free 130 Hội Thảo Quốc Gia 2015 về Điện  Tử,  Truyền  Thông và Công Nghệ Thông  Tin (ECIT 2015) Nâng cao c hất lượng gom cụm kết q uả tìm kiếm video sử dụng kết hợp đặc trưng âm thanh, đặc trưng thị giác và thông tin v ăn bản Nguyễn Quang Phúc, Nguy ễn Thị Anh Thư, Ngô Đức Thành, Lê Đình Duy, Nguy ễn Hoàng T ú Anh Phòng Thí nghiệm Truyền thông Đa phương tiện Đại học Công nghệ Thông tin, ĐHQG-HCM Thành phố Hồ Chí Minh, Việt Nam Email: {phucnq,thunta,thanhnd,ldduy,anhnht}@uit.edu.vn T óm tắt —Bài báo này đề xuất phương pháp sử dụng kết hợp đặc trưng âm thanh, đặc trưng thị giác và thông tin văn bản để nâng cao chất lượng gom cụm kết q uả tìm kiếm video. Mặc dù hướng tiếp cận kết hợp đa đặc trưng đã được giới thiệu trong các lớp bài toán như tìm kiếm video (video retrieval), phân lớp video (video classification) nhưng đóng góp chính của bài báo này là phân tíc h ưu điểm của từng loại đặc trưng cụ thể làm cơ sở cho việc kết hợp đa đặc trưng và là công trình đầu tiên sử dụng kết hợp đặc trưng âm thanh, đặc trưng thị giác và thông tin văn bản đi kèm video để giải quy ết bài toán gom cụm kết quả tìm kiếm video. Các thí nghiệm được tiến hành trên kết quả tìm kiếm video của Y ouT ube với phương pháp k ết hợp đề xuất cho kết quả tốt hơn so v ới việc chỉ áp dụng từng loại đặc trưng riêng lẻ trong quá trình gom cụm video. T ừ khóa —gom cụm video, đặc trưng âm thanh, đặc trưng thị giác, độ tương tự kết hợp. I. GIỚI THIỆU Ngày na y, v ới sự phát triển mạnh mẽ của công nghệ truyền thông và kỹ thuật số cùng v ới sự bùng nổ của mạng Internet, số lượng video được chia sẻ trên Web ngày càng nhiều. Để tìm kiếm video trên W eb, người dùng phải cung cấp từ khóa tìm kiếm trên các công cụ tìm kiếm video (ví dụ như Y ouT ube, Google Video). Kết quả tìm kiếm được trình bày như một danh sách phẳng với các video được xếp theo độ liên q uan với từ khóa truy vấn. Để tìm được video mong muốn, người dùng phải “tốn công” duyệt qua toàn bộ danh sách. Hơn nữa, các kết quả tìm kiếm là rất đa đạng và thường bị phân mảnh hoặc bị chi phối bởi các video không phù hợp (đặc biệt trong những trường hợp như người dùng gửi truy vấn quá ngắn hoặc truy vấn mơ hồ do tính đa nghĩa của từ khóa truy vấn). Giả định người dùng đang quan tâm tới một vấn đề cụ thể nhưng không đưa ra được từ khóa phù hợp. Do đó, kết quả tìm kiếm video trả về có thể thuộc nhiều thể loại, chủ đề khác nhau và gâ y khó khăn trong việc tìm kiếm. Trường hợp xấu hơn xả y ra khi kết quả của các chủ đề khác áp đảo chủ đề mà người dùng quan tâm. Trong kịch bản như v ậy , việc gom cụm kết quả tìm kiếm video là cần thiết nhằm giúp người dùng dễ dàng xác định video cần tìm. Nói cách khác, thay vì phải duy ệt qua một danh sách phẳng kết quả tìm kiếm gồm nhiều video thuộc nhiều chủ đề trộn lẫn với nhau thì người dùng được cung cấp một cái nhìn trực quan hơn thông qua kết quả gom cụm video theo từng chủ đề cụ thể. Qua đó, người dùng có thể dễ dàng xác định được video mà họ quan tâm một cách nhanh chóng và bỏ q ua các cụm video không thích hợp. T óm lại, với một danh sách video trả v ề từ kết quả tìm kiếm của một truy vấn bất kỳ trên các kênh video trực tuyến, bài toán gom cụm kết quả tìm kiếm video là xác định các video có nội dung tương tự nhau và gom chúng lại trong cùng một cụm. Dữ liệu đầu vào và đầu ra của bài toán được minh họa trực quan ở Hình 1. Đầu vào là danh sách video trả v ề từ kết quả tìm kiếm video trên Web, đầu ra là các cụm video. Gom cụm kết quả tìm kiếm trên W eb được nghiên cứu rộng rãi trước đây . Các công trình chủ yếu tập trung vào dữ liệu văn bản (phổ biến là gom cụm trang W eb) [6], [8], [9] và dữ liệu hình ảnh [3], [5], [11]. Gần đây , có một số công trình nghiên cứu gom cụm kết quả tìm kiếm video [1], [7], [12]. So với dữ liệu dạng văn bản ha y hình ảnh thì dữ liệu video có cấu trúc phức tạp hơn. Nội dung của video chứa đựng đồng thời các đặc trưng về âm thanh (audio), thị giác (visual) hay văn bản (te xtual). Điều này đặt ra nhiều thách thức trong việc biểu diễn và so khớp video. Trong [12], tác giả biểu diễn video dựa trên đặc trưng thị giác. Cụ thể, mỗi frame được biểu diễn thành một véc tơ đặc trưng trong không gian màu HSV (Hue Saturation V alue). Sau đó, video được biểu diễn bởi một véc tơ đặc trưng được tính bằng cách lấ y 130 Hội Thảo Quốc Gia 2015 về Điện Tử, Truyền Thông và Công Nghệ Thông Tin (ECIT 2015) ISBN: 978-604-67-0635-9 131 Hội Thảo Quốc Gia 2015 về Điện  Tử,  Truyền  Thông và Công Nghệ Thông  Tin (ECIT 2015) Hình 1. Minh họa trực quan dữ liệu đầu vào và đầu ra cho bài toán gom cụm kết quả tìm kiếm video. trung bình tất cả các véc tơ biểu diễn cho các frame của video. Độ tương đồng giữa các video được quy về việc tính khoảng cách giữa các véc tơ biểu diễn chúng. V ới hướng tiếp cận này thì tính ngữ nghĩa trong thông tin văn bản đi kèm video (ví dụ như tiêu đề (title), mô tả (description), các thẻ từ khóa (tags)) không được xem xét. T rong [1], [7], các tác giả đã khai thác các thông tin được trích xuất từ đặc trưng thị giác và thông tin văn bản đi kèm video nhằm cải thiện chất lượng gom cụm video. T uy nhiên, các phương pháp rút trích đặc trưng biểu diễn video được sử dụng vẫn còn khá đơn giản và hiệu quả của từng loại đặc trưng trong quá trình gom cụm video chưa được phân tích rõ ràng. Trong bài báo nà y, chúng tôi tập trung v ào việc phân tích ưu điểm của từng loại đặc trưng cụ thể làm cơ sở cho việc kết hợp đa đặc trưng. T ừ đó, đề xuất phương pháp kết hợp đặc trưng âm thanh, đặc trưng thị giác và thông tin văn bản đi kèm video nhằm nâng cao chất lượng gom cụm video. Các mục tiếp theo của bài báo được tổ chức như sau: mục II giới thiệu phương pháp kết hợp đặc trưng đề xuất, mục III trình bày các thực nghiệm, mục IV thảo luận về hướng phát triển. II. PHƯƠNG PHÁP ĐỀ XU ẤT A. Mô hình kết hợp đặc trưng âm thanh, đặc trưng thị giác và thông tin văn bản Theo quan sát trực quan, các video có nội dung tương tự nhau thường có thể hiện thị giác (sự xuất hiện của các đối tượng, hình ảnh) giống nhau. Vì vậy , việc sử dụng đặc trưng thị giác để gom cụm video sẽ trở nên hiệu quả. T uy nhiên, với sự đa dạng của dữ liệu video trên Web, những video thuộc cùng một c hủ đề có thể có những đối tượng và hình ảnh khác nhau. Khi đó, việc khai thác nội dung ngữ nghĩa được trích xuất từ thông tin văn bản đi kèm video có thể giúp gom các video tương đồng ngữ nghĩa về cùng một cụm. Do đó, đặc trưng thị giác và thông tin văn bản đi kèm video sẽ hỗ trợ, bổ sung cho nhau để biểu diễn video một cách hiệu quả giúp nâng cao chất lượng gom cụm video. T uy nhiên, việc tận dụng nội dung ngữ nghĩa của thông tin văn bản đi kèm video sẽ thực sự hiệu quả khi chúng được mô tả đúng với nội dung thực sự của video. Dữ liệu video trên các kênh video trực tuyến thường được tải lên bởi nhiều người dùng, các thông tin văn bản đi kèm video cũng được người dùng khai báo. Trong thực tế, vì những mục đích riêng (ví dụ như thu hút lượt xem) hoặc do cảm nhận chủ quan, người dùng có thể mô tả các thông tin văn bản đi kèm không đúng v ới nội dung thực sự của video. Trong những trường hợp tương tự như vậ y , chúng tôi tin rằng việc khai thác kết hợp đặc trưng âm thanh được trích xuất trực tiếp từ nội dung video (ví dụ như những video về ca nhạc thường có các âm thanh như tiếng reo hò, tiếng vỗ tay ; những video đua xe thì âm thanh đi kèm là tiếng động cơ xe, ...) sẽ g óp phần cải thiện chất lượng gom cụm video. Để làm rõ những phân tích trên, một ví dụ minh họa được thể hiện ở Hình 2. Trong ví dụ nà y, cả bốn video đều giới thiệu về “xe hơi” nên sẽ được gom v ào cùng một cụm. Video 1 và video 3 có thể hiện thị giác tương đối giống nhau nên việc khai thác đặc trưng thị giác sẽ giúp gom 2 video này với nhau. T uy nhiên, video 2 v à video 4 có thể hiện thị giác khác so với video 1 và video 3. Khi đó, việc tận dụng thông tin văn bản đi kèm video cùng với đặc trưng âm thanh được trích xuất từ nội dung video (như tiếng động cơ xe) sẽ giúp gom video 2 và video 4 vào chung cụm v ới video 1 và video 3. T ừ những phân tích trên, chúng tôi đề xuất mô hình kết hợp đặc trưng âm thanh, đặc trưng thị giác và thông Hình 2. Minh họa cụm bốn video thuộc chủ đề “xe hơi” từ danh sách kết quả tìm kiếm video của truy vấn “ Aston”. 131 Hội Thảo Quốc Gia 2015 về Điện Tử, Truyền Thông và Công Nghệ Thông Tin (ECIT 2015) 132 Hội Thảo Quốc Gia 2015 về Điện  Tử,  Truyền  Thông và Công Nghệ Thông  Tin (ECIT 2015) Hình 3. Mô hình kết hợp đặc trưng âm thanh, đặc trưng thị giác và thông tin văn bản giải quyết bài toán gom cụm kết q uả tìm kiếm video. tin văn bản đi kèm video nhằm nâng cao chất lượng gom cụm video (xem Hình 3). B. T rích xuất đặc trưng, biểu diễn và so khớp video 1) Đặc trưng âm thanh (Audio): Như đã phân tích ở trên, đặc trưng âm thanh đóng một vai trò quan trọng trong quá trình gom cụm video. Trong bài báo này , chúng tôi sử dụng MFCC (Mel-Frequency Cepstral Coefficients) [13] như là một loại đặc trưng âm thanh được trích xuất từ video. Mượn ý tưởng từ mô hình BoW (Bag-of-W ords) trong biểu diễn dữ liệu văn bản, sau khi đặc trưng âm thanh (biểu diễn dạng tập các véc tơ) được trích xuất từ tập dữ liệu video, quá trình gom cụm các đặc trưng tạo từ điển được tiến hành. Cuối cùng, mỗi video sẽ được biểu diễn bởi một véc tơ đặc trưng với số chiều tương ứng với số từ trong từ điển. Độ tương tự giữa các video được tính là khoảng cách giữa các véc tơ đại diện chúng. Quá trình tính độ tương tự video dựa trên đặc trưng âm thanh được thể hiện ở Hình 4. Hình 4. Minh họa quá trình tính độ tương tự video dựa trên đặc trưng âm thanh (MFCC) được biểu diễn theo mô hình BoW . 2) Đặc trưng thị giác (Visual): Để tăng độ c hính xác so khớp video thì một trong những yêu cầu quan trọng là các điểm đặc trưng cục bộ (local keypoint f eatures) được rút trích từ các frame phải bất biến với những biến đổi về độ sáng, tỉ lệ co giãn, phép x oay , .... Một trong những phương pháp rút trích và mô tả các đặc trưng cục bộ đáp ứng yêu cầu trên được sử dụng phổ biến nhất hiện nay là Scale-Inv ariant Feature Transf orm (SIFT) [4] Hình 5. Minh họa quá trình tính độ tương tự video dựa trên đặc trưng thị giác (SIFT) được biểu diễn theo mô hình BoW . bao gồm các bước chính là phát hiện và mô tả các điểm đặc trưng. Các điểm đặc trưng sẽ được phát hiện và mô tả trên từng frame của mỗi video. Để phát hiện các điểm đặc trưng, chúng tôi sử dụng bộ phát hiện đặc trưng phổ biến Hessian-Affine [10]. V ới mỗi đặc trưng, một véc tơ 128 chiều được tạo ra từ bộ mô tả SIFT . Như vậy , mỗi frame của video sẽ được biểu diễn bao gồm một tập các véc tơ đặc trưng 128 chiều. Video được biểu diễn bằng tập hợp tập các véc tơ đặc trưng biểu diễn cho từng frame. Tương tự như quá trình biểu diễn video với đặc trưng âm thanh, chúng tôi cũng sử dụng mô hình BoW để biểu diễn và tính độ tương tự video theo đặc trưng thị giác. Quá trình tính độ tương tự video dựa trên đặc trưng thị giác được thể hiện ở Hình 5. 3) Thông tin văn bản (T extual): Thông tin văn bản đi kèm video (ví dụ như tiêu đề (title), mô tả (description), các thẻ từ khóa (tags)) góp phần quan trọng thể hiện nội dung ngữ nghĩa video giúp cải thiện chất lượng gom cụm video. T uy nhiên, vấn đề đặt ra là thông tin văn bản có ý nghĩa tương tự nhau nhưng có thể được diễn đạt với nhiều từ ngữ khác nhau (điều này chủ y ếu là do tính linh hoạt vốn có của ngôn ngữ tự nhiên cho phép người dùng thể hiện cùng một nội dung nhưng với các ngôn từ khác nhau). Trong bài báo nà y, chúng tôi đề xuất sử dụng từ điển W ordNet [2] để tính độ tương tự ngữ nghĩa giữa các từ thể hiện trong thông tin văn bản đi kèm video. Sau khi nghiên cứu rộng rãi một số phương pháp, chúng tôi đề xuất sử dụng phương pháp của Li để tính độ tương tự ngữ nghĩa giữa các từ, phương pháp này có sự tương quan tốt nhất với sự đánh giá của con người v ề mức độ tương tự ngữ nghĩa giữa các từ như được trình bày trong báo cáo [15]. Độ tương tự giữa các video dựa trên thông tin văn bản đi kèm sử dụng từ điển W ordNet được thể hiện ở Hình 6. 132 Hội Thảo Quốc Gia 2015 về Điện Tử, Truyền Thông và Công Nghệ Thông Tin (ECIT 2015) 133 Hội Thảo Quốc Gia 2015 về Điện  Tử,  Truyền  Thông và Công Nghệ Thông  Tin (ECIT 2015) Hình 6. Minh họa quá trình tính độ tương tự video dựa trên thông tin văn bản đi kèm sử dụng từ điển W ordNet. C. Gom cụm video Quá trình gom cụm video dựa trên sự kết hợp đặc trưng âm thanh, đặc trưng thị giác và thông tin văn bản được thực hiện qua 3 bước sau: Bước 1.  Độ tương tự giữa các video theo từng đặc trưng cụ thể sẽ được tính theo các phương pháp được trình bày ở mục trước đó. Bước 2.  Với hai video bất kỳ  X và  Y , độ tương tự kết hợp đa đặc trưng được tính theo công thức sau: Sim  ( X, Y )=   mỗi đặc trưng i w i ∗ Sim i ( X, Y ) (1) trong đó,  Sim  ( X, Y ) là độ tương tự kết hợp đa đặc trưng giữa hai video  X và  Y , Sim i ( X, Y ) là độ tương tự giữa hai video  X và  Y theo đặc trưng  i , w i là trọng số của đặc trưng  i . Bước 3.  Áp dụng thuật toán gom cụm dữ liệu để thực hiện gom cụm video dựa trên độ tương tự kết hợp đa đặc trưng. III. THỰC NGHIỆM A. Bộ dữ liệu video Chúng tôi sử dụng phần mềm mã nguồn mở T ubeKit 1 để tải dữ liệu video thực từ Y ouT ube thông qua Y ouT ube API. Chúng tôi tải về khoảng 80 đến 100 video (thời lượng mỗi video từ 2 đến 10 phút) cho mỗi truy vấn và thực hiện loại bỏ một số video biệt lập, ít liên quan đến truy vấn tìm kiếm. Sự loại bỏ này là hợp lý bởi vì chúng tôi đang thử nghiệm tính năng hậu xử lý gom cụm kết quả tìm kiếm video chứ không phải là tìm kiếm chính xác của một công cụ tìm kiếm video. Thí nghiệm được tiến hành trên bộ dữ liệu gồm 884 video của 10 truy vấn với các từ khóa khác nhau. Thông tin chi tiết về bộ dữ liệu video được mô tả ở Bảng I. 1 www.tubekit.org Bảng I BỘ DỮ LIỆU VIDEO THỬ NGHIỆM Truy vấn Số video Số chủ đề 1. Aston 82 4 2. Cobra 92 5 3. Jaguar 86 4 4. Leopard 95 5 5. Lion 89 4 6. Lotus 91 6 7. Mustang 83 5 8. Scorpion 90 6 9. Venus 89 7 10. Viper 87 5 B. Phương pháp đánh giá Để đánh giá chất lượng gom cụm video. Chúng tôi sử dụng 2 độ đo phổ biến là Entropy và Purity [14]. Entropy của mỗi cụm phản ánh sự phân tán video thuộc các chủ đề trong mỗi cụm, giá trị Entropy đánh giá chất lượng gom cụm tổng thể được tính là trung bình cộng của tất cả các Entropy của các cụm. V ới tập dữ liệu gồm  n video thuộc  k loại (chủ đề) được gán nhãn thủ công, ký hiệu là  C j , j =1 , ..., k và thuật toán gom cụm  n video vào  k cụm  P i với  i =1 , ..., k . Entropy đánh giá chất lượng gom cụm toàn cục cho tất cả các cụm được tính toán theo công thức sau: Entr opy  = −  i n i n  j n ij n i log  n ij n i (2) trong đó  n i là số video trong cụm  P i , n ij  là số video trong cụm  P i thuộc chủ đề  C j và  n là tổng số video trong tất cả các cụm. Kết quả gom cụm là hoàn hảo nếu mỗi cụm chỉ chứa video thuộc cùng một chủ đề duy nhất. Khi đó, giá trị Entropy sẽ bằng không. Nói một cách tổng q uát, giá trị Entropy càng nhỏ thì cho chất lượng gom cụm tốt hơn. Purity phản ánh độ tinh khiết của các cụm. Purity của một cụm được xác định dựa trên số video thuộc chủ đề mà xuất hiện nhiều nhất trong cụm đó. Purity đánh giá chất lượng gom cụm toàn cục cho tất cả các cụm được tính toán theo công thức sau với các ký hiệu có ý nghĩa tương tự như trong công thức tính Entropy: P urity =  i n i n ( max j n ij n i ) (3) Ngược lại với Entropy , giá trị Purity càng lớn thì cho kết quả gom cụm tốt hơn. C. Cài đặt thực nghiệm Nhằm mục đích so sánh, đánh giá kết quả gom cụm video với phương pháp đề xuất, chúng tôi tiến hành cài 133 Hội Thảo Quốc Gia 2015 về Điện Tử, Truyền Thông và Công Nghệ Thông Tin (ECIT 2015) 134 Hội Thảo Quốc Gia 2015 về Điện  Tử,  Truyền  Thông và Công Nghệ Thông  Tin (ECIT 2015) đặt các thí nghiệm sau: • A (Audio): Gom cụm video dựa trên độ tương tự theo đặc trưng âm thanh. • V (Visual): Gom cụm video dựa trên độ tương tự theo đặc trưng thị giác. • T (T extual): Gom cụm video dựa trên độ tương tự theo thông tin văn bản đi kèm video. • A- V -T (A udio-V isual- Te xtual) (hướng tiếp cận của chúng tôi): Gom cụm video dựa trên độ tương tự kết hợp đặc trưng âm thanh, đặc trưng thị giác và theo thông tin văn bản đi kèm video. Để xem xét sự tương q uan giữa các đặc trưng trong mô hình kết hợp, chúng tôi thử nghiệm phương pháp kết hợp đặc trưng đề xuất với các bộ trọng số khác nhau ứng với từng đặc trưng được thể hiện trong Bảng II. Bảng II TRỌNG SỐ KẾT HỢP CÁ C ĐẶC TRƯNG Trọng số Âm thanh (Audio) Thị giác (Visual)  V ăn bản (Te xtual)  Tổng tsA 0.60 0.30 0.10 1.00 tsB 0.35 0.50 0.15 1.00 tsC 0.20 0.60 0.20 1.00 Để gom cụm video, chúng tôi sử dụng thuật toán K - Medoids (một thuật toán gom cụm phổ biến) vì hai lý do sau: (i) trọng tâm của cụm là một đối tượng cụ thể (tâm thật), (ii) độ tương tự giữa các đối tượng chỉ cần tính một lần (điều này là phù hợp với đầu v ào là độ đo tương tự kết hợp đa đặc trưng giữa các video được xử lý tính toán trước đó). Đối với bài toán gom cụm tổng quát thì số cụm được khai báo linh động bởi người dùng. Số cụm càng ít thì tỷ lệ các đối tượng khác nhau được gom về cùng một cụm càng cao, số cụm càng nhiều thì tỷ lệ các đối tượng giống nhau được gom vào các cụm khác nhau càng lớn. Trong bài báo nà y, để công bằng trong việc đánh giá giữa các phương pháp thực nghiệm, chúng tôi tiến hành thử nghiệm thuật toán gom cụm với số cụm đầu vào tương ứng với số chủ đề của mỗi truy vấn. D. Kết quả thực nghiệm Kết quả gom cụm video ứng với các truy v ấn khác nhau đánh giá theo Entropy và Purity được thể hiện ở Hình 7 và Hình 8. Kết quả thể hiện ở Hình 7 cho thấ y trên hầu hết các bộ dữ liệu video của các truy vấn, phương pháp sử dụng đặc trưng âm thanh (A) và đặc trưng thị giác (V) cho kết quả gom cụm video tốt hơn (đạt giá trị Entropy thấp hơn) so với phương pháp sử dụng thông tin văn bản đi kèm (T). Điều này cho thấ y xu hướng những video Hình 7. Kết quả gom cụm video được đánh giá theo Entropy . tương tự nhau (thuộc cùng chủ đề) thường có những đối tượng hình ảnh cụ thể và âm thanh tương tự nhau. T uy nhiên, kết quả thí nghiệm cũng cho thấ y rằng mỗi đặc trưng đều có ưu thế riêng đối với mỗi bộ dữ liệu video của từng truy vấn. Phương pháp kết hợp đa đặc trưng của chúng tôi (A- V -T) được thử nghiệm v ới các bộ trọng số khác nhau cho mỗi đặc trưng đều cho kết quả gom cụm video tốt hơn so với các phương pháp sử dụng từng đặc trưng riêng lẻ (A), (V), (T). Điều này chứng minh tính hiệu quả của việc kết hợp đặc trưng âm thanh, đặc trưng thị giác và thông tin văn bản đi kèm video trong quá trình gom cụm video. Phương pháp A- V- T (tsA), A- V- T (tsB) cho k ết quả gom cụm video tốt nhất (đạt giá trị Entropy thấp nhất chứng minh xác suất phân bố các video thuộc cùng một chủ đề vào các cụm khác nhau là thấp nhất). Điều này cho thấ y rằng trong chiến lược kết hợp đa đặc trưng giải quyết bài toán gom cụm kết q uả tìm kiếm video thì đặc trưng âm thanh và đặc trưng thị giác chiếm ưu thế hơn so với thông tin v ăn bản đi kèm video. Kết quả thể hiện ở Hình 8 cho thấ y phương pháp A- V- T (tsA), A- V -T (tsB) cũng cho k ết quả gom cụm video tốt nhất (đạt giá trị Purity cao nhất chứng minh tỉ lệ phân bố những video thuộc cùng một chủ đề vào cùng một cụm là cao nhất). Minh họa kết quả trực quan gom cụm video được thể hiện ở Hình 9. Kết quả gom cụm video thể hiện ở Hình 9 bao gồm 6 cụm video liên quan đến truy vấn “Scorpion”. Cụm 1 bao gồm những video ca nhạc thể hiện bởi ban nhạc Scorpions. Cụm 2 bao gồm những video liên quan đến động vật (con bọ cạp). Cụm 3 bao gồm những video game. Cụm 4 gồm những video giới thiệu về mũ bảo hiểm thương hiệu Scorpion. Cụm 5 gồm những video tập Y oga (Scorpion Pose). Cụm 6 gồm những video liên 134 Hội Thảo Quốc Gia 2015 về Điện Tử, Truyền Thông và Công Nghệ Thông Tin (ECIT 2015) 135 Hội Thảo Quốc Gia 2015 về Điện  Tử,  Truyền  Thông và Công Nghệ Thông  Tin (ECIT 2015) Hình 8. Kết quả gom cụm video được đánh giá theo Purity. Hình 9. Minh họa trực quan một phần kết quả gom cụm video với truy vấn “Scorpion”. quan đến một loại xe chuy ên dụng thu hoạch gỗ thông (Ponsse Scorpion). T ừ kết quả trực quan gom cụm video, chúng tôi quan sát thấy rằng đa số các video thuộc cùng chủ đề đều được gom trong cùng một cụm. Thông qua kết quả gom cụm video, người dùng có thể xác định được những video mà họ quan tâm một cách dễ dàng hơn thay vì phải duyệt q ua một danh sách phẳng các kết quả tìm kiếm như trước đây . IV . KẾT LUẬN V À HƯỚN G PHÁT TRIỂN Trong bài báo nà y, chúng tôi đề xuất phương pháp k ết hợp đặc trưng âm thanh, đặc trưng thị giác được trích xuất trực tiếp từ nội dung video cùng với các thông tin văn bản đi kèm video dựa trên những phân tích v ề ưu điểm của từng loại đặc trưng. Kết quả thí nghiệm cho thấy rằng phương pháp kết hợp đề xuất giúp cải thiện chất lượng gom cụm video so với các phương pháp sử dụng từng đặc trưng riêng lẻ. Hướng phát triển tiếp theo là có thể khai thác thêm các thông tin được trích xuất từ đặc trưng chuyển động (motion features) của video. Thử nghiệm và đánh giá kết quả gom cụm video dựa trên việc kết hợp các bộ đặc trưng khác nhau nhằm xây dựng bộ đặc trưng phù hợp cho bài toán gom cụm kết quả tìm kiếm video. LỜI CẢM ƠN Nghiên cứu được tài trợ bởi Đại học Quốc gia Thành phố Hồ Chí Minh (ĐHQG-HCM) trong khuôn khổ Đề tài mã số C2015-26-02. TÀI LIỆU THAM KHẢ O [1] A. Hindle, J. Shao, D. Lin, J. Lu and R. Zhang, “Clustering Web Video Search Results Based on Integration of Multiple Features,” In WWW, pp. 53-73, 2011. [2] C. Fellbaum, ed., “WordNet: An electronic lexical database, ” Language, Speech, and Communication. MIT Press, Cambridge, USA, 1998. [3] D. Cai, X. He, Z. Li, W.Y . Ma, J.R. W en, “Hierarchical clustering of www image search results using visual, textual and link information,” In A CM Multimedia, pp. 952-959, 2004. [4] D. G. Lowe, “Distinctiv e Image Features from Scale-Inv ariant Keypoints,” International Journal of Computer Vision, 60, 2, pp. 91-110, 2004. [5] F. Jing, C. W ang, Y . Y ao, K. Deng, L. Zhang, W.Y . Ma, “Igroup: web image search results clustering,” In A CM Multimedia, pp. 377-384, 2006. [6] G. Mecca, S. Raunich, A. Pappalardo, “A new algorithm for clustering search results,” Data Know l, Eng.62(3), pp. 504-522, 2007. [7] H. Huang, Y . Lu, F. Zhang, and S. Sun, “ A multi-modal clustering method for web videos, ” In Trustworthy Computing and Services, pp. 163-169, 2013. [8] H. Zeng, Q. He, Z. Chen, W . Ma, and J. Ma, “Learning to cluster web search results,” In Proceedings of A CM SIGIR ’04, 2004. [9] J. Park, X. Gao, and P. Andreae, “Query directed web page clustering using suffix tree and wikipedia links,” In Advanced Data Mining and Applications, pp. 91-99, 2012. [10] K. Mikolajczyk, T. T uytelaars, C. Schmid, A. Zisserman, J. Matas, F. Schaff alitzky, T . Kadir, and L. V an Gool, “ A comparison of affine region detectors,” International journal of computer vision, vol. 65, no. 1-2, pp. 43-72, 2005. [11] M. Rege, M. Dong, and J. Hua, “Clustering W eb Images with Multi-modal Features,” In Proceedings of the 15th International Conference on Multimedia, pp. 317-320, 2007. [12] S. Liu, M. Zhu, Q. Zheng, “Mining similarities for clustering web video clips,” In CSSE (4), pp. 759-762, 2008. [13] U. Srinivasan, S. Pfeiff er, S. Nepal, M. Lee, L. Gu, S. Barrass, “A Surv ey of Mpeg-1 Audio, Video and Semantic Analy sis Tec hniques,” Multimedia T ools and Applications, 27(1), pp. 105- 141, 2005. [14] Y . Zhao, G. Karypis, “Criterion functions for document clus- tering: experiments and analysis,” T echnical Report TR01-40, Department of Computer Science, University of Minnesota, 2001. [15] Y .H. Li, Z. Bandar and D. McLean, “An approach f or measuring semantic similarity using multiple information sources,” IEEE Transactions on Know ledge and Data Engineering, vol. 15, no. 4, pp. 871-882, 2003. 135 Hội Thảo Quốc Gia 2015 về Điện Tử, Truyền Thông và Công Nghệ Thông Tin (ECIT 2015) Citations Citations 0 References References 15 Query Directed Web Page Clustering Using Suffix Tree and Wikipedia Links [Show abstract]  [Hide abstract]  ABSTRACT: Recent research on Web page clustering has shown that the user query plays a critical role in guiding the categorisation of web search results. This paper combines our Query Directed Clustering algorithm (QDC) with another existing algorithm, Suffix Tree Clustering (STC), to identify common phrases shared by documents for base cluster identification. One main contribution is the utilising of a new Wikipedia link based measure to estimate the semantic relatedness between query and the base cluster labels, which has shown great promise in identifying the good base clusters. Our experimental results show that the performance is improved by utilising suffix trees and Wikipedia links. Chapter  · Dec 2012  · Data & Knowledge Engineering  John Park Xiaoying Gao Peter Andreae Read A Multi-modal Clustering Method for Web Videos [Show abstract]  [Hide abstract]  ABSTRACT: The prevalence of video sharing websites brings the explosion of web videos and poses a tough challenge to the web video clustering for their indexing. This paper proposes a flexible multi-modal clustering method for web videos. This method achieves web video representation and similarity measurement by integrating the extracted visual features, semantic features and text features of videos to describe a web video more accurately. With the multi-modal combined similarity as input, the affinity propagation algorithm is employed for the clustering procedure. The clustering method is evaluated by experiments conducted on web video dataset and has a better performance than existing methods. Article  · Jan 2013  Haiqi Huang Yueming Lu Fangwei Zhang Songlin Sun Songlin Sun Read WordNet: An electronic lexical database (Language, Speech, and Communication). Cambridge, MA: The MIT Press [Show abstract]  [Hide abstract]  ABSTRACT: with a preface by George Miller \nWordNet, an electronic lexical database, is considered to be the most important resource available to researchers in computational linguistics, text analysis, and many related areas. Its design is inspired by current psycholinguistic and computational theories of human lexical memory. English nouns, verbs, adjectives, and adverbs are organized into synonym sets, each representing one underlying lexicalized concept. Different relations link the synonym sets. \nThe purpose of this volume is twofold. First, it discusses the design of WordNet and the theoretical motivations behind it. Second, it provides a survey of representative applications, including word sense identification, information retrieval, selectional preferences of verbs, and lexical chains. \nContributors: Reem Al-Halimi, Robert C. Berwick, J. F. M. Burg, Martin Chodorow, Christiane Fellbaum, Joachim Grabowski, Sanda Harabagiu, Marti A. Hearst, Graeme Hirst, Douglas A. Jones, Rick Kazman, Karen T. Kohl, Shari Landes, Claudia Leacock, George A. Miller, Katherine J. Miller, Dan Moldovan, Naoyuki Nomura, Uta Priss, Philip Resnik, David St-Onge, Randee Tengi, Reind P. van de Riet, Ellen Voorhees. Book  · May 1998  · Data & Knowledge Engineering  Christiane Fellbaum Read A Survey of MPEG1 Audio, Video and Semantic Analysis Techniques [Show abstract]  [Hide abstract]  ABSTRACT: Digital audio & video data have become an integral part of multimedia information systems. To reduce storage and bandwidth requirements, they are commonly stored in a compressed format, such as MPEG-1. Increasing amounts of MPEG encoded audio and video documents are available online and in proprietary collections. In order to effectively utilise them, we need tools and techniques to automatically analyse, segment, and classify MPEG video content. Several techniques have been developed both in the audio and visual domain to analyse videos. This paper presents a survey of audio and visual analysis techniques on MPEG-1 encoded media that are useful in supporting a variety of video applications. Although audio and visual feature analyses have been carried out extensively, they become useful to applications only when they convey a semantic meaning of the video content. Therefore, we also present a survey of works that provide semantic analysis on MPEG-1 encoded videos. Full-text  ·  Article  · Sep 2005  Uma Srinivasan Silvia Pfeiffer Surya Nepal + 1  more author ... Michael Lee Read  full-text A new algorithm for clustering search results [Show abstract]  [Hide abstract]  ABSTRACT: We develop a new algorithm for clustering search results. Differently from many other clustering systems that have been recently proposed as a post-processing step for Web search engines, our system is not based on phrase analysis inside snippets, but instead uses latent semantic indexing on the whole document content. A main contribution of the paper is a novel strategy – called dynamic SVD clustering – to discover the optimal number of singular values to be used for clustering purposes. Moreover, the algorithm is such that the SVD computation step has in practice good performance, which makes it feasible to perform clustering when term vectors are available. We show that the algorithm has very good classification performance, and that it can be effectively used to cluster results of a search engine to make them easier to browse by users. The algorithm has being integrated into the Noodles search engine, a tool for searching and clustering Web and desktop documents. Article  · Sep 2007  Giansalvatore Mecca Salvatore Raunich Alessandro Pappalardo Read Hierarchical clustering of WWW image search results using visual, textual and link analysis [Show abstract]  [Hide abstract]  ABSTRACT: We consider the problem of clustering Web image search results. Generally, the image search results returned by an image search engine contain multiple topics. Organizing the results into different semantic clusters facilitates users' browsing. In this paper, we propose a hierarchical clustering method using visual, textual and link analysis. By using a vision-based page segmentation algorithm, a web page is partitioned into blocks, and the textual and link information of an image can be accurately extracted from the block containing that image. By using block-level link analysis techniques, an image graph can be constructed. We then apply spectral techniques to find a Euclidean embedding of the images which respects the graph structure. Thus for each image, we have three kinds of representations, i.e. visual feature based representation, textual feature based representation and graph based representation. Using spectral clustering techniques, we can cluster the search results into different semantic clusters. An image search example illustrates the potential of these techniques. Full-text  ·  Conference Paper  · Jan 2004  · Data & Knowledge Engineering  Deng Cai Xiaofei He Zhiwei Li + 1  more author ... Wei-Ying Ma Read  full-text Show more Recommended publications Conference Paper Gom cụm dữ liệu web video dựa trên hướng tiếp cận early fusion cho đặc trưng văn bản June 2014 Gom cụm kết quả tìm kiếm video là bài toán nhằm giúp người dùng xác định được các video clip mà họ quan tâm một cách hiệu quả hơn. Các nghiên cứu gom cụm video trước đó chủ yếu tập trung khai thác độ tương tự đặc trưng hình ảnh của dữ liệu video cho độ chính xác không cao. Tuy nhiên, bên cạnh đặc trưng hình ảnh thì đặc trưng văn bản cũng mang nhiều thông tin ngữ nghĩa giúp tăng độ chính xác... [Show full abstract] Read more Conference Paper Using Textual Semantic Similarity to Improve Clustering Quality of Web Video Search Results October 2015 Clustering Web video search results is to help users locating videos of interest in more effective manner. To cluster returned videos, existing works proposed to use textual and visual similarity of videos. However, one of their limitations is that semantic similarity of textual metadata was not considered. Meanwhile, metadata of videos are usually annotated by users with words of high... [Show full abstract] Read more Conference Paper Clustering web video search results with convolutional neural networks September 2016 Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems giving state-of-the-art results on recognition, detection, segmentation, classification and retrieval. Encouraged by these results, we develop our previous work [14] by implementing deep neural network architecture for extracting and representing visual features to improve... [Show full abstract] Read more Data PhucNQ-NICS-2016 November 2016 Read more Discover more Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. This publication is from a journal that may support self archiving. Learn more © 2008- 2017  ResearchGate GmbH. All rights reserved. About us  ·  Help Center  ·  Careers  ·  Developers  ·  News  ·  Contact us  ·  Privacy  ·  Terms  ·  Copyright  |  Advertising  ·  Recruiting or Discover by subject area Join for free Log in  ResearchGate is the professional network for scientists and researchers. Join for free",
          "url": "https://www.researchgate.net/publication/305984223_Nang_cao_chat_luong_gom_cum_ket_qua_tim_kiem_video_su_dung_ket_hop_dac_trung_am_thanh_dac_trung_thi_giac_va_thong_tin_van_ban"
        },
        {
          "title": "Unsupervised Machine Learning 1: Các thuật toán phân cụm",
          "relevance": "1",
          "content": "Giới thiệu về các thuật toán phân cụm Các thuật toán phân cụm được xếp vào nhóm phương pháp học không giám sát ( Unsupervided Learning ) vì mục tiêu chủ yếu của nó là tìm ra các thông tin hữu ích tiềm ẩn từ tập dữ liệu hoặc giảm chiều dữ liệu (Dimension Reduction) từ một số rất lớn ban đầu. Các thuật toán phân cụm (Clustering) nói riêng và cách tiếp cận của phương pháp học không giám sát nói chung hiện có vai trò quan trọng trong nhiều lĩnh vực nghiên cứu như Y Học, Marketing cũng như nghiên cứu Kinh Tế, Tài Chính. Trong series này về một số thuật toán phân cụm, những cách tiếp cận (thuật toán sau sẽ được giới thiệu): Phân cụm K Means (K Mean Clustering). Phâm cụm cấp bậc (Hierarchical Clustering). Phân tích thành phần chính PCA (Principal Component Analysis) Trước khi đi chi tiết vào ba thuật toán phân cụm trên chúng ta sẽ nghiên cứu tác động của việc lựa chọn phương pháp đo khoảng cách lên kết quả của thuật toán phân cụm K means clustering (cũng như một số thuật toán phân cụm khác). Các phương pháp đo khoảng cách Việc lựa chọn phương pháp đo khoảng cách là bước quan trọng khi thực hiện thuật toán phân cụm. Khoảng cách được hiểu tổng quát trong trường hợp này là “mức độ tương tự” giữa hai quan sát. Có hai nhóm phương pháp chính đo khoảng cách giữa hai quan sát. Nhóm thứ nhất bao gồm: Khoảng cách Euclidean. Khoảng cách Manhattan. Nhóm thứ hai là đo  khoảng cách dựa trên tương quan (correlation-based distances)  bao gồm: Dựa trên tương quan Pearson (Pearson correlation distance). Dựa trên tương quan cosin Eisen (Eisen cosine correlation distance). Dự trên tương quan Spearman. Dựa trên tương quan Kendall. Chúng ta xét ngay ví dụ sau về khoảng cách Euclidean với bộ số liệu  USArrests  bằng cách lấy ba quan sát đầu tiên: # Load dữ liệu\r\ndata(USArrests)\r\n(df1 <- USArrests[1:3, ]) ##         Murder Assault UrbanPop Rape\r\n## Alabama   13.2     236       58 21.2\r\n## Alaska    10.0     263       48 44.5\r\n## Arizona    8.1     294       80 31.0 # \"Khoảng cách\" giữa Alabama và Alaska: \r\n\r\nalab <- df1[1, ]\r\nalas <- df1[2, ]\r\n(d1 <- sqrt(sum((alab - alas)^2))) ## [1] 37.17701 # \"Khoảng cách\" giữa Alabama và và Arizona: \r\n\r\nariz <- df1[3, ]\r\n(d2 <- sqrt(sum((alab - ariz)^2))) ## [1] 63.00833 # Khoảng cách giữa Alaska và Arizona: \r\n(d3 <- sqrt(sum((alas - ariz)^2))) ## [1] 46.59249 Khoảng cách và vài trò của đồng bộ hóa (Scaling) hay chuẩn hóa (Standalization) thước đo Vì rằng tỉ lệ giết người, tội ác liên quan đến bạo lực, tỉ lệ án hiếp dâm và đô thị hóa trong ví dụ trên được đo bằng các đơn vị (hay thước đo) khác nhau nên việc chuẩn hóa (hay đồng bộ hóa) các thước đo là quan trọng và việc này có ảnh hưởng đến khoảng cách giữa các quan sát. Nói thế này cho dễ hiểu: Cân nặng đo bằng Kg, tuổi đo bằng năm nên ta cần một “đơn vị đo chung” cho chúng. Có nhiều cách thức chuẩn hóa “thước đo” và dưới đây chúng ta xem xét tác động của việc biến các tọa độ (các biến) sao cho chúng có mean là 0 và độ lệch chuẩn là 1 với số liệu ở trên: # Viết hàm chuẩn hóa và chuẩn hóa bộ dữ liệu: \r\n\r\nmy_scale <- function(x) {(x - mean(x))/sd(x)}\r\ndf1 <- sapply(df1, my_scale) \r\nsummary(df1) ##      Murder           Assault            UrbanPop            Rape        \r\n##  Min.   :-0.9053   Min.   :-0.97624   Min.   :-0.8552   Min.   :-0.9431  \r\n##  1st Qu.:-0.5367   1st Qu.:-0.51109   1st Qu.:-0.5498   1st Qu.:-0.5243  \r\n##  Median :-0.1681   Median :-0.04594   Median :-0.2443   Median :-0.1054  \r\n##  Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.0000  \r\n##  3rd Qu.: 0.4526   3rd Qu.: 0.48812   3rd Qu.: 0.4276   3rd Qu.: 0.4716  \r\n##  Max.   : 1.0734   Max.   : 1.02218   Max.   : 1.0995   Max.   : 1.0485 # Lúc này, các khoảng cách sẽ là: \r\n\r\nalab <- df1[1, ]\r\nalas <- df1[2, ]\r\n(d1 <- sqrt(sum((alab - alas)^2))) ## [1] 2.59743 # \"Khoảng cách\" giữa Alabama và và Arizona: \r\n\r\nariz <- df1[3, ]\r\n(d2 <- sqrt(sum((alab - ariz)^2))) ## [1] 3.22747 # Khoảng cách giữa Alaska và Arizona: \r\n(d3 <- sqrt(sum((alas - ariz)^2))) ## [1] 2.614727 Tất nhiên, thay vì tính toán thủ công như trên chúng ta có thể dùng hàm  dist  như sau: library(tidyverse)\r\ndf1 %>% dist(method = \"euclidean\") %>% head() ## [1] 2.597430 3.227470 2.614727 # Các con số 2.597430, 3.227470 và 2.614727 chính  là các kết quả thu được ở trên.  Nhược điểm của hàm dist ở trên là không áp dụng được cho dữ liệu factor, ordinal. Trong những tình huống này chúng ta sử dụng hàm  daisy()  . Trở lại với bộ số liệu USArrests: data(USArrests)\r\n(df1 <- USArrests[1:3, ]) ##         Murder Assault UrbanPop Rape\r\n## Alabama   13.2     236       58 21.2\r\n## Alaska    10.0     263       48 44.5\r\n## Arizona    8.1     294       80 31.0 # Load gói cluster package\r\nlibrary(cluster)\r\n\r\n# euclidean và không chuẩn hóa: \r\ndaisy(df1, metric = \"euclidean\", stand = FALSE) ## Dissimilarities :\r\n##          Alabama   Alaska\r\n## Alaska  37.17701         \r\n## Arizona 63.00833 46.59249\r\n## \r\n## Metric :  euclidean \r\n## Number of objects : 3 # euclidean và chuẩn hóa thước đo nhưng  theo  một kiểu khác là subtracting the variable's mean value and dividing by the variable's mean absolute deviation: \r\ndaisy(df1, metric = \"euclidean\", stand = TRUE) ## Dissimilarities :\r\n##          Alabama   Alaska\r\n## Alaska  3.699459         \r\n## Arizona 4.587244 3.654346\r\n## \r\n## Metric :  euclidean \r\n## Number of objects : 3 # Nếu muốn sử dụng kiểu chuẩn hóa quen thuộc của chúng ta: \r\ndf1 <- sapply(df1, my_scale)\r\ndaisy(df1, metric = \"euclidean\", stand = FALSE) ## Dissimilarities :\r\n##          1        2\r\n## 2 2.597430         \r\n## 3 3.227470 2.614727\r\n## \r\n## Metric :  euclidean \r\n## Number of objects : 3 Thế ý nghĩa của các khoảng cách này là gì? Là nếu, ví dụ, bạn chọn một ngưỡng là “khoảng cách bé hơn 2.6” thì bạn có thể “nhóm” Alabama và Alaska vào một cụm (Cluster). Còn riêng ông Arizona đứng lẻ loi một mình - tức thuộc một cụm khác. Đây là cách giải thích trực quan nhất (tuy chưa hoàn toàn chính xác) cho  k mean clustering  - một thuật toán phân cụm dựa trên khoảng cách. Dưới đây trình bày một ví dụ trực quan khác về phân cụm với bộ số liệu  WordCities  về 23018 thành phố trên thế giới. Tuy nhiên chúng ta chỉ lọc ra 4000 thành phố lớn nhất theo tiêu chí số dân và phân cụm chúng thành 6 nhóm dự trên kinh độ và vĩ độ. Chi tiết kĩ thuật của R codes các bạn chưa cần quan tâm tại thời điểm này. library(mdsr)\r\ndata(\"WorldCities\")\r\nBigCities <- WorldCities %>% \r\n  arrange(desc(population)) %>% \r\n  select(longitude, latitude) %>% \r\n  slice(1:4000)\r\n\r\nhead(BigCities) ##   longitude  latitude\r\n## 1 121.45806  31.22222\r\n## 2 -58.37723 -34.61315\r\n## 3  72.88261  19.07283\r\n## 4 -99.12766  19.42847\r\n## 5  67.08220  24.90560\r\n## 6  28.94966  41.01384 library(mclust)\r\nset.seed(29)\r\n\r\ncity_clusts <- BigCities %>% \r\n  kmeans(centers = 6) %>% \r\n  fitted(\"classes\") %>% \r\n  as.character()\r\n\r\nBigCities <- BigCities %>% \r\n  mutate(cluster = city_clusts) \r\n\r\nBigCities %>% \r\n  ggplot(aes(x = longitude, y = latitude)) + \r\n  geom_point(aes(color = cluster), alpha = 0.5) Nếu dựa trên kinh độ và vĩ độ có thể thây 4000 thành phố này được phân thành các cụm rõ ràng. Ví dụ, các thành phố ở châu Mĩ là tách biệt rõ ràng với phần còn lại. Thuật toán K means Clustering Về cơ bản đây là thuật toán phân chia bộ dữ liệu gồm n quan sát ban đầu thành K cụm (Cluster) sao cho sự đồng nhất (homogeneous) giữa các quan sát trong nhóm là cao nhất có thể. Hay nói cách khác, thuật toán này nhóm các quan sát thành K cụm khác nhau sao cho sự khác biệt giữa các quan sát trong mỗi cụm là thấp nhất. Sự khác biệt ấy có thể là các một đặc tính hay một nhóm đạc tính nào đó (thường gọi là attributes) của các quan sát. Sự đồng nhất (hay khác biệt giữa các quan sát) ấy được lượng hóa bằng tổng các “khoảng cách” giữa các quan sát trong một cụm con (Sub Cluster) mà chúng ta đã đề cập ở trên và sẽ là tối ưu khi tổng này bé nhất có thể được. Để hiểu sâu hơn về những khái niệm rắc rối này, chúng ta sẽ nghiên cứu một ví dụ trực quan lấy từ cuốn  R for Marketing Research and Analytics . Mini Project 1: K Means Clustering trong Marketing cho phân loại khách hàng Giới thiệu bài toán Mục tiêu của chúng ta là tìm cách phân loại những khách hàng tiêu dùng tiềm năng thành các nhóm (trong Marketing, việc này gọi là Segmentation) dựa trên các thông tin như thu nhập, các đặc điểm chủng tộc và cá nhân khác nhau của họ. path <- dir(\"E:/R_projects/Marketing\", full.names = TRUE)\r\nseg.raw <- read.csv(path[[8]])\r\nhead(seg.raw) ##        age gender   income kids ownHome subscribe    Segment\r\n## 1 47.31613   Male 49482.81    2   ownNo     subNo Suburb mix\r\n## 2 31.38684   Male 35546.29    1  ownYes     subNo Suburb mix\r\n## 3 43.20034   Male 44169.19    0  ownYes     subNo Suburb mix\r\n## 4 37.31700 Female 81041.99    1   ownNo     subNo Suburb mix\r\n## 5 40.95439 Female 79353.01    3  ownYes     subNo Suburb mix\r\n## 6 43.03387   Male 58143.36    4  ownYes     subNo Suburb mix dim(seg.raw) ## [1] 300   7 str(seg.raw) ## 'data.frame':    300 obs. of  7 variables:\r\n##  $ age      : num  47.3 31.4 43.2 37.3 41 ...\r\n##  $ gender   : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 1 1 2 2 2 1 1 ...\r\n##  $ income   : num  49483 35546 44169 81042 79353 ...\r\n##  $ kids     : int  2 1 0 1 3 4 3 0 1 0 ...\r\n##  $ ownHome  : Factor w/ 2 levels \"ownNo\",\"ownYes\": 1 2 2 1 2 2 1 1 1 2 ...\r\n##  $ subscribe: Factor w/ 2 levels \"subNo\",\"subYes\": 1 1 1 1 1 1 1 1 1 1 ...\r\n##  $ Segment  : Factor w/ 4 levels \"Moving up\",\"Suburb mix\",..: 2 2 2 2 2 2 2 2 2 2 ... Nếu chúng ta biết trước các nhóm khách hàng, thì hình ảnh sau cung cấp nhiều thông tin ý nghĩa: df1 <- seg.raw %>% select(age, income, kids, Segment)\r\npar(mfrow = c(1, 3)) \r\npar(bg = \"grey98\") \r\nfor (i in 1:3) {\r\n  boxplot(df1[, i] ~ df1$Segment, \r\n          main = names(df1[i]), \r\n          col = rainbow(4)) \r\n}  par(mfrow = c(1, 1))  Thực vậy, có vẻ như là nếu căn cứ theo độ tuổi thì những quan sát này có thể được phân thành 4 nhóm riêng biệt. Tất nhiên trong thực tế chúng ta không biết trước 4 nhóm này và do vậy nhiệm vụ của chúng ta là phân cụm các khách hàng này. Bài toán này, nếu giải quyết được thì VP Bank hay các tổ chức kinh doanh nói chung sẽ có cách thức tiếp cận và có những chiến lược Marketing phù hợp. Đánh giá khả năng sử dụng thuật toán phân cụm Tuy nhiên, trước khi thực hiện bất kì thuật toán phân cụm nào thì chúng ta cần trả lời câu hỏi: liệu bộ dữ liệu của chúng ta có thể áp dụng được các thuật toán phân cụm hay không? Để làm việc này, chúng ta sử dụng một thao tác gọi là đánh giá xu hướng cụm (Clustering tendency assessment) bằng thống kê  Hopkins  bằng hàm  hopkins()  của gói  clustertend  : # Loại ra biến phân loại    khách hàng  vì thực  tế chúng ta chưa biết thông tin này: \r\ndf1_raw <- seg.raw %>% select(-Segment) \r\n# Chuyển hóa dữ liệu: \r\ndf1_raw <- df1_raw %>% mutate_if(is.factor, as.numeric) \r\n\r\nlibrary(clustertend)\r\nset.seed(29)\r\nhopkins(df1_raw, n = nrow(df1_raw) - 1) ## $H\r\n## [1] 0.2049759 Giá trị của thống kê này thấp hơn ngưỡng 0.5 nên có thể sử dụng các thuật toán phân loại cho bộ dữ liệu trên. Lựa chọn k tối ưu Bước kết tiếp là lựa chọn một k tối ưu cho thuật toán phân cụm. Một giá trị k lớn sẽ gia tăng tính đồng nhất của các quan sát thuộc một cụm cụ thể nhưng sẽ dấn đến khả năng overfiting. Do vậy lựa chọn k phù hợp là một bước quan trọng. K tối ưu sẽ là một giá trị nằm đâu đó trong khoảng từ 2 đến căn bậc hai của n/2 với n là số quan sát trong bộ dữ liệu (Lantz, 2015). Có nhiều thuật toán được sử dụng để tìm k tối ưu nhưng trong bài này chúng ta sử dụng phương pháp Elbow (Elbow method): set.seed(123)\r\n\r\nk.max <- round(sqrt(nrow(df1_raw) / 2))\r\ndata <- sapply(df1_raw, my_scale)\r\n\r\nwss <- sapply(1:k.max, \r\n        function(k){kmeans(data, k, nstart = 10 )$tot.withinss})\r\nplot(1:k.max, wss,\r\n       type=\"b\", pch = 19, frame = FALSE, \r\n       xlab=\"Number of clusters K\",\r\n       ylab=\"Total within-clusters sum of squares\")\r\n\r\n# Căn cứ theo  phương pháp này chúng ta có  thể chọn k là 4 hoặc 5. Vẽ thêm cho vui: \r\nabline(v = 4, lty =2) Thực hiện phân cụm và hình ảnh hóa # Nếu chọn k =  4\r\nset.seed(29)\r\nkm.res4 <- kmeans(data, 4, nstart = 25)\r\n\r\nlibrary(factoextra)\r\nfviz_cluster(km.res4, \r\n             data = data, \r\n             geom = \"point\",\r\n             stand = FALSE, \r\n             frame.type = \"norm\") # Nếu chọn k =  5: \r\n\r\nkm.res5 <- kmeans(data, 5, nstart = 25)\r\nfviz_cluster(km.res5,\r\n             data = data, \r\n             geom = \"point\",\r\n             stand = FALSE, \r\n             frame.type = \"norm\") (CÒN NỮA)",
          "url": "http://rstudio-pubs-static.s3.amazonaws.com/264432_95f817c998b24d23bba6ed5057257aa5.html"
        }
      ]
    },
    {
      "description": "Liệt kê những ứng dụng của xử lý ngôn ngữ tự nhiên vào các nhu cầu thực tế.",
      "query": "những ứng dụng của xử lý ngôn ngữ tự nhiên",
      "sites": [
        {
          "content": "Trang chủ Đăng ký Đăng nhập Liên hệ Tài Liệu Môn Học Tổng hợp tài liệu các môn học chuyên ngành cho sinh viên Ứng dụng xử lý ngôn ngữ tự nhiên trong hệ tìm kiếm thông tin trên văn bản tiếng Việt File đính kèm: Ứng dụng xử lý ngôn ngữ tự nhiên trong hệ tìm kiếm thông tin trên văn bản tiếng Việt.pdf Tài liệu liên quan Ứng dụng xử lý ngôn ngữ tự nhiên trong hệ tìm kiếm thông tin trên văn bản tiếng Việt 8 trang | Lượt xem: 995 | Lượt tải: 1 \r\n            Copyright © 2016  MonHoc.vn",
          "relevance": "0",
          "title": "Ứng dụng xử lý ngôn ngữ tự nhiên trong hệ tìm kiếm thông tin trên văn bản tiếng Việt",
          "url": "http://monhoc.vn/tai-lieu/ung-dung-xu-ly-ngon-ngu-tu-nhien-trong-he-tim-kiem-thong-tin-tren-van-ban-tieng-viet-2860/"
        },
        {
          "content": "Công nghệ Lập trình Lập trình ứng dụng Lập trình web Tools & Tips Sự kiện Chuyên gia nói Tâm sự coder Devvui Tìm kiếm Sign in Đăng nhập tài khoản Tài khoản mật khẩu của bạn Forgot your password? Get help Password recovery Khởi tạo mật khẩu email của bạn Mật khẩu đã được gửi vào email của bạn. Tech Talk Điều gì sẽ xảy ra khi cập nhật phần mềm mỗi… Trí tuệ nhân tạo của Google tự đánh cờ vây với… 8 xu hướng công nghệ sẽ thống trị trong giai đoạn… Những kĩ năng cần có của một developer thành công Bản update Windows 10 Fall Creators không tương thích với laptop… Tất cả Lập trình ứng dụng Lập trình web Framework Microsoft .Net 4.7.1 có gì mới? 7 lí do để loại bỏ React’s Functional Components Tại sao C # là một trong những ngôn ngữ lập… Vì sao JavaScript là ngôn ngữ lập trình quyến rũ Getting Started with Entity Framework 6 Code First using MVC 5 Thử một lần code mà không dùng If xem nào? 5 cách giúp lập trình viên tăng năng suất làm việc Lập trình viên, liệu bạn đã đủ “hấp dẫn” trong mắt… [TÀI LIỆU] Beginning Mobile App Development with React Native “Ở Việt Nam, cơ hội để thực sự làm về Trí… [ Học bổng] Học tiếng Nhật miễn phí cùng ” Akira”-… VWS2017 – Xây dựng Decentralized web trên nền tảng Blockchain Họp báo ra mắt sự kiện Vietnam Web Summit 2017 [Miễn phí] Tham gia sự kiện App Analytics Tools: con đường… 4 sai lầm có thể khiến các lập trình viên rời… Lương kỹ sư IT đi Nhật có thể lên đến 165… Phỏng vấn chuyên gia Machine Learning từ AdAsia về ứng dụng… 5 điều phiền toái nhất của CSS Thương hiệu cá nhân Dev quan trọng hơn bạn tưởng Tâm sự một coder: hãy dũng cảm thành thật với con… Web Dev: Cố gắng hoàn hảo sẽ cản trở bạn Thay đổi code, thay đổi thế giới Thương hiệu cá nhân Dev quan trọng hơn bạn tưởng Những tình huống “đứng hình” trong JavaScript Trang Chủ Công nghệ Xử lý ngôn ngữ tự nhiên (Natural Language Processing) là gì? Xử lý ngôn ngữ tự nhiên (Natural Language Processing) là gì? June 26, 2016 1010 Chia sẻ Facebook Tweet Các doanh nghiệp hiện nay đang đối mặt với “cơn lũ” dữ liệu về mọi mặt: feedback của khách hàng, thông tin đối thủ cạnh tranh, emails của khách hàng, tweets, thông tin họp báo, hồ sơ pháp lý, các văn bản về sản phẩm và kĩ thuật. Việc khai thác được những dữ liệu này là điểm mấu chốt để các doanh nghiệp có thể triển khai nhanh chóng các quyết định của mình so với đối thủ cạnh tranh. Vấn đề ở đây là gì? Có quá nhiều thông tin để xử lý cùng lúc (hơn 85% dữ liệu trên thế giới không có cấu trúc), và kích thước dữ liệu ngày càng tăng. Đối với nhiều doanh nghiệp, điều này là bất khả thi để điều động nhân sự đọc tất cả mọi thứ được cho là quan trọng (các khách hàng đang nói gì về sản phẩm, những đối thủ cạnh tranh của chúng ta đang làm gì). Xử lý ngôn ngữ tự nhiên (NLP) giúp máy tính làm thay những việc trên Được xây dựng dựa trên ngôn ngữ học phức tạp, các nguyên lý thống kê, và thuật toán mạng nơ ron (neural network algorithms). Chương trình NLP có khả năng đọc và hiểu được văn bản với tốc độ cao. Do đó, dù bạn có 1000 tài liệu hay thậm chí hàng tỉ văn bản, chương trình NLP có thể “tiêu hoá” nhanh chóng tất cả các thông tin này, từ đó có thể rút trích ra được những tri thức (knowledge) đáng giá cho doanh nghiệp của bạn như: tri thức về các khách hàng, tri thức về những đối thủ cạnh tranh, tri thức về các hoạt động trong doanh nghiệp như điều hành, marketings, sales, kĩ thuật, và sản phẩm. Xử lý ngôn ngữ tự nhiên cung cấp những thông tin giá trị NLP được sử dụng cho hàng loạt các ngành công nghiệp để giải quyết những bài toán mấu chốt như cung cấp những thông tin giá trị và rõ ràng từ hàng đống tài liệu phi cấu trúc (dữ liệu CRM, social media, tin tức, hồ sơ bằng sáng chế, thông tin tài chính). Thông qua các thuật toán tiên tiến, NLP chỉ ra được ai, cái gì, khi nào, và ở đâu trong những nội dung phi cấu trúc, từ đó có thể cung cấp các cấp độ hiểu biết cao hơn về công việc kinh doanh của bạn. Các ứng dụng của NLP vào lĩnh vực kinh tế Marketing : Voice of the customer, social media analysis, churn analysis, market research, survey analysis Business :   Competitive intelligence, document categorization, human resources (voice of the employee), records retention, risk analysis, website faceted navigation Industry specific :  Fraud detection, e-discovery, warranty analysis, medical analytics research Nguồn:  http://www.alchemyapi.com/resources/natural-language-processing Tham khảo thêm: You Must Allow Me To Tell You How Ardently I Admire and Love Natural Language Processing . --------------------  Thông tin cho Dev  -------------------- Người viết  ông xuân hồng  TAGS ngongngutunhien Facebook Twitter Bài viết trước Gulp, Vũ khí tối mật cho lập trình web (P.1): tự động hóa trong JavaScript Bài kế tiếp Chân dung nhà lập trình game  trong thời đại công nghệ di động Những quy tắc gây sốc trong phát triển phần mềm March 22, 2017 FinTech Việt Nam sẽ phát triển mạnh mẽ trong thời gian sắp tới March 9, 2017 Những khoảnh khắc đáng nhớ của sự kiện Inside the mind of a Product Manager April 3, 2017 Những lời nói dối phổ biến của lập trình viên July 11, 2017 Tại sao giới lập trình thù ghét Internet Explorer? March 26, 2017 Tại sao bạn cứ ám ảnh về đồng lương? July 25, 2017 “Ở Việt Nam, cơ hội để thực sự làm về Trí tuệ nhân tạo còn quá ít, trong khi những thứ mang hình thù... October 21, 2017 [ Học bổng] Học tiếng Nhật miễn phí cùng ” Akira”- chắp cánh ước mơ October 19, 2017 VWS2017 – Xây dựng Decentralized web trên nền tảng Blockchain October 19, 2017 VỀ CHÚNG TÔI • Giấy phép thiết lập Mạng xã hội số 569/GP-BTTTT do Bộ Thông Tin và Truyền Thông cấp.\r\n• Cơ quan chủ quản: Công ty Cổ phần Applancer.\r\nTrụ sở: 179 Đường Nguyễn Đình Chính, Phường 11, Quận PN, TP.HCM  Liên hệ:  hieuld@applancer.net - Tel: 028 6264 5022 THEO CHÚNG TÔI",
          "relevance": "1",
          "title": "Xử lý ngôn ngữ tự nhiên (Natural Language Processing) là gì?",
          "url": "https://techtalk.vn/xu-ly-ngon-ngu-tu-nhien-natural-language-processing-la-gi.html"
        },
        {
          "content": "Tìm kiếm trang web này Trang chủ LY NAM PHONG Lưu Tuấn Anh Nguyễn Văn Hải Các công cụ xử lý Trích lọc tiếng Việt từ HTML DongDu Download dữ liệu Giới thiệu về các nghiên cứu mới [Máy học]Learning Combination Features with L1 Regularization [phân loại]Text Categorization with All Substring Features  [in Japanese] Automatic Tree and String Based Wrapper Generation for Semi-structured Documents Extracting Structured Data from Web Pages Online Feature Selection using Grafting Kiến thức cơ bản về xử lý ngôn ngữ tự nhiên Khởi đầu NLP với Python Liblinear-thư viện học máy Lựa chọn đặc trưng (Feature selection) Machine Learning trong NLP Mô hình ngôn ngữ NLP là gì ? Phân nhóm dữ liệu (Clustering) Thuật toán tách từ (Tokenizer) Xử lý tiếng Việt bằng Python (1) Ứng dụng Pointwise để tách từ Nghiên cứu của tác giả Bài toán thêm dấu cho tiếng Việt Việt hoá Mecab Nhập môn Linux SHELL là gì SHELL mạnh nhất : zsh Tài nguyên ngôn ngữ tiếng Việt Khái yếu về corpus Khái yếu về từ điển Kế hoạch xây dựng tự động corpus từ nguồn Web Đặc trưng của tiếng Việt Tạp đàm seminar là gì Sơ đồ trang web Hoạt động gần đây của trang web Tác giả trang Van Hai Nguyen tháng hai 17, 2015 anh tháng một 17, 2012 山本和英 tháng một 16, 2012 Trang chủ [English] [日本語] Vietnamese Natural Language Processing      Xử lý ngôn ngữ là một kĩ thuật quan trọng nhằm giúp máy tính hiểu được ngôn ngữ của con người, qua đó hướng dẫn máy tính thực hiện và giúp đỡ con người trong những công việc có liên quan đến ngôn ngữ như : dịch thuật, phân tích dữ liệu văn bản, nhận dạng tiếng nói, tìm kiếm thông tin, ...    XLNN cũng đóng một vai trò quan trọng trong việc đẩy mạnh sự phát triển của CNTT Việt Nam để sánh ngang với các cường quốc khác.    Tuy nhiên, XLNN tiếng Việt (XLNNTV) cũng vấp phải vô vàn khó khăn, mà lớn nhất phải kể đến sự khó khăn về nhân sự. Những người nắm giữ những kiến thức về XLNNTV quả thực không nhiều, và cũng không có được 1 mạng lưới liên kết, trao đổi và hỗ trợ một cách hiệu quả. Ngoài ra, những khó khăn khác như không có dữ liệu đủ lớn, thiếu những nghiên cứu nền tảng, ... cũng hạn chế không ít sự phát triển của XLNNTV.    Trang web này được tạo ra để chia sẻ và tập hợp những thông tin về nghiên cứu XLNN nói chung, và XLNNTV nói riêng. Thông qua trang web này, chúng tôi muốn kêu gọi sự đoàn kết và giúp đỡ của những người quan tâm đến XLNNTV.    Đầu tiên, chúng tôi muốn tạo ra 1 mailing list để tăng cường sự hiểu biết và liên kết giữa mọi người. Tiếp đó, với sự cộng tác này, chúng tôi sẽ thực hiện những nghiên cứu cơ bản, viết sách cho người mới học, giới thiệu những nghiên cứu mới, tạo dữ liệu và chia sẻ với mọi người.    Nếu bạn có sự quan tâm đến XLNNTV, hãy tham gia vào mailing list của chúng tôi.   Bạn có thể gửi mail về địa chỉ anh(a)jnlp.org. Chúng tôi xin trân trọng cảm ơn sự quan tâm của các bạn.    Tác giả Website này được quản lý bởi  Lưu Tuấn Anh , hiện đang học tập và làm việc tại  Nagaoka University of Technology ,  Natural Language Processing Yamamoto-lab  (Japan). Address: Nagaoka City, Niigata, 940-2188 JAPAN E-mail:  viet@jnlp.org Download :  Download dữ liệu tự vựng(corpus) Bài toán tự động thêm dấu cho tiếng Việt.   Tài liệu : Automatic Diacritics Restoration for Vietnamese Text (IALP2012).  Độ chính xác đạt 94.7%.  Bài viết mới :      Bài toán tự động thêm dấu cho tiếng Việt.   Thứ tự nên đọc : 1.  NLP là gì ? 2.  Khởi đầu NLP với Python 3.  Xử lý tiếng Việt bằng Python 4.  Khái yếu về từ điển 5.  Khái yếu về corpus 6.  Mô hình ngôn ngữ 7.  Thuật toán tách từ (Tokenizer) 8.  Machine Learning trong NLP Sign in | Recent Site Activity | Report Abuse | Print Page | Powered By Google Sites",
          "relevance": "0",
          "title": "Vietnamese Natural Language Processing",
          "url": "http://viet.jnlp.org/"
        },
        {
          "content": "Trang chủ Đăng ký Đăng nhập Liên hệ Tài Liệu Môn Học Tổng hợp tài liệu các môn học chuyên ngành cho sinh viên Môn học Lập Trình Cơ Sở Dữ Liệu Hệ Điều Hành Đồ Họa Tin Học Văn Phòng Công Nghệ Thông Tin Công Nghệ Phần Mềm Hệ Điều Hành Quản Lý Dự Án Mạng Máy Tính Đồ Họa Máy Tính Xử Lý Ảnh Cấu Trúc Dữ Liệu & Giải Thuật Kiến Trúc Máy Tính Tin Học Đại Cương Trí Tuệ Nhân Tạo Lập Trình Hướng Đối Tượng Phân Tích Thiết Kế Hệ Thống Lý Thuyết Thông Tin Nguyên Lý & Phương Pháp Lập Trình Công Nghệ Tri Thức & Máy Học Phân Tích & Thiết Kế Thuật Toán Các hệ cơ sở tri thức Biểu diễn tri thức và suy luận Các hệ giải bài toán thông minh Hệ thống đa tác tử Khai thác dữ liệu và ứng dụng Lập trình symbolic trong trí tuệ nhân tạo Xử lý ngôn ngữ tự nhiên Ngôn ngữ học máy tính Ngôn ngữ học ngữ liệu Các kỹ thuật xử lý ngôn ngữ tự nhiên Các hệ thống hỏi đáp Dịch máy Máy học trong xử lý ngôn ngữ tự nhiên Công nghệ web và ứng dụng Công nghệ .NET Phương pháp phát triển phần mềm hướng đối tượng Xử lý tín hiệu số Logic mờ và ứng dụng Lý thuyết automat và ứng dụng Một số ứng dụng của ngôn ngữ tự nhiên Truy xuất thông tin Bảo mật Internet Lập trình mạng An toàn mạng máy tính Truyền dữ liệu Thiết bị mạng và truyền thông đa phương tiện Mật mã học Hệ thống thông tin địa lý Thiết kế hướng đối tượng với UML Hệ quản trị cơ sở dữ liệu Kho dữ liệu và OLAP Hệ thống thông tin kế toán Phát triển ứng dụng web Cơ sở dữ liệu phân tán Lập trình trên thiết bị di động Phân tích không gian Hệ cơ sở dữ liệu không gian Mạng xã hội Hệ thống thông tin quản lý Thương mại điện tử An toàn và bảo mật Hệ thống thông tin Hoạch định nguồn lực doanh nghiệp Phân tích dữ liệu kinh doanh Mẫu thiết kế Điện toán đám mây Dữ liệu lớn Lập trình trực quan Phương pháp mô hình hóa Phần mềm và hệ thống nhúng Đặc tả hình thức Kiểm chứng phần mềm Phát triển, vận hành, bảo trì phần mềm Giao tiếp người máy Lập trình Game Xử lý phân bố E-learning Xử lý song song Kỹ thuật lập trình nhúng Lập trình đồ họa Vi xử lý – vi điều khiển Thiết kế vi mạch số Điều khiển tự động Thiết kế vi mạch Trình biên dịch Thiết kế mạng Công nghệ thoại IP Quản trị hệ thống mạng Lập trình hệ thống và mạng Tính toán lưới Thiết Kế Website Đại Cương TÀI LIỆU MỚI Tài liệu Xử lý ngôn ngữ tự nhiên Xử lý ngôn ngữ tự nhiên giới thiệu những cơ sở khoa học, các hướng nghiên cứu và ứng dụng của xử lý ngôn ngữ tự nhiên với tư cách là một chuyên ngành của khoa học máy tính. Sinh viên được làm quen với một số kỹ thuật phân tích cú pháp đơn giản, cài đặt trên Prolog.\r\n\r\nXử lý ngôn ngữ tự nhiên cung cấp cho sinh viên những kiến thức căn bản về lĩnh vực xử lý ngôn ngữ tự nhiên. Thông qua môn học, sinh viên có thể nắm được một số phương pháp và kỹ thuật nền tảng để định nghĩa văn phạm và phân tích cú pháp.\r\n\r\nXử lý ngôn ngữ tự nhiên (Natural Language Processing – NLP), là một lĩnh vực của khoa học máy tính, nghiên cứu sự tương tác giữa máy tính và ngôn ngữ tự nhiên, tập trung vào việc nghiên cứu các ứng dụng trên ngôn ngữ của con người.Trong trí tuệ nhân tạo thì xử lý ngôn ngữ tự nhiên là một trong những phần khó nhất vì nó liên quan đến việc phải hiểu ý nghĩa ngôn ngữ-công cụ hoàn hảo nhất của tư duy và giao tiếp. Xử lý ngôn ngữ tự nhiên tập trung ứng dụng trí tuệ nhân tạo vào việc phân tích, nhận biết, tổng hợp ngôn ngữ tự nhiên. Đó là cơ sở chính để đi vào các hướng: hiểu ngôn ngữ, dịch ngôn ngữ, xử lý tiếng nói, tóm tắt văn bản, tìm kiếm thông tin … Luận văn Tìm hiểu về xử lý ngôn ngữ tự nhiên  Lượt xem: 1989 1 \r\n            Copyright © 2016  MonHoc.vn",
          "relevance": "0",
          "title": "Tài liệu Xử lý ngôn ngữ tự nhiên",
          "url": "http://monhoc.vn/tai-lieu/xu-ly-ngon-ngu-tu-nhien/"
        },
        {
          "content": "Tìm kiếm trang web này Trang chủ LY NAM PHONG Lưu Tuấn Anh Nguyễn Văn Hải Các công cụ xử lý Trích lọc tiếng Việt từ HTML DongDu Download dữ liệu Giới thiệu về các nghiên cứu mới [Máy học]Learning Combination Features with L1 Regularization [phân loại]Text Categorization with All Substring Features  [in Japanese] Automatic Tree and String Based Wrapper Generation for Semi-structured Documents Extracting Structured Data from Web Pages Online Feature Selection using Grafting Kiến thức cơ bản về xử lý ngôn ngữ tự nhiên Khởi đầu NLP với Python Liblinear-thư viện học máy Lựa chọn đặc trưng (Feature selection) Machine Learning trong NLP Mô hình ngôn ngữ NLP là gì ? Phân nhóm dữ liệu (Clustering) Thuật toán tách từ (Tokenizer) Xử lý tiếng Việt bằng Python (1) Ứng dụng Pointwise để tách từ Nghiên cứu của tác giả Bài toán thêm dấu cho tiếng Việt Việt hoá Mecab Nhập môn Linux SHELL là gì SHELL mạnh nhất : zsh Tài nguyên ngôn ngữ tiếng Việt Khái yếu về corpus Khái yếu về từ điển Kế hoạch xây dựng tự động corpus từ nguồn Web Đặc trưng của tiếng Việt Tạp đàm seminar là gì Sơ đồ trang web Hoạt động gần đây của trang web Tác giả trang anh tháng một 17, 2012 Kiến thức cơ bản về xử lý ngôn ngữ tự nhiên ‎ > ‎\n   NLP là gì ?   NLP(Natural Language Processing)  là khái niệm\nđể chỉ các kĩ thuật, phương pháp thao tác trên ngôn ngữ tự nhiên bằng máy tính.\nBạn cần phân biệt ngôn ngữ tự nhiên (ví dụ như tiếng Việt, tiếng Anh, tiếng Nhật…\nlà những ngôn ngữ trong giao tiếp thường ngày) và ngôn ngữ nhân tạo ( như ngôn\nngữ lập trình, ngôn ngữ máy, …).    Trong NLP có  2 quan điểm cơ bản  :  1. Xử lý các từ ngữ bằng máy tính.  2. Làm cho máy tính hiểu được các từ ngữ.  Hiện tại, cả 2 hướng này đều đang được tích\ncực nghiên cứu và phát triển, nhờ đó rất nhiều các hệ thống hiệu quả đã và đang\nđược tạo ra.    Các ứng dụng cơ bản của NLP :  1. Chế tạo các hệ thống Máy dịch, ví dụ như\nGoogle translation. 2. Xử lý văn bản và ngôn ngữ.  3. Tìm kiếm thông tin.  4. Chiết suất thông tin.  5. Tóm tắt văn bản.  6. Phân loại văn bản.  7. Data mining, web mining.  Các bạn có thể đọc thêm về NLP tại  Wikipedia   . Comments Sign in | Recent Site Activity | Report Abuse | Print Page | Powered By Google Sites",
          "relevance": "1",
          "title": "NLP là gì ?",
          "url": "http://viet.jnlp.org/kien-thuc-co-ban-ve-xu-ly-ngon-ngu-tu-nhien/nlp-la-gi"
        },
        {
          "content": "Diễn đàn Tìm kiếm Bài mới Camera Bài mới Ảnh mới Gallery Mua Bán Camera Facebook Rao vặt Mua bán điện thoại Mua bán máy tính Mua bán máy tính bảng Mua bán camera Mua bán đồ công nghệ Mua bán xe Điện máy Mua bán sim, sim 3g Sony Camera iPhone X Bài mới Điện thoại Máy tính Camera Xe KHCN Bạn bè Sự kiện Thiết lập Tên tài khoản hoặc Email: Mật khẩu: Quên mật khẩu?  Nhớ đăng nhập Đăng ký Đăng nhập bằng Facebook Đăng nhập bằng Google Menu Diễn đàn Diễn đàn Liên kết nhanh Tìm kiếm Bài mới Camera Camera Liên kết nhanh Bài mới Ảnh mới Gallery Mua Bán Camera Facebook Rao vặt Rao vặt Liên kết nhanh Mua bán điện thoại Mua bán máy tính Mua bán máy tính bảng Mua bán camera Mua bán đồ công nghệ Mua bán xe Điện máy Mua bán sim, sim 3g Sony Camera iPhone X Menu Đăng nhập Tên tài khoản hoặc Email: Mật khẩu: Quên mật khẩu?  Nhớ đăng nhập Đăng ký Đăng nhập bằng Facebook Đăng nhập bằng Google  Chì tìm tiêu đề Viết bởi thành viên: Các tên cách nhau bởi dấu phẩy(,) Mới hơn:  Chỉ tìm kiếm trong chủ đề này  Chỉ tìm trong khu vực này  Hiển thị kết quả theo chủ đề Thêm... Mục tìm hữu ích Thảo luận mới Tinhte.vn Diễn đàn Khoa học công nghệ > Khoa học > [Computer Science] Xử lý ngôn ngữ tự nhiên (Phần 1) \n\tThảo luận trong ' Khoa học ' bắt đầu bởi  Wosea ,  19/8/16 .\nTrả lời: 1, Xem: 4989.\n\n Chia sẻ Facebook Share Wosea Thành viên\n\n Tham gia: 19/8/16 Được thích: 13 Hi bạn, mở đầu loạt bài viết với chủ đề  Computer Science , mình sẽ giới thiệu về  Xử lý ngôn ngữ tự nhiên  (  Natural Language Processing - NLP ). Đây là một lĩnh vực của Khoa học máy tính (Computer Science), Trí tuệ nhân tạo (Artificial Intelligence) và Ngôn ngữ học tính toán (Computational Linguistics) liên quan tới sự tương tác giữa ngôn ngữ con người (natural language) với máy tính. \nChúng ta sẽ được làm quen những kỹ thuật, các văn phạm được sử dụng để phân tích một câu (ngôn ngữ người). Bài viết có tham khảo bài giảng của thầy  Nguyễn Tuấn Đăng  (ĐH CNTT -ĐHQG HCM), cùng những kinh nghiệm có được của bản thân trong quá trình học tập và trao đổi với bạn bè. Khó tránh khỏi những sơ sót và những vấn đề bạn cảm thấy chưa được sáng tỏ trong bài viết, đừng ngần ngại hãy để lại bình luận cho mình biết nha. - - - - \n- Để phân tích cú pháp chúng ta dựa trên: \n+ Văn phạm hình thức: Văn phạm phi ngữ cảnh (Context Free Grammar -CFG). \n+  Definite Clause Grammar  - DCG  =>Prolog. \n+ Phương pháp phân tích cú pháp. \n- Biểu diễn bằng: Biểu thức chính quy; Văn phạm chính quy. \n- Bây giờ ta thử phân tích một câu tiếng Việt cơ bản như sau: \"Nam học bài\". Câu này có 3 từ, gồm 3 loại từ (Danh từ riêng, động từ và danh từ chung). Danh từ riêng (Nam) giữ chức năng chủ ngữ. Từ \"học\", động từ ở Tiếng Việt khác với trong Tiếng Anh (Các  Thì : Hiện tại, quá khứ,...;  Thể : Chủ động, bị động;  Ngôi : số 3 số ít/nhiều;..) Từ \"bài\" giữ chức năng bổ ngữ làm rõ nghĩa cho cả câu. \"học bài\" là ngữ động từ =>vị ngữ. Tương tự bạn hãy phân tích câu: \"Nam học chăm chỉ\". \nQua ví dụ phân tích câu trên, bạn đã được ôn lại cách phân tích một câu trong ngôn ngữ. Bây giờ chúng ta tìm hiểu sơ lược về lịch sử một tí: \nĐể có thể giải quyết vấn đề xử lý được ngôn ngữ tự nhiên trên máy tính, Leonard Bloomfield (1887-1949) đã tiến hành nghiên cứu và đưa ra \" Mô hình phân tích thành tố trực tiếp \". Học trò của ông ta là Zellig Harris (1909-1992) và người học trò của Z. Harris là Noam Chomsky (1928 -). Chúng ta sẽ tiếp thu kiến thức từ những công trình nghiên cứu và kết quả của ông Noam Chomsky trong phần chủ đề này. \nNăm 1957, N.Chomsky đã trở thành nhân vật nổi bật trong lĩnh vực ngôn ngữ học bằng thuật ngữ \"Syntactic Structures - Ngữ pháp cấu trúc ngữ đoạn\". Cụ thể hơn chúng ta sẽ tiếp cận với \"Phrase Structure Grammar - Ngữ pháp/văn phạm cấu trúc ngữ đoạn\". \nKhá sơ lược những gì chúng ta sẽ tìm hiểu. Ở phần 1, chúng ta cần nắm được cách thức giải thích câu theo \"Mô hình phân tích thành tố trực tiếp\" và \" Văn phạm phi ngữ cảnh \". 1)  Giải thích câu :  \"Nam học bài\"  theo  \"Mô hình phân tích thành tố trực tiếp\". \nĐể phân tích một câu nào đó theo mô hình này, chúng ta phải hiểu câu đó, ngôn ngữ đó đang sử dụng là gì (Anh, Pháp, Hoa, Việt,...). Trong quá trình phân tích, chúng ta không quan tâm các từ được phân rã giữ chức năng, từ loại nào. Giải:  \n​      Tương tự ta phân tích câu: \"Nam đang học toán\" \n​ 2)  Giải thích câu :  \"Nam học bài\"  theo  Phrase Structure Grammar  _  CFG  (Context-Free Grammar - Văn phạm phi ngữ cảnh). \n​ Một số quy tắc ký hiệu (sẽ khác so với một số nguồn tài liệu): NNP : Danh từ riêng;  NP : Danh ngữ;  NN : Danh từ chung;  PRP : Đại danh từ (nó,họ,...);  VP : Ngữ động từ;  VB : Động từ;     RB : Trạng từ;  IN : Giới từ;  PP : Giới ngữ;  CC : Liên từ(và, với);   S : Câu \nDựa vào cây cú pháp trên, Chomsky viết như sau: \n\n\t\t\n​ \nTương tự ta phân tích câu: \"Nam đang học toán\" (Lời giải: Trong  phần 2  ) *Note : Có 4 lớp văn phạm gồm \"Văn phạm chính quy\"; \"Context- Free Grammar(s)\"; \"Context- Sensitive Grammar(s)\"; \"Văn phạm tự do\". \n- - - -  Tài liệu tham khảo: \n[1] Nguyễn Tuấn Đăng,  Xử lý ngôn ngữ tự nhiên  (Bài giảng), mã lớp CS221.G11, lớp Cử nhân Chính quy Khoa học máy tính, Trường Đại học Công Nghệ Thông Tin, ĐHQG-HCM. \n[2] Nguyễn Tuấn Đăng, Ngôn ngữ học máy tính (Giáo trình), NXB Đại học quốc gia TP. Hồ Chí Minh, 2015. \n[3]  Kiến thức cơ bản về xử lý ngôn ngữ tự nhiên http://viet.jnlp.org/  Phần mềm tham khảo ngữ nghĩa từ: \nTừ điển  LẠC VIỆT mtd2002  – EVA  \nTừ điển  Lingoes v2.9.2  (2014-08-16)  \nOK! Kết phần 1, bạn đã nắm được các ký tự và quy tắc cú pháp theo CFG. Ở phần 2 chúng ta sẽ tìm hiểu làm thế nào máy tính có thể chạy được các quy tắc này. \nCảm ơn bạn đã theo dõi bài viết! Nguồn  Sâu non tìm lá ​ \nBài viết tiếp theo:  [Computer Science] Xử lý ngôn ngữ tự nhiên (Phần 2) ​   Chia sẻ Facebook Share #1 Wosea , 19/8/16 \n\t\t\t\t\t\n\t\t\t\t\t\tSửa lần cuối:  24/8/16 Phạm Bá Nhật Minh  thích nội dung này.\n\t\t hackerry Dự bị\n\n Tham gia: 11/1/10 Được thích: 2 \n\t\t\t\t\t\t\n\n\n\t\n\t\n\n\n\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\nup <3\n\t\t\t\t\t\t   #2 hackerry , 7/9/17 (Bạn phải đăng nhập hay đăng ký tài khoản để đăng bài ở đây) Hiện nội dung bỏ qua Show more posts \n\tTag:\n\t\n\t\t computerscience naturallanguage wosea xử lý ngôn ngữ tự nhiên computer science natural language processing - nlp Chia sẻ Facebook Share Đăng nhập bằng Facebook Đăng nhập bằng Google Tên tài khoản hoặc Email: Bạn đã có tài khoản? \n\t\t\t\t\t\tChưa, tôi tạo tài khoản mới. \n\t\t\t\t\t\tCó, mật khẩu của tôi là: Quên mật khẩu?  Nhớ đăng nhập Đăng ký thành viên Cần bạn trả lời \nBộ phát wifi 4G nào mạnh nhất\n traibao1616 \nTư vấn tai nghe wireless\n Jakerr \nAe cho e hỏi về mạng phát wf ạ\n Gấu Đenn \nSo sánh ƯU-NHƯỢC điểm của các loại nhà thông minh 2017\n hachbachcongtu \nXin mọi người tư vấn loa\n eragon1492 \n\t\t\t\n\t\t\t\tCó thể bạn quan tâm\n\t\t\t\n\t\t Lợn đột biến gen siêu cơ bắp tại Campuchia Con người sẽ ra sao nếu chỉ sống với 1 quả thận đến cuối đời? Sự hình thành vũ trụ - Giả thuyết bởi DuongHero [Hỏi Tinh tế] Anh em có dùng cục phát wifi di động? Trên tay màn hình QLED 49\" 32:9 của Samsung cho game thủ \n\t\t\t\n\t\t\t\tCùng chuyên mục\n\t\t\t\n\t\t Lá thư từ nạn nhân Titanic đấu giá với số tiền 166 ngàn đô: \"Con tàu lớn, đồ ăn ngon và nhạc hay\" Trên tay bình nước không thể ngã đổ MightyMug, tự hít vào mặt bàn, giá 32 đô la Một cặp song sinh, một người mẹ, 2 người cha và sự phức tạp của việc song sinh Dùng giọng nói để điều khiển Alexa tắt/mở đèn, mở nhạc... [Mẹo vặt] Công thức tính nhẩm đổi nhanh giữa độ C và độ F không cần máy tính \n\t\t\t\n\t\t\t\tSôi động trong tuần\n\t\t\t\n\t\t [Nhận xét chung về RX10 IV: một chiếc máy all... [Hỏi Tinh tế] Anh em đã lên Windows 10 Fall Creators chưa, cảm nhận thế nào? Đã có Windows 10 Fall Creators + file ISO, lên nào anh em! Tổng hợp lỗi Windows 10 Fall Creators, mời anh em chia sẻ thêm Cảm nhận Win 10 Fall Creators: nhanh hơn, thiết kế Fluent thiếu đồng nhất, nhiều cải tiến nhỏ mà tốt So sánh hiệu năng thực tế giữa iPhone 8 Plus với Galaxy Note 8: lịch sử có lặp lại? Đã có iOS11.1 Beta 4 mời anh em tham gia Dev tải về Màn hình P-OLED của Pixel 2 XL có bị nhạt màu, ánh xanh và vân cầu vồng? Mac Mini đã 3 năm rồi chưa được làm mới, có phải Apple đã bỏ dòng sản phẩm này? [Windows 10] Hãy dùng bộ GoTiengViet để tránh các lỗi chậm, giật khi gõ nhanh Trang chủ Giúp đỡ Điều khoản & Quy định Lên đầu \n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\tTinhte.vn\n\t\t\t\t\t\t\t Trang nhất Diễn đàn Hỗ trợ - hướng dẫn Thư viện Gallery Sự kiện Tinhte.vn RSS Liên hệ Quảng cáo \n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\tDiễn đàn\n\t\t\t\t\t\t\t Thông tin Máy tính Điện thoại Camera Xe Khoa học công nghệ Tinh tế \n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\tNhật tảo\n\t\t\t\t\t\t\t Mua bán điện thoại Mua bán máy tính Mua bán máy tính bảng Mua bán camera Mua bán đồ công nghệ Mua bán xe Mua bán điện máy Mua bán sim, sim 3g \n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\tLiên kết\n\t\t\t\t\t\t\t Cafe Tinhte.vn Khacten.com 1TuDien.com Nhaccuatui.com 5Giay.vn Webtretho.com Biker Vietnam \n\t\t\t\t Chịu trách nhiệm nội dung: Trần Mạnh Hiệp • © 2015 Công ty Cổ phần MXH Tinh tế  Địa chỉ: 209 Đường Nam Kỳ Khởi Nghĩa, Phường 7, Quận 3, TP.HCM • Giấy phép MXH số 385/GP-TTTT do Bộ TTTT cấp.\n\t\t\t\t\n\t\t\t\t\n\t\t\t Đang tải... Tinhte.vn Diễn đàn Khoa học công nghệ > Khoa học >",
          "relevance": "0",
          "title": "[Computer Science] Xử lý ngôn ngữ tự nhiên (Phần 1)",
          "url": "https://tinhte.vn/threads/computer-science-xu-ly-ngon-ngu-tu-nhien-phan-1.2634689/"
        },
        {
          "content": "Tin FPT Chuyên gia FPT viết Artificial Intelligence Internet of Things Cloud Computing Mobility Security Khác Dự án Công nghệ quanh ta Sự kiện Tài liệu Về chúng tôi Giới thiệu Định hướng công nghệ Cán bộ công nghệ Search FPT TechInsight Ban công nghệ tập đoàn FPT tuyển dụng chuyên gia phần… Giám đốc Công nghệ FPT Retail đạt danh hiệu Trạng nguyên… Đoàn FPT kéo quân vào cố đô tham dự KSE 2017 FPT Telecom phối hợp cùng ISC tạo ra IQC – Ứng… Chinh phục thử thách, hẹn hò cùng CyRadar Tất cả Artificial Intelligence Internet of Things Cloud Computing Mobility Security Khác Giới thiệu về Xử lí Ngữ nghĩa trong Ngôn ngữ Tự… Phụ nữ là nhân tố quan trọng giúp Việt Nam cất… Kỹ thuật Attention trong mô hình Sequence-to-Sequence và ứng dụng trong… Nghị định 58 về kinh doanh mật mã dân sự và… Chung tay xây dựng kho dữ liệu lớn nhất Việt Nam… Giải quyết ùn tắc giao thông bằng hệ thống iBus Triển khai giải pháp tối ưu hóa nguồn lực của FPT… Ứng dụng Trí tuệ nhân tạo trong giải pháp Camera thông… Phát triển Search Engine với TensorFlow Bức tranh khái quát về Mạng từ và Mạng từ tiếng… Facebook tung ra hàng loạt tính năng mới cho người dùng Alibaba đầu tư 15 tỷ USD vào R&D với 7 phòng… iPhone 8 – Nên hay không móc hầu bao cho siêu… Airbus sẽ bắt đầu các tuyến taxi bay vào năm 2018 “Bot of the Year” – Xây dựng Chatbot, nhận ngay 50… Tiến sĩ công nghệ chia sẻ thông tin từ Hội nghị… Một số thông tin từ Hội nghị ACL – Chia sẻ… Hội thảo “Tiền điện tử và chuỗi khối blockchain” Công nghệ tương tác giọng nói và vai trò của tương… Tài liệu Xử lý ngôn ngữ tự nhiên thu thập từ… 7 ngày free với khóa học mới về Deep Learning từ… Khám phá những cuốn sách hay về Python Bí kíp giải quyết cấu trúc dữ liệu Tài liệu hướng dẫn sử dụng API Vietnam AI Hackathon Về chúng tôi Giới thiệu Định hướng công nghệ Cán bộ công nghệ Chuyên gia FPT viết Xử lí ngôn ngữ tự nhiên – Những điều cần biết Xử lí ngôn ngữ tự nhiên – Những điều cần biết 08/12/2016 1040 Xử lý ngôn ngữ tự nhiên (NLP) là một lĩnh vực liên ngành nghiên cứu sự tương tác giữa máy tính và ngôn ngữ tự nhiên của con người. Mục tiêu của lĩnh vực này là làm cho máy tính thực hiện hiệu quả những nhiệm vụ liên quan đến ngôn ngữ của con người như giao tiếp giữa người và máy, cải thiện hiệu quả giao tiếp giữa người với người, hoặc đơn giản là nâng cao hiệu quả xử lý văn bản và lời nói. X ử lý ngôn ngữ tự nhiên (Natural Language Processing) – Khái niệm Xử lý ngôn ngữ tự nhiên (NLP) là một nhánh của Trí tuệ nhân tạo, tập trung vào việc nghiên cứu sự tương tác giữa máy tính và ngôn ngữ tự nhiên của con người. Mục tiêu của lĩnh vực này là giúp máy tính hiểu và thực hiện hiệu quả những nhiệm vụ liên quan đến ngôn ngữ của con người như: tương tác giữa người và máy, cải thiện hiệu quả giao tiếp giữa con người với con người, hoặc đơn giản là nâng cao hiệu quả xử lý văn bản và lời nói. Xử lý ngôn ngữ tự nhiên ra đời từ những năm 1940, với rất nhiều công trình nghiên cứu theo hai hướng chính là: 1) ô-tô-mát (automaton) và các mô hình xác suất (probabilistic models) vào những năm 1950; 2) các phương pháp dựa trên ký hiệu (symbolic) và các phương pháp ngẫu nhiên (stochastic) vào những năm 1970. Giai đoạn tiếp theo (1970-1983) chứng kiến sự bùng nổ trong nghiên cứu về xử lý tiếng nói và ngôn ngữ. Ngày nay với sự phát triển nhanh chóng, học máy (machine learning) đã trở thành trung tâm của phần lớn các lĩnh vực thuộc khoa học máy tính, bao gồm xử lý ảnh và thị giác máy tính (computer vision), tin sinh học (bioinformatics), các hệ tư vấn (recommender systems), kỹ nghệ phần mềm, và cả xử lý ngôn ngữ tự nhiên. Những khó khăn trong lĩnh vực xử lý ngôn ngữ tự nhiên Xử lý ngôn ngữ tự nhiên liên quan tới tương tác giữa máy tính và ngôn ngữ của con người. Ngôn ngữ tự nhiên xuất phát từ cảm xúc, vì thế thường không có quy luật hay tuân thủ theo tính hợp lí logic, kể cả về mặt cú pháp, ngữ nghĩa, và diễn đạt ngôn từ. Nó có tính nhập nhằng cao ở tất cả các mức, bao gồm mức từ vựng, mức cú pháp, mức ngữ nghĩa và mức văn bản. Ta nói rằng ngôn ngữ là nhập nhằng nếu có nhiều cấu trúc ngôn ngữ khác nhau phù hợp với nó. Sự nhập nhằng của ngôn ngữ tự nhiên khiến việc xử lý ngôn ngữ tự nhiên trên máy tính trở nên khó khăn. Hãy cùng xem xét những ví dụ sau đây: Ví dụ 1: They  book  that hotel. (S1) They read that  book . (S2) Đầu tiên, từ  book  là nhập nhằng về mặt từ loại.  Book  có thể là một động từ (trong câu S1) hoặc một danh từ (trong câu S2) tùy thuộc vào ngữ cảnh xuất hiện của nó. Hiện tượng này gây khó khăn cho bài toán gán nhãn từ loại, một bước trong xử lý cú pháp. Không chỉ vậy,  book  cũng nhập nhằng về mặt ngữ nghĩa.  Book  có thể là một hành động đặt hàng thứ gì đó (trong câu S1) hoặc có thể là một văn bản viết được xuất bản dưới dạng in ấn hay điện tử (trong câu S2). Hiện tượng này gây khó khăn cho bài toán xác định nghĩa của từ, là một bước trong xử lý ngữ nghĩa. Ví dụ 2: A computer understands you like your mother.  (S3) Hình 1: Một ví dụ của sự không rõ ràng ở góc độ cú pháp Ở góc độ ngữ pháp, câu này có thể được giải thích theo hai cây cú pháp như trên Hình 1. Những cấu trúc khác nhau dẫn đến những cách hiểu khác nhau:  “a computer understands you like your mother does”  hoặc  “a computer understands that you like your mother”.   Hiện tượng này gây khó khăn cho cả hai bài toán là phân tích cú pháp và phân tích ngữ nghĩa. Ví dụ 3: “ I  voted for  Nader  because  he  was most aligned with  my  value,”  she  said.  (S4) Đây là một ví dụ của phép đồng tham chiếu, trong đó “ I ”, “ my ”, và “ she ” cùng đề cập đến một chủ thể, “ Nader ” và “ he ” cùng đề cập đến một chủ thể. Một số lý do khác khiến cho việc xử lý ngôn ngữ tự nhiên trở nên khó khăn có thể là: Ngôn ngữ tự nhiên sử dụng ngữ cảnh một cách phức tạp và tinh tế để truyền đạt ý nghĩa. Ngôn ngữ tự nhiên thường gây nhầm lẫn. Ngôn ngữ tự nhiên liên quan tới suy luận về thế giới. Ngôn ngữ tự nhiên là một phần quan trọng trong việc tương tác giữa con người với nhau (một hệ thống mang tính xã hội). Những bài toán cơ bản trong NLP Xử lý ngôn ngữ tự nhiên bao gồm hiểu ngôn ngữ tự nhiên (Natural Language Understanding – NLU) và sinh ngôn ngữ tự nhiên (Natural Language Generation – NLG). Trong đó, hiểu ngôn ngữ tự nhiên (NLU)bao gồm 4 bước chính sau đây: Phân tích hình vị:  là sự nhận biết, phân tích, và miêu tả cấu trúc của những hình vị trong một ngôn ngữ cho trước và các đơn vị ngôn ngữ khác, như từ gốc, biên từ, phụ tố, từ loại,… Có hai loại bài toán điển hình trong phần này, bao gồm bài toán tách từ (word segmentation) và gán nhãn từ loại (POS). Phân tích cú pháp:  là quy trình phân tích một chuỗi các biểu tượng, ở dạng ngôn ngữ tự nhiên hoặc ngôn ngữ máy tính, tuân theo văn phạm hình thức. Văn phạm hình thức thường dùng trong phân tích cú pháp của ngôn ngữ tự nhiên bao gồm Văn phạm phi ngữ cảnh (Context-free grammar – CFG), Văn phạm danh mục kết nối (Combinatory categorial grammar – CCG), và Văn phạm phụ thuộc (Dependency grammar – DG). Đầu vào của quá trình phân tích là một câu gồm một chuỗi từ và nhãn từ loại của chúng, và đầu ra là một cây phân tích thể hiện cấu trúc cú pháp của câu đó. Các thuật toán phân tích cú pháp phổ biến bao gồm CKY, Earley, Chart, và GLR. Phân tích ngữ nghĩa:  là quá trình liên hệ cấu trúc ngữ nghĩa, từ cấp độ cụm từ, mệnh đề, câu và đoạn đến cấp độ toàn bài viết, với ý nghĩa độc lập của chúng. Nói cách khác, việc này nhằm tìm ra ngữ nghĩa của đầu vào ngôn từ. Phân tích ngữ nghĩa bao gồm hai mức độ: Ngữ nghĩa từ vựng biểu hiện các ý nghĩa của những từ thành phần, và phân biệt nghĩa của từ; Ngữ nghĩa thành phần liên quan đến cách thức các từ liên kết để hình thành những nghĩa rộng hơn. Phân tích diễn ngôn:  Ngữ dụng học là môn nghiên cứu về mối quan hệ giữa ngôn ngữ và ngữ cảnh sử dụng (context-of-use). Ngữ cảnh sử dụng bao gồm danh tính của người hoặc vật, và vì thế ngữ dụng học bao gồm những nghiên cứu về cách ngôn ngữ được dùng để đề cập (hoặc tái đề cập) tới người hoặc vật. Ngữ cảnh sử dụng bao gồm ngữ cảnh diễn ngôn, vì vậy ngữ dụng học cũng bao gồm những nghiên cứu về cách thức cấu tạo nên diễn ngôn, và cách người nghe hiểu người đang đối thoại với mình. Khía cạnh thứ hai của NLP là sinh ngôn ngữ tự nhiên (NLG). Đây là một nhiệm vụ trong quá trình xử lý ngôn ngữ tự nhiên trong việc sinh ra ngôn ngữ tự nhiên từ một hệ thống máy biểu diễn như một cơ sở tri thức hoặc một dạng biểu diễn logic. NLG đóng vai trò quan trọng trong rất nhiều ứng dụng NLP, bao gồm sinh hội thoại, tương tác người – máy, dịch thuật máy, và tóm tắt văn bản tự động. Một số ứng dụng của NLP Truy xuất thông tin  (Information Retrieval – IR) có nhiệm vụ tìm các tài liệudưới dạng không có cấu trúc (thường là văn bản) đáp ứng nhu cầu về thông tin từ những nguồn tổng hợp lớn. Những hệ thống truy xuất thông tin phổ biến nhất bao gồm các công cụ tìm kiếm như Google, Yahoo, hoặc Bing search. Những công cụ này cho phép tiếp nhận một câu truy vấn dưới dạng ngôn ngữ tự nhiên làm đầu vào và cho ra một danh sách các tài liệu được sắp xếp theo mức độ phù hợp. Trích chọn thông tin  (Information Extraction) nhận diện một số loại thực thể được xác định trước, mối quan hệ giữa các thực thể và các sự kiện trong văn bản ngôn ngữ tự nhiên. Khác với truy xuất thông tin trả về một danh sách các văn bản hợp lệ thì trích chọn thông tin trả về chính xác thông tin mà người dùng cần. Những thông tin này có thể là về con người, địa điểm, tổ chức, ngày tháng, hoặc thậm chí tên công ty, mẫu sản phẩm hay giá cả. Trả lời câu hỏi  (QA) có khả năng tự động trả lời câu hỏi của con người ở dạng ngôn ngữ tự nhiên bằng cách truy xuất thông tin từ một tập hợp tài liệu. Một hệ thống QA đặc trưng thường bao gồm ba mô đun: Mô đun xử lý truy vấn (Query Processing Module) – tiến hành phân loại câu hỏi và mở rộng truy vấn; Mô đun xử lý tài liệu (Document Processing Module) – tiến hành truy xuất thông tin để tìm ra tài liệu thích hợp; và Mô hình xử lý câu trả lời (Answer Processing Module) – trích chọn câu trả lời từ tài liệu đã được truy xuất. Tóm tắt văn bản tự động  là bài toán thu gọn văn bản đầu vào để cho ra một bản tóm tắt ngắn gọn với những nội dung quan trọng nhất của văn bản gốc. Có hai phương pháp chính trong tóm tắt, là phương pháp trích xuất (extractive) và phương pháp tóm lược ý (abstractive). Những bản tóm tắt trích xuất được hình thành bằng cách ghép một số câu được lấy y nguyên từ văn bản cần thu gọn. Những bản tóm lược ý thường truyền đạt những thông tin chính của đầu vào và có thể sử dụng lại những cụm từ hay mệnh đề trong đó, nhưng nhìn chung được thể hiện ở ngôn ngữ của người tóm tắt. Dịch máy  (Machine translation – MT) là việc sử dụng máy tính để tự động hóa một phần hoặc toàn bộ quá trình dịch từ ngôn ngữ này sang ngôn ngữ khác. Các phương pháp dịch máy phổ biến bao gồm dịch máy dựa trên ví dụ (example-based machine translation – EBMT), dịch máy dựa trên luật (rule-based machine translation – RBMT), và dịch máy thống kê (statistical machine translation – SMT). Những nghiên cứu gần đây tập trung vào dịch máy thống kê bởi nhiều ưu điểm của nó so với các phương pháp khác. Dịch dựa trên từ (word-based translation), dịch dựa trên cú pháp (syntax-based translation), dịch dựa trên cụm từ (phrase-based translation), và dịch dựa trên cụm từ phân cấp (hierarchical phrase-based translation) là những mô hình dịch máy thống kê thành công nhất. Tham khảo: ACL Anthology: A Digital Archive of Research Papers in Computational Linguistics.  http://aclweb.org/anthology// Daniel Jurafsky, James H. Martin.  Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics . 2 nd  edition. Prentice-Hall, 2009. Christopher Manning and Hinrich Schütze.  Foundations of Statistical Natural Language Processing . MIT Press, 1999. Christopher Manning, Hinrich Schütze, and Prabhakar Raghavan.  Introduction to Information Retrieval , Cambridge University Press, 2008. Box – About Author: Mr. Ngo Xuan Bach – PhD at Japan  Advanced Institute of Science and Technology The research Interests: Statistical NLP, Legal Text Processing, Discourse Processing, Paraphrasing, Sentiment Analysis, Recommender Systems, Machine Learning. He is the author of the book “A Joint Model for Vietnamese Part-of-Speech Tagging Using Dual Decomposition” and dozens of international journal article. – BachNX – FPT Software –   TAGS nlp natural language processing Facebook Twitter Tin trước Javascript module pattern Tin sau Top 10 vấn đề IT có thể giải quyết nhờ LepideMigrator for Documents Please enter your name here You have entered an incorrect email address! Please enter your email address here Tin liên quan Vấn đề tách từ trong văn bản tiếng Việt – Phần 1 08/12/2016 Vấn đề tách từ trong văn bản tiếng Việt – Phần 2 08/12/2016 Nóng cùng Micro-Service tại Vietnam Web Summit 2016 12/12/2016 Công nghệ Kinh doanh thông minh – Kỳ 1 08/12/2016 Những cuốn sách về Java bạn không nên bỏ qua 16/02/2017 Phạm Quang Việt: “CEO mà có lúc trong túi không đủ... 13/10/2017 Ban công nghệ tập đoàn FPT tuyển dụng chuyên gia phần... 19/10/2017 Giải quyết ùn tắc giao thông bằng hệ thống iBus 18/10/2017 Chuyên trang về công nghệ của tập đoàn FPT, cung cấp các thông tin về những công nghệ mới, công nghệ đặc thù, các xu hướng công nghệ mới nhất cho cộng đồng công nghệ. Ban công nghệ FPT Địa chỉ: Tầng 4, Tòa Nhà FPT,  Duy Tân, Cầu Giấy, Hà Nội  Email:  Techinsight@fpt.com.vn Điện thoại: (84-4) 7300 7300 - 48825 Edit with Live CSS Save Write CSS OR LESS and hit save. CTRL + SPACE for auto-complete.",
          "relevance": "1",
          "title": "Xử lí ngôn ngữ tự nhiên – Những điều cần biết",
          "url": "https://tech.fpt.com.vn/xu-li-ngon-ngu-tu-nhien-nhung-dieu-can-biet/"
        },
        {
          "content": " Log In VNLP: bộ công cụ Xử lý ngôn ngữ tự nhiên cho tiếng Việt hacker news datascientist bigdata Karmi_Phuc \n           (Karmi Phúc)\n            \n             2016-03-11 09:40:44 UTC\n            #1 Trước khi biết về VNPL thì mình sẽ nói trước về ứng dụng của bộ công cụ này. Baomoi  thì chắc nhiều bạn cũng biết hoặc đang sử dụng hàng ngày rồi. Nói ngắn gọn thì  Baomoi  là một aggregator tổng hợp tin từ các tờ báo khác rồi phân tích, gom nhóm thêm cho các nguồn tin đó. Bộ máy bên trong của  Baomoi  được tạo nên bởi ePI LAB, dựa trên bộ công cụ VNLP \"cây nhà lá vườn,\" giúp xử lý ngôn ngữ tự nhiên tiếng Việt. Ngoài ra, ứng dụng hay ho nữa của VNLP chính là  Social Listening . Hiện tại BitBucket Repo này không còn được cập nhật thường xuyên, nhưng những gì mà nó cung cấp như hiện giờ cũng đủ để giúp ích cho các bạn làm trong ngành Big Data hay Data Analytics: https://bitbucket.org/epilab/vnlp/wiki/Home Home Categories FAQ/Guidelines Terms of Service Privacy Policy Powered by  Discourse , best viewed with JavaScript enabled  Đăng ký học C++ qua Videos  Học C++ Free? Click  Blog  Dạy Nhau Học  Tự Học Lập Trình",
          "relevance": "0",
          "title": "VNLP: bộ công cụ Xử lý ngôn ngữ tự nhiên cho tiếng Việt",
          "url": "https://daynhauhoc.com/t/vnlp-bo-cong-cu-xu-ly-ngon-ngu-tu-nhien-cho-tieng-viet/21940"
        },
        {
          "content": "Gamek Kenh14 Cafebiz Mobile Điện thoại Máy tính bảng INTERNET Digital Marketing Media KHÁM PHÁ Lịch sử Tri thức TRÀ ĐÁ CÔNG NGHỆ Tản mạn Ý tưởng sáng tạo TIN ICT THỦ THUẬT Sống APPS - GAMES ĐỒ CHƠI SỐ VIDEO Mobile Tin ICT Internet Khám phá Video Trang chủ › Tin ICT \r\nKỹ sư trưởng Google Translate đến Việt Nam chia sẻ về công nghệ xử lý ngôn ngữ tự nhiên và máy học\r\n Ngocmiz  ,  Theo  Trí Thức Trẻ Bình luận  0 Chia sẻ  Trí tuệ nhân tạo dự đoán ông Trump thắng cử, nổi danh hơn Obama Xem trí tuệ nhân tạo của MIT biến những hình ảnh bình thường trở nên ma mị đáng sợ như thế nào Tại Việt Nam, nhiều kỹ sư nghiên cứu, công ty khởi nghiệp và cả những tập đoàn lớn như Viettel, VNPT, VNG, VCCorp,… cũng đang quan tâm tới công nghệ này như một xu thế phát triển tất yếu bởi khả năng ứng dụng vào nhiều lĩnh vực khác nhau. Vào tối ngày 28/10 tại UP Co-working Space, Hà Nội, Cinnamon AI Labs đã hợp tác cùng Airpoli và UP đưa một buổi chia sẻ chuyên sâu tới cộng đồng nghiên cứu công nghệ trí tuệ nhân tạo với chủ đề: Ứng dụng công nghệ máy học (machine learning) vào xử lý ngôn ngữ tự nhiên. Vị diễn giả đặc biệt đến từ Google chính là Keith Stevens – kỹ sư trưởng của Google Translate và hiện đang cùng một nhóm kỹ sư của mình làm việc tại Google Japan, Nhật Bản. Kỹ sư Keith Stevens. Keith Stevens là kỹ sư trưởng của Google Translate, một trong những sản phẩm hàng đầu về công nghệ xử lý ngôn ngữ tự nhiên và máy học. Keith đã tốt nghiệp Tiến sĩ tại Đại học California, chuyên ngành Khoa học máy tính và tính toán ngôn ngữ học. Trong 6 năm tại Google, Keith cùng với đội ngũ xây dựng Google Translate như ta sử dụng ngày nay. Ngoài ra, Keith từng là giảng viên tại Đại học California trong 3 năm. Hiện tại, Keith đã và đang tư vấn về kỹ thuật cho một số sản phẩm sử dụng công nghệ xử lý ngôn ngữ tự nhiên của Việt Nam như ứng dụng học ngoại ngữ - Airpoli. Các khán giả tham gia chương trình Công nghệ xử lý ngôn ngữ tự nhiên và công nghệ máy học đang phát triển nhanh chóng. Ngày nay, có nhiều công ty khởi nghiệp trên thế giới xây dựng rô-bốt trả lời tự động và ứng dụng chúng vào nhiều lĩnh vực trong cuộc sống. Theo báo cáo của CBInsights, chỉ tính riêng 7 tháng đầu năm 2016, có 15 công ty khởi nghiệp xây dựng công nghệ robot trả lời tự động đã nhận được đầu tư. Còn tại Việt Nam, nhiều kỹ sư nghiên cứu, công ty khởi nghiệp và cả những tập đoàn lớn như Viettel, VNPT, VNG, VCCorp,… cũng đang quan tâm tới công nghệ này như một xu thế phát triển tất yếu bởi khả năng ứng dụng vào nhiều lĩnh vực như: Lĩnh vực tài chính : Trợ lý ảo tự động trò chuyện với khách hàng về chỉ số chứng khoán hay trợ lý thực hiện các truy xuất thông tin cơ bản Lĩnh vực y tế : Trợ lý ảo tự động tư vấn bệnh, tư vấn địa chỉ phòng khám theo triệu chứng bệnh của khách hàng, nhắc lịch uống thuốc, sắp xếp lịch khám và tài khám Lĩnh vực tuyển dụng, nhân sự : Trợ lý ảo sắp xếp lịch họp, lịch phỏng vấn, hồ sơ ứng viên; Lĩnh vực dịch vụ bán hàng : Trợ lý ảo tự động đặt vé xem phim, hòa nhạc, vé máy bay; Lĩnh vực giáo dục : Trợ giảng ảo hỗ trợ luyện tập ngoại ngữ. Trong buổi chia sẻ, Keith đã giới thiệu mô hình xây dựng Google Translate dựa trên nền tảng công nghệ học sâu (Deep Learning) và quá trình xử lý ngôn ngữ tự nhiên (Natural Language Processing). Kĩ sư trưởng của Google Translate cũng chia sẻ những ứng dụng thực tế khác của nền tảng công nghệ này, đồng thời giới thiệu các dự án tiềm năng phát triển trong tương lai của Google Translate. Keith nhận định công nghệ trí tuệ nhân tạo là xu hướng tất yếu của sự phát triển xã hội, bằng chứng là sự tham gia và cạnh tranh quyết liệt của những ông lớn như Facebook, Google, IBM, Yahoo, Intel hay Apple. Trong đó Google, ngoài việc tự đầu tư nghiên cứu công nghệ, cũng đã có 11 thương vụ mua lại các dự án nghiên cứu về trí tuệ nhân tạo tính đến nay. Điển hình như năm 2014, Google thâu tóm công ty công nghệ Deepmind của Anh với giá 600 triệu đô-la Mỹ, và giờ đây Google DeepMind đã trở nên nổi tiếng với việc đánh bại địch thủ cờ vây thế giới. Bằng kỹ năng của một giảng viên Đại học, Keith đã có những lời khuyên bổ ích tới các bạn kỹ sư trẻ của Việt Nam về phương pháp học tập và nghiên cứu chuyên sâu về trí tuệ nhân tạo. Keith chia sẻ: “Hiện nay trên thế giới có rất nhiều mô hình công nghệ với những điểm mạnh và yếu khác nhau. Trước khi đi sâu vào nghiên cứu một hướng cụ thể, các kỹ sư Việt Nam nên tìm hiểu và đánh giá phương pháp nào có tính ứng dụng và giải quyết tốt nhất bài toàn của mình.” Buổi chia sẻ của Keith Stevens có sự tham gia của các doanh nghiệp có sản phẩm sử dụng công nghệ trí tuệ nhân tạo, các kỹ sư công nghệ đến từ những tập đoàn lớn nhưu VCCorp, Viettel, FPT hoặc đã từng có kinh nghiệm làm việc tại Amazon, Microsoft. Ngoài ra sự kiện không thể thiếu sự quan tâm của sinh viên, kỹ sư nghiên cứu và giảng viên đến từ trường Đại học Bách Khoa và Đại học Công Nghệ Hà Nội. Cinnamon AI Labs nhận thấy rằng những cơ hội được chia sẻ trực tiếp với các chuyên gia đầu ngành như sự kiện này sẽ là hoạt động thúc đẩy quan trọng để các kỹ sư tài năng tại Việt Nam bắt kịp với xu hướng công nghệ trên thế giới. Được thành lập bởi  Cinnamon , một công ty công nghệ đặt trụ sở tại Singapore, Cinnamon AI Labs là dự án khởi nghiệp phòng nghiên cứu và phát triển ứng dụng công nghệ trí tuệ nhân tạo tại Đông Nam Á, hoạt động với 3 mô hình chính: - Đào tạo và phát triển cộng đồng thông qua các chương trình đào tạo hợp tác với các tổ chức khác, xây dựng các chuỗi sự kiện và thúc đẩy chia sẻ từ cộng đồng - Phát triển đội ngũ chuyên gia nghiên cứu trí tuệ nhân tạo thông qua việc nhận các dự án nghiên cứu từ các công ty tại châu Á - Ươm mầm các công ty khởi nghiệp sử dụng nền tảng cốt lõi là trí tuệ nhân tạo Kỷ nguyên của Trí tuệ nhân tạo - Ngay bây giờ hoặc quá muộn Tags: google translate máy học machine learning cinnamon ai labs Xem theo ngày Ngày 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 Tháng Tháng 1 Tháng 2 Tháng 3 Tháng 4 Tháng 5 Tháng 6 Tháng 7 Tháng 8 Tháng 9 Tháng 10 Tháng 11 Tháng 12 Năm 2017 2016 2015 2014 2013 Xem Nokia quay lại với Microsoft để tiếp tục \"kết nối mọi người\" Chia tay không phải là hết, Nokia vừa tái hợp với người cũ Microsoft để phát triển một dự án mang đậm tính nhân văn. Gặp gỡ công ty công nghệ muốn khiến bạn ngày càng nghiện điện thoại hơn nữa Nostalgia SpiderumTrong bối cảnh người người muốn cai nghiện điện thoại, thì công ty này thậm chí còn muốn khiến bạn nghiện hơn, tuy nhiên, theo một cách đáng trân trọng. Lỗ nặng nhưng vẫn IPO thành công rực rỡ và được định giá 5,3 tỷ... 21 giờ trước Từ Uber, Grab, Bitcoin đến câu chuyện về kinh tế số ở Việt Nam... 21 giờ trước Sản phẩm điện tử Hàn Quốc, Trung Quốc, Đài Loan áp đảo thị... 22 giờ trước CEO thung lũng Silicon vừa bị bắt vì giết người và lạm dụng tình... 22 giờ trước TIN NỔI BẬT Thiết bị phần cứng của Google chỉ là \"Con ngựa thành Troy\" Đây là nhà sản xuất smartphone lớn thứ hai thế giới sau Samsung mà có thể bạn chưa từng biết tới Không cần phải tốn thời gian cài đặt lại, Windows 10 Fall Creators sẽ cung cấp cho bạn một giải pháp hay hơn rất nhiều Đè bẹp cả Samsung lẫn Google, Apple đang thống trị \"bãi tha ma\" smartwatch/wearable như thế nào? Hãy biến thanh điều hướng ảo nhàm chán của Android trở nên sinh động hơn với hiệu ứng ảnh động vui mắt mà không cần root Video Mobile Tin ICT Internet Khám phá Trà đá công nghệ Thủ thuật Apps - Game Đồ chơi số \r\nChịu trách nhiệm quản lý nội dung: Bà Nguyễn Bích Minh \r\nHà Nội: Tầng 20, Tòa nhà Center Building - Hapulico Complex, Số 1 Nguyễn Huy Tưởng, Thanh Xuân, Hà Nội. \r\nEmail:  info@genk.vn \r\nĐiện thoại: 024.73095555, máy lẻ 62374 \r\nVPĐD tại TP.HCM: Tầng 4, Tòa nhà 123\r\n \r\nVõ Văn Tần, Phường 6, Quận 3, Tp. Hồ Chí Minh\r\n © Copyright 2010 - 2017 - Công ty Cổ phần VCCorp \r\nTầng 17, 19, 20, 21 Toà nhà Center Building - Hapulico Complex, Số 1 Nguyễn Huy Tưởng, Thanh Xuân, Hà Nội. \r\nTrang tin điện tử trên internet: Giấy phép số 460/GP-TTĐT do Sở Thông tin và Truyền thông Hà Nội cấp ngày 03/02/2016 Liên hệ quảng cáo \r\nHotline hỗ trợ quảng cáo: 0942 86 11 33  \r\nEmail:  giaitrixahoi@admicro.vn \r\nHỗ trợ & CSKH: Admicro\r\n  \r\n \r\nAddress: Tầng 20, Tòa nhà Center Building - Hapulico Complex, Số 1 Nguyễn Huy Tưởng, Thanh Xuân, Hà Nội.",
          "relevance": "1",
          "title": "Kỹ sư trưởng Google Translate đến Việt Nam chia sẻ về công nghệ xử lý ngôn ngữ tự nhiên và máy học",
          "url": "http://genk.vn/ky-su-truong-google-translate-den-viet-nam-chia-se-ve-cong-nghe-xu-ly-ngon-ngu-tu-nhien-va-may-hoc-20161031115316122.chn"
        },
        {
          "content": "Ông Xuân Hồng Chia sẻ kiến thức và thông tin về Machine learning Menu Skip to content Data Science Big Data Deep learning Exploratory Data Analysis Getting and cleaning data Machine learning Lập trình Python Spark R Weka Kiến thức Toán Statistical Inference Xử lý ngôn ngữ tự nhiên – Natural Language Processing (NLP) Chia sẻ Dự án About Giới thiệu các công cụ Xử lý ngôn ngữ tự nhiên Tháng Hai 6, 2016 Tháng Hai 24, 2017 Ông Xuân Hồng 4 phản hồi NLP tools Intuitive interfaces easy to plug in your own input corpus/datastream (trivial streaming API) easy to extend with other Vector Space algorithms (trivial transformation API) Facebook Twitter Google LinkedIn Like this: Số lượt thích Đang tải... Liên quan Điều hướng bài viết ←  Các thuật ngữ trong Xử lý ngôn ngữ tự nhiên Dành cho các bạn sắp bước chân vào lĩnh vực Công nghệ thông tin  → Mr Ping nói: \n\t\t\t\t\t\t\t\tTháng Ba 31, 2017 lúc 7:09 sáng\t\t\t\t\t\t\t Chào anh, Theo anh sử dụng công cụ gì để chuyển đổi từ chuỗi quan điểm tiếng việt người dùng  sang vector để hỗ trợ phân loại SVM bằng Weka. Số lượt thích Số lượt thích Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Ba 31, 2017 lúc 7:12 sáng\t\t\t\t\t\t\t Hi, bạn có thể dùng TF-IDF  https://ongxuanhong.wordpress.com/2017/01/16/truy-van-van-ban-document-retrieval/ Số lượt thích Liked by  1 person Phản hồi Mr Ping nói: \n\t\t\t\t\t\t\t\tTháng Tư 2, 2017 lúc 10:09 sáng\t\t\t\t\t\t\t anh có thể chỉ dẫn em được không. Hiện em đang dùng weka để phân lớp các quan điểm bằng tiếng việt. Em không biết cách convert string to word vector như thế nào để weka nhận theo các từ ví dụ :”tuyệt vời” mà không phải “tuyệt” và “vời” Số lượt thích Số lượt thích Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Tư 2, 2017 lúc 12:14 chiều\t\t\t\t\t\t\t Bài toán này lại liên quan đến word tokenization. Trong Tv, ta thường lập ngữ liệu từ vựng như “tuyet_voi” để training. Nếu bỏ qua bài toán này ta có thể truyền trực tiếp “tuyet_voi” vào bộ convert vector Số lượt thích Số lượt thích Phản hồi Nhập bình luận của bạn tại đây... Mời bạn điền thông tin vào ô dưới đây hoặc kích vào một biểu tượng để đăng nhập: Thư điện tử  (bắt buộc) (Địa chỉ của bạn được giấu kín) Tên  (bắt buộc) Trang web  Bạn đang bình luận bằng tài khoản WordPress.com  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Twitter  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Facebook  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Google+  (  Đăng xuất  /  Thay đổi  ) Hủy bỏ Connecting to %s Notify me of new comments via email. Thông báo cho tôi bằng email khi có bài đăng mới. Bạn Không Thể Sống Một Mình Nếu ngày mai tớ trở thành người nổi tiếng. Gặp tớ giữa đường bạn đừng ngại tớ nghe. Cứ nhìn tớ như ngày xưa bạn thấy. Đừng nên nghĩ tớ quá xa vời.  Nếu một ngày tớ là kẻ ăn xin. Thì bố thí cho tớ vài kỷ niệm. Dù rất nhỏ nhưng đừng nên khinh tớ. Vì tớ vẫn là tớ của ngày xưa.  Nếu ngày mai tớ chẳng biết đường về. Hãy chỉ giùm tớ con đường đi đúng đắn. Đừng để tớ sa chân nơi bùn nhớp. Lúc rút chân lên thì đã bẩn mất rồi.  Nếu ngày mai tớ gặp bạn giữa đường. Thì ta sẽ gặp nhau và cười nhé. Đừng nên trách và đừng nên đánh mất. Đôi khi có một số người lướt qua cuộc đời bạn và ngay tức khắc bạn nhận ra rằng sự có mặt của họ ý nghĩa như thế nào. Họ đã dạy bạn những bài học, đã giúp bạn nhận ra giá trị của chính mình hoặc trở thành con người mà bạn từng mơ ước. Có lẽ bạn sẽ không biết được những con người này từ đâu đến ( bạn cùng phòng, người hàng xóm, vị giáo sư, người bạn mất liên lạc từ lâu hay thậm chí là một người hoàn toàn xa lạ ). Nhưng khi bạn thờ ơ với họ, hãy nhớ rằng trong từng khoảnh khắc họ sẽ ảnh hưởng rất sâu sắc đến cuộc đời bạn.  …Cuộc sống không hề làm khó bạn mà chính bạn đã tự làm khó mình bằng cách đặt ra những câu hỏi đại loại như: “Tại sao lại thế này?… Tại sao lại thế kia?…”. Không ai có thể trả lời giúp bạn mà chính bạn phải tự đi tìm lời giải đáp cho mình. Hãy luôn nghĩ về người khác trước khi nghĩ đến bản thân, chắc chắn lúc đó bạn sẽ hài lòng với câu trả lời của chính mình. Và đừng quên luôn đặt câu hỏi: “Mình đã làm được gì?” trước khi tự hỏi: “Mình đã nhận được gì?” nhé! Tôi tin là bạn sẽ thành công!   Thật ra trên đời, con người luôn cần có nhau để giải quyết rất nhiều vấn đề. Nhờ những lúc qua lại giúp đỡ đó mà hoá giải bao muộn phiền, nảy sinh các tình cảm, cùng trải nghiệm biết bao điều thú vị, hiểu biết thêm nhiều tri thức mới. Đây chính là cuộc sống! Vì thế hãy trân trọng những người đang “làm phiền” bên cạnh bạn nhé! Đừng để đến khi mất đi rồi mới nghẹn ngào lại bảo “giá như” … ĐĐ. GS. Thích Phước Tiến \n__(())__ Namo Bụt Sakyamuni Nhận Email khi có bài viết mới Liên hệ Email (bắt buộc) Comment (bắt buộc) Top bài viết Bài viết mới FAQ: Big data cho người mới bắt đầu Tháng Mười 2, 2017 Làm việc với Spark DataFrames – Truy vấn nâng cao (Subqueries, Cube, Rollup) Tháng Chín 15, 2017 Thống kê ứng dụng 3: Các vấn đề trong thống kê Tháng Chín 14, 2017 Thống kê ứng dụng 2: Suy luận thống kê Tháng Chín 12, 2017 Thống kê ứng dụng 1: Quan sát dữ liệu Tháng Chín 7, 2017 AI, Machine Learning, Deep Learning phân biệt như thế nào cho đúng Tháng Chín 4, 2017 Information extraction – Bài toán rút trích thông tin trong văn bản Tháng Tám 28, 2017 Bắt đầu nghiên cứu big data từ đâu và như thế nào Tháng Tám 3, 2017 Data Science – Mỏ vàng của Kỉ nguyên số Tháng Tám 3, 2017 SMA 2017 – Lý thuyết ra quyết định Tháng Sáu 17, 2017 Big Data Chia sẻ Data Science Deep learning Dự án Exploratory Data Analysis Getting and cleaning data Kiến thức Lập trình Machine learning Python R Spark Statistical Inference Toán Weka Xử lý ngôn ngữ tự nhiên - Natural Language Processing (NLP) This slideshow requires JavaScript. Thư viện Tháng Mười 2017  (1) Tháng Chín 2017  (5) Tháng Tám 2017  (3) Tháng Sáu 2017  (3) Tháng Năm 2017  (3) Tháng Ba 2017  (1) Tháng Một 2017  (3) Tháng Mười Hai 2016  (1) Tháng Mười Một 2016  (2) Tháng Mười 2016  (2) Tháng Chín 2016  (1) Tháng Tám 2016  (2) Tháng Bảy 2016  (2) Tháng Năm 2016  (1) Tháng Tư 2016  (1) Tháng Ba 2016  (2) Tháng Hai 2016  (2) Tháng Một 2016  (1) Tháng Mười Hai 2015  (6) Tháng Mười Một 2015  (5) Tháng Mười 2015  (6) Tháng Chín 2015  (11) Tháng Tám 2015  (16) Tháng Bảy 2015  (25) Tháng Sáu 2015  (4) Thống kê 360,709 hits Tháng Hai 2016 H B T N S B C « Th1   Th3 » 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29   Data Science Lập trình Kiến thức Chia sẻ Dự án About Tạo một website miễn phí hoặc 1 blog với WordPress.com. Post to Hủy bỏ %d  bloggers like this:",
          "relevance": "0",
          "title": "Menu",
          "url": "https://ongxuanhong.wordpress.com/2016/02/06/gioi-thieu-cac-cong-cu-xu-ly-ngon-ngu-tu-nhien/"
        },
        {
          "content": "\n\t\tKhóa học Đổi mới toàn diện Bản thân cùng Tư duy ĐỘT PHÁ 2016 - Ts. Lê Thẩm Dương.  Đăng ký hôm nay nhận ngay Coupon Giảm 50% Danh mục   Tài Liệu Tham Khảo Kinh Doanh-Marketing Kinh Tế - Quản Lý Tài Chính - Ngân Hàng Công Nghệ Thông Tin Ngoại Ngữ Kỹ Thuật - Công Nghệ Khoa Học Tự Nhiên Khoa Học Xã Hội Văn Hoá - Nghệ Thuật Biểu mẫu - Văn bản Sáng kiến kinh nghiệm Ôn thi ĐH-CĐ Giáo án điện tử Bài giảng điện tử Đề thi - Kiểm tra Bài tập SGK Bộ Sưu Tập Luận Văn & Đề Tài Mẫu Slide Powerpoint Khóa Học Khóa Học Online Tài Liệu Phổ Thông Giáo Án Điện Tử Bài Giảng Điện Tử Đề Thi - Kiểm Tra Tư liệu Học phổ thông Online Chương trình Hỏi đáp Tư liệu NÂNG CẤP Đăng Nhập \n\t\t\t | \n\t\t\t Đăng Ký Chủ đề  » \n\t Luận văn Thạc sĩ Giáo dục học Luận án Tiến sĩ Kinh tế Luận văn Thạc sĩ Kinh tế Luận văn thạc sĩ kế toán   Luận văn cao học   Luyện thi THPT QG 2018 Trang Chủ \n\t\t\t\t\t   »  Luận Văn - Báo Cáo \n\t\t\t\t\t   »  Thạc sĩ - Tiến sĩ - Cao học  LUẬN VĂN:  Xử lý ngôn ngữ tự nhiên \n\t\t\t\t\t\tXử lý ngôn ngữ tự nhiên (natural language processing - NLP) là một nhánh của\ntrí tuệ nhân tạo tập trung vào các ứng dụng trên ngôn ngữ của con người. Trong trí tuệ\nnhân tạo thì xử lý ngôn ngữ tự nhiên là một trong những phần khó nhất vì nó liên quan\nđến việc phải hiểu ý nghĩa ngôn ngữ - công cụ hoàn hảo nhất của tư duy và giao tiếp.\t\t\t\t\t\t 48p chieuwindows23 01-06-2013 240 119   Download LUẬN VĂN: Tìm hiểu về xử lý ngôn ngữ tự nhiên và máy dịch. Viết chương trình mô phỏng từ điển Việt-Anh \n\t\t\t\t\t\tTham khảo luận văn - đề án 'luận văn: tìm hiểu về xử lý ngôn ngữ tự nhiên và máy dịch. viết chương trình mô phỏng từ điển việt-anh', luận văn - báo cáo, công nghệ thông tin phục vụ nhu cầu học tập, nghiên cứu và làm việc hiệu quả\t\t\t\t\t\t 70p chieuwindows23 01-06-2013 136 62   Download uận văn: Tìm hiểu về xử lý ngôn ngữ tự nhiên và viết chương trình mô phỏng sửa lỗi từ vựng trong việc sử dụng câu tiếng Anh \n\t\t\t\t\t\tTham khảo luận văn - đề án 'uận văn: tìm hiểu về xử lý ngôn ngữ tự nhiên và viết chương trình mô phỏng sửa lỗi từ vựng trong việc sử dụng câu tiếng anh', luận văn - báo cáo, công nghệ thông tin phục vụ nhu cầu học tập, nghiên cứu và làm việc hiệu quả\t\t\t\t\t\t 66p samsung_12 06-05-2013 106 40   Download Xử lý ngôn ngữ tự nhiên \n\t\t\t\t\t\tXử lý ngôn ngữ tự nhiên giới thiệu đến các bạn những vấn đề về sự tiến hóa của ngôn ngữ, cơ sở của ngôn ngữ, khả năng phát sinh, vấn đề nói và nghe, hai mô hình của giao tiếp, mô hình bản tin mã hóa, mô hình tình huống giao tiếp, giao tiếp sử dụng ngôn ngữ hình thứ,... Mời các bạn cùng tham khảo để nắm bắt nội dung chi tiết.\t\t\t\t\t\t 31p pechi1412 01-12-2015 34 13   Download Ứng dụng xử lý ngôn ngữ tự nhiên trong dịch máy - TS. Nguyễn Chí Hiếu \n\t\t\t\t\t\tỨng dụng xử lý ngôn ngữ tự nhiên trong dịch máy do TS. Nguyễn Chí Hiếu biên soạn trình bày phương pháp cải thiện chất lượng đối sánh từ trong bước tiền xử lý bằng cách chuyển đổi cấu trúc câu nguồn theo trật tự của câu đích để rút trích cụm danh từ song ngữ, đề xuất phương pháp xây dựng bảng từ và cụm từ song ngữ từ ngữ liệu song ngữ với các nét ngữ nghĩa, xác suất   xuất hiện và luật chuyển đổi... Mời các bạn cùng tham khảo.\t\t\t\t\t\t 14p pechi1412 01-12-2015 25 5   Download Ứng dụng xử lý ngôn ngữ tự nhiên trong hệ tìm kiếm thông tin trên văn bản tiếng việt \n\t\t\t\t\t\tTrong các hệ thống tìm kiếm thông tin văn bản (Text Information Retrieval System), tiến trình \nquan trọng nhất là tiến trình phân tích nội dung văn bản để xác định tập chỉ mục biểu diễn tốt \nnhất nội dung của văn bản (tiến trình lập chỉ mục - indexing). Để có thể phân tích và rút trích \nđược các chỉ mục (index term / term) tốt người ta thường ứng dụng các kết quả của lĩnh vực xử lý \nngôn ngữ tự nhiên vào tiến trình này.  ...\t\t\t\t\t\t 8p xuantruong 12-06-2009 558 132   Download Báo cáo Khoa học: Nghiên cứu phát triển công nghệ nhận dạng, tổng hợp và xử lý ngôn ngữ tiếng Việt \n\t\t\t\t\t\tMục đích của đề tài \"Nghiên cứu phát triển công nghệ nhận dạng, tổng hợp và xử lý ngôn ngữ tiếng Việt\" dưới đây là nghiên cứu khảo sát xây dựng các phương pháp hiệu quả cho tổng hợp, nhận dạng và xử lý ngôn ngữ tiếng Việt. Ba nội dung chính quan hệ chặt chẽ với nhau được nghiên cứu trong đề tài là: Nhận dạng và tổng hợp tiếng Việt, nhận dạng chữ Việt in và viết tay có hạn chế, xử lý ngôn ngữ tự nhiên tiếng Việt.\t\t\t\t\t\t 121p votinhdon91 28-08-2014 92 41   Download LUẬN VĂN: NGHIÊN CỨU XÂY DỰNG TIÊU CHUẨN BẢN RÕ TIẾNG ANH CỦA NGÔN NGỮ TỰ NHIÊN \n\t\t\t\t\t\tNhận dạng ngôn ngữ là một trong những nghiên cứu quan trọng được ứng dụng trong Internet và trong nhiều lĩnh vực xử lý ngôn ngữ tự nhiên khác như nhận dạng tiếng nói, nhận dạng chữ viết. Đặc biệt, xây dựng tiêu chuẩn bản rõ ứng dụng trong phân tích mật mã các bản mã khối ( cổ điển và hiện đại như: Des, 3-Des, AES…).\t\t\t\t\t\t 56p chieu_mua 24-08-2012 79 29   Download BÁO CÁO ĐỀ TÀI  SINH NGÔN NGỮ TỰ NHIÊN \n\t\t\t\t\t\tTrong những năm gần đây, công nghệ thông tin phát triển như vũ bão, đóng vai trò ngày càng quan trọng trong đời sống xã hội.Trí tuệ nhân tạo và đặc biệt là xử lý ngôn ngữ tự nhiên được nghiên cứu rộng rãi mang lại nhiều ứng dụng quan trọng trong đời sống. Trong đó sinh ngôn ngữ tự nhiên là một lĩnh vực nổi bật có khả năng tạo ra những hệ thống đáp ứng người sử dụng như chính ngôn ngữ con người nó ra. Chính vì những lợi ích to lớn mà nó mang lại, trong...\t\t\t\t\t\t 32p duylong2091 29-03-2013 74 29   Download Đề tài: “Tìm hiểu kỹ thuật xây dựng giao diện người dùng với cơ chế phản hồi ngôn ngữ tự nhiên” \n\t\t\t\t\t\tNgày nay những nghiên cứu về lĩnh vực xử lý ngôn ngữ tự nhiên nhằm tạo cho máy tính khả năng hiểu giao tiếp được bằng ngôn ngữ tự nhiên với con người đã không chỉ còn là lý thuyết nữa mà đã đi vào xây dựng rất nhiều ứng dụng có hiệu quả.Hơn nữa quá trình ứng dụng này còn có sự kết hợp giao thao của các nghành chuyên môn khác nhau trong lĩnh vực tin học,có các kỹ thuật ứng dụng lập trình logic,giao diện người dùng vào lĩnh xử lý ngôn ngữ tự nhiên.\t\t\t\t\t\t 153p caphe_123 17-07-2013 73 22   Download Luận văn: ìm hiểu về xử lý ngôn ngữ tự nhiên và viết chương trình mô phỏng kiểm tra lỗi từ vựng trong việc sử dụng câu tiếng Anh \n\t\t\t\t\t\tXử lý ngôn ngữ chính là xử lý thông tin khi đầu vào là “dữ liệu ngôn ngữ”\n(dữ liệu cần biến đổi), tức dữ liệu “văn bản” hay “tiếng nói”. Các dữ liệu liên quan\nđến ngôn ngữ viết (văn bản) và nói (tiếng nói) đang dần trở nên kiểu dữ liệu chính\ncon ngƣời có và lƣu trữ dƣới dạng điện tử.\t\t\t\t\t\t 68p samsung_12 06-05-2013 63 20   Download Truy vấn ngôn ngữ tự nhiên không hoàn chỉnh đối với cơ sở dữ liệu quan hệ \n\t\t\t\t\t\tNgôn ngữ tự nhiên là một trong các phương tiện truyền thông mạng nhất. Một lĩnh vực mà hệ xử lý ngôn ngữ tự nhiên tỏ ra có hiệu quả là hệ truy vấn cơ sở dữ liệu. bài báo trình bày một cách tiếp cận biến đổi các truy vấn ngôn ngữ tự nhiên không hoàn chỉnh đối với các cơ sở dữ liệu quan hệ thành SQL và sau đó cơ sở dữ liệu này có thể được tra cứu và một câu trả lờ thích hợp có thể được sinh ra.\t\t\t\t\t\t 6p thuplato 17-05-2016 26 2   Download LUẬN VĂN:SO SÁNH MỘT SỐ PHƯƠNG PHÁP HỌC MÁY CHO BÀI TOÁN GÁN NHÃN TỪ LOẠI TIẾNG VIỆT \n\t\t\t\t\t\tGán nhãn từ loại (Part-of-Speech Tagging) là một trong hai bài toán nền tảng, đóng vai trò quan trọng trong các hệ thống xử lý ngôn ngữ tự nhiên. Ở Việt Nam đã có một số nghiên cứu về bài toán này, tuy nhiên kết quả đạt được vẫn còn ở mức khiêm tốn so với nhiều ngôn ngữ khác. Việc tìm hiểu các phương pháp gán nhãn từ loại trong tiếng Anh cho thấy hướng tiếp cận dựa theo phương pháp học máy cho kết quả tốt hơn cả trong các phương pháp đã được công bố.\t\t\t\t\t\t 68p chieu_mua 26-08-2012 209 95   Download LUẬN VĂN:NHẬN BIẾT CÁC LOẠI THỰC THỂ TRONG VĂN BẢN TIẾNG VIỆT NHẰM HỖ TRỢ WEB NGỮ NGHĨA VÀ TÌM KIẾM HƯỚNG THỰC THỂ \n\t\t\t\t\t\tNhận biết các loại thực thể là một bước cơ bản trong trích chọn thông tin từ\nvăn bản và xử lý ngôn ngữ tự nhiên. Nó được ứng dụng nhiều trong dịch tự động, tóm\ntắt văn bản, hiểu ngôn ngữ tự nhiên , nhận biết tên thực thể trong sinh/y học và đặc\nbiệt ứng dụng trong việc tích hợp tự động các đối tượng, thực thể từ môi trường Web\nvào các ontology ngữ nghĩa và các cơ sở tri thức....\t\t\t\t\t\t 58p bluesky_12 26-12-2012 86 47   Download Cải tiến một số giải thuật phân tích cú pháp trong xử lý ngôn ngữ tự nhiên. \n\t\t\t\t\t\tCải tiến một số giải thuật phân tích cú pháp trong xử lý ngôn ngữ tự nhiên.  Nó gần giống nhưng không đồng nhất với khái niệm entropi nhiệt động của một hệ vật lý để đo khoảng cách tới trạng thái cân bằng (equilibrium)\r\nĐịnh luật 2 của nhiệt động học được Shanon phát biểu mở rộng thành định luật 10 của Shannon trong ngữ cảnh phi nhiệt động như sinh học, sinh thái học, kinh tế học, xã hội học và kinh tế học......\t\t\t\t\t\t 6p butmaucam 27-08-2013 49 9   Download Văn phạm nét và tiếp cận ngữ nghĩa trong xử lý ngôn ngữ tự nhiên tiếng Việt. \n\t\t\t\t\t\tVăn phạm nét và tiếp cận ngữ nghĩa trong xử lý ngôn ngữ tự nhiên tiếng Việt. Một định nghĩa mang tính triết học hơn được Louis Couffignal gợi ý năm 1956 (cũng là một nhà tiên phong của điều khiển học) gọi điều khiển học là \"Nghệ thuật đảm bảo cho hoạt động hiệu quả\". Cùng với thời gian, điều khiển học dần dần được hiểu là nghiên cứu và đưa ra những nguyên lý trừu tượng của việc tổ chức những hệ thống từ đơn giản tới phức tạp. ...\t\t\t\t\t\t 13p butmaucam 28-08-2013 36 7   Download LUẬN VĂN:PHÂN TÍCH CÂU HỎI TRONG HỆ THỐNG HỎI ĐÁP TIẾNG VIỆT \n\t\t\t\t\t\tTrong mười năm gần đây, hệ thống hỏi đáp tự động đã nhận được sự quan tâm đặc biệt của các nhà nghiên cứu, các công ty (Yahoo, Google, Microsoft, IBM…), các hội nghị lớn về trích chọn thông tin, xử lý ngôn ngữ tự nhiên (TREC, CLEF, ACL,..) và đã đạt được những kết quả nhất định. Tuy nhiên các nghiên cứu về hệ thống hỏi đáp cho tiếng Việt vẫn còn rất nhiều hạn chế.\t\t\t\t\t\t 71p chieu_mua 25-08-2012 128 53   Download LUẬN VĂN:MỞ RỘNG BỘ DỮ LIỆU HUẤN LUYỆN CHO QUÁ TRÌNH XỬ LÝ NHẬP NHẰNG NGHĨA CỦA TỪ \n\t\t\t\t\t\tTrong các chủ đề thuộc lĩnh vực xử lý ngôn ngữ tự nhiên, xử lý nhập nhằng nghĩa của từ là một chủ đề dành được nhiều sự quan tâm chú ý của những nhà nghiên cứu, phát triển, ứng dụng khoa học máy tính. Lý do là mặc dù nếu đứng một mình, xử lý nhập nhằng nghĩa của từ ít đem lại lợi ích cụ thể trong đời sống hàng ngày, nhưng nó lại có một vai trò quan trọng trong nhiều ứng dụng xử lý ngôn ngữ tự nhiên hữu ích khác như dịch máy, tìm kiếm...\t\t\t\t\t\t 51p chieu_mua 28-08-2012 68 29   Download Báo cáo nghiên cứu khoa học \" MỘT SỐ CẢI TIẾN GIẢI THUẬT EARLEY CHO VIỆC PHÂN TÍCH CÚ PHÁP TRONG XỬ LÝ NGÔN NGỮ TỰ NHIÊN \" \n\t\t\t\t\t\t Giải thuật Earley [1, 2] là một trong số những giải thuật được sử dụng để phân tích cú pháp trong xử lý ngôn ngữ tự nhiên. Nó là một giải thuật tổng quát, có thể phân tích bất kỳ văn phạm phi ngữ cảnh nào. Nhưng giải thuật này vẫn còn nhiều hạn chế cần khắc phục. Đầu tiên, Kilbury [3] đã nhận xét rằng giải thuật Earley là không hiệu quả trong xử lý ngôn ngữ tự nhiên.\t\t\t\t\t\t 20p meomeongon 06-01-2012 43 18   Download Giáo trình Nhập môn trí tuệ nhân tạo - PGS.TS. Nguyễn Quang Hoan \n\t\t\t\t\t\t(NB) Giáo trình Nhập môn trí tuệ nhân tạo giới thiệu đến các bạn những nội dung chính như sau: tổng quan khoa học trí tuệ nhân tạo, các phương pháp giải quyết vấn đề, biểu diễn tri thức và suy diễn, xử lý ngôn ngữ tự nhiên, các kỹ thuật trí tuệ nhân tạo hiện đại.\t\t\t\t\t\t 171p susuqb 17-12-2015 60 23   Download + Xem thêm  2179 Xử lý ngôn ngữ tự nhiên  khác CHỦ ĐỀ BẠN MUỐN TÌM Dẫn luận ngôn ngữ Ngôn ngữ lập trình C Phát triển ngôn ngữ cho trẻ Ngôn ngữ của bé Ngôn ngữ điện ảnh Địa lý tự nhiên Ngôn ngữ báo chí Lập trình ngôn ngữ tư duy Ngôn ngữ cơ thể trong giao tiếp Ngôn ngữ nghệ thuật Phong cách ngôn ngữ Ngôn ngữ học cấu trúc Ngôn ngữ trong giao tiếp Ngôn ngữ và văn hóa Sử dụng ngôn ngữ Ngôn ngữ học xã hội Ngôn ngữ đức Lập trình ngôn ngữ html Lập trình ngôn ngữ php Ngôn ngữ học THÔNG TIN Về chúng tôi Quy định bảo mật Thỏa thuận sử dụng Quy chế hoạt động TRỢ GIÚP Hướng dẫn sử dụng Upload tài liệu Hỏi và đáp   HỖ TRỢ KHÁCH HÀNG Liên hệ Hỗ trợ trực tuyến Liên hệ quảng cáo   Theo dõi chúng tôi Giấy phép ICP số: 670/GP-BTTTT cấp ngày 30/11/2015 Copyright © 2009-2015 TaiLieu.VN. All rights reserved. TaiLieu.VN hiển thị tốt nhất với trình duyệt Chrome, Firefox, Internet Explorer 8.",
          "relevance": "0",
          "title": "Xử lý ngôn ngữ tự nhiên",
          "url": "http://tailieu.vn/tag/xu-ly-ngon-ngu-tu-nhien.html"
        },
        {
          "content": "Đăng ký Đăng nhập Liên hệ LuanVan.net.vn - Luận văn, đồ án, tiểu luận, luận án, đề tài, đề án, chuyên đề thực tập, tốt nghiệp Thư viện luận văn, đồ án, tiểu luận, luận án, báo cáo, bài tập lớn, đề tài, đề án, chuyên đề thực tập, tốt nghiệp, thạc sĩ, tiến sĩ, cao học Trang Chủ Tài Liệu Upload Đồ án Tìm hiểu về xử lý ngôn ngữ tự nhiên và máy dịch - Viết chương trình mô phỏng từ điển Việt - Anh \r\n                    Xử lý ngôn ngữ chính là xử lý thông tin khi đầu vào là “dữ liệu ngôn ngữ” \r\n(dữ liệu cần biến đổi), tức dữ liệu “văn bản” hay “tiếng nói”. Các dữ liệu liên quan \r\nđến ngôn ngữ viết (văn bản) và nói (tiếng nói) đang dần trở nên kiểu dữ liệu chính \r\ncon người có và lưu trữ dưới dạng điện tử. Đặc điểm chính của các kiểu dữ liệu này \r\nlà không có cấu trúc hoặc nửa cấu trúc và chúng không thể lưu trữ trong các khuôn \r\ndạng cố định như các bảng biểu. Theo đánh giá của công ty Oracle, hiện có đến \r\n80% dữ liệu không cấu trúc trong lượng dữ liệu của loài người đang có [Oracle \r\nText]. Với sự ra đời và phổ biến của Internet, của sách báo điện tử, của máy tính cá \r\nnhân, của viễn thông, của thiết bị âm thanh, người người ai cũng có thể tạo ra dữ \r\nliệu văn bản hay tiếng nói. Vấn đề là làm sao ta có thể xử lý chúng, tức chuyển \r\nchúng từ các dạng ta chưa hiểu được thành các dạng ta có thể hiểu và giải thích \r\nđược, tức là ta có thể tìm ra thông tin, tri thức hữu ích cho mình\r\n                 70 trang  |  Chia sẻ:  lvbuiluyen  | Ngày: 21/10/2013  | Lượt xem: 2722  | Lượt tải: 5 Bạn đang xem nội dung tài liệu  Đồ án Tìm hiểu về xử lý ngôn ngữ tự nhiên và máy dịch - Viết chương trình mô phỏng từ điển Việt - Anh , để tải tài liệu về máy bạn click vào nút DOWNLOAD ở trên BỘ GIÁO DỤC VÀ ĐÀO TẠO \r\nTRƯỜNG…………….. \r\nLUẬN VĂN \r\nTìm hiểu về xử lý ngôn ngữ tự \r\nnhiên và máy dịch. Viết chương \r\ntrình mô phỏng từ điển Việt-Anh \r\n Đồ án tốt nghiệp \r\n 1 \r\nLời cảm ơn \r\nTrước hết em xin chân thành cảm ơn thầy giáo Ths. Vũ Mạnh Khánh, là \r\nngười đã hướng dẫn em rất nhiều trong suốt quá trình tìm hiểu nghiên cứu và hoàn \r\nthành khóa luận này từ lý thuyết đến ứng dụng. Sự hướng dẫn của các thầy đã giúp \r\nem có thêm được những hiểu biết về xử lý ngôn ngữ tự nhiên và các ứng dụng của \r\nnó. \r\nĐồng thời em cũng xin chân thành cảm ơn các thầy cô trong bộ môn công \r\nnghệ thông tin cũng như các thầy cô trong trường đã trang bị cho em những kiến \r\nthức cơ bản cần thiết để em có thể hoàn thành tốt khóa luận này. \r\nEm xin gửi lời cảm ơn đến các thành viên lớp CT1002, những người bạn đã \r\nluôn ở bên cạnh động viên, tạo điều kiện thuận lợi và cùng em tìm hiểu, hoàn thành \r\ntốt khóa luận. \r\nSau cùng, em xin gửi lời cảm ơn đến gia đình, bạn bè đã tạo mọi điều kiện để \r\nem xây dựng thành công khóa luận này. \r\nHải Phòng, ngày…….tháng……năm 2010 \r\n Sinh viên \r\nNguyễn Văn Thành \r\n Đồ án tốt nghiệp \r\n 2 \r\nMục lục \r\n Đồ án tốt nghiệp \r\n 3 \r\nArticle I. MỞ ĐẦU \r\n Xử lý ngôn ngữ tự nhiên (natural language processing - NLP) là một nhánh \r\ncủa trí tuệ nhân tạo tập trung vào các ứng dụng trên ngôn ngữ của con người. Trong \r\ntrí tuệ nhân tạo thì xử lý ngôn ngữ tự nhiên là một trong những phần khó nhất vì nó \r\nliên quan đến việc phải hiểu ý nghĩa ngôn ngữ - công cụ hoàn hảo nhất của tư duy \r\nvà giao tiếp. \r\nXử lý ngôn ngữ chính là xử lý thông tin khi đầu vào là “dữ liệu ngôn ngữ” \r\n(dữ liệu cần biến đổi), tức dữ liệu “văn bản” hay “tiếng nói”. Các dữ liệu liên quan \r\nđến ngôn ngữ viết (văn bản) và nói (tiếng nói) đang dần trở nên kiểu dữ liệu chính \r\ncon người có và lưu trữ dưới dạng điện tử. Đặc điểm chính của các kiểu dữ liệu này \r\nlà không có cấu trúc hoặc nửa cấu trúc và chúng không thể lưu trữ trong các khuôn \r\ndạng cố định như các bảng biểu. \r\nĐể máy tính có thể hiểu và thực thi một chương trình được viết bằng ngôn \r\nngữ cấp cao, ta cần phải có một trình biên dịch thực hiện việc chuyển đổi chương \r\ntrình đó sang chương trình ở dạng ngôn ngữ đích. \r\nXử lý ngôn ngữ tự nhiên là một lĩnh vực nghiên cứu nhằm giúp cho các hệ \r\nthống máy tính hiểu và xử lý được ngôn ngữ con người. Dịch máy là một trong \r\nnhững ứng dụng chính của xử lý ngôn ngữ tự nhiên. Mặc dù dịch máy đã được \r\nnghiên cứu và phát triển trong hơn 50 năm qua, song vẫn tồn tại nhiều vấn đề cần \r\nnghiên cứu. \r\n Đồ án tốt nghiệp \r\n 4 \r\nArticle II. Chương 1 : Giới thiệu về xử lý ngôn ngữ tự \r\nnhiên \r\n1.1. Tổng quan \r\nXử lý ngôn ngữ chính là xử lý thông tin khi đầu vào là “dữ liệu ngôn ngữ” \r\n(dữ liệu cần biến đổi), tức dữ liệu “văn bản” hay “tiếng nói”. Các dữ liệu liên quan \r\nđến ngôn ngữ viết (văn bản) và nói (tiếng nói) đang dần trở nên kiểu dữ liệu chính \r\ncon người có và lưu trữ dưới dạng điện tử. Đặc điểm chính của các kiểu dữ liệu này \r\nlà không có cấu trúc hoặc nửa cấu trúc và chúng không thể lưu trữ trong các khuôn \r\ndạng cố định như các bảng biểu. Theo đánh giá của công ty Oracle, hiện có đến \r\n80% dữ liệu không cấu trúc trong lượng dữ liệu của loài người đang có [Oracle \r\nText]. Với sự ra đời và phổ biến của Internet, của sách báo điện tử, của máy tính cá \r\nnhân, của viễn thông, của thiết bị âm thanh,… người người ai cũng có thể tạo ra dữ \r\nliệu văn bản hay tiếng nói. Vấn đề là làm sao ta có thể xử lý chúng, tức chuyển \r\nchúng từ các dạng ta chưa hiểu được thành các dạng ta có thể hiểu và giải thích \r\nđược, tức là ta có thể tìm ra thông tin, tri thức hữu ích cho mình. \r\nGiả sử chúng ta có các câu sau trong các tiếng nước ngoài: \r\n- “We meet here today to talk about Vietnamese language and speech \r\nprocessing.” \r\n- “Aujourd'hui nous nous réunissons ici pour discuter le traitement de langue \r\net de parole vietnamienne.” \r\n- “Mы встрачаемся здесь сегодня, чтобы говорить о вьетнамском \r\nязыке и обработке речи.” \r\nNếu có ai đó dịch, hoặc có một chương trình máy tính dịch (biến đổi) chúng \r\nra tiếng Việt, ta sẽ hiểu nghĩa các câu trên đều là: “Hôm nay chúng ta gặp nhau ở \r\nđây để bàn về xử lý ngôn ngữ và tiếng nói tiếng Việt.”. Nếu các câu này được lưu \r\ntrữ như các tệp tiếng Anh, Pháp, Nga và Việt như ta nhìn thấy ở trên, ta có các dữ \r\nliệu “văn bản”. Nếu ai đó đọc các câu này, ghi âm lại, ta có thể chuyển chúng vào \r\n Đồ án tốt nghiệp \r\n 5 \r\nmáy tính dưới dạng các tệp các tín hiệu (signal) “tiếng nói”. Tín hiệu sóng âm của \r\nhai âm tiết tiếng Việt có thể nhìn thấy như sau: \r\n Hình 1.1 : Tín hiệu sóng âm của hai âm tiêt Tiếng Việt \r\nTuy nhiên, một văn bản thật sự (một bài báo khoa học chẳng hạn) có thể có \r\nđến hàng nghìn câu, và ta không phải có một mà hàng triệu văn bản. Web là một \r\nnguồn dữ liệu văn bản khổng lồ, và cùng với các thư viện điện tử − khi trong một \r\ntương lai gần các sách báo xưa nay và các nguồn âm thanh được chuyển hết vào \r\nmáy tính (chẳng hạn bằng các chương trình nhận dạng chữ, thu nhập âm thanh, hoặc \r\ngõ thẳng vào máy) − sẽ sớm chứa hầu như toàn bộ kiến thức của nhân loại. Vấn đề \r\nlà làm sao “xử lý” (chuyển đổi) được khối dữ liệu văn bản và tiếng nói khổng lồ này \r\nqua dạng khác để mỗi người có được thông tin và tri thức cần thiết từ chúng. \r\nXử lý ngôn ngữ tự nhiên đã được ứng dụng trong thực tế để giải quyết các \r\nbài toán như : nhận dạng chữ viết, nhận dạng tiếng nói, tổng hợp tiếng nói, dịch tự \r\nđộng, tìm kiếm thông tin, tóm tắt văn bản, khai phá dữ liệu và phát hiện tri thức. \r\nSection 2.01 1.2. Cơ sở khoa học \r\n1.2.1 Một số khái niệm cơ bản \r\n1.2.1.1. Ngôn ngữ tự nhiên \r\n Ngôn ngữ là hệ thống để giao thiệp hay suy luận dùng một cách biểu diễn \r\nphép ẩn dụ và một loại ngữ pháp theo logic, mỗi cái đó bao hàm một tiêu chuẩn hay \r\nsự thật thuộc lịch sử và siêu việt. Nhiều ngôn ngữ sử dụng điệu bộ, âm thanh, ký \r\nhiệu, hay chữ viết, và cố gắng truyền khái niệm, ý nghĩa, và ý nghĩ, nhưng mà nhiều \r\nkhi những khía cạnh này nằm sát quá, cho nên khó phân biệt nó. \r\n(a) 1.2.1.2. Xử lý ngôn ngữ tự nhiên \r\n Đồ án tốt nghiệp \r\n 6 \r\n Xử lý ngôn ngữ tự nhiên (natural language processing - NLP) là một nhánh \r\ncủa trí tuệ nhân tạo tập trung vào các ứng dụng trên ngôn ngữ của con người. Trong \r\ntrí tuệ nhân tạo thì xử lý ngôn ngữ tự nhiên là một trong những phần khó nhất vì nó \r\nliên quan đến việc phải hiểu ý nghĩa ngôn ngữ - công cụ hoàn hảo nhất của tư duy \r\nvà giao tiếp. \r\n(b) 1.2.1.3. Trí tuệ nhân tạo \r\nTrí tuệ nhân tạo hay trí thông minh nhân tạo (tiếng Anh: artificial \r\nintelligence hay machine intelligence, thường được viết tắt là AI) là trí tuệ được \r\nbiểu diễn bởi bất cứ một hệ thống nhân tạo nào. Thuật ngữ này thường dùng để nói \r\nđến các máy tính có mục đích không nhất định và ngành khoa học nghiên cứu về \r\ncác lý thuyết và ứng dụng của trí tuệ nhân tạo. \r\n(c) 1.2.1.4. Nhập nhằng \r\nNhập nhằng trong ngôn ngữ học là hiện tượng thường gặp, trong giao tiếp \r\nhàng ngày con người ít để ý đến nó bởi vì họ xử lý tốt hiện tượng này. Nhưng trong \r\ncác ứng dụng liên quan đến xử lý ngôn ngữ tự nhiên khi phải thao tác với ý nghĩa từ \r\nvựng mà điển hình là dịch tự động nhập nhằng trở thành vấn đề nghiêm trọng . Ví \r\ndụ trong một câu cần dịch có xuất hiện từ “đường” như trong câu “ra chợ mua cho \r\nmẹ ít đường” vấn đề nảy sinh là cần dịch từ này là road hay sugar, con người xác \r\nđịnh chúng khá dễ dàng căn cứ vào văn cảnh và các dấu hiệu nhận biết khác nhưng \r\nvới máy thì không. Một số hiện tượng nhập nhằng: Nhập nhằng ranh giới từ, Nhập \r\nnhằng từ đa nghĩa, Nhập nhằng từ đồng âm (đồng tự), Nhập nhằng từ loại. \r\n1.2.1.5. Dịch máy \r\nDịch máy là một trong những ứng dụng chính của xử lý ngôn ngữ tự nhiên, \r\ndùng máy tính để dịch văn bản từ ngôn ngữ này sang ngôn ngữ khác. Mặc dù dịch \r\nmáy đã được nghiên cứu và phát triển hơn 50 năm qua, xong vẫn tồn tại nhiều vấn \r\nđề cần nghiên cứu. Ở Việt Nam, dịch máy đã được nghiên cứu hơn 20 năm, nhưng \r\ncác sản phẩm dịch máy hiện tại cho chất lượng dịch còn nhiều hạn chế. Hiện nay, \r\n Đồ án tốt nghiệp \r\n 7 \r\ndịch máy được phân chia thành một số phương pháp như: dịch máy trên cơ sở luật, \r\ndịch máy thống kê và dịch máy trên cớ sở ví dụ. \r\n1.2.2 Lý thuyết thông tin \r\n(d) 1.2.2.1. Khái niệm \r\nLý thuyết thông tin nghiên cứu về: Áp dụng các công cụ toán học trong việc \r\nlượng hóa dữ liệu cho mục đích lưu trữ và truyền dữ liệu. Độ đo thông tin là \r\nEntropy, là số lượng bít trung bình cần thiết để cho việc lưu trữ hay truyền dữ liệu. \r\nĐóng vai trò quan trọng trong xử lý thông tin bằng các phương pháp thống kê, đặc \r\nbiệt trong NLP. \r\n(e) 1.2.2.2. Entropy \r\n Entropy là một độ đo thông tin. Entropy ~ hỗn độn, mờ, trái nghĩa với \r\norder... \r\n Đo độ không chắc chắn: Entropy thấp -> Đo độ không chắc chắn thấp; \r\nEntropy cao -> Đo độ không chắc chắn cao. Trong vật lý: Entropy giảm khi năng \r\nlượng được sử dụng. Ký hiệu p(x) là một phân bố của một biến ngẫu nhiên X. là \r\nkhông gian mẫu của X. Entropy được tính như sau: \r\nH(X) = - ∑ x p(x) log2p(x). \r\nĐơn vị: bits (log10: nats). Kí hiệu: H(X) = Hp(X) = H(p). \r\n(f) 1.2.2.3. Perplexity - Cross Entropy \r\n1. Entropy liên quan thế nào đến hiểu ngôn ngữ? \r\n Liên quan đến sự không chính xác: một vấn đề càng có nhiều thông tin thì \r\nEntropy càng thấp. Có nhiều mô hình -> entropy đo chất lượng của các mô hình? \r\nVí dụ: mô hình mã hóa ký tự với trung bình số bít sử dụng trên mỗi ký tự là 2.5 . \r\nĐây là mô hình ngôn ngữ 0-gram, nếu đặt trong sự liên kết của các âm tiết thì chúng \r\nta có thể sinh được mô hình tốt hơn, chẳng hạn cho entropy 1.22 bít trên một ký tự. \r\n Đồ án tốt nghiệp \r\n 8 \r\n2. Perplexity \r\n Entropy của một phân bố p(X) là: Hp(X) thì giá trị 2H được gọi là perplexity \r\nperplexity là số lượng mẫu trung bình mà một biến phải lựa chọn. Perlexity càng bé \r\n(tức là entropy càng bé) thì mô hình càng tốt số bít dùng để mã hóa thông tin \r\ncàng bé. \r\nVí dụ : Cho 8 con ngựa với xác suất lựa chọn như sau: \r\nNgựa 1: 1/2 ngựa 2: 1/4 ngựa 3: 1/8 ngựa 4: 1/16 \r\nNgựa 5: 1/64 ngựa 2: 1/64 ngựa 3: 1/64 ngựa 4: 1/64 \r\n3. Entropy rate \r\n Tính entropy của một dãy các từ trong một ngôn ngữ L \r\nH(w1,...,wn) = - W L p(W1n)log(W1n) \r\nEntropy rate được coi như per-word entropy. Coi một ngôn ngữ như một quá trình \r\nngẫu nhiên sản xuất một dãy các từ. Cần quan tâm đến một dãy vô hạn từ. Entropy \r\nrate H(L) được định nghĩa như sau: \r\n),...,(log),...,(\r\n1\r\nlim),...,(\r\n1\r\nlim)( 111 nn\r\nL\r\nn\r\nn\r\nn\r\nwwpwwp\r\nn\r\nwwH\r\nn\r\nLH\r\n4 . Cross Entropy \r\n Cross entropy được sử dụng khi chúng ta không biết phân bố thật p. \r\n Cross-entropy của phân bố m của phân bố thật p được định nghĩa: \r\n),...,(log\r\n1\r\nlim),...,(log),...,(\r\n1\r\nlim),( 111 n\r\nn\r\nL\r\nnn\r\nn\r\nwwm\r\nn\r\nwwmwwp\r\nn\r\nmpH\r\n (theo lý thuyết Shannon-McMillan-Breiman) \r\n Đồ án tốt nghiệp \r\n 9 \r\n5. Cross entropy để so sánh các mô hình : H(p) ≤ H(p,m) \r\n Cross entropy H(p,m) là cận trên của entropy H(p); \r\n Mô hình m càng chính xác thì cross entropy H(p,m) càng gần với entropy \r\nH(p); \r\n Độ khác nhau H(p,m) và H(p) đo độ chính xác của mô hình m; \r\n6. Các công thức Cross Entropy \r\n Cross entropy giữa biến X với phân bố xác suất đúng p(x) và một phân bố m \r\nđược tính như sau: \r\n)(log)()||()(),( xmxpmpDXHmXH\r\nx\r\nChú ý: D(p||q) = ∑x p(x) log2 (p(x)/q(x)) \r\n1.3 Quy trình xử lý ngôn ngữ tự nhiên \r\nĐể máy tính có thể hiểu và thực thi một chương trình được viết bằng ngôn \r\nngữ cấp cao, ta cần phải có một trình biên dịch thực hiện việc chuyển đổi chương \r\ntrình đó sang chương trình ở dạng ngôn ngữ đích. Chương này trình bày một cách \r\ntổng quan về cấu trúc của một trình biên dịch và mối liên hệ giữa nó với các thành \r\nphần khác - “họ hàng” của nó - như bộ tiền xử lý, bộ tải và soạn thảo liên kết,v.v. \r\nCấu trúc của trình biên dịch được mô tả trong chương là một cấu trúc mức quan \r\nniệm bao gồm các giai đoạn: Phân tích từ vựng, Phân tích cú pháp, Phân tích ngữ \r\nnghĩa, Sinh mã trung gian, Tối ưu mã và Sinh mã đích. Nói một cách đơn giản, trình \r\nbiên dịch là một chương trình làm nhiệm vụ đọc một chương trình được viết bằng \r\nmột ngôn ngữ - ngôn ngữ nguồn (source language) - rồi dịch nó thành một chương \r\ntrình tương đương ở một ngôn ngữ khác - ngôn ngữ đích (target languague). Một \r\nphần quan trọng trong quá trình dịch là ghi nhận lại các lỗi có trong chương trình \r\nnguồn để thông báo lại cho người viết chương trình. \r\n Đồ án tốt nghiệp \r\n 10 \r\nHình 1.2 : Một trình biên dịch \r\n(g) 1.3.1 Phân tích từ vựng (Lexical Analysis) \r\nTrong một trình biên dịch, giai đọan phân tích từ vựng sẽ đọc chương trình \r\nnguồn từ trái sang phải (quét nguyên liệu - scanning) để tách ra thành các thẻ từ \r\n(token). \r\nVí dụ 1.2: Quá trình phân tích từ vựng cho câu lệnh gán position := initial + rate * \r\n60 sẽ tách thành các token như sau: \r\n 1. Danh biểu position \r\n 2. Ký hiệu phép gán := \r\n3. Danh biểu initial \r\n4. Ký hiệu phép cộng (+) \r\n5. Danh biểu rate \r\n6. Ký hiệu phép nhân (*) \r\n7. Số 60 \r\nTrong quá trình phân tích từ vựng các khoảng trắng (blank) sẽ bị bỏ qua. \r\n(h) 1.3.2 Phân tích cú pháp (Syntax Analysis) \r\nGiai đoạn phân tích cú pháp thực hiện công việc nhóm các thẻ từ của chương \r\ntrình nguồn thành các ngữ đoạn văn phạm (grammatical phrase), mà sau đó sẽ được \r\ntrình biên dịch tổng hợp ra thành phẩm. Thông thường, các ngữ đoạn văn phạm này \r\nđược biểu diễn bằng dạng cây phân tích cú pháp (parse tree) với: \r\n- Ngôn ngữ được đặc tả bởi các luật sinh. \r\n- Phân tích cú pháp dựa vào luật sinh để xây dựng cây phân tích cú pháp. \r\n Đồ án tốt nghiệp \r\n 11 \r\n Ví dụ 1.3: Giả sử ngôn ngữ đặc tả bởi các luật sinh sau: \r\n Stmt → id := expr \r\n expr → expr + expr | expr * expr | id | number \r\nVới câu nhập: position := initial + rate * 60, cây phân tích cú pháp được xây \r\ndựng như sau: \r\nHình 1.3 : Một cây phân tích cú pháp \r\nCấu trúc phân cấp của một chương trình thường được diễn tả bởi quy luật đệ qui. \r\nVí dụ 1.4: \r\n 1) Danh biểu (identifier) là một biểu thức (expr). \r\n 2) Số (number) là một biểu thức. \r\n 3) Nếu expr1 và expr2 là các biểu thức thì: \r\n expr1 + expr2 \r\n expr1 * expr2 \r\n (expr) \r\n 4) Cũng là những biểu thức. Câu lệnh (statement) cũng có thể định nghĩa đệ qui : \r\n Nếu id1 là một danh biểu và expr2 là một biểu thức thì id1 := expr2 là một \r\nlệnh (stmt). \r\n Đồ án tốt nghiệp \r\n 12 \r\nNếu expr1 là một biểu thức và stmt2 là một lệnh thì while (expr1) do stmt2 \r\nvà if (expr1) then stmt2: đều là các lệnh. Người ta dùng các qui tắc đệ qui như \r\ntrên để đặc tả luật sinh (production) cho ngôn ngữ. Sự phân chia giữa quá trình \r\nphân tích từ vựng và phân tích cú pháp cũng tuỳ theo công việc thực hiện. \r\n(i) 1.3.3 Phân tích ngữ nghĩa (Semantic Analysis) \r\nGiai đoạn phân tích ngữ nghĩa sẽ thực hiện việc kiểm tra xem chương trình \r\nnguồn có chứa lỗi về ngữ nghĩa hay không và tập hợp thông tin về kiểu cho giai \r\nđoạn sinh mã về sau. Một phần quan trọng trong giai đoạn phân tích ngữ nghĩa là \r\nkiểm tra kiểu (type checking) và ép chuyển đổi kiểu. \r\nVí dụ 1.5: Trong biểu thức position := initial + rate * 60 \r\nCác danh biểu (tên biến) được khai báo là real, 60 là số integer vì vậy trình \r\nbiên dịch đổi số nguyên 60 thành số thực 60.0 \r\n. \r\n Hình 1. 4 : Chuyển đổi kiểu trên cây phân tích cú pháp \r\n(j) 1.3.4 Các giai đoạn của trình biên dịch \r\nMột trình biên dịch được chia thành các giai đoạn, mỗi giai đoạn chuyển \r\nchương trình nguồn từ một dạng biểu diễn này sang một dạng biểu diễn khác. \r\n Đồ án tốt nghiệp \r\n 13 \r\nVÍ DỤ: Một cách phân rã điển hình trình biên dịch được trình bày trong hình \r\n: \r\n Hình 1.5 : Các giai đoạn của một trình biên dịch \r\nViệc quản lý bảng ký hiệu và xử lý lỗi được thực hiện xuyên suốt qua tất cả \r\ncác giai đoạn. Các giai đoạn mà chúng ta đề cập ở trên là thực hiện theo trình tự \r\nlogic của một trình biên dịch. Nhưng trong thực tế, cài đặt các hoạt động của nhiều \r\nhơn một giai đoạn có thể được nhóm lại với nhau. Thông thường chúng được nhóm \r\nthành hai nhóm cơ bản, gọi là: Kỳ đầu (Front end) và kỳ sau (Back end). \r\n1. Kỳ đầu (Front End) \r\nKỳ đầu bao gồm các giai đoạn hoặc các phần giai đoạn phụ thuộc nhiều vào \r\nngôn ngữ nguồn và hầu như độc lập với máy đích. Thông thường, nó chứa các giai \r\nđoạn sau: Phân tích từ vựng, Phân tích cú pháp, Phân tích ngữ nghĩa và Sinh mã \r\ntrung gian. Một phần của công việc tối ưu hóa mã cũng được thực hiện ở kỳ đầu. \r\nFront end cũng bao gồm cả việc xử lý lỗi xuất hiện trong từng giai đoạn. \r\n2. Kỳ sau (Back End) \r\nKỳ sau bao gồm một số phần nào đó của trình biên dịch phụ thuộc vào máy \r\nđích và nói chung các phần này không phụ thuộc vào ngôn ngữ nguồn mà là ngôn \r\nngữ trung gian. Trong kỳ sau, chúng ta gặp một số vấn đề tối ưu hoá mã, phát sinh \r\nmã đích cùng với việc xử lý lỗi và các thao tác trên bảng ký hiệu. \r\n Đồ án tốt nghiệp \r\n 14 \r\n1.3.5 Một số thuật toán phân tích cú pháp \r\n(k) 1.3.5.1 Topdown \r\nPhân tích từ trên xuống, từ trái qua phải; \r\nKhi gặp một từ (terminal) thì phân tích nút tiếp theo; \r\nKhi không tương ứng với input word thì quay lui; \r\n(l) 1.3.5.2 Bottom-up \r\nLà một dạng của shift-reduce actions; \r\nKhi gặp vế phải của một luật thì thu gọn thành vế trái; \r\nKhi không phân tích được tiếp thì quay lui; \r\n(m) 1.3.5.3 CYK (Cocke-Younger-Kasami) \r\nVăn phạm dạng chuẩn Chomsky (Chomsky Normal Form); \r\n Các luật thuộc một trong 2 dạng: \r\n A -> B C \r\n A -> a \r\nVí dụ: \r\n S -> X Y \r\n X -> X A | a | b \r\n Y -> A Y | a \r\n A -> a \r\nPhân tích câu “babaa” -> không sinh ra câu \r\n“baaa” -> sinh ra câu \r\n Đồ án tốt nghiệp \r\n 15 \r\nXác định các đặc điểm sau đây: \r\n1)Sinh ra giá trị một nút như thế nào? \r\nA[i,j] <- ? + ? \r\n2)Lưu lại đường đi như thế nào để sinh lại cây \r\nTính nhập nhằng: Một A[,] có thể có nhiều tag, mỗi tag lại được dẫn xuất bằng \r\nnhiều cách. \r\n3)Tại sao thuật toán CYK lại cần văn phạm dạng chuẩn Chomsky. \r\nPhân tích câu: \r\n“book that flight” \r\n“book the flight through Houston” \r\n Đồ án tốt nghiệp \r\n 16 \r\nChuyển từ văn phạm CFG sang văn phạm dạng chuẩn Chomsky \r\n1) A -> B C D \r\nA -> X D \r\n X -> B C \r\n2) Bỏ luật dạng A -> B \r\nVới mọi B -> , sinh luật A -> \r\n Đồ án tốt nghiệp \r\n 17 \r\nHình 1.6. Thử sinh ra một văn phạm tương ứng \r\n1.4 Các ứng dụng của xử lý ngôn ngữ tự nhiên \r\n1. Nhận dạng tiếng nói (speech recognition): Từ sóng tiếng nói, nhận biết và \r\nchuyển chúng thành dữ liệu văn bản tương ứng. Giúp thao tác của con người trên \r\ncác thiết bị nhanh hơn và đơn giản hơn, chẳng hạn thay vì gõ một tài liệu nào đó \r\nbạn đọc nó lên và trình soạn thảo sẽ tự ghi nó ra. Đây cũng là bước đầu tiên cần \r\nphải thực hiện trong ước mơ thực hiện giao tiếp giữa con người với robot. Nhận \r\ndạng tiếng nói có khả năng trợ giúp người khiếm thị rất nhiều. \r\n2. Tổng hợp tiếng nói (speech synthesis): Từ dữ liệu văn bản, phân tích và \r\nchuyển thành tiếng người nói. Thay vì phải tự đọc một cuốn sách hay nội dung một \r\ntrang web, nó tự động đọc cho chúng ta. Giống như nhận dạng tiếng nói, Tổng hợp \r\ntiếng nói là sự trợ giúp tốt cho người khiếm thị, nhưng ngược lại nó là bước cuối \r\ncùng trong giao tiếp giữa người với robot. \r\n3. Nhận dạng chữ viết (optical character recognition, OCR): Từ một văn bản \r\nin trên giấy, nhận biết từng chữ cái và chuyển chúng thành một tệp văn bản trên \r\nmáy tính. có hai kiểu nhận dạng: Thứ nhất là nhận dạng chữ in như nhận dạng chữ \r\ntrên sách giáo khoa rồi chuyển nó thành dạng văn bản điện tử như dưới định dạng \r\n Đồ án tốt nghiệp \r\n 18 \r\ndoc của Microsoft Word chẳng hạn. Phức tạp hơn là nhận dạng chữ viết tay, có khó \r\nkhăn bởi vì chữ viết tay không có khuôn dạng rõ ràng thay đổi từ người này sang \r\nngười khác.Với chương trình nhận dạng chữ viết in có thể chuyển hàng ngàn đầu \r\nsách trong thư viện thành văn bản điện tử trong thời gian ngắn. Nhận dạng chữ viết \r\ncủa con người có ứng dụng trong khoa học hình sự và bảo mật thông tin (nhận dạng \r\nchữ ký điện tử). \r\n4. Dịch tự động (machine translation): Từ một tệp dữ liệu văn bản trong một \r\nngôn ngữ (tiếng Anh chẳng hạn), máy tính dịch và chuyển thành một tệp văn bản \r\ntrong một ngôn ngữ khác. Một phần mềm điển hình về tiếng Việt của chương trình \r\nnày là Evtrans của Softex, dịch tự động từ tiếng Anh sang tiếng Việt và ngược lại, \r\nphần mềm từng được trang web vdict.com mua bản quyền, đây cũng là trang đầu \r\ntiên đ Luận văn liên quan Đồ án Phân tích thiết kế và xây dựng hệ thống quản lý bán hàng 84 trang | Lượt xem: 1215 | Lượt tải: 12 Quản lý khách sạn bán hàng 27 trang | Lượt xem: 598 | Lượt tải: 1 Đồ án Nghiên cứu một giải pháp giấu văn bản trong ảnh 32 trang | Lượt xem: 1076 | Lượt tải: 10 Sưu tầm thủ thuật máy tính - Phần 2 54 trang | Lượt xem: 746 | Lượt tải: 2 Khóa luận xây dựng hệ thống thông tin tổ chức, quản lý các giải thưởng/cuộc thi qua mạng internet 93 trang | Lượt xem: 747 | Lượt tải: 1 Đồ án Xây dựng website kinh doanh thời trang trực tuyến 29 trang | Lượt xem: 1479 | Lượt tải: 5 Đề tài Nghiên cứu một số vấn đề về phụ thuộc dữ liệu và khai phá dữ liệu trong cơ sở dữ liệu quan hệ 73 trang | Lượt xem: 1409 | Lượt tải: 4 Đề tài Quản lý học sinh trường THPT Lý Nhân - Hà Nam 59 trang | Lượt xem: 2267 | Lượt tải: 19 10 lỗi thiết kế bảo mật mạng thường gặp 11 trang | Lượt xem: 913 | Lượt tải: 2 Nhận dạng ký tự quang sử dụng mạng nơron Kohonen 68 trang | Lượt xem: 645 | Lượt tải: 5 \r\n            Copyright © 2016 LuanVan.net.vn \r\n            Website đang trong thời gian thử nghiệm, chờ xin giấy phép của Bộ TT & TT. \r\n            Thư viện  tài liệu  và \r\n             ebook  cho sinh viên. \r\n             Thư viện tài liệu \r\n            Chia sẻ: ",
          "relevance": "0",
          "title": "Đồ án Tìm hiểu về xử lý ngôn ngữ tự nhiên và máy dịch - Viết chương trình mô phỏng từ điển Việt - Anh",
          "url": "http://luanvan.net.vn/luan-van/do-an-tim-hieu-ve-xu-ly-ngon-ngu-tu-nhien-va-may-dich-viet-chuong-trinh-mo-phong-tu-dien-viet-anh-45974/"
        },
        {
          "content": "Trang chủ Đăng ký Đăng nhập Liên hệ Tài liệu - Ebook Thư viện tài liệu, ebook, đồ án, luận văn, giáo trình tham khảo cho học sinh, sinh viên Mục lục Lời nói đầu Chương 11: Giới thiệu về xử lý ngôn ngữ tự nhiên 1.1 Tổng quan 1.2 Cơ sở khoa học 1.3 Quy trình xử lý ngôn ngữ tự nhiên 1.4 Các ứng dụng của ngôn ngữ tự nhiên Chương 2: Ngữ pháp tiếng anh 2.1 Các thì trong tiếng anh 2.2 Cách sử dụng một số thì Chương 3: Chương trình thực nghiệm 3.1 Giới thiệu ngôn ngữ lập trình C# 3.2 Chương trình Kết luận Tài liệu tham khảo Các file đính kèm theo tài liệu này: Tìm hiểu xử lý ngôn ngữ tự nhiên và viết chương trình mô phỏng kiểm tra cấu trúc câu trong tiếng Anh.pdf Tài liệu liên quan Bài giảng Ngôn ngữ truy vấn SQL 38 trang | Lượt xem: 969 | Lượt tải: 8 Giáo trình Hình thành đoạn mã ứng dụng nguyên lý sử dụng kỹ thuật lập trình trong access với PHP code 50 trang | Lượt xem: 654 | Lượt tải: 2 Bài giảng Cơ sở dữ liệu phân tán 44 trang | Lượt xem: 1726 | Lượt tải: 10 Bài giảng SQL server 2000: các giao dịch và truy vấn phân tán (Distributed Queries Transactions) 19 trang | Lượt xem: 1287 | Lượt tải: 4 Bài tập về Phụ thuộc hàm 8 trang | Lượt xem: 7276 | Lượt tải: 22 Giao tác và xử lý tranh chấp trong truy xuất xuất đồng đồng thời với MS SQL Server 44 trang | Lượt xem: 3647 | Lượt tải: 8 Hướng dẫn sử dụng Hệ quản trị cơ sở dữ liệu MS SQL Server 27 trang | Lượt xem: 12415 | Lượt tải: 27 Giáo trình Nhập môn Hệ quan trị cơ sở dữ liệu DB2 224 trang | Lượt xem: 860 | Lượt tải: 6 Bài giảng Toán rời rạc 18 trang | Lượt xem: 2391 | Lượt tải: 15 Bài giảng Lập trình web asp.net 9 trang | Lượt xem: 1579 | Lượt tải: 11 Copyright © 2014  Doc.edu.vn",
          "relevance": "0",
          "title": "Tìm hiểu xử lý ngôn ngữ tự nhiên và viết chương trình mô phỏng kiểm tra cấu trúc câu trong tiếng Anh",
          "url": "http://doc.edu.vn/tai-lieu/tim-hieu-xu-ly-ngon-ngu-tu-nhien-va-viet-chuong-trinh-mo-phong-kiem-tra-cau-truc-cau-trong-tieng-anh-7074/"
        },
        {
          "content": "\nKieu Trong Khanh\n - Chúng tôi hướng tới việc cung cấp các tutorial và lý thuyết liên quan đến công nghệ Java, đặc biệt là J2EE/JavaEE\n- Chúng tôi không chủ trương cung cấp source code, video trên web site này bởi vì chúng tôi mong muốn các bạn làm từng bước một để cảm nhận được kiến thức, hiểu biết và kết quả. Cách tiếp cận của chúng tôi là làm từng bước bằng hướng dẫn\n- Hướng tiếp cận của chúng tôi theo hướng thể hiện các lý thuyết thông qua ví dụ để giúp tiết kiệm thời gian Categories Advanced (14) Alice (12) Android (5) Capstones (54) EJB (21) GoogleAPI (7) GoogleWebToolKit (2) Hibernate (8) JavaFX (4) JSF (5) Problems&Solution (7) Servlet&JSP (15) Spring (3) Struts (13) Video_EJB2 (2) Video_EJB3 (7) Video_JSP (9) Video_Servlet (8) Video_Struts (6) Video_XML (6) Web Services (11) XML&Java (23) Giới\nthiệu về Natural Language Processing (NLP) và API.ai – Công nghệ\ntích hợp xử lí ngôn ngữ tự nhiên. Tác\ngiả:Huỳnh Thành Đạt Mục\nđích:  Chủ đề của bài này\ngiới thiệu về Xử Lý Ngôn Ngữ Tự Nhiên (Tiếng\nViệt) – Một phương pháp mới để giúp cho\nmáy có thể hiểu và phân tích được câu nói của\ncon người và có thể chắc lọc ra những thông\ntin cần thiết. Bên cạnh đó, chủ đề này\ncòn giới thiệu về công nghệ API.ai – công nghệ xử\nlý ngôn ngữ tự nhiên và giúp lập trình viên tương\ntác trao đổi thông qua RESTful Web Services. Yêu\ncầu về các kiến thức cơ bản cho các khái niệm\nvề NLP và API.ai: ·        \n Nắm\nvững kiến thức về Web Services – RESTful (có thể\ntham khảo bài tại địa chỉ  http://www.kieutrongkhanh.net/2016/08/gioi-thieu-ve-restful-web-services-cong.html ) ·        \n Nắm\nvững về cấu trúc từ vựng tiếng Việt ·        \n Nắm\nvững về ngôn ngữ lập trình JAVA, lập trình thao\ntác đối tượng         \n I.            \n Khái niệm về Natural Language\nProcessing Natural\nLanguage Processing (NLP) – Xử lý ngôn ngữ tự nhiên là một\nnhánh của trí tuệ nhân tạo, tập trung vào việc\ntương tác giữa máy tính và ngôn ngữ tự nhiên của\ncon người (ở chủ đề này là tiếng việt)\nđể từ đó máy tính có thể hiểu và thực\nthi đúng yêu cầu của con người. Vậy\nlàm thế nào để máy tính có thể hiểu được\ncâu nói của con người? Sau\nđây, chúng tôi sẽ hướng dẫn bạn đọc\ncách giúp máy tính hiểu được ngữ nghĩa của\nmột câu nói. Đầu\ntiên chúng tôi có một vài khái niệm: ·        \n Lexical Category  – Nhóm từ vựng\nhọc o   \n Khái\nniệm này giúp định danh cho một tập các từ\nhoặc cụm từ cùng mang một ý nghĩa hay đề\ncập đến một nội dung cụ thể. o   \n Khái\nniệm này giúp chúng ta có thể phân tích các thành phần trong\nmột câu thành dạng tổng quát để có người\nkhác có thể diễn đạt bằng nhiều cách hay sử\ndụng từ địa phương, từ lóng, … thì nó\ncũng cùng chung một ngữ nghĩa. o   \n Ví du : Food là một\nLexical Category bao gồm các từ như bún bò, hủ tiếu,\n… o   \n Để\nhiểu rõ khái niệm này, chúng ta cùng phân tích một câu hỏi\nđơn giản như sau: §  \n Câu\nhỏi trên được phân tích thành các Lexical Category\nnhư sau   §  \n Action:  dùng để chỉ\ncác hành động của con người có liên quan tới\nthức ăn. §  \n Food:  dùng để chỉ\ncác món ăn. §  \n Where:  dùng để xác\nđịnh các từ để hỏi địa điểm. §  \n Delicious:  là tập hợp\ncác từ chỉ độ ngon của thức ăn. §  \n …:  các lexical category khác\ntùy theo sự phức tạp của của một câu\nđược phân tích §  \n Từ\ncâu ví dụ trên chúng ta có thể gom lại thành một chuỗi\ncác Lexical Category như sau: [Action][Food][Where][Delicious] ·        \n Pattern –  cú pháp hay ngữ\npháp hình thành trong một câu o   \n Vi dụ:    [Action][Food][Where][Delicious] . o   \n Mục\nđích của pattern giúp xác định mẫu câu được\ndùng trong giao tiếp. Đến\nđây, chúng ta đã hình thành nên khái niệm mẫu câu, ngữ\npháp của câu để sử dụng trong giao tiếp và\ntruyền đạt theo kiểu chúng ta được học\ntrong ngôn ngữ tiếng Việt hay tiếng Anh. Tuy\nnhiên,  ngôn ngữ tự nhiên  được hiểu\ntheo ngữ cảnh – các câu giao tiếp có ý nghĩa khác nhau\ntùy theo không gian, địa điểm và nội dung đang\nđược trao đổi, và không theo một một quy\ntắc nào. Vì vậy,  pattern  chưa đảm bảo\nviệc xác định chính xác nghĩa một câu. ·        \n Intent : xác định ý\nđịnh, hay mục đích của câu được\nphân tích dựa trên ngữ cảnh giao tiếp. o   \n Ví dụ :  với câu hỏi\n “Ăn phở ở đâu ngon?”, chúng ta hiểu\nintent “câu nói mong muốn xác định vị trí quán phở\nở đâu là ngon”.  Tổng\nkết : muốn xử lý ngôn ngữ tự nhiên\nchúng ta cần phải xác định 03 thành phần cơ bản\nlần lượt là Lexical Category, Pattern, Intent để\nthông qua đó chúng ta sẽ dạy cho máy hiểu được\ncác câu được chuyển tải trong quá trình giao tiếp   \n II.            \n API.ai ·        \n Api.ai\nlà một framework hỗ trợ xử lý ngôn ngữ tự\nnhiên (hiện tại, bộ này chưa hỗ trợ tiếng\nViệt) nhằm hỗ trợ người lập trình xây\ndựng một công cụ liên quan đến giao tiếp tự\nđộng giữa người và máy tính.  ·        \n Các\ntính năng §  \n Api.ai’s\nSpeech Recognition: Hỗ trợ nhận diện giọng nói, chuyển\nđổi âm thanh – sound thành dạng văn bản – text. §  \n Natural\nLanguage Understanding and Conversation Management: Xử lý ngôn ngữ\ntự nhiên và hỗ trợ giao tiếp. §  \n Một\nsố khái niệm cơ bản trong API.ai mà chúng ta sẽ sử\ndụng trong lúc cài đặt ứng dụng: Khái niêm Mô tả Agent Tương\n  đương như một ứng dụng trong api.ai.\n  Đây cũng là nơi chúng ta tích hợp vào ứng dụng\n  của mình để có thể dạy và test bot. Entity Khái niệm\n  tương tự như Lexical Category đã nói trên. Intent Xác định\n  ngữ cảnh của câu và ứng xử trong giao tiếp.\n  Có ý nghĩa tương tự như phần giải thích\n  về  intent  trên Action Khi một  intent \n  được trigger thì  action  sẽ được\n  thực hiện. Action đỏi hỏi các thông tin (parameter)\n  tương ứng được tổng hợp từ\n  các pattern kết hợp với các  intent . Context Xác\n  định ngữ cảnh của câu được phân\n  tích hay giao tiếp.  Context  bao gồm các  intent , cho\n  biết các câu nói đó thuộc những ngữ cảnh\n  tương ứng để có cách ứng xử cho phù hợp. ·        \n Cách\nsử dụng: chúng ta có thể tích hợp api với ứng\ndụng của chúng ta bằng cách sử dụng REST-like\nAPI. ·        \n Lý\ndo sử dụng api: Tuy API.ai không hỗ trợ tiếng việt,\nAPI.ai hỗ trợ việc quản lý giao tiếp cực kỳ\ntốt với hiệu xuất xử lý cao. Bên cạnh\nđó, api.ai hỗ trợ đầy đủ cách thức\nxử lý ngôn ngữ tự nhiên tiệm cận theo đúng\ncác khái niệm trong xử lý ngôn tự nhiên. ·        \n Ứng\ndụng api.ai vào ứng dụng “tìm món ăn”  o   \n Các\nbước để phát triển ứng dụng  §  \n Bước 1:  tạo account\ntrên trang api.ai §  \n Bước 2:  tạo một\nAgent (một ứng dụng trong api.ai) §  \n Bước 3 : tạo entity ·        \n Chúng\nta tiến hành xây dựng bộ từ vựng cho ứng dụng\nthông qua entity. ·        \n Khởi\ntạo đầy đủ các bộ entities cho ứng dụng\ncủa mình. ·        \n Ở\nbước này, chúng ta cần xác định rõ nghiệp vụ\n- business của ứng dụng cần xây dựng để\ntừ đó xác định tập tự vựng\ntương ứng ·        \n Trong\nví dụ này chúng ta đang xây dựng ứng dụng “tìm món\năn”, chúng ta sẽ có các nhóm từ vựng (entity name –\nlexical category trong khái niệm của xử lý ngôn ngữ tự\nnhiên) như sau: o   \n Food;\nAction; Delicious, Feeling, Hungry, Location, Nearby, Where, Hello. o   \n Chi\ntiết bên trong bộ entity, quí vị có thể download theo\nđịa chỉ  http://img.tobebetter.info/khanhkt/Advanced/final/NLP-API_Edited_files/entities.rar §  \n Bước 4 : tạo intent ·        \n Intent\ntrong api.ai là nơi lưu trữ các  pattern  hoặc các dạng\n example  ( example – còn gọi là example mode, nội dung này\nsẽ được trình bày rõ ràng hơn ở các phần\ntiếp theo) -  để thực\nhiện dạy cho hệ thống biết cách ứng xử\nphù hợp – machine learning) ·        \n Trước\ntiên, chúng ta xác định các  intent  cần thiết cho ứng\ndụng. ·        \n Ví\ndụ: một vài intent sẽ sử dụng trong ứng dụng\n“tìm món ăn” được mô tả ở trên như sau o   \n Unknown : là intent mô tả\nhành động mà hệ thống sẽ ứng xử với\nnhững câu nói mà bot chưa có cách ứng xử. o   \n RatingRequestLocation_HaveFood :\nlà intent mô tả hành động mà hệ thống sẽ ứng\nxử với những câu nói liên quan tới việc xác\nđịnh địa điểm của những món\năn ngon. o   \n Greeting : là intent mô tả\nhành động mà hệ thống sẽ ứng xử với\nnhững câu chào hỏi của người dùng. o   \n Bên dưới là cách tạo intent bằng\ncách chọn Intents bên menu trái, click vào dấu +, nhập tên\nIntent, xác định các pattern tại mục User says và định\nnghĩa các parameter lên quan tại New Parameter, nhấn Save\nđể lưu trữ ·        \n Api.ai\ncó 2 định dạng: o   \n Example mode (“) : là nơi chứa\ncác câu nói thông thường kết hợp với các lexical\ncategory (entity) nhằm huấn luyện hay dạy cho hệ\nthống cách thức hiểu và ứng xử  à \nđây cũng là nơi machine learning được áp dụng.\nTuy nhiên, điều này chỉ có hiệu lực đối\nvới những ngôn ngữ mà api.ai hỗ trợ. §  \n Ở\nô vuông số (1) trên hình, chúng ta nhập một câu, ví dụ\nnhư “tôi muốn ăn mì”, Api sẽ tự động tô\nmàu và nhận biết được các parameter name như ở\nô vuông số (2). Điều này sẽ cực kì hay khi sử\ndụng machine learning ở cái công tắc phía trên hình. Tuy\nnhiên, khi sử dụng Tiếng Việt, chúng ta không nên sử\ndụng tính năng này vì api.ai không hỗ trợ dẫn\nđến giới hạn sự hiểu biết của hệ\nthống khi ứng xử với tính đa dạng khác nhau\ntrong câu nói trong quá trình giao tiếp o   \n Templete mode (@):  đây là\nnơi chứa  pattern . Chúng ta sẽ quản lý tập\ncâu nói của người dùng dưới dạng pattern và lưu\ntrữ lại.  §  \n Chúng\nta thực hiện tạo pattern bằng cách ·        \n Click\nvào số (1). Sau đó ghi một pattern mình muốn vào dưới\ndạng: @[Entity_Name]:[Parameter_Name]. ·        \n Trong\nquá trình thực hiện, Api.ai sẽ hỗ trợ chúng ta\nauto complete các entity tương ứng. ·        \n Tương\ntự như vậy, các bạn tạo cho mình bộ pattern\ntrong các intent đã phân tích trên hoặc có thể download bộ\nintent tại địa chỉ  http://img.tobebetter.info/khanhkt/Advanced/final/NLP-API_Edited_files/intents.rar ·        \n Chúng\nta đã tạo xong bộ xử lý ngôn ngữ tự nhiên kết\nhợp với api.ai ·        \n Bên\ncạnh đó, API.ai hỗ trợ chúng ta việc test ứng\ndụng như mô tả trong hình bên dưới   ·        \n Ngoài\nra, chúng ta có thể sử dụng RESTful để\ntương tác với api. ·        \n Tiếp\ntheo, chúng tôi sẽ hướng dẫn quí vị bổ sung\ncác từ, cụm từ vào entities và đồng thời bổ\nsung thêm pattern vào intent. o   \n Đầu\ntiên, chúng ta cần lấy access key tương tác với\napi. Quá trình được thực hiện như chú thích của\nhình bên dưới o   \n Chúng\nta sẽ sử dụng Restful để tương tác với\nAPI (Quí vị có thể tham khảo các khái niệm và cách thức\nsử dụng Restful tại đia chị  http://www.kieutrongkhanh.net/2016/08/gioi-thieu-ve-restful-web-services-cong.html  ) o   \n Tạo\njava class để tương tác với api như hình vẽ\nbên dưới o   \n Tạo\ntiếp một class thừa kế từ AbstractBuilder để\ntạo sự tương tác với entity của agent. o   \n Sau\nkhi hoàn tất việc tương tác với entity, chúng ta thực\nhiện chạy thử bằng cách bổ sung vào Entity  Delicious\n từ khóa “ăn được” §  \n api.ai\ntrước khi thực hiện: §  \n Tạo\nmột java main class để thực hiện việc bổ\nsung từ khóa “ăn được” vào entity Delicious §  \n Kết\nquả trên api.ai sau khi chượng trình test chạy o   \n Tương\ntự, chúng ta sẽ insert pattern cho câu “Quán phở ở\nđâu ngon” §  \n Api.ai\ntrước khi thực hiện bổ sung §  \n Tạo\nclass IntentBuilders kế thừa thừa AbstractBuilder để\ntương tác với api  §    Tạo\nJava main class để test thử việc insert một\npattern cho câu “Quán phở ở đâu ngon” §  \n Trong\ncác ví dụ trên, json của intent  RatingRequestLocation_HaveFood \n được lấy về bởi vì toàn bộ pattern\nđược lưu trong chuỗi đó. Ngoài ra, một\nkhi chúng ta thêm pattern vào  templetes  thì chúng ta cũng cần\nxóa đi một số thành phần (userSays, auto, lastUpdate,..)\nđể đảm bảo thông tin trong chuỗi json sẽ\nđược cập nhật.  Chúc\nmừng quí vị đã hoàn tất và nắm vững khái niệm\nsơ bộ về xử lý ngôn ngữ tự nhiên và cách sử\ndụng API.ai – Framework hỗ trợ xử lý ngôn ngữ tự\nnhiên.  Quí\nvị có thể tìm hiểu thêm về API.ai tại địa\nchỉ  https://docs.api.ai Chúng\ntôi hy vọng nội dung của bài này giúp ích quí vị trong\nviệc thao thác với các ứng dụng có xử lý ngôn ngữ\ntự nhiên và có thể vận dụng api.ai linh hoạt\ntrong quá trình xử lý và hiện thực ứng dụng. Rất\nmong sự đóng góp của quí vị và hẹn gặp lại\nquí vị ở một chủ đề khác. \nĐược đăng bởi\n Khanh Kieu \nvào lúc\n 08:50 Gửi email bài đăng này BlogThis! Chia sẻ lên Twitter Chia sẻ lên Facebook Chia sẻ lên Pinterest \nNhãn:\n Advanced \nĐăng ký:\n Đăng Nhận xét (Atom) About Us \n                     1. Họ và tên: KIỀU TRỌNG KHÁNH    Trình độ chuyên môn:    Bachelor   of Engineer in Information Technology - Software E...\n Giới thiệu bản thân \nKhanh Kieu\n Kieu Trong Khanh    http://www.kieutrongkhanh.net/ Xem hồ sơ hoàn chỉnh của tôi Lưu trữ Blog \n\n        ► \n      \n \n2017\n (53) \n\n        ► \n      \n \ntháng chín\n (3) \n\n        ► \n      \n \ntháng tám\n (1) \n\n        ► \n      \n \ntháng bảy\n (5) \n\n        ► \n      \n \ntháng sáu\n (12) \n\n        ► \n      \n \ntháng năm\n (3) \n\n        ► \n      \n \ntháng hai\n (11) \n\n        ► \n      \n \ntháng một\n (18) \n\n        ▼ \n      \n \n2016\n (190) \n\n        ► \n      \n \ntháng mười hai\n (27) \n\n        ► \n      \n \ntháng mười một\n (21) \n\n        ► \n      \n \ntháng mười\n (10) \n\n        ▼ \n      \n \ntháng chín\n (25) Xử lý lỗi Unable to compile class for JSP trên JBo... Giới thiệu về Facebook Messager Platform - Cộng ng... Estimote Beacon Tutorial Giới thiệu về Natural Language Processing (NLP) và... ỨNG DỤNG CÔNG THỨC VẬT LÝ ĐỂ TẠO HIỆU ỨNG ĐỐI TƯỢNG DI CHUYỂN THEO PHƯƠNG NGANG TRONG CẢNH T... PHỐI HỢP CÁC HÌNH ẢNH, SỬ DỤNG BILLBOARD VÀ VEHICL... RÁP CẢNH GIAO TIẾP GIỮA CÁC BIẾN CỐ THÔNG QUA OBJECT CÁCH THỨC ĐƯA OBJECT MỚI CỦA TURBO SQUID VÀO TRONG... CÁCH THỨC ĐƯA OBJECT MỚI CỦA GOOGLE  SKETCHUP VÀO ... SỬ DỤNG THUỘC TÍNH SET SKIN TEXTURE CỦA CÁC HÌNH Ả... KỸ THUẬT TẠO HIỆU ỨNG ÁNH SÁNG TRONG ALICE SỬ DỤNG POSE ĐỂ TẠO HOẠT ĐỘNG HAY HÀNH VI CỦA MỘT ... KỸ THUẬT ĐIỀU HƯỚNG DUMMY OBJECT CHUYỂN CẢNH TRONG ALICE SỬ DỤNG DUMMY OBJECT VÀ SE... Cơ chế không phụ thuộc ngày của thiết bị Sử dụng thiết bị di động sử dụng Android để consum... Gửi và nhận tin nhắn SMS với Twilio Gửi tin nhắn SMS với hoiio API Tạo ứng dụng mobile thực hiện quản lý chi tiêu của... Tạo ứng dụng mobile thực hiện quản lý chi tiêu của... Tạo ứng dụng mobile thực hiện quản lý chi tiêu của... Tạo ứng dụng mobile thực hiện quản lý chi tiêu của... Xây dựng một hệ thống ứng dụng sử dụng trên Web Si... \n\n        ► \n      \n \ntháng tám\n (107) \nChủ đề Đơn giản. Được tạo bởi  Blogger .\n",
          "relevance": "0",
          "title": "Categories",
          "url": "http://www.kieutrongkhanh.net/2016/09/gioi-thieu-ve-natural-language.html"
        },
        {
          "content": "\r\n                Giới thiệu \r\n                Tin tức - Sự kiện \r\n                Sản phẩm - Dịch vụ\r\n             \r\n                Văn hóa nội bộ \r\n                Đối tác\r\n             \r\n                Tuyển dụng \r\n                Liên hệ Tuyển dụng Các tin khác Lập trình viên Frontend - Zamba Lập trình viên Backend - Zamba System Administrator - SohaGame NODE JS FULL STACK DEVELOPER - VCCLOUD PHP FULL STACK DEVELOPER - VCCLOUD Python Full stack developer - VCCloud PROJECT COORDINATOR - VCCLOUD Business Analyst - VCCloud Biên tập viên SEO - Afamily \r\n        01/09/2016 00:00 \r\n            Admicro tuyển Kỹ sư xử lý ngôn ngữ tự nhiên \r\n        Để phục vụ cho dự án Search Engine, chúng tôi cần tuyển kỹ sư xử lý ngôn ngữ tự nhiên – Natural Language Processing Engineer. Số lượng: 10 Mô tả công việc: Lập trình phát triển theo yêu cầu của dự án. Báo cáo công việc định kỳ cho trưởng nhóm Yêu cầu ứng tuyển: Đã tốt nghiệp đại học hoặc tương đương liên quan đến CNTT. Sử dụng thành thạo một trong các ngôn ngữ như: Java, Python. Khả năng tư duy, học hỏi nhanh. Có kinh nghiệm xây dựng, làm việc với các hệ thống xử lý ngôn ngữ tự nhiên: Word segmentation, Automatic Summarization, POS tagging, NER, Topic Modeling, Sentiment analysis… Có thể làm việc trên các nền tảng dữ liệu lớn như Hadoop/Spark/Hive. Có khả năng đo kiểm chất lượng, tối ưu thuật toán. Kỹ năng đọc/viết tài liệu kỹ thuật bằng tiếng Anh. Quyền lợi ứng viên: Được làm việc các hệ thống lớn hàng đầu của Việt nam. Làm việc trong môi trường năng động, nhiều thử thách, có cơ hội được đào tạo nâng cao nghiệp vụ thường xuyên Được cung cấp các trang thiết bị hiện đại, laptop, máy tính cá nhân... cần thiết để nâng cao hiệu quả làm việc Mức lương hấp dẫn (luôn cao hơn thị trường) Chính sách BHXH, BHYT và các phúc lợi khác theo đúng quy định của Luật lao động Cách thức ứng tuyển: HỒ SƠ BAO GỒM: (Vui lòng ghi rõ vị trí dự tuyển bên ngoài hồ sơ) CV Tiếng Việt /Tiếng Anh(Sơ yếu lý lịch) có ghi rõ về quá trình học tập, kinh nghiệm làm việc, các điểm mạnh, điểm yếu của bản thân…) Nơi nhận:  Phòng Hành chính Nhân sự - Công ty VCCORP - Tầng 19 tòa nhà Center Building, số 01 Nguyễn Huy Tưởng, Thanh Xuân, Hà Nội hoặc qua địa chỉ mail: yendaothi@admicro.vn    Link hay Mua chung SohaPay Sàn nhạc Mua rẻ SohaGame SoLo Admicro AutoPro CafeF F139 Én bạc Rồng bay Kênh14 Afamily Biz GameK GenK Ming Soha",
          "relevance": "0",
          "title": "Admicro tuyển Kỹ sư xử lý ngôn ngữ tự nhiên",
          "url": "https://vccorp.vn/tuyen-dung/admicro-tuyen-ky-su-xu-ly-ngon-ngu-tu-nhien-20160217144044386.htm"
        },
        {
          "content": "\n      Slideshare uses cookies to improve functionality and performance, and to provide you with relevant advertising. If you continue browsing the site, you agree to the use of cookies on this website. See our  User Agreement  and  Privacy Policy .\n     \n      Slideshare uses cookies to improve functionality and performance, and to provide you with relevant advertising. If you continue browsing the site, you agree to the use of cookies on this website. See our  Privacy Policy  and  User Agreement  for details.\n     SlideShare Explore Search You Upload Login Signup Submit Search Home Presentation Courses PowerPoint Courses EdTech & E-Learning Courses \n                  by  LinkedIn Learning \n    Các bài toán xử lý ngôn ngữ tự nhiên trong phát triển hệ thống chatbot\n   Upcoming SlideShare Loading in … 5 × 1 1  of  37 Like this presentation? Why not share! Share Email     Introduction to natural language pr... by Minh Quang-Nhat Pham 846 views How to Become a Thought Leader in Y... by Leslie Samuel 570999 views Natural language processing by Hansi Thenuwara 1007 views Surveillance Built on Mesh Wi-Fi: E... by Amy Blasnick 1547 views Hype vs. Reality: The AI Explainer by Luminary Labs 456930 views Visual Design with Data by Seth Familian 670887 views Share SlideShare Facebook Twitter LinkedIn Email Email sent successfully! Embed Size (px) Start on Show related SlideShares at end WordPress Shortcode Link \n                  Các bài toán xử lý ngôn ngữ tự nhiên trong phát triển hệ thống chatbot\n                 \n                  3,438 views Share Like \n                  Download\n                 Minh Quang-Nhat Pham , Researcher at FPT Technology Research Institute (FTRI) - FPT University  Follow\n                         \n              Published on  Mar 30, 2017 \n                    Trình bày về những bài toán xử lý ngôn ngữ tự nhiên trong phát triển hệ thống chatbot theo mô hình truy xuất thông tin. Ngoài ra mô hình sinh hội thoại sử dụng mạng Neural cũng được đề cập (neural chatbot)\n                   \n                    ... Published in: Data & Analytics \n                    0 Comments\n                 \n                  20 Likes\n                 \n                Statistics\n               \n                Notes\n               Full Name \n                          Comment goes here.\n                         12 hours ago   \n\n                         Delete Reply Spam Block Are you sure you want to Yes No \n                          Your message goes here\n                         Post Be the first to comment Huỳnh Tuyên \n                                 at \n                                 Volgograd State Technical University - ВолгГТУ \n                              4 days ago\n                             Lap Nguyen \n                              1 month ago\n                             Hao Tran \n                                , \n                                 R&D, Mobile, IoT, AI, Microservices, Products. Learning Biotechnology! \n                                 at \n                                 Softfoundry International Pte Ltd \n                              4 months ago\n                             Tung Nguyen \n                                , \n                                 Assistant Professor at Utah State University \n                                 at \n                                 Utah State University \n                              6 months ago\n                             Tung Nguyen \n                                , \n                                 Assistant Professor at Utah State University \n                                 at \n                                 Utah State University \n                              6 months ago\n                             \n                    Show More\n                     No Downloads Views Total views \n                      3,438\n                     On SlideShare \n                      0\n                     From Embeds \n                      0\n                     Number of Embeds \n                      7\n                     Actions Shares 0 Downloads \n                      165\n                     Comments \n                      0\n                     Likes \n                      20\n                     \n                    Embeds\n                     0 No embeds No notes for slide \n                  Các bài toán xử lý ngôn ngữ tự nhiên trong phát triển hệ thống chatbot\n               \n      1.\n    Các bài toán x lý ngôn ng t nhiên\ntrong phát tri n h th ng chatbot\nPh m Quang Nh t Minh\nVi n nghiên c u công ngh FPT (FTRI)\nminhpqn2@fe.edu.vn\nNgày 30 tháng 3 năm 2017\n \n   \n        2.\n       \n    M t s h th ng giao ti p t đ ng\nHình: Chatbot Symptomate1\ncho ch n đoán b nh và tr lý o Cortana\n1\nhttps://www.facebook.com/Symptomate\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 2/37\n \n   \n        3.\n       \n    Vì sao nhu c u s d ng chatbot ngày càng tăng?\nChatbot h u ích trong các h th ng tr c tuy n v i s lư ng\nl n ngư i dùng:\nH tr ho c thay th ngư i trong m t s tác v : chăm sóc\nkhách hàng, h tr đ t hàng, tr l i câu h i, giáo d c,. . .\nR t nhi u các n n t ng h tr trò chuy n tr c tuy n:\nFacebook messenger, Slack, Skype, Telegram,...\nS lư ng ngư i dùng đi n tho i thông minh ngày càng nhi u\nNh ng bư c ti n m i trong ngành AI:\nX lý ngôn ng t nhiên\nX lý ti ng nói\nH c máy\nNgày càng nhi u platform h tr t o chatbot\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 3/37\n \n   \n        4.\n       \n    N n t ng h tr phát tri n chatbot\nM t s n n t ng h tr phát tri n chatbot:\nWatson Conversation Service\nMicrosoft’s LUIS\nGoogle Natural Language API\nWit.ai\nApi.ai\nAmazon Lex\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 4/37\n \n   \n        5.\n       \n    Ki n trúc cơ b n c a m t h th ng tr l i t đ ng\nHình: Ki n trúc cơ b n c a m t h th ng tr l i t đ ng2\n2\nnh l y t cu n sách c a Daniel Jurafsky và James Martin\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 5/37\n \n   \n        6.\n       \n    Hình: Ví d v c u trúc h i tho i c a chatbot. (Ngu n: stanfy.com:\nhttp://tinyurl.com/mdfsa6h)\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 6/37\n \n   \n        7.\n       \n    N i dung trình bày\n1 Bài toán xác đ nh ý đ nh ngư i dùng (intent detection)\n2 Bài toán trích xu t thông tin (named entity extraction)\n3 Qu n lý h i tho i (dialogue management)\n4 Mô hình sinh h i tho i cho chatbot\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 7/37\n \n   \n        8.\n       \n    N i dung trình bày\n1 Bài toán xác đ nh ý đ nh ngư i dùng (intent detection)\n2 Bài toán trích xu t thông tin (named entity extraction)\n3 Qu n lý h i tho i (dialogue management)\n4 Mô hình sinh h i tho i cho chatbot\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 8/37\n \n   \n        9.\n       \n    Ý đ nh (intent) là gì?\nIntent: Đi u ngư i dùng mong mu n chatbot th c hi n (h\ntr ) khi đưa ra câu h i tho i\nVí d , khi ngư i dùng mu n chatbot đưa ra thông tin v th i\nti t hôm nay\nTh i ti t hôm nay th nào ad?\nHà N i hôm nay có mưa không v y?\nTr i hôm nay th nào b n?\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 9/37\n \n   \n        10.\n       \n    T i sao xác đ nh intent quan tr ng?\nIntent đư c xác đ nh s quy t đ nh c u trúc (frame) và k ch\nb n (script) c a đo n h i tho i ti p theo.\nN u chatbot xác đ nh sai intent:\nPh n h i không thích h p → Ngư i dùng không hài lòng →\nR i b h th ng\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 10/37\n \n   \n        11.\n       \n    Các v n đ trong bài toán xác đ nh intent (1)\nLàm sao đ nh n bi t nh ng cách di n đ t khác nhau cho cùng\nm t intent?\nVí d , t “lag“ và “ch m“ trong 2 câu sau mang cùng ng\nnghĩa\nAd ơi, sao m ng nhà em d o này ch m th ?\nM ng lag l m, dùng r t c ch .\nNgư i dùng có th dùng câu h i, câu c u khi n hay câu than\nphi n khi đưa ra yêu c u h tr\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 11/37\n \n   \n        12.\n       \n    Các v n đ trong bài toán xác đ nh intent (2)\nX lý v n đ sai chính t\nVí d : “Ad ơi, m ng chaamj l m“\nTrong ti ng Vi t, chúng ta có th ph i x lý v n đ ti ng Vi t\nkhông d u và mix gi a có d u và không d u.\nX lý các t vi t t t, ngôn ng chat\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 12/37\n \n   \n        13.\n       \n    Cách ti p c n h c máy cho bài toán xác đ nh intent\nTrong nh ng nh ng mi n ng d ng đóng (closed domain)\nS lư ng intent là h u h n\nMô hình phân l p văn b n (text classification)\nHu n luy n mô hình phân l p intent t m t t p d li u hu n\nluy n\nintent text\norder.pizza i want a small pizza with tomatos\norder.pizza i want a pizza with bbq souce\norder.pizza pizza delivery\ngreeting Hi\ngreeting Hello\n...\nB ng: Ví d v d li u hu n luy n cho chatbot ph c v order pizza\n(Api.ai)\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 13/37\n \n   \n        14.\n       \n    Mô hình phân l p intent\nHình: Ki n trúc h th ng phân l p intent\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 14/37\n \n   \n        15.\n       \n    Ti n x lý d li u\nTách t (word segmentation)\nX lý t gõ sai chính t (ví d m ng chaamj)\nX lý t vi t t t (ví d : gõ ip thay vì iphone)\nPOS Tagging\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 15/37\n \n   \n        16.\n       \n    Trích xu t đ c trưng\nBag-of-words\nHi n t i intent detection t i FPT.AI đang s d ng bag-of-word\nfeatures\ntf-idf features\n...\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 16/37\n \n   \n        17.\n       \n    Thu t toán hu n luy n mô hình phân l p\nThu t toán phân l p ph bi n:\nSupport Vector Machines (SVM)\nRandom Forest\nQuora đang s d ng đ phát hi n câu h i trùng l p\nNeural Networks\nFPT.AI s d ng mô hình m ng Neural Feed Forward v i m t\nt ng n\nS d ng thư vi n keras v i Theano backed end.\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 17/37\n \n   \n        18.\n       \n    Đánh giá đ chính xác c a h th ng phân l p intent\nMô hình đánh giá 5-fold cross validation\nPhân chia t p d li u thành 5 t p con\nM i l n s d ng 4 t p con cho hu n luy n mô hình và test\ntrên t p còn l i\nĐ đo: accuracy trên t p test\nTính trung bình cho 5 l n ch y\nEngine Đ chính xác\nFpt.ai 0.85424\nWit.ai 0.83419\nB ng: Đ chính xác trên t p d li u FTel\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 18/37\n \n   \n        19.\n       \n    N i dung trình bày\n1 Bài toán xác đ nh ý đ nh ngư i dùng (intent detection)\n2 Bài toán trích xu t thông tin (named entity extraction)\n3 Qu n lý h i tho i (dialogue management)\n4 Mô hình sinh h i tho i cho chatbot\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 19/37\n \n   \n        20.\n       \n    Hình: Ví d v c u trúc h i tho i c a chatbot. (Ngu n: stanfy.com:\nhttp://tinyurl.com/mdfsa6h)\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 20/37\n \n   \n        21.\n       \n    Các lo i th c th mà NLU thư ng h tr\nV trí (Location)\nTh i gian (Datetime)\nS (Number)\nĐ a ch liên l c (Contact)\nKho ng cách (Distance)\nKho ng th i gian (Duration)\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 21/37\n \n   \n        22.\n       \n    Ví d đ u vào & đ u ra c a module trích xu t thông tin\nTôi mu n đ t vé máy bay đi Phú Qu c t sân bay N i Bài lúc 8\ngi t i ngày mai.\nTôi mu n đ t vé máy bay đi <ENTITY TYPE=\"LOCATION\">\nPhú Qu c </ENTITY> t sân bay <ENTITY\nTYPE=\"LOCATION\"> N i Bài </ENTITY> lúc <ENTITY\nTYPE=\"TIME\"> 8 gi t i ngày mai </ENTITY>.\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 22/37\n \n   \n        23.\n       \n    Mô hình gán nhãn chu i - sequence labeling\nSequence labeling là mô hình hay đư c s d ng cho bài toán\ntrích xu t thông tin.\nChúng ta có m t t p d li u bao g m các câu đư c gán nhãn\ncho t ng t trong câu\nTôi/O mu n/O đ t/O vé/O máy/O bay/O đi/O\nPhú_Qu c/B-LOCATION t /NA sân/NA bay/NA\nN i_Bài/B-LOCATION lúc/NA 8/B-TIME gi /I-TIME\nt i/I-TIME ngày/I-TIME mai/I-TIME.\nMô hình gán nhãn B-I-O\nB: Beginning, I: Inside, O: Outside\nBài toán: Hu n luy n m t mô hình gán nhãn cho m t câu m i\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 23/37\n \n   \n        24.\n       \n    Mô hình Conditional Random Fields (CRF)\nÝ tư ng chính: Mô hình hoá xác su t đi u ki n c a m t chu i các\nnhãn v i m t dãy các t cho trư c.\nMô hình hoá P(Y |X) t d li u.\nV i m t câu X cho trư c, dãy các nhãn đư c ch n sao cho giá tr\nP(Y |X) đ t giá tr c c đ i.\nCông c : CRFsuite3, CRF++4, Mallet5,etc\n3\nhttp://www.chokkan.org/software/crfsuite/\n4\nhttps://taku910.github.io/crfpp/\n5\nhttp://mallet.cs.umass.edu/\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 24/37\n \n   \n        25.\n       \n    N i dung trình bày\n1 Bài toán xác đ nh ý đ nh ngư i dùng (intent detection)\n2 Bài toán trích xu t thông tin (named entity extraction)\n3 Qu n lý h i tho i (dialogue management)\n4 Mô hình sinh h i tho i cho chatbot\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 25/37\n \n   \n        26.\n       \n    Vai trò c a qu n lý h i tho i (Dialogue Manager)\nNh n đ u vào t thành ph n NLU\nQu n lý các tr ng thái h i tho i (dialogue state)\nQu n lý ng c nh h i tho i (dialogue context)\nTruy n đ u ra cho thành ph n sinh ngôn ng\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 26/37\n \n   \n        27.\n       \n    Các mô hình qu n lý h i tho i ph bi n\nMô hình d a trên máy tr ng thái h u h n (Finite State\nAutomata - FSA)\nMô hình d a trên frame (frame-based hay form-based)\nK t h p gi a hai mô hình trên\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 27/37\n \n   \n        28.\n       \n    Mô hình FSA\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 28/37\n \n   \n        29.\n       \n    Mô hình qu n lý dialogue d a trên Frame\nGi i quy t như c đi m c a FSA khi ngư i dùng đưa cùng lúc\nnhi u thông tin\nT i m i tr ng thái, FSA ch nh n 1 câu tr l i cho 1 câu h i\nD a trên các frame đ nh s n đ đ nh hư ng cu c h i tho i\nM i Frame s bao g m các thông tin (slot) c n đi n và các\ncâu h i tương ng\nDialogue Manager s h i cho đ n khi các slot đư c đi n h t\nSlot Câu h i\nH tên Xin quý khách cho bi t h tên\nS đi n tho i S đi n tho i c a quý khách là gì ?\nTên gói Internet Gói Internet mà quý khách đang dùng là\ngì ?\nT c đ Internet th c t T c đ vào Internet c a quý khách\nhi n th i là bao nhiêu ?\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 29/37\n \n   \n        30.\n       \n    Mô hình qu n lý dialogue d a trên Frame\nTrong m t h i tho i có th có nhi u hơn 1 frame\nVí d : khách hàng có th v a h i v v n đ m ng ch m, v a\nmu n đ i gói Internet\nDialogue manager c n bi t khi nào thì c n chuy n sang frame\nkhác\nĐ nh nghĩa các lu t (production rule) đ chuy n gi a các frame\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 30/37\n \n   \n        31.\n       \n    N i dung trình bày\n1 Bài toán xác đ nh ý đ nh ngư i dùng (intent detection)\n2 Bài toán trích xu t thông tin (named entity extraction)\n3 Qu n lý h i tho i (dialogue management)\n4 Mô hình sinh h i tho i cho chatbot\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 31/37\n \n   \n        32.\n       \n    Ý tư ng chính c a mô hình sinh h i tho i (1)\nNhư c đi m c a mô hình truy xu t thông tin (phát hi n\nintent và tr l i d a trên các m u có s n)\nT n công xây d ng d li u hu n luy n và các m u tr l i\nKhông t n d ng đư c các h i tho i đã có\nVí d : gi a các nhân viên chăm sóc khách hàng và ngư i dùng\nÝ tư ng: T n d ng các h i tho i đã có đ t đ ng sinh h i\ntho i\nChatbot h c theo cách tr l i c a con ngư i trong các đo n\nh i tho i trong quá kh\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 32/37\n \n   \n        33.\n       \n    Ý tư ng chính c a mô hình sinh h i tho i (2)\nVí d v đo n h i tho i trong d li u IT Helpdesk [Vinyals\nand Le 2015]6\nDescribe your problem: i am having issues accessing vpn\nMachine: could you please let me know what are the operating\nsystems you are using ?\nHuman: i am using linux\nMachine: ok it was in the ssl vpn on which you stated it was\nbroken\nMachine: please check if you are logged in to corp network\nMachine: what is the error that you are running please\nHuman: i am seeing an error related to vpn\nMachine: what is the error message that you are getting when\nconnecting to vpn using network connect ?\nHuman: connection refused or something like that\nMachine: may i know the version of network connect you\nconnect ?\n6\nhttps://arxiv.org/abs/1506.05869\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 33/37\n \n   \n        34.\n       \n    Ý tư ng chính c a mô hình sinh h i tho i (3)\nChatbot theo mô hình sinh b t chư c các t ng trong câu\nh i tho i c a nhân viên h tr\n....\nMachine: try this\nMachine: goto <URL> page and enter your name and\npassword\nHuman: done\nMachine: try logging in with <NAME> and check if its\nworking fine now\nHuman: yes , now it works !\nMachine: great\n(The <URL> indeed contains information about vpn access.)\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 34/37\n \n   \n        35.\n       \n    Mô hình sinh h i tho i s d ng Deep Learning\nMô hình \"sequence to sequence\"\nGi i thi u trong bài báo \"Sequence to Sequence Learning with\nNeural Networks\"7\nHình: Minh ho mô hình sequence-to-sequence8\n7\nhttp://tinyurl.com/kr57pde\n8\nhttps://github.com/farizrahman4u/seq2seq\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 35/37\n \n   \n        36.\n       \n    Đ c đi m c a mô hình sinh h i tho i (neural chatbot)\nKhông c n d li u gán nhãn ho c c n lư ng d li u gán nhãn\nít hơn r t nhi u so v i mô hình chatbot truy n th ng\nM t mô hình duy nh t h c tr c ti p t d li u h i tho i\nC n d li u h i tho i l n\nMô hình không th c s ph n nh b n ch t c a h i tho i gi a\nngư i v i ngư i\nCh t lư ng h i tho i sinh ra ph thu c l n vào d li u h i\ntho i\nChatbot Tay AI c a Microsoft phân bi t ch ng t c do b ngư i\ndùng \"hu n luy n\"\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 36/37\n \n   \n        37.\n       \n    Tóm t t n i dung đã trình bày\nCác bài toán NLP cơ b n trong cách ti p c n truy n th ng\ntrong phát tri n chatbot\nXác đ nh intent\nTrích xu t thông tin\nQu n lý h i tho i\nCách ti p c n truy n th ng\nLà mô hình chatbot ph bi n trong các s n ph m chatbot\nth c t\nTheo mô hình truy xu t thông tin\nC n nhi u d li u hu n luy n và lu t chu n b b ng tay\nThích h p cho mi n ng d ng đóng\nNeural chatbot: cách ti p c n m i trong phát tri n chatbot\nH c t d li u h i tho i theo mô hình “sequence to sequence“\ntrong Deep learning.\nPh m Quang Nh t Minh Các v n đ NLP trong chatbot 37/37\n \n   \n        Learning the Basics of Branding\n       Online Course - LinkedIn Learning \n        Gamification of Learning\n       Online Course - LinkedIn Learning \n        Test Prep: PSAT\n       Online Course - LinkedIn Learning \n        Introduction to natural language processing\n       Minh Quang-Nhat Pham \n        How to Become a Thought Leader in Your Niche\n       Leslie Samuel \n        Natural language processing\n       Hansi Thenuwara \n        Surveillance Built on Mesh Wi-Fi: EnGenius AP MeshCam\n       Amy Blasnick \n        Hype vs. Reality: The AI Explainer\n       Luminary Labs \n        Visual Design with Data\n       Seth Familian \n        3 Things Every Sales Team Needs to Be Thinking About in 2017\n       Drift English\n                     Español\n                     Português\n                     Français\n                     Deutsch\n                     About Dev & API Blog Terms Privacy Copyright Support LinkedIn Corporation © 2017 × Share Clipboard × Email Email sent successfully.. Facebook Twitter LinkedIn Link Public clipboards featuring this slide × \n    No public clipboards found for this slide\n   × Save the most important slides with Clipping Clipping is a handy way to collect and organize the most important slides from a presentation. You can keep your great finds in clipboards organized around topics. Start clipping No thanks. Continue to download. Select another clipboard × Looks like you’ve clipped this slide to   already. Create a clipboard You just clipped your first slide! \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.\n       Name*\n           Description\n           Visibility\n         Others can see my Clipboard Cancel Save",
          "relevance": "0",
          "title": "Admicro tuyển Kỹ sư xử lý ngôn ngữ tự nhiên",
          "url": "https://www.slideshare.net/minhpqn/cc-bi-ton-x-l-ngn-ng-t-nhin-trong-pht-trin-h-thng-chatbot"
        },
        {
          "content": "Tiếng Việt Tiếng Anh Trang Chủ Tìm Việc Làm IT Tạo Hồ Sơ Blog IT Đăng nhập Đăng ký Chào bạn, Đăng nhập xem việc làm phù hợp Đăng nhập Đăng ký Trang Chủ Tìm Việc Làm IT Tạo Hồ Sơ Blog IT Blog IT Các thuật ngữ trong Xử lý ngôn ngữ tự nhiên Các thuật ngữ trong Xử lý ngôn ngữ tự nhiên \r\n\t\t  \r\n\t\tVai trò của Xử lý ngôn ngữ tự nhiên-XLNNTN (Natural Language Processing-NLP) trong khai thác Big Data là không thể phủ nhận trong bối cảnh phát triển của doanh nghiệp hiện nay. Đối với ngôn ngữ tiếng Anh, ta đã được kế thừa nhiều tri thức cũng như nhiều công cụ có sẵn để áp dụng ngay vào thực tiễn. Tuy nhiên, đối với ngôn ngữ tiếng Việt, ta vẫn còn gặp nhiều khó khăn (nhân sự có chuyên môn còn hạn chế, ngữ liệu để huấn luyện chưa đủ lớn) bên cạnh những cơ hội rất lớn (thị trường Việt Nam chưa được khai thác) cho những ai đam mê lĩnh vực này. \r\n\t\tVì vậy, trong bài viết này, tôi xin lập ra danh sách các thuật ngữ thường gặp trong NLP để tiện tham khảo cũng như giúp cho những bạn mới bắt đầu có thể nhanh chóng tra cứu sơ để tiến hành nghiên cứu ngay các tài liệu khoa học. Bài viết sẽ luôn được cập nhật. Nếu có các thuật ngữ chưa rõ, các bạn có thể comment để chúng ta tiếp tục mở rộng thêm danh sách này. \r\n\t\t  Natural Language Processing (NLP)  – Xử lý ngôn ngữ tự nhiên là lĩnh vực Khoa học máy tính kết hợp giữa Trí tuệ nhân tạo (Artificial Intelligence) và Ngôn ngữ học tính toán (Computational Linguistics) nhằm tập trung xử lý tương tác giữa con người và máy tính sao cho máy tính có thể hiểu hay bắt chước được ngôn ngữ của con người. Các ứng dụng thường thấy như hiện nay là Siri, Cortana và Google Now. \r\n\t\tSiri vs Google Now vs Cortana \r\n\t\t\t  Ambiguity  – nhập nhằng (ở nhiều cấp độ: lexical – từ vựng, morphological – hình vị, syntactic – cú pháp, semantic – ngữ nghĩa, domain – lĩnh vực). Ví dụ nhập nhằng từ “đậu” đại diện cho một hành động hay “đậu” đại diện cho một loài thực vật trong câu  “Con ruồi đậu mâm xôi đậu” . Pre-processing –  tiền xử lý dữ liệu, xử lý sơ bộ văn bản: xóa bỏ những kí tự, những mã điều khiển, những vùng không cần thiết cho hệ thống gồm: tách đoạn/câu/từ (paragraph/sentence/word segmentation), làm sạch (cleaning), tích hợp (integreation), chuyển đổi (transformation), giảm số chiều (reduction). \r\n\t\t  \r\n\t\tETL input output \r\n\t\t  Morphological analysis (Phân tích hình thái) Phân tích phụ tố (affix):  ví dụ anti-comput-er-iza-tion Xử lý từ ghép (compound word):  ví dụ carry out, out of sight, out of mind Xử lý các trường hợp tỉnh lược (ellipsis):  I’m, o’clock, Dr. Nhận diện tên riêng:  John, Bush, IBM Nhân diện ranh giới từ (word boundary):  tiếng Việt một từ có nhiều tiếng. Ví dụ: chúm chím, tuổi tác, hỏi han, tối om, giáo viên, hiện đại hóa, … Parser (Phân tích ngữ pháp) Gán nhãn từ loại (Part Of Speech – POS tagging):  một từ có nhiều từ loại (Danh từ, Động từ, Tính từ, …) Gán nhãn ranh giới ngữ:  đâu là bắt đầu, kết thúc của các ngữ (phrase). Ví dụ ngữ danh từ, ngữ động từ, … Gán nhãn quan hệ ngữ pháp (grammatical relation) Gán nhãn cây cú pháp (parse tree) \r\n\t\t  \r\n\t\tParse tree \r\n\t\t  Anaphora –  khử nhập nhằng thế đại từ. Ví dụ “The monkey ate the banana because  it  was hungry”. Đại từ  “it”  thay thế cho monkey hay banana. Pragmatics –  phân tích ngữ dụng: từ  “sentence”  trong phân tích văn phạm có nghĩa là câu, trong luật pháp có nghĩa là án tù. Do vậy, ta cần xem xét toàn bộ văn bản để đưa ra ý nghĩa chính xác. Corpus/Corpora  – “ngữ liệu” là những “dữ liệu, cứ liệu của ngôn ngữ”, tức là những chứng cứ thực tế sử dụng ngôn ngữ, được dùng để kiểm chứng các quy luật của ngôn ngữ trong quá trình phân tích thông kê hay kiểm định giả thuyết thống kê của các mô hình dự đoán. \r\n\t\t  \r\n\t\tCorpus \r\n\t\t  Information Extraction  – là tiến trình rút trích ra các thông tin có cấu trúc một cách tự động từ các nguồn dữ liệu không cấu trúc hay bán cấu trúc (unstructured/semi-structure) ví dụ như các tài liệu văn bản hay các trang web. \r\n\t\tInformation Extraction \r\n\t\t  Named Entity Recognition (NER)  – là tiến trình xác định và phân loại các phần tử trong văn bản vào các danh mục được định nghĩa trước như tên người, tên tổ chức, địa điểm, giá trị tiền tệ, tỷ lệ phần trăm,… \r\n\t\tNamed Entity Recognition \r\n\t\t  Sentiment Analysis  -sử dụng các kĩ thuật NLP để rút trích thông tin chủ quan của người dùng từ một câu nói hay một văn bản. Đây cũng là kĩ thuật khai thác ý kiến người dùng xem họ đang có thái độ tích cực hay tiêu cực về sản phẩm của công ty. Sentiment Analysis \r\n\t\t  Bag of Words  -mô hình thường dùng trong các tác vụ phân lớp văn bản (Text Classification). Thông tin sẽ được biểu diễn thành tập các từ đi kèm với tần xuất xuất hiện của mỗi từ này trong văn bản. Bag of Words được dùng như feature để huấn luyện cho classifier. \r\n\t\t  \r\n\t\tBag of Words \r\n\t\t  Explicit Semantic Analysis (ESA)  -là tiến trình giúp máy hiểu được ý nghĩa của văn bản, được sử dụng trong Information Retrieval, Document Classification, Semantic Relatedness calculation (độ tương tự về ý nghĩa giữa các từ hay văn bản) Latent Semantic Analysis (LSA)  -tiến trình phân tích quan hệ giữa các văn bản và các từ. Đầu ra là mối liên quan giữa các khái niệm, văn bản, và các từ. LSA giả sử các từ gần nhau về mặt ý nghĩa sẽ xuất hiện trong các văn bản tương tự. \r\n\t\tLatent Semantic Analysis \r\n\t\t  Latent Dirichlet Allocation (LDA)  – kĩ thuật Topic Modeling thường dùng, ý tưởng của LDA dựa trên nguyên lý mỗi topic là phân bố của các từ, mỗi văn bản là sự trộn lẫn giữa nhiều topic, và mỗi từ phân bố vào một trong những topic này. \r\n\t\tLatent Dirichlet Allocation \r\n\t\t  \r\n\t\tTra cứu nhanh bảng thuật ngữ \r\n\t\t\t\t\tThuật ngữ \r\n\t\t\t\t\tÝ nghĩa \r\n\t\t\t\t\tambiguity \r\n\t\t\t\t\ttính nhập nhằng \r\n\t\t\t\t\tcomputer \r\n\t\t\t\t\tngành máy tính \r\n\t\t\t\t\tlinguistics \r\n\t\t\t\t\tngôn ngữ học \r\n\t\t\t\t\tcomputational linguistics \r\n\t\t\t\t\tngôn ngữ học tính toán \r\n\t\t\t\t\tapplied linguistics \r\n\t\t\t\t\tngôn ngữ học ứng dụng \r\n\t\t\t\t\tmathematical linguistics \r\n\t\t\t\t\tngôn ngữ học toán \r\n\t\t\t\t\tacl – association for computational linguistics \r\n\t\t\t\t\thiệp hội ngôn ngữ học máy tính \r\n\t\t\t\t\tspelling checker \r\n\t\t\t\t\tkiểm lỗi chính tả \r\n\t\t\t\t\tgrammar checker \r\n\t\t\t\t\tkiểm lỗi văn phạm \r\n\t\t\t\t\tthesaurus \r\n\t\t\t\t\ttừ điển đồng nghĩa \r\n\t\t\t\t\ttext analyzer \r\n\t\t\t\t\tphân tích văn bản \r\n\t\t\t\t\ttext classification \r\n\t\t\t\t\tphân loại văn bản \r\n\t\t\t\t\ttext summarization \r\n\t\t\t\t\ttóm tắt văn bản \r\n\t\t\t\t\tvoice synthesis \r\n\t\t\t\t\ttổng hợp tiếng nói \r\n\t\t\t\t\tautomatic translation \r\n\t\t\t\t\tdịch tự động \r\n\t\t\t\t\tinterlingual \r\n\t\t\t\t\tliên ngôn ngữ nhằm biểu diễn chung cho tất cả các ngôn ngữ chính trên thế giới để tạo điều kiện thuận lợi trong việc trao đổi thông tin. \r\n\t\t\t\t\tformal language \r\n\t\t\t\t\tngôn ngữ hình thức \r\n\t\t\t\t\tformalization \r\n\t\t\t\t\thình thức hóa \r\n\t\t\t\t\tmachine readable dictionary \r\n\t\t\t\t\ttừ điển điện tử dành cho máy \r\n\t\t\t\t\tcorpus – linguistics \r\n\t\t\t\t\tngôn ngữ học ngữ liệu \r\n\t\t\t\t\tcorpus – based \r\n\t\t\t\t\tdựa trên ngữ liệu \r\n\t\t\t\t\tstatistical linguistics \r\n\t\t\t\t\tngôn ngữ học thống kê \r\n\t\t\t\t\ttagset \r\n\t\t\t\t\thệ thống nhãn \r\n\t\t\t\t\ttoolkit \r\n\t\t\t\t\tcác công cụ \r\n\t\t\t\t\tpragmatic relation \r\n\t\t\t\t\tquan hệ võ đoán (quan hệ mà không thể giải thích được lý do, quan hệ chỉ do quy ước,thói quen của cộng đồng) \r\n\t\t\t\t\tphonetics \r\n\t\t\t\t\tâm vị-đơn vị âm thanh nhỏ nhất để cấu tạo và khu biệt về mặt biểu hiện vật chất (âm thanh) của các đơn vị khác. Ví dụ: k-a-d(card);b-i-g(big) \r\n\t\t\t\t\tmorpheme \r\n\t\t\t\t\thình vị-đơn vị nhỏ nhất mang nghĩa (nghĩa ngữ pháp hay nghĩa từ vựng) được cấu tạo bởi các âm vị. Ví dụ: read-ing;book-s \r\n\t\t\t\t\tword \r\n\t\t\t\t\ttừ–đơn vị mang nghĩa độc lập; được cấu tạo bởi (các) hình vị; có chức năng định danh. Ví dụ: I-am-reading-my–books. \r\n\t\t\t\t\tphrase \r\n\t\t\t\t\tngữ-gồm hai hay nhiều từ có quan hệ ngữ pháp hay ngữ nghĩa với nhau. Vídụ: bức thư, mạng máy tính, computer system,… \r\n\t\t\t\t\tsentence \r\n\t\t\t\t\tcâu-gồm các từ/ngữ có quan hệ ngữ pháp hay ngữ nghĩa với nhau và có chức năng cơ bản là thông báo. Ví dụ: I am reading my books. \r\n\t\t\t\t\ttext \r\n\t\t\t\t\tvăn bản-hệ thống các câu được liên kết với nhau về mặt hình thức, ngữ pháp, ngữ nghĩa và ngữ dụng. \r\n\t\t\t\t\thieararchical relation \r\n\t\t\t\t\tquan hệ cấp bậc \r\n\t\t\t\t\tsyntagmatical relation \r\n\t\t\t\t\tquan hệ ngữ đoạn \r\n\t\t\t\t\tassociation relation \r\n\t\t\t\t\tquan hệ liên tưởng \r\n\t\t\t\t\tmorphology \r\n\t\t\t\t\thình thái-mối quan hệ giữa đơn vị ngôn ngữ với hình thức cấu tạo của đơn vị đó \r\n\t\t\t\t\tgrammar \r\n\t\t\t\t\tngữ pháp-mối quan hệ giữa đơn vị ngôn ngữ này với các đơn vị ngôn ngữ hữu quan \r\n\t\t\t\t\tsemantic \r\n\t\t\t\t\tngữ nghĩa-mối quan hệ giữa đơn vị ngôn ngữ với nội dung (mặt ý nghĩa) của đơn vị đó. Xác định nghĩa của từng từ và tổ hợp của chúng để tạo nghĩa của câu. Thí dụ trong phân tích (Ônggià) (đi) (nhanhquá), động từ “đi” có thể có nghĩa “bước đi”, hay “chết” hay “điều khiển”(khi đánh cờ),…và tương ứng ta có các nghĩa khác nhau của câu. \r\n\t\t\t\t\tpragmatic \r\n\t\t\t\t\tngữ dụng-mối quan hệ giữa đơn vị ngôn ngữ với mục đích sử dụng của đơn vị đó. Mối quan hệ giữa ngôn ngữ và ngữ cảnh sử dụng ngôn ngữ (contextofuse). Ngữ dụng như vậy nghiên cứu việc ngôn ngữ được dùng để nói về người và vật như thế nào. \r\n\t\t\t\t\tflexional \r\n\t\t\t\t\tngôn ngữ hòa kết \r\n\t\t\t\t\tagglutinate \r\n\t\t\t\t\tngôn ngữ chắp dính \r\n\t\t\t\t\tisolate \r\n\t\t\t\t\tngôn ngữ đơn lập \r\n\t\t\t\t\tpolysynthetic \r\n\t\t\t\t\tngôn ngữ đa tổng hợp \r\n\t\t\t\t\tclassifier \r\n\t\t\t\t\ttừ chỉ loại-phó danh từ chỉ loại: cái bàn, cuốn sách, bức thư, con chó, con sông, vì sao,… \r\n\t\t\t\t\taffix \r\n\t\t\t\t\tphụ tố \r\n\t\t\t\t\tcomparative linguistics \r\n\t\t\t\t\tngôn ngữ học so sánh \r\n\t\t\t\t\tlexicology \r\n\t\t\t\t\ttừ vựng học \r\n\t\t\t\t\tetymology \r\n\t\t\t\t\ttừ nguyên học-nghiên cứu lịch sử của từ \r\n\t\t\t\t\tencyclopedia \r\n\t\t\t\t\tbách khoa toàn thư \r\n\t\t\t\t\tdenotative meaning \r\n\t\t\t\t\tnghĩa biểu vật-liên hệ giữa từ và sự vật (hiện tượng, thuộc tính, hành động,…) \r\n\t\t\t\t\tsignificative meaning \r\n\t\t\t\t\tnghĩa biểu niệm-liên hệ giữa từ và ý (ý nghĩa, ý niệm, biểu niệm,…) \r\n\t\t\t\t\tpragmatical meaning \r\n\t\t\t\t\tnghĩa ngữ dụng-còn gọi là nghĩa biểu thái, nghĩa hàm chỉ (connotative meaning) là mối liên hệ giữa từ với thái độ chủ quan, cảm xúc của người nói. \r\n\t\t\t\t\tstructural meaning \r\n\t\t\t\t\tnghĩa cấu trúc-là mối quan hệ giữa từ với các từ khác trong hệ thống từ vựng. Quan hệ giữa từ này với từ khác thể hiện trên hai trục: trục đối vị (paradigmatial axis) và trục ngữ đoạn (syntagmatical axis) \r\n\t\t\t\t\tstem \r\n\t\t\t\t\tthân từ-có thể bao gồm một hay nhiều hình vị gốc. Ví dụ: babysit \r\n\t\t\t\t\tinflection \r\n\t\t\t\t\tbiến cách-là dạng mà trong đó có một hình vị ràng buộc kết hợp vào một từ để thể hiện những ý nghĩa ngữ pháp như: thì(tense),số (number), giống (gender), cách (case),… \r\n\t\t\t\t\tderivation \r\n\t\t\t\t\tdẫn xuất-là dạng từ mới được hình thành trên cơ sở từ gốc kết hợp với các phụ tố nhằm thể hiện những ý nghĩa từ vựng, như: lặp lại (re-), chống (anti-), người/vật thực hiện (-er/-or),… \r\n\t\t\t\t\tdouble consonant \r\n\t\t\t\t\tgấp đôi phụ âm \r\n\t\t\t\t\tsyntactic group \r\n\t\t\t\t\tđoản ngữ-một nhóm những từ có liên hệ trực tiếp với nhau ở trong câu gọi là tổ hợp từ, và loại tổ hợp từ có quan hệ chính phụ được gọi là đoản ngữ. Đoản ngữ có vai trò quan trọng trong việc phân tích cú pháp và mô hình hóa câu để hiểu câu dễ dàng. \r\n\t\t\t\t\tpos tagging \r\n\t\t\t\t\txác định loại từ-xem mỗi từ trong câu là loại gì (danh từ, động từ, giới từ,…) \r\n\t\t\t\t\tchunking \r\n\t\t\t\t\txác định cụm từ-thí dụ “ông già” là cụm danh từ, “đi” là cụm động từ, “nhanh quá” là cụm trạng từ. Như vậy câu trên có hai phân tích (Ông già)(đi)(nhanh quá) hoặc (Ông)(già đi)(nhanh quá) \r\n\t\t\t\t\tparsing \r\n\t\t\t\t\txác định quan hệ ngữ pháp-(Ông già)(đi)(nhanh quá) là quan hệ chủ ngữ-vị ngữ-trạng ngữ. \r\n\t\t\t\t\tshallow parsing \r\n\t\t\t\t\tphân tích sơ bộ \r\n\t\t\t\t\tfully parsing \r\n\t\t\t\t\tphân tích đầy đủ-phân tích cả tầng ngữ nghĩa \r\n\t\t\t\t\tacoustic \r\n\t\t\t\t\tâm học \r\n\t\t\t\t\ttext to speech \r\n\t\t\t\t\ttổng hợp tiếng nói \r\n\t\t\t\t\talphabet set \r\n\t\t\t\t\tbộ chữ-là bất kỳ một tập ký hiệu nào, tập này không nhất thiết phải hữu hạn hay đếm được (nhưng trên thực tế những tập này là hữu hạn) \r\n\t\t\t\t\tstring \r\n\t\t\t\t\tchuỗi (sigma)-định nghĩa một cách hình thức những chuỗi trên một bộ chữ (alphabet) \r\n\t\t\t\t\tlanguage \r\n\t\t\t\t\tngôn ngữ-là một tập những chuỗi có chiều dài hữu hạn trên một bộ chữ hữu hạn (sigma) nào đó) \r\n\t\t\t\t\tgrammar \r\n\t\t\t\t\tvăn phạm \r\n\t\t\t\t\tunrestricted grammar \r\n\t\t\t\t\tvăn phạm không hạn chế-được đoán nhận bằng một máy Turing. Đây là văn phạm loại 0 \r\n\t\t\t\t\tcontext-sensitive grammar \r\n\t\t\t\t\tvăn phạm cảm ngữ cảnh-được đoán nhận bằng một máy Turing. Đây là văn phạm loại 1. \r\n\t\t\t\t\tcontext-free grammar \r\n\t\t\t\t\tvăn phạm phi ngữ cảnh-sự áp dụng các luật sản sinh trong P thì hoàn toàn không bị điều kiện gì về ngữ cảnh ràng buộc. Được đoán nhận bằng PDA-push down acceptor. \r\n\t\t\t\t\tderivation sequence \r\n\t\t\t\t\tdãy suy dẫn \r\n\t\t\t\t\tderivation tree \r\n\t\t\t\t\tcây suy dẫn \r\n\t\t\t\t\tcategorized grammar \r\n\t\t\t\t\tvăn phạm mục \r\n\t\t\t\t\tChomsky normal form \r\n\t\t\t\t\tdạng chính tắc \r\n\t\t\t\t\tpharagraph segmentation \r\n\t\t\t\t\ttách đoạn-tách văn bản thành các đoạn và xem đoạn văn là một khối liên tục các câu. \r\n\t\t\t\t\ttoken \r\n\t\t\t\t\tmột dãy tuần tự các ký tự trong bảng chữ cái, hoặc dãy tuần tự các con số (một chữ số có chứa dấu chấm là dấu chấm thập phân được xem như là một token), hoặc một ký tự không nằm trong bảng chữ cái (như dấu chấm câu, dấu ngoặc kép, các ký tự mở rộng,…) \r\n\t\t\t\t\tsigmoid \r\n\t\t\t\t\thàm “nén” \r\n\t\t\t\t\tback propagation \r\n\t\t\t\t\tlan truyền ngược \r\n\t\t\t\t\tellipsis \r\n\t\t\t\t\ttỉnh lược-ví dụ: I’m, o’clock, Dr. \r\n\t\t\t\t\tTBL-Transformation Based Learning \r\n\t\t\t\t\tgiải thuật học cải biến \r\n\t\t\t\t\tStochastic transduction \r\n\t\t\t\t\tchuyển dịch trạng thái có xác suất \r\n\t\t\t\t\tacceptor \r\n\t\t\t\t\tmáy đoán nhận \r\n\t\t\t\t\ttransducer \r\n\t\t\t\t\tchuyển dịch \r\n\t\t\t\t\tparser \r\n\t\t\t\t\tphân tích ngữ pháp bao gồm phân tích từ pháp (ngữ pháp của từ-POS tagger) và phân tích cú pháp (ngữ pháp của câu), bước trung gian là phân đoạn ngữ (phrase–chunker) \r\n\t\t\t\t\ttransformation rules \r\n\t\t\t\t\tluật cải biến \r\n\t\t\t\t\twordnet \r\n\t\t\t\t\tcơ sở tri thức khổng lồ về ngữ nghĩa của từ vựng theo hướng liệt kê nét nghĩa \r\n\t\t\t\t\tLDOCE-Longman Dictionary Of Contemporary \r\n\t\t\t\t\thệ thống nhãn ngữ nghĩa LDOCE English \r\n\t\t\t\t\tpolysemy \r\n\t\t\t\t\ttừ đa nghĩa \r\n\t\t\t\t\thamonymy \r\n\t\t\t\t\ttừ đồng nghĩa \r\n\t\t\t\t\tcontrastive \r\n\t\t\t\t\tnghĩa không liên quan với nhau \r\n\t\t\t\t\tcomplementary \r\n\t\t\t\t\tnghĩa có liên quan một cách hệ thống với nhau \r\n\t\t\t\t\thomograph \r\n\t\t\t\t\tnghĩa của từ đồng tự \r\n\t\t\t\t\tprimitives \r\n\t\t\t\t\tsơ cấp \r\n\t\t\t\t\tselectional restriction \r\n\t\t\t\t\tràng buộc ngữ nghĩa \r\n\t\t\t\t\tcontent words \r\n\t\t\t\t\ttừ thực \r\n\t\t\t\t\tontology \r\n\t\t\t\t\thệ thống nhãn ngữ nghĩa, bản thể học để phân loại tri thức \r\n\t\t\t\t\tcollocation \r\n\t\t\t\t\tngôn từ-xét đến hình thái và ngữ nghĩa của các từ lân cận. Chẳng hạn khi thấy “bank…river” -> “bờ sông”, “bank…account/money”-> “ngân hàng” \r\n\t\t\t\t\tanaphora \r\n\t\t\t\t\tthế đại từ \r\n\t\t\t\t\tgranularity \r\n\t\t\t\t\tđộ mịn \r\n\t\t\t\t\tsyntactic tree transfer \r\n\t\t\t\t\tchuyển đổi cây cú pháp \r\n\t\t\t\t\tPp-attachment \r\n\t\t\t\t\tkhử nhập nhằng ngữ giới từ \r\n\t\t\t\t\tentry \r\n\t\t\t\t\tmục từ trong từ điển \r\n\t\t\t\t\tidiom \r\n\t\t\t\t\tthành ngữ \r\n\t\t\t\t\tsubcategory \r\n\t\t\t\t\ttiểu từ loại như danh từ thuộc loại con nào (danh từ đếm được, không đếm được,…), động từ loại con nào (tha động từ, tự động từ,…) \r\n\t\t\t\t\tcase role \r\n\t\t\t\t\tngữ pháp cách: agent (human), instrument (object) \r\n\t\t\t\t\tcategories \r\n\t\t\t\t\tdanh từ chỉ loài \r\n\t\t\t\t\tsubcategories \r\n\t\t\t\t\tchủng loại \r\n\t\t\t\t\tmodality \r\n\t\t\t\t\ttình thái: từ này dùng trong cảnh huống nào: trịnh trọng, thân mật, thông tục,… \r\n\t\t\t\t\tinterlingual MT \r\n\t\t\t\t\tdịch qua ngôn ngữ trung gian \r\n\t\t\t\t\tdemo \r\n\t\t\t\t\tbiểu diễn \r\n\t\t\t\t\tconcordance \r\n\t\t\t\t\ttừ đồng hiện \r\n\t\t\t\t\tphoneme synthesis \r\n\t\t\t\t\ttổng hợp âm vị \r\n\t\t\t\t\tpunctuation \r\n\t\t\t\t\tngừng nghỉ \r\n\t\t\t\t\tintonation \r\n\t\t\t\t\tngữ điệu lên xuống \r\n\t\t\t\t\talignment \r\n\t\t\t\t\tliên kết với nhau trong ngữ liệu song song \r\n\t\t\t\t\tcategory \r\n\t\t\t\t\tchủng loại như thông tin về số (ít/nhiều), về thời, đếm được. \r\n\t\t\t\t\ttags \r\n\t\t\t\t\tNhãn \r\n\t\t\t\t\tempiricism \r\n\t\t\t\t\tchủ nghĩa kinh nghiệm \r\n\t\t\t\t\trationalism \r\n\t\t\t\t\tchủ nghĩa lý luận \r\n\t\t\t\t\tdata driven \r\n\t\t\t\t\tdữ liệu thực tiễn \r\n\t\t\t\t\ttheory driven \r\n\t\t\t\t\tmô hình lý thuyết \r\n\t\t\t\t\tdeductive \r\n\t\t\t\t\tnghiên cứu theo phương pháp xác suất thống kê \r\n\t\t\t\t\tinductive \r\n\t\t\t\t\tnghiên cứu theo phương pháp luật suy diễn \r\n\t\t\t\t\tchildren language acquisition \r\n\t\t\t\t\tnhận biết ngôn ngữ của trẻ \r\n\t\t\t\t\tlanguage performance \r\n\t\t\t\t\tsự thực hiện ngôn ngữ \r\n\t\t\t\t\tLanguage competence \r\n\t\t\t\t\tnăng lực ngôn ngữ \r\n\t\t\t\t\tparole \r\n\t\t\t\t\tlời nói \r\n\t\t\t\t\trationalism \r\n\t\t\t\t\tnghiên cứu dựa theo lý luận \r\n\t\t\t\t\tbilingual parallel corpora \r\n\t\t\t\t\tngữ liệu song ngữ \r\n\t\t\t\t\tparallel corpora \r\n\t\t\t\t\tngữ liệu song song \r\n\t\t\t\t\testimation maximization \r\n\t\t\t\t\tước lượng cực đại \r\n\t\t\t\t\tmarginal phenomena \r\n\t\t\t\t\tnhững trường hợp ngoại lệ mà không tuân theo luật chính \r\n\t\t\t\t\tflip flop \r\n\t\t\t\t\tlà hiện tượng mà khi hệ thống có một sự thay đổi nào đó để khắc phục một lỗi sai này, nhưng hệ thống sẽ dẫn đễn lỗi sai khác mà ta không ngờ tới) \r\n\t\t\t\t\tpost edit \r\n\t\t\t\t\thiệu đính \r\n\t\t\t\t\tfertility \r\n\t\t\t\t\tgiá trị sản sinh \r\n\t\t\t\t\tgreedy decoding \r\n\t\t\t\t\ttìm kiếm tham lam \r\n\t\t\t\t\tbaseline \r\n\t\t\t\t\tgán nhãn sơ khởi \r\n\t\t\t\t\ttemplate \r\n\t\t\t\t\tkhung luật định sẵn \r\n\t\t\t\t\tpipeline style \r\n\t\t\t\t\tcông việc thực hiện nối tiếp nhau \r\n\t\t\t\t\tconcept \r\n\t\t\t\t\tthực thể cùng loại \r\n\t\t\t\t\tinstance-based learning \r\n\t\t\t\t\thọc dựa trên trường hợp (similarity, example, memory-based) \r\n\t\t\t\t\tfitness \r\n\t\t\t\t\thàm đánh giá \r\n\t\t\t\t\tensembles of classifier \r\n\t\t\t\t\ttập hợp phân lớp \r\n\tNguồn: Techtalk Tag xu ly ngon ngu tu nhien trong Big Data  Natural Language Processing  các thuật ngữ thường gặp trong xu ly ngon ngu tu nhien Bài viết nổi bật Hàng loạt ngân hàng cảnh báo \"trạng thái Red Alert 2.0\" Thành công trong việc đánh bại Apple và Samsung, giờ là lúc Vivo vươn ra ngoài Trung Quốc Không chỉ bạn nhìn thấy quảng cáo, quảng cáo cũng đang nhìn thấy bạn! Google chính thức ra mắt chương trình bảo mật cao cấp nhất dành cho người dùng Lơ là bảo mật, website bán hàng online dễ bị hacker hạ gục Các thiết bị Internet of Things sẽ phải ngày càng bảo mật hơn Google công bố bộ ảnh đẹp ảo diệu chụp từ Pixel 2 và 2 XL Cả Apple lẫn Google nên cảm thấy lo lắng về chiếc smartphone \"biến hình\" Galaxy X đi là vừa! Đà Nẵng: Sẽ khởi công dự án khu Công viên phần mềm số 2 tại tuần lễ APEC Ngân hàng Đông Á: Người dùng không được sử dụng thiết bị jailbreak/root truy cập eBanking Chức Năng Tài Khoản Tạo / Đăng Hồ Sơ Tạo Thông Báo Việc Làm Việc làm phù hợp với bạn Phản hồi từ nhà tuyển dụng Talent Community CareerBuilder.vn Về CareerBuilder.vn Thỏa Thuận Sử Dụng Quy Định Bảo Mật Chính Sách BV Thông Tin Cá Nhân Quy chế sàn giao dịch Qui trình giải quyết tranh chấp Kết nối với CareerBuilder.vn  Facebook  Linkedin  Youtube CareerBuilder App Android CareerBuilder App Store Website Đối Tác Vieclam.tuoitre.vn Liên Hệ Trợ Giúp CareerBuilder.vn - Mạng Việc làm & Tuyển dụng lớn nhất thế giới Công Ty Cổ Phần CareerBuilder  Trụ̣ sở: 139 Pasteur, Phường 6, Quận 3, TP.HCM MST: 0303284985 Ngày cấp: 25/04/2013 Nơi cấp: Sở Kế Hoạch Và Đầu Tư Thành Phố Hồ Chí Minh Điện thoại: (84.28) 3822-6060  Email: contact@careerbuilder.vn",
          "relevance": "0",
          "title": "Các thuật ngữ trong Xử lý ngôn ngữ tự nhiên",
          "url": "https://vieclamit.careerbuilder.vn/advices/cac-thuat-ngu-trong-xu-ly-ngon-ngu-tu-nhien.35A4EA20.html"
        },
        {
          "content": "Đăng nhập Phiên bản Mobile Tin nóng Tin mới Đề xuất Xã hội Thời sự Giao thông Môi trường - Khí hậu Thế giới Văn hóa Nghệ thuật Ẩm thực Du lịch Kinh tế Kinh doanh Lao động - Việc làm Tài chính Chứng khoán Giáo dục Học bổng - Du học Đào tạo - Thi cử Thể thao Bóng đá Quần vợt Giải trí Âm nhạc Thời trang Điện ảnh - Truyền hình Pháp luật Hình sự - Dân sự An ninh - Trật tự KH - CN CNTT - Viễn thông Khoa học - Tự nhiên Thiết bị - Phần cứng Đời sống Dinh dưỡng - Làm đẹp Tình yêu - Hôn nhân Sức khỏe - Y tế Xe cộ Nhà đất Quản lý - Quy hoạch Không gian - Kiến trúc \nxử lý ngôn ngữ tự nhiên\n Chuyển đổi vận hành số để kích hoạt tăng trưởng dịch vụ tài chính CAND 7 liên quan Chuyển đổi vận hành số để kích hoạt tăng trưởng dịch vụ tài chính \n81% doanh nghiệp khối dịch vụ tài chính tin tưởng cần chuyển đổi số để nắm bắt thành công.\n 5 công nghệ cần thiết để phát triển trong kỷ nguyên số Kỷ Nguyên Số 7 liên quan 5 công nghệ cần thiết để phát triển trong kỷ nguyên số \n81% lãnh đạo khối dịch vụ tài chính tin tưởng cần chuyển đổi sốđể nắm bắt thành công.\n Samsung ra mắt Bixby 2.0 Công Luận 1 liên quan Samsung ra mắt Bixby 2.0 \nSamsung vừa giới thiệu thế hệ 2 của Bixby nhắm chủ yếu vào thiết bị gia dụng thông minh trong gia đình, hay nói cách khác, bạn có thể ra lệnh cho Bixby để nó điều khiển các món đồ...\n 81% lãnh đạo khối dịch vụ tài chính thấy cần chuyển đổi số VietTimes 3 đăng lại 7 liên quan 81% lãnh đạo khối dịch vụ tài chính thấy cần chuyển đổi số \nLãnh đạo khối dịch vụ tài chính (FSI) Châu Á Thái Bình Dương đang tiến bước vào cuộc CMCN 4.0, theo một báo cáo mới đây của Microsoft. 81% lãnh đạo tin tưởng rằng cần chuyển đổi...\n Trợ lý ảo của Samsung được cập nhật lên 2.0 để cạnh tranh với đối thủ VietTimes 5 liên quan Trợ lý ảo của Samsung được cập nhật lên 2.0 để cạnh tranh với đối thủ \nSamsung vừa tiến hành nâng cấp ứng dụng trợ lý ảo Bixby. Samsung đã công bố việc nâng cấp đối với Bixby trong hội thảo các nhà lập trình được tổ chức ở San Francisco ngày hôm nay.\n Dù ở đâu, cũng có thể đóng góp cho quê hương TG&VN Dù ở đâu, cũng có thể đóng góp cho quê hương \nGiáo sư Ngô Thanh Nhàn chia sẻ, những việc ông làm ở Mỹ nhưng liên quan và hướng về đất nước, nghĩa là có đóng góp trực tiếp nào đó cho Việt Nam, là giúp cho quê hương...\n Shutterstock tích hợp trí tuệ nhân tạo để tìm kiếm ảnh Thanh Niên 1 đăng lại 3 liên quan Shutterstock tích hợp trí tuệ nhân tạo để tìm kiếm ảnh \nSau khi sử dụng trí tuệ nhân tạo (AI) chống lại việc loại bỏ dấu bản quyền, Shutterstock tiếp tục áp dụng AI để hỗ trợ tìm kiếm ảnh phù hợp dựa vào các thành phần trong ảnh.\n Alibaba tham vọng lật đổ Thung lũng Silicon XHTT 1 liên quan Alibaba tham vọng lật đổ Thung lũng Silicon \nTập đoàn Alibaba (Trung Quốc) đang đổ 15 tỷ USD vào một mạng lưới các phòng thí nghiệm toàn cầu nhằm đối đầu với Thung lũng Silicon trong tương lai, theo TechCrunch.\n Alibaba sẽ đầu tư 15 tỉ USD vào nghiên cứu công nghệ NCĐT 5 liên quan Alibaba sẽ đầu tư 15 tỉ USD vào nghiên cứu công nghệ \nNguồn ảnh: Quartz Alibaba công bố kế hoạch đầu tư 15 tỷ USD trong 3 năm tới vào một sáng kiến ​​nghiên cứu và phát triển toàn cầu được gọi là Alibaba DAMO Academy.\n Alibaba mạnh tay chi hơn 15 tỷ USD phát triển công nghệ mới Nhà Quản Trị 1 liên quan Alibaba mạnh tay chi hơn 15 tỷ USD phát triển công nghệ mới \nAlibaba vừa thông báo ngày thứ Tư (11/10) rằng, tập đoàn này sẽ đầu tư hơn 15 tỷ USD trong 3 năm tới vào một chương trình nghiên cứu và phát triển (R&D) toàn cầu.\n Gartner: Nhiều thương hiệu lớn như Facebook có thể 'biến mất' Zing 1 đăng lại 1 liên quan Gartner: Nhiều thương hiệu lớn như Facebook có thể 'biến mất' \nĐó là một trong những dự đoán của Gartner từ nay tới năm 2023, theo đó nhiều thương hiệu lớn như Apple, Facebook, Google, Microsoft, Amazon, Alibaba có thể sẽ biến mất.\n Cách mạng 4.0: Trí tuệ nhân tạo và nạn thất nghiệp ANTG Cách mạng 4.0: Trí tuệ nhân tạo và nạn thất nghiệp \nChưa bao giờ thế giới lại nhắc nhiều đến cách mạng công nghiệp 4.0 như bây giờ. Tại sao cuộc cách mạng công nghiệp 4.0 lại được nhiều quốc gia như Đức, Mỹ, Anh, Nhật Bản, Hàn Quốc,...\n Facebook Messenger tham vọng trở thành ứng dụng tin nhắn mặc định trên Android và iOS Pháp Luật Plus 10 liên quan Facebook Messenger tham vọng trở thành ứng dụng tin nhắn mặc định trên Android và iOS \nFacebook Messenger muốn trở thành trình tin nhắn mặc định trên iPhone và Android - đó chính xác là những gì Phó Chủ tịch mảng Messenger của Facebook David Marcus phát biểu hôm thứ...\n AI có thể trở thành ngành công nghiệp 14 tỷ USD vào năm 2023 VietTimes 4 liên quan AI có thể trở thành ngành công nghiệp 14 tỷ USD vào năm 2023 \nMộtbáo cáo gần đây khẳng định rằng ngành công nghiệp trí tuệ nhân tạo (AI) sẽ đạttốc độ tăng trưởng hàng năm là 17,2% vào năm 2023. Trong 6 năm tới, thị trườngsẽ tăng từ mức 525...\n Startup thử sức với công nghệ tài chính Nhân Dân 4 liên quan Startup thử sức với công nghệ tài chính \nMảng công nghệ luôn hấp dẫn các bạn trẻ khởi nghiệp (startup). Trong đó, Fintech (viết tắt của từ financial technology - công nghệ trong tài chính) đang được các nhà đầu tư đặc...\n 8 chiến lược thuật toán của Robot giao dịch chứng khoán ĐTCK 1 liên quan 8 chiến lược thuật toán của Robot giao dịch chứng khoán \nTiếp sau bài giới thiệu Robot giao dịch chứng khoán, Báo Đầu tư Chứng khoán sẽ thống kêxin giới thiệu 8 chiến lược phổ biến nhất trên thị trường chứng khoán quốc tế và tại Việt Nam...\n Apple tìm kiếm kỹ sư phát triển Siri có kiến thức tâm lý học VnReview 1 liên quan Apple tìm kiếm kỹ sư phát triển Siri có kiến thức tâm lý học \nApple đang tìm kiếm các kỹ sư phát triển Siri có nền tảng sâu sắc về tâm lý học và khả năng tạo dựng mối liên kết sâu sắc giữa con người và máy tính.\n Thương mại điện tử: Giao thoa giữa 'ảo' và 'thật' Bnews Thương mại điện tử: Giao thoa giữa 'ảo' và 'thật' \nThương mại điện tử trở thành một phần không thể thiếu của nền kinh tế toàn cầu và đóng một vai trò quan trọng trong việc thay đổi về cơ bản cách thức mà con người tương tác với...\n Khởi nghiệp với trí tuệ nhân tạo NCĐT 1 đăng lại 1 liên quan Khởi nghiệp với trí tuệ nhân tạo \nNguồn ảnh: Nhân vật cung cấp Sau nửa năm ra mắt, với nhiều hợp đồng hàng ngàn đô và tốc độ tăng trưởng 15-20%/tháng, Cinnamon đang tạo được lợi thế nhất định trong cuộc đua startup...\n Xu hướng Smart Home hiện diện rõ nét tại IFA 2017 Số Hóa Xu hướng Smart Home hiện diện rõ nét tại IFA 2017 \nTại triển lãm IFA ở Berlin, khách tham quan dễ dàng thấy rõ xu hướng nhà thông minh trong loạt sản phẩm từ tủ lạnh cho đến loa.Dù cải tiến thiết kế, nâng cấp camera và cấu hình,...\n Walmart bắt tay Google đấu với Amazon Đấu Thầu 1 đăng lại 1 liên quan Walmart bắt tay Google đấu với Amazon \nWalmart cho phép khách hàng mua sắm bằng giọng nói nhờ công cụ của Google, cạnh tranh trực tiếp với Amazon trong thu hút khách hàng.\n Đại gia bán lẻ Wal-Mart và Google bắt tay để 'đấu' với Amazon BizLIVE 1 đăng lại 6 liên quan Đại gia bán lẻ Wal-Mart và Google bắt tay để 'đấu' với Amazon \nCuộc chiến công nghệ giữa các \"ông lớn\" trở nên gay gắt hơn khi tập đoàn bán lẻ kỳ cựu Wal-Mart của Mỹ hợp tác với Google để cạnh tranh với gã khổng lồ thương mại điện tử Amazon,...\n Startup Việt ứng dụng công nghệ trí tuệ nhân tạo thế nào? Đầu Tư 1 đăng lại Startup Việt ứng dụng công nghệ trí tuệ nhân tạo thế nào? \nCác startup công nghệ còn nhiều cơ hội tham gia thị trường AI nếu mô hình kinh doanh thông minh, lộ trình phát triển sản phẩm rõ ràng.\n IBM Watson, hệ trí tuệ nhân tạo giúp chẩn đoán nhanh ung thư não Một Thế Giới IBM Watson, hệ trí tuệ nhân tạo giúp chẩn đoán nhanh ung thư não \nTheo IEEE Spectrum, IBM Watson, hệ trí tuệ nhân tạo có gắn tên với công ty IBM, chỉ cần vỏn vẹn 10 phút để phân tích bộ gien của bệnh nhân mắc bệnh ung thư não và đề xuất phác đồ...\n 1 2 » Tin nóng Chủ tịch Quốc hội nêu 12 thách thức của đất nước PLO Chủ tịch Quốc hội nêu 12 thách thức của đất nước Vì sao Mỹ không thể 'ra tay' với Triều Tiên? Infonet Vì sao Mỹ không thể 'ra tay' với Triều Tiên? Bất ngờ phiên tòa vụ VN Pharma quay lại xét hỏi thay vì tuyên án Lao Động Bất ngờ phiên tòa vụ VN Pharma quay lại xét hỏi thay vì tuyên án 'Luật ngầm' trên sông Thanh Niên 'Luật ngầm' trên sông Hàng loạt công ty bán hàng đa cấp 'khai tử' PLO Hàng loạt công ty bán hàng đa cấp 'khai tử' Những sao Việt nỗ lực lấy lại hình ảnh sau scandal 'để đời' PNNews Những sao Việt nỗ lực lấy lại hình ảnh sau scandal 'để đời' Mẹ tìm thấy con gái sau 43 năm bị trao nhầm tại nhà hộ sinh ở Hà Nội SaoStar Mẹ tìm thấy con gái sau 43 năm bị trao nhầm tại nhà hộ sinh ở Hà Nội Tòa tiếp tục xét hỏi vụ VN Pharma NLĐ Tòa tiếp tục xét hỏi vụ VN Pharma Tranh cãi gay gắt: Thu nhập 30-40 triệu/ tháng có nên mua xe ô tô trả góp không? VTC Tranh cãi gay gắt: Thu nhập 30-40 triệu/ tháng có nên mua xe ô tô trả góp không? Bắt được cá trê 'khủng' dài gần 1m Thanh Niên Bắt được cá trê 'khủng' dài gần 1m Video Trước thủ tướng Áo, thế giới đã có những nhà lãnh đạo trẻ nổi tiếng nào? iOne Trước thủ tướng Áo, thế giới đã có những nhà lãnh đạo trẻ nổi tiếng nào? Cách làm món bánh mì chuối bổ dưỡng PNNews Cách làm món bánh mì chuối bổ dưỡng Tiêu chí chọn bạn trai dài 2 trang A4 của cô giáo tiếng Anh VietnamNet Tiêu chí chọn bạn trai dài 2 trang A4 của cô giáo tiếng Anh Clip: Phóng ngược chiều trên cầu vượt, xe máy suýt tông vào đầu ô tô Người Đưa Tin Clip: Phóng ngược chiều trên cầu vượt, xe máy suýt tông vào đầu ô tô Ngày 26/10, Quốc hội phê chuẩn bổ nhiệm Bộ trưởng Giao thông vận tải và Tổng Thanh tra Chính phủ VTC Ngày 26/10, Quốc hội phê chuẩn bổ nhiệm Bộ trưởng Giao thông vận tải và Tổng Thanh tra Chính phủ Putin trổ tài nói tiếng Anh trước thanh niên thế giới PNNews Putin trổ tài nói tiếng Anh trước thanh niên thế giới Ngoại trưởng Mỹ kêu gọi các nước Vùng Vịnh cô lập Iran Zing Ngoại trưởng Mỹ kêu gọi các nước Vùng Vịnh cô lập Iran Công Phượng lại muốn ra nước ngoài thi đấu VTC Công Phượng lại muốn ra nước ngoài thi đấu Xác ướp quấn dây thừng kì dị bất ngờ dạt bờ sông ở Anh Tiền Phong Xác ướp quấn dây thừng kì dị bất ngờ dạt bờ sông ở Anh Những màn mix&match dở tệ ở Hoa hậu Hoàn vũ Việt Nam tập 4 2Sao Những màn mix&match dở tệ ở Hoa hậu Hoàn vũ Việt Nam tập 4 Nóng trong ngày Trong tuần Theo dõi Báo Mới Báo Mới \nFanpage Báo Mới \nGiải Trí Gửi tin nóng \ncho bạn Google \nPlus Xu hướng đảng dân chủ tự do Thanh Thắng Sân Cẩm Phả FLC Thanh Hóa Hoàng Công Truyện Công an xã Đỉnh Sơn VN Pharma Than Quảng Ninh Phan Văn Sáu Sở Y tế Thừa Thiên Tottenham 4-1 Liverpool Everton 2-5 Arsenal Công an huyện Quỳnh Phụ Abe Shinzō Công an tỉnh Đồng Nai Võ Đình Thường hạ nghị viện Nguyễn Thị Kim Ngân Trần Văn Phú bộ trưởng bộ y tế PHIÊN BẢN KHÁC Báo Mới APPS Báo Mới ENGLISH Báo Mới BLOG LIÊN HỆ Giới thiệu Điều khoản sử dụng Quảng cáo NGƯỜI DÙNG Nhúng tin vào trang web Thống kê & So sánh \nBÁO MỚI thực hiện việc tổng hợp\n \nvà sắp xếp các thông tin tự động\n \nbởi chương trình máy tính\n Giấy phép số 46/GP-TTĐT do Sở Thông tin và Truyền thông Hà Nội cấp ngày 13/01/2012 Đơn vị chủ quản: Công ty Cổ phần Công nghệ EPI * Chịu trách nhiệm: Nguyễn Thanh Tùng Địa chỉ: Tầng 7, Tòa nhà Báo Sinh Viên VN, D29 Phạm Văn Bạch, Yên Hòa, Cầu Giấy, Hà Nội \nTel: (04) 3-212-3232 ext. 2947\n Cung cấp trong: 29.46 ms. Đăng nhập Đăng nhập tài khoản Facebook Đăng nhập tài khoản Google ×",
          "relevance": "0",
          "title": "xử lý ngôn ngữ tự nhiên",
          "url": "http://www.baomoi.com/tag/x%E1%BB%AD-l%C3%BD-ng%C3%B4n-ng%E1%BB%AF-t%E1%BB%B1-nhi%C3%AAn.epi"
        },
        {
          "content": "\r\n\t\t\t\t\t\tVietlex have developed this website for freely access to all people who want to study, research, and understand linguistics generally and Vietnamese specifically. Vietnamese Corpus of Vietlex with more than 150.000.000 syllables.  \r\n\t\t\t\t\t\tAll home and foreign collaborations and cooperations with Our Centre are welcome ! \r\n\t\t\t\t\t\r\n\t\t\t\t\t Lịch vạn niên Tra từ điển Kho ngữ liệu Đặt mua sách Sản phẩm Xử lí ngôn ngữ Ngôn ngữ học Trang chủ Giới thiệu \r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tBài trong chuyên mục\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  NHẬN DẠNG THỰC THỂ ĐỊNH DANH TRONG VĂN BẢN TIẾNG VIỆT  XÂY DỰNG MÔ HÌNH NGỮ NGHĨA ÁP DỤNG CHO VIỆC BIÊN SOẠN TỪ ĐIỂN GIẢI THÍCH  Hướng tới việc xây dựng MẠNG TỪ tiếng Việt  Xây dựng kho ngữ liệu áp dụng cho phân tích, xử lí ngôn ngữ và biên soạn từ điển  Ứng dụng phương pháp Pointwise vào bài toán tách từ cho tiếng Việt  Xác định dãy từ đồng nghĩa khi xây dựng Wordnet tiếng Việt  Xác định ĐƠN VỊ TỪ VỰNG MỚI xuất hiện trong văn bản tiếng Việt  Ngôn ngữ học máy tính và việc xây dựng từ điển  Về xử lý tiếng Việt trong công nghệ thông tin  Automated Extraction of Tree Adjoining Grammars from a Treebank for Vietnamese  Building a Large Syntactically-Annotated Corpus of Vietnamese  A Lexicon for Vietnamese Language Processing  IEEE VÀ HỘI NGHỊ QUỐC TẾ IEEE-RIVF’10 TẠI HÀ NỘI  Tìm hiểu mô hình từ điển dùng cho xử lí ngôn ngữ tự nhiên  Lexical descriptions for Vietnamese language processing  Developing Tools and Building Linguistic Resources  for Vietnamese Morpho-Syntactic Processing  A Case Study in POS Tagging of Vietnamese Texts  Sử dụng bộ gán nhãn từ loại xác suất QTAG cho văn bản tiếng Việt  Quy tắc sắp xếp đơn vị từ vựng trong từ điển tiếng Việt  Quy tắc đặt dấu thanh trong tiếng Việt \r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tXử lí ngôn ngữ\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t NHẬN DẠNG THỰC THỂ ĐỊNH DANH TRONG VĂN BẢN TIẾNG VIỆT (Bài báo cập nhật trên cơ sở bài gửi tham gia  Hội thảo khoa học  Giữ gìn sự trong sáng của tiếng Việt trên các phương tiện thông tin đại chúng . 5-11-2016 ) \r\n\t  \r\n\t1. Giới thiệu Nhận dạng thực thể định danh  (Named Entity Recognition – NER), còn gọi là  nhận dạng thực thể có tên , là nhiệm vụ nhận biết các từ xuất hiện trong văn bản là tên gọi của một đối tượng nào đó, như  tên người  (nhân danh),  tên đất  (địa danh, địa điểm),  tên tổ chức, tên tác phẩm, tên sự kiện, thời gian, tiền tệ , v.v. NER đóng vai trò quan trọng trong các ứng dụng tự động trích xuất thông tin, khai phá dữ liệu, dịch máy, v.v. \r\n\tNER đã được quan tâm nghiên cứu trên thế giới từ đầu những năm 1990. Hiện nay các hệ thống NER cho tiếng Anh, tiếng Đức, tiếng Hà Lan, v.v. đã được xây dựng và được đánh giá cao. Điểm chung của các hệ thống NER này là tập trung đi vào nhận dạng 3 ...        VŨ XUÂN LƯƠNG – Vietlex Chi tiết   XÂY DỰNG MÔ HÌNH NGỮ NGHĨA ÁP DỤNG CHO VIỆC BIÊN SOẠN TỪ ĐIỂN GIẢI THÍCH \r\n\t1. ĐẶT VẤN ĐỀ \r\n\tQuá trình con người nhận biết, hiểu biết về thế giới khách quan hầu như không có sự khác biệt giữa các dân tộc trên thế giới. Có chăng đó là sự khác biệt về cách thức tư duy, tức là cách thức sử dụng các đơn vị ngôn ngữ để phản ánh và tái hiện hiện thực vào trong tư duy. Người ta có thể hiểu biết về một đối tượng nào đó không có ở trước mặt khi nhắc đến tên gọi của nó. Tên gọi đó được biểu thị thông qua đơn vị từ. Như vậy, giữa từ và các đối tượng của hiện thực có một mối liên hệ với nhau. Tất cả những gì được thể hiện qua mối liên hệ ấy chính là nội dung của từ, cũng tức là ý nghĩa của từ. “Ý nghĩa là cái quyết định, là lí do tồn tại của ngôn ngữ. Không một đối tượng nào của ngôn ngữ học mà không liên hệ với ý nghĩa.” [3] \r\n\tCách thức tư duy ở mỗi cộng ...        VŨ XUÂN LƯƠNG – Vietlex Chi tiết   Hướng tới việc xây dựng MẠNG TỪ tiếng Việt \r\n\tSáng ngày 19-3-2014, tại Viện Hàn lâm Khoa học Xã hội Việt Nam (Số 1, Liễu Giai, Hà Nội) đã diễn ra Hội thảo khoa học với chủ đề “ Hướng tới việc xây dựng Mạng từ tiếng Việt ” , thuộc Đề tài Khoa học Công nghệ cấp Nhà nước: “ Nghiên cứu, xây dựng và phát triển một số tài nguyên và công cụ thiết yếu cho xử lí văn bản tiếng Việt ” do Công ty Cổ phần dịch vụ CNTT NaisCorp tổ chức. Hội thảo đã thu hút được đông đảo các chuyên gia hàng đầu về ngôn ngữ học và tin học từ các viện nghiên cứu và các trường đại học tới dự, như: Viện Ngôn ngữ học, Viện Từ điển học và Bách khoa thư, Viện Công nghệ Thông tin, Trường Đại học Công nghệ, Trường Đại học Khoa học Tự nhiên, Trường Đại học Bách khoa, Học viện Kĩ thuật quân sự, Trung tâm Từ điển học, v.v. Vietlex xin đăng  Lời giới thiệu  của TS Nguyễn Phương Thái (chủ nhiệm Đề ...  Chi tiết   Xây dựng kho ngữ liệu áp dụng cho phân tích, xử lí ngôn ngữ và biên soạn từ điển \r\n\tSUMMARY \r\n\tThere has been much articlemention the importance of the  corpus  for the development of  corpus linguistics  in the last ten years. However, the situation of Vietnam now, not much deeply articleon this issue. The authors noted that corpus is useful for language study, especially for compiling the dictionary. This article presented an overview about the concepts  corpus  and  corpus linguistics , and also tries to present more deeply about how to build common corpus today. \r\n\t  \r\n\tTÓM TẮT \r\n\tĐã có nhiều bài viết nói về tầm quan trọng của  kho ngữ liệu  (corpus) đối với sự phát triển của  ngôn ngữ học ngữ liệu  (corpus linguistics) trong khoảng 10 năm trở lại đây. Tuy nhiên, trong bối cảnh Việt Nam hiện nay, chưa có nhiều bài nghiên cứu đi sâu vào vấn đề này. Nhận thấy kho ngữ liệu ngày càng trở nên hữu ích cho nghiên cứu ngôn ngữ, đặc biệt là cho biên soạn từ điển, bài báo sẽ giới thiệu khái quát về khái niệm  kho ngữ ...        VŨ XUÂN LƯƠNG – Vietlex Chi tiết   Ứng dụng phương pháp Pointwise vào bài toán tách từ cho tiếng Việt \r\n\tNatural Language Processing Laboratory Department of Electrical Engineering \r\n\tNagaoka University of Technology 940-2188, Nagaoka City, Niigata, Japan \r\n\t  Abstract \r\n\tTrong tiếng Việt, dấu cách (space) không được sử dụng như 1 kí hiệu phân tách từ, nó chỉ có ý nghĩa phân tách các âm tiết với nhau. Vì thế, để xử lý tiếng Việt, bài toán tách từ (word segmentation) là 1 trong những bài toán cơ bản và quan trọng bậc nhất. Ngoài tiếng Việt, có khá nhiều các ngôn ngữ châu Á khác cũng cần bước tách từ, ví dụ như: tiếng Nhật, tiếng Trung, tiếng Hàn,… do đó vấn đề này nhận được sự quan tâm rộng rãi và có nhiều hướng tiếp cận khác nhau. Bài viết này sẽ tập trung phân tích hướng tiếp cận pointwise dựa trên máy học SVM: phân loại từng dấu cách một cách độc lập vào 2 loại: SPACE (kí hiệu tách từ) và UNDERSCORE (kí hiệu liên kết 2 âm tiết). Với phương pháp này, chúng tôi đã đạt được độ chính xác 98.2% trong thực ...        Lưu Tuấn Anh, Yamamoto Kazuhide Chi tiết   1 2 3 4 \r\n\t\t\t\t\t© Copyrights 2000 - 2017 Trung tâm Từ điển học - Vietnam Lexicography Centre (Vietlex).   \r\n\t\t\t\t\t© Ghi rõ nguồn \"Vietlex\" khi phát hành lại thông tin từ Website này.\r\n\t\t\t\t\t\t\t\t\r\n\t\t\t\t",
          "relevance": "0",
          "title": "Lịch vạn niên",
          "url": "http://www.vietlex.com/xu-li-ngon-ngu"
        },
        {
          "content": "Toggle navigation DEHA VIETNAM DEHA SOLUTIONS Xử Lý Ngôn Ngữ Tự Nhiên với Python – Phần 1 2017-08-25 Hoang Giang Bien \n\tXin chào anh em, đợt này mình có tham gia một dự án khá thú vị về AI. Vai trò của mình trong dự án và thiết kế các thành phần \"biên\", hiểu đơn giản là những thứ râu ria bên ngoài hệ thống Trí tuệ nhân tạo kia. Ví dụ viết Mobile App, Web quảng bá, xử lí truy cập API, xử lí dữ liệu đầu vào… Cũng là cơ may được làm việc với ngôn ngữ Python và đặc biệt là xử lí ngôn ngữ tự nhiên với thư viện NLTK. Sau đây mình sẽ chia sẻ với các bạn những trải nghiệm của mình với việc \"Xử Lý Ngôn Ngữ Tự Nhiên – NLP\" cũng như Python và NLTK trong thời gian qua. Đây hầu hết là những kiến thức cơ bản về NLP dành cho Developer, không cần các bạn phải giỏi những kỹ thuật chuyên sâu hay các thuật toán phức tạp. Vì chúng ta cũng biết, NLP là một nhánh của Trí Tuệ Nhân Tạo và phải nói là khó nhất. Hy vọng bài viết sẽ mang lại những kiến thức hữu ích, giúp bạn tự tin hơn trong việc tìm hiểu AI nói chung và NLP nói riêng. Nào chúng ta cùng bắt đầu!\n \n\t1. Ngôn ngữ tự nhiên là gì? \n\tNgôn ngữ tự nhiên là ngôn ngữ mà các loài động vật sáng tạo ra để giao tiếp với đồng loại. Con người cũng là một loại động vật sử dụng ngôn ngữ để giao tiếp. Thế giới ngôn ngữ của con người rất phong phú, theo thông kê của các nhà khoa học thì có tới hàng ngàn ngôn ngữ tồn tại trên trái đất. Ngôn ngữ tự nhiên có 2 dạng là chữ viết và âm thanh (tức tiếng nói). Ngôn ngữ của mỗi dân tộc, quốc gia lại khác nhau bao gồm khác nhau cả về cách viết cũng như cách phát âm.\n \n\t2. Tại sao cần phải \"Xử Lý Ngôn Ngữ Tự Nhiên\". \n\tXử Lý Ngôn Ngữ Tự Nhiên có vai trò hết sức quan trọng trong ngành Khoa Học Máy Tính. Nó có vô vàn ứng dụng hữu ích trong cuộc sống cũng như nghiên cứu. Chúng ta có thể điểm qua một vài ứng dụng của xử lý ngôn ngữ tự nhiên như:\n \n\t\t\tNhận dạng chữ viết: Có hai kiểu nhận dạng, thứ nhất là nhận dạng chữ in, ví dụ nhận dạng chữ trên sách giáo khoa rồi chuyển nó thành dạng văn bản điện tử như dưới định dạng doc của Microsoft Word chẳng hạn. Phức tạp hơn là nhận dạng chữ viết tay, có khó khăn bởi vì chữ viết tay không có khuôn dạng rõ ràng và thay đổi từ người này sang người khác. Với chương trình nhận dạng chữ viết in có thể chuyển hàng ngàn đầu sách trong thư viện thành văn bản điện tử trong thời gian ngắn. Nhận dạng chữ viết của con người có ứng dụng trong khoa học hình sự và bảo mật thông tin (nhận dạng chữ ký điện tử).\n\t\t \n\t\t\tNhận dạng tiếng nói: Nhận dạng tiếng nói rồi chuyển chúng thành văn bản tương ứng. Giúp thao tác của con người trên các thiết bị nhanh hơn và đơn giản hơn, chẳng hạn thay vì gõ một tài liệu nào đó bạn đọc nó lên và trình soạn thảo sẽ tự ghi nó ra. Đây cũng là bước đầu tiên cần phải thực hiện trong ước mơ thực hiện giao tiếp giữa con người với robot. Nhận dạng tiếng nói có khả năng trợ giúp người khiếm thị rất nhiều.\n\t\t \n\t\t\tTổng hợp tiếng nói: Từ một văn bản tự động tổng hợp thành tiếng nói. Thay vì phải tự đọc một cuốn sách hay nội dung một trang web, nó tự động đọc cho chúng ta. Giống như nhận dạng tiếng nói, tổng hợp tiếng nói là sự trợ giúp tốt cho người khiếm thị, nhưng ngược lại nó là bước cuối cùng trong giao tiếp giữa robot với người.\n\t\t \n\t\t\tDịch tự động (Machine translate): Như tên gọi đây là chương trình dịch tự động từ ngôn ngữ này sang ngôn ngữ khác. Một phần mềm điển hình về tiếng Việt của chương trình này là Evtrans của Softex, dịch tự động từ tiếng Anh sang tiếng Việt và ngược lại, phần mềm từng được trang web vdict.com mua bản quyền, đây cũng là trang đầu tiên đưa ứng dụng này lên mạng. Tháng 10 năm 2008 có hai công ty tham gia vào lĩnh vực này cho ngôn ngữ tiếng Việt là công ty Lạc Việt (công ty phát hành từ điển Lạc Việt) và Google, một thời gian sau đó Xalo.vn cũng đưa ra dịch vụ tương tự.\n\t\t \n\t\t\tTìm kiếm thông tin (Information retrieval): Đặt câu hỏi và chương trình tự tìm ra nội dung phù hợp nhất. Thông tin ngày càng đầy lên theo cấp số nhân, đặc biệt với sự trợ giúp của Internet việc tiếp cận thông tin trở lên dễ dàng hơn bao giờ hết. Việc khó khăn lúc này là tìm đúng nhất thông tin mình cần giữa bề bộn tri thức và đặc biệt thông tin đó phải đáng tin cậy. Các máy tìm kiếm dựa trên giao diện web như Google hay Yahoo hiện nay chỉ phân tích nội dung rất đơn giản dựa trên tần suất của từ khoá và thứ hạng của trang và một số tiêu chí đánh giá khác để đưa ra kết luận, kết quả là rất nhiều tìm kiếm không nhận được câu trả lời phù hợp, thậm chí bị dẫn tới một liên kết không liên quan gì do thủ thuật đánh lừa của các trang web nhằm giới thiệu sản phẩm (có tên tiếng Anh là SEO viết tắt của từ Search Engine Optimization). Thực tế cho đến bây giờ chưa có máy tìm kiếm nào hiểu được ngôn ngữ tự nhiên của con người trừ trang  www.ask.com  được đánh giá là \"hiểu\" được những câu hỏi có cấu trúc ở dạng đơn giản nhất. Mới đây cộng đồng mạng đang xôn xao về trang Wolfram Alpha, được hứa hẹn là có khả năng hiểu ngôn ngữ tự nhiên của con người và đưa ra câu trả lời chính xác. Lĩnh vực này hứa hẹn tạo ra bước nhảy trong cách thức tiếp nhận tri thức của cả cộng đồng.\n\t\t \n\t\t\tTóm tắt văn bản: Từ một văn bản dài tóm tắt thành một văn bản ngắn hơn theo mong muốn nhưng vẫn chứa những nội dung thiết yếu nhất.\n\t\t \n\t\t\tKhai phá dữ liệu (Data mining) và phát hiện tri thức: Từ rất nhiều tài liệu khác nhau phát hiện ra tri thức mới. Thực tế để làm được điều này rất khó, nó gần như là mô phỏng quá trình học tập, khám phá khoa học của con người, đây là lĩnh vực đang trong giai đoạn đầu phát triển. Ở mức độ đơn giản khi kết hợp với máy tìm kiếm nó cho phép đặt câu hỏi để từ đó công cụ tự tìm ra câu trả lời dựa trên các thông tin trên web mặc cho việc trước đó có câu trả lời lưu trên web hay không (giống như trang Yahoo! hỏi và đáp, nơi chuyên đặt các câu hỏi để người khác trả lời), nói một cách nôm na là nó đã biết xử lý dữ liệu để trả lời câu hỏi của người sử dụng, thay vì máy móc đáp trả những gì chỉ có sẵn trong bộ nhớ. \n\t\t\t(Nguồn: Wikipedia)\n\t\t \n\t3. Tại sao lại sử dụng Python trong xử lý ngôn ngữ tự nhiên. \n\tPython ra đời năm 1991, và là một ngôn ngữ thông dịch. Trải qua hơn 20 năm phát triển, Python là một trong những ngôn ngữ được sử dụng nhiều nhất trong dậy lập trình và nghiên cứu khoa học. Rất nhiều trường đại học sử dụng Python để dậy về lập trình cho các sinh viên ngành Khoa Học Máy Tính. Rất nhiều công ty lớn sử dụng Python để xây dựng hệ thống như Google, Youtube, Instagram, Dropbox, Atlassian… Python là một ngữ sử dụng được cho nhiều mô hình lập trình, đơn giản khi học và sử dụng. Tôi sử dụng Python chưa lâu nhưng khi so sánh việc Code sử dụng Pythong thì nó ngắn hơn rất nhiều so với khi viết bằng PHP hoặc Java. Bạn có thể bay bổng tự do với Python hoặc cũng có thể bắt nó trở lên vững chắc và mạnh mẽ như Java. Theo những thông tin mà tôi được biết thì Python cũng là một ngôn ngữ rất phát triển trong lĩnh vực Data Science và Machine Learning. Python cũng cung cấp những hàm và thư viện xử lý ngôn ngữ tuyệt vời. Scikit-learn và Tensor-flow là 2 thư viện Machine Learning nổi tiếng được viêt bằng Python. Đứng ở góc độ người tiếp cận sau, cá nhân tôi thấy Python là một lựa chọn hợp lý khi làm Xử Lý Ngôn Ngữ Tự Nhiên.\n \n\t4. Giới thiệu về NLTK. \n\tNLTK hay Natural Language Toolkit – Bộ công cụ ngôn ngữ tự nhiên, là một thư viện được viết bằng Python hỗ trợ xử lý ngôn ngữ tự nhiên. Bằng cách cung cấp các cơ chế và kỹ thuật xử lý ngôn ngữ phổ biến, nó giúp cho việc xử lý ngôn ngữ tự nhiên trở lên dễ dàng và nhanh chóng hơn. Được viết bởi Steven Bird và Edward Loper, làm việc tại Khoa Máy Tính, Đại Học Pennsylvania, Hoa Kỳ và năm 2001. Ngoài việc hỗ trợ xử lý ngôn ngữ, NLTK còn có các mô phỏng đồ hoạ và dữ liệu mẫu hữu ích. NLTK cung cấp các xử lý như classification, tokenization, stemming, tagging, parsing, và semantic reasoning… Những ứng dụng này chúng ta sẽ dần được tìm hiểu ở những bài viết sau. Ngoài việc phục vụ xử lý ngôn ngữ tự nhiên, NLTK còn được sử dụng trong Machine Learning với tác dụng làm sạch dữ liệu, xử lý dữ liệu đầu vào cho các thuật toán Machine Learning.\n \n\t5. Tổng kết. \n\tTrên đây, tôi đã giới thiếu cho các bạn sơ lược về NLP và những thứ chúng ta cần để bắt đầu việc xử lý ngôn ngữ tự nhiên bằng Pyhthon và NLTK. Ở bài viết sau, tôi sẽ hướng dẫn các bạn cách cài đặt Python 3 và NLTK. NLP .  permalink .\n\t\t\t\t\t\t Post navigation  Tại sao các công ty Product không thích nhân sự Outsourcing? Làm người tử tế: Xây dựng và phát triển phong trào một cách tử tế tử tế  Leave a Reply  Cancel reply Your email address will not be published.  Required fields are marked  * Comment Name  * Email  * Website Currently you have JavaScript disabled. In order to post comments, please make sure JavaScript and Cookies are enabled, and reload the page. Click here for instructions on how to enable JavaScript in your browser. Search for: Recent Posts Làm người tử tế: Làm sếp, suy nghĩ cho tử tế Làm người tử tế: Lập trình viên tử tế Làm người tử tế: Xây dựng và phát triển phong trào một cách tử tế tử tế Xử Lý Ngôn Ngữ Tự Nhiên với Python – Phần 1 Tại sao các công ty Product không thích nhân sự Outsourcing? Recent Comments test  on  Làm người tử tế: Lập trình viên tử tế Kien  on  Nỗi cực Tại sao các công ty Product không thích nhân sự Outsourcing? – DEHA's blog  on  Together we make a difference Coder  on  SOLID – Thiết kế code chuyên nghiệp Vinh Pham  on  Gương mặt của tuần Archives October 2017 September 2017 August 2017 June 2017 April 2017 March 2017 February 2017 January 2017 November 2016 October 2016 September 2016 May 2016 Categories Books DevOps Git How to ? Japanese NLP Soft Skills Uncategorized Meta Log in Entries  RSS Comments  RSS WordPress.org \n\t\t\t\t\tdazzling\t\t\t\t\tTheme by  Colorlib  Powered by  WordPress",
          "relevance": "1",
          "title": "Xử Lý Ngôn Ngữ Tự Nhiên với Python – Phần 1",
          "url": "http://seal.deha.vn/xu-ly-ngon-ngu-tu-nhien-voi-python-phan-1/"
        },
        {
          "content": "Ông Xuân Hồng Chia sẻ kiến thức và thông tin về Machine learning Menu Skip to content Data Science Big Data Deep learning Exploratory Data Analysis Getting and cleaning data Machine learning Lập trình Python Spark R Weka Kiến thức Toán Statistical Inference Xử lý ngôn ngữ tự nhiên – Natural Language Processing (NLP) Chia sẻ Dự án About \n\t\t\t\t\tXử lý ngôn ngữ tự nhiên – Natural Language Processing (NLP)\t\t\t\t Tổng hợp kiến thức Xử lý ngôn ngữ tự nhiên – Natural Language Processing (NLP) Information extraction – Bài toán rút trích thông tin trong văn bản Tháng Tám 28, 2017 Tháng Tám 29, 2017 Ông Xuân Hồng 3 phản hồi Facebook Twitter Google LinkedIn Like this: Số lượt thích Đang tải... Gán nhãn từ loại (Part-of-Speech tagging POS) Tháng Chín 2, 2016 Ông Xuân Hồng 6 phản hồi Tagging problem Trong nhiều tác vụ của Xử lý ngôn ngữ tự nhiên (XLNNTN), ta mong muốn xây dựng được một mô hình mà chuỗi các quan sát đầu vào (từ, ngữ, câu,…) đi kèm với chuỗi các nhãn đầu ra (từ loại, ranh giới ngữ, tên thực thể,…) gọi là  pairs of sequences . Gán nhãn từ loại (Part-of-speech tagging – POS) có lẽ là bài toán sớm nhất được nghiên cứu và được mọi người biết đến khi nhập môn chuyên ngành XLNNTN. Trong bài viết này, ta sẽ tìm hiểu về bài toán gán nhãn từ loại, các hướng tiếp cận và thuật toán cơ bản để giải quyết vấn đề này. Tiếp tục đọc  → Facebook Twitter Google LinkedIn Like this: Số lượt thích Đang tải... Language Modeling là gì Tháng Tám 27, 2016 Tháng Tám 28, 2016 Ông Xuân Hồng 2 phản hồi Language model Trong bài viết này, ta sẽ tìm hiểu thế nào là một mô hình ngôn ngữ (language modeling). Làm sao để xây dựng được một mô hình ngôn ngữ từ tập các mẫu câu của một ngôn ngữ bất kỳ (Anh, Việt, Nhật, …). Mô hình ngôn ngữ ban đầu được ứng dụng trong nhận dạng tiếng nói (speech recognition) và đã được áp dụng vào trong những tác vụ khác liên quan trong lĩnh vực Xử lý ngôn ngữ tự nhiên (Natural Language Processing – NLP) như gán nhãn từ loại (tagging), phân tích cây cú pháp (parsing), dịch máy (machine translation), … Tại sao chúng ta cần mô hình ngôn ngữ? Lý do thứ nhất, mô hình này cung cấp cho bạn thông tin về phân bố xác suất tiền nghiệm (prior distribution)   để xét xem câu gồm các từ đầu vào có phù hợp hay không với ngôn ngữ xác định. Ví dụ, ta sẽ có xác suất của câu   nhờ vậy mà ta xác định được câu  “tối nay được đi chơi rồi vui quá”  sẽ phù hợp hơn với ngôn ngữ tiếng Việt hơn câu hai  “quá rồi vui đi chơi tối” . Thứ hai, các kĩ thuật liên quan đến ước lượng tham số cho mô hình thông qua tập dữ liệu huấn luyện cho trước được sử dụng trong các mô hình khác như Hidden Markov Model, Natural Language Parsing. Và cuối cùng, đây là một trong những cơ sở kiến thức để các bạn đọc hiểu được các bài viết liên quan đến Long short-term memory (LSTM). Tiếp tục đọc  → Facebook Twitter Google LinkedIn Like this: Số lượt thích Đang tải... Giới thiệu các công cụ Xử lý ngôn ngữ tự nhiên Tháng Hai 6, 2016 Tháng Hai 24, 2017 Ông Xuân Hồng 4 phản hồi NLP tools Nếu bạn đang làm việc và nghiên cứu trên ngôn ngữ tiếng Anh thì ta có thể sử dụng các thư viện/module NLP của Python được liệt kê bên dưới. Mục đích của bài viết này được dùng để liệt kê những thư viện/module và những chức năng hữu ích trong NLP. Các bạn có thể tham khảo danh sách các thuật ngữ liên quan đến các chức năng ở  bài viết này . Tiếp tục đọc  → Facebook Twitter Google LinkedIn Like this: Số lượt thích Đang tải... Các thuật ngữ trong Xử lý ngôn ngữ tự nhiên Tháng Hai 5, 2016 Tháng Mười Một 11, 2016 Ông Xuân Hồng 10 phản hồi Natural Language Processing Vai trò của  Xử lý ngôn ngữ tự nhiên-XLNNTN (Natural Language Processing-NLP)  trong khai thác Big Data là không thể phủ nhận trong bối cảnh phát triển của doanh nghiệp hiện nay. Đối với ngôn ngữ tiếng Anh, ta đã được kế thừa nhiều tri thức cũng như nhiều công cụ có sẵn để áp dụng ngay vào thực tiễn. Tuy nhiên, đối với ngôn ngữ tiếng Việt, ta vẫn còn gặp nhiều khó khăn (nhân sự có chuyên môn còn hạn chế, ngữ liệu để huấn luyện chưa đủ lớn) bên cạnh những cơ hội rất lớn (thị trường Việt Nam chưa được khai thác) cho những ai đam mê lĩnh vực này. Vì vậy, trong bài viết này, tôi xin lập ra danh sách các thuật ngữ thường gặp trong NLP để tiện tham khảo cũng như giúp cho những bạn mới bắt đầu có thể nhanh chóng tra cứu sơ để tiến hành nghiên cứu ngay các tài liệu khoa học. Bài viết sẽ luôn được cập nhật. Nếu có các thuật ngữ chưa rõ, các bạn có thể comment để chúng ta tiếp tục mở rộng thêm danh sách này. Tiếp tục đọc  → Facebook Twitter Google LinkedIn Like this: Số lượt thích Đang tải... Natural Language Processing MindMap Bộ sưu tập Tháng Chín 28, 2015 Tháng Tám 18, 2016 Ông Xuân Hồng Bạn nghĩ gì về bài viết này? Facebook Twitter Google LinkedIn Like this: Số lượt thích Đang tải... Xử lý ngôn ngữ tự nhiên (Natural Language Processing) là gì? Tháng Tám 13, 2015 Tháng Sáu 19, 2017 Ông Xuân Hồng Bạn nghĩ gì về bài viết này? natural langage processing Các doanh nghiệp hiện nay đang đối mặt với “cơn lũ” dữ liệu về mọi mặt: feedback của khách hàng, thông tin đối thủ cạnh tranh, emails của khách hàng, tweets, thông tin họp báo, hồ sơ pháp lý, các văn bản về sản phẩm và kĩ thuật. Việc khai thác được những dữ liệu này là điểm mấu chốt để các doanh nghiệp có thể triển khai nhanh chóng các quyết định của mình so với đối thủ cạnh tranh. Vấn đề ở đây là gì? Có quá nhiều thông tin để xử lý cùng lúc (hơn 85% dữ liệu trên thế giới không có cấu trúc), và kích thước dữ liệu ngày càng tăng. Đối với nhiều doanh nghiệp, điều này là bất khả thi để điều động nhân sự đọc tất cả mọi thứ được cho là quan trọng (các khách hàng đang nói gì về sản phẩm, những đối thủ cạnh tranh của chúng ta đang làm gì). Tiếp tục đọc  → Facebook Twitter Google LinkedIn Like this: Số lượt thích Đang tải... Bạn Không Thể Sống Một Mình Nếu ngày mai tớ trở thành người nổi tiếng. Gặp tớ giữa đường bạn đừng ngại tớ nghe. Cứ nhìn tớ như ngày xưa bạn thấy. Đừng nên nghĩ tớ quá xa vời.  Nếu một ngày tớ là kẻ ăn xin. Thì bố thí cho tớ vài kỷ niệm. Dù rất nhỏ nhưng đừng nên khinh tớ. Vì tớ vẫn là tớ của ngày xưa.  Nếu ngày mai tớ chẳng biết đường về. Hãy chỉ giùm tớ con đường đi đúng đắn. Đừng để tớ sa chân nơi bùn nhớp. Lúc rút chân lên thì đã bẩn mất rồi.  Nếu ngày mai tớ gặp bạn giữa đường. Thì ta sẽ gặp nhau và cười nhé. Đừng nên trách và đừng nên đánh mất. Đôi khi có một số người lướt qua cuộc đời bạn và ngay tức khắc bạn nhận ra rằng sự có mặt của họ ý nghĩa như thế nào. Họ đã dạy bạn những bài học, đã giúp bạn nhận ra giá trị của chính mình hoặc trở thành con người mà bạn từng mơ ước. Có lẽ bạn sẽ không biết được những con người này từ đâu đến ( bạn cùng phòng, người hàng xóm, vị giáo sư, người bạn mất liên lạc từ lâu hay thậm chí là một người hoàn toàn xa lạ ). Nhưng khi bạn thờ ơ với họ, hãy nhớ rằng trong từng khoảnh khắc họ sẽ ảnh hưởng rất sâu sắc đến cuộc đời bạn.  …Cuộc sống không hề làm khó bạn mà chính bạn đã tự làm khó mình bằng cách đặt ra những câu hỏi đại loại như: “Tại sao lại thế này?… Tại sao lại thế kia?…”. Không ai có thể trả lời giúp bạn mà chính bạn phải tự đi tìm lời giải đáp cho mình. Hãy luôn nghĩ về người khác trước khi nghĩ đến bản thân, chắc chắn lúc đó bạn sẽ hài lòng với câu trả lời của chính mình. Và đừng quên luôn đặt câu hỏi: “Mình đã làm được gì?” trước khi tự hỏi: “Mình đã nhận được gì?” nhé! Tôi tin là bạn sẽ thành công!   Thật ra trên đời, con người luôn cần có nhau để giải quyết rất nhiều vấn đề. Nhờ những lúc qua lại giúp đỡ đó mà hoá giải bao muộn phiền, nảy sinh các tình cảm, cùng trải nghiệm biết bao điều thú vị, hiểu biết thêm nhiều tri thức mới. Đây chính là cuộc sống! Vì thế hãy trân trọng những người đang “làm phiền” bên cạnh bạn nhé! Đừng để đến khi mất đi rồi mới nghẹn ngào lại bảo “giá như” … ĐĐ. GS. Thích Phước Tiến \n__(())__ Namo Bụt Sakyamuni Nhận Email khi có bài viết mới Liên hệ Email (bắt buộc) Comment (bắt buộc) Top bài viết Bài viết mới FAQ: Big data cho người mới bắt đầu Tháng Mười 2, 2017 Làm việc với Spark DataFrames – Truy vấn nâng cao (Subqueries, Cube, Rollup) Tháng Chín 15, 2017 Thống kê ứng dụng 3: Các vấn đề trong thống kê Tháng Chín 14, 2017 Thống kê ứng dụng 2: Suy luận thống kê Tháng Chín 12, 2017 Thống kê ứng dụng 1: Quan sát dữ liệu Tháng Chín 7, 2017 AI, Machine Learning, Deep Learning phân biệt như thế nào cho đúng Tháng Chín 4, 2017 Information extraction – Bài toán rút trích thông tin trong văn bản Tháng Tám 28, 2017 Bắt đầu nghiên cứu big data từ đâu và như thế nào Tháng Tám 3, 2017 Data Science – Mỏ vàng của Kỉ nguyên số Tháng Tám 3, 2017 SMA 2017 – Lý thuyết ra quyết định Tháng Sáu 17, 2017 Big Data Chia sẻ Data Science Deep learning Dự án Exploratory Data Analysis Getting and cleaning data Kiến thức Lập trình Machine learning Python R Spark Statistical Inference Toán Weka Xử lý ngôn ngữ tự nhiên - Natural Language Processing (NLP) This slideshow requires JavaScript. Thư viện Tháng Mười 2017  (1) Tháng Chín 2017  (5) Tháng Tám 2017  (3) Tháng Sáu 2017  (3) Tháng Năm 2017  (3) Tháng Ba 2017  (1) Tháng Một 2017  (3) Tháng Mười Hai 2016  (1) Tháng Mười Một 2016  (2) Tháng Mười 2016  (2) Tháng Chín 2016  (1) Tháng Tám 2016  (2) Tháng Bảy 2016  (2) Tháng Năm 2016  (1) Tháng Tư 2016  (1) Tháng Ba 2016  (2) Tháng Hai 2016  (2) Tháng Một 2016  (1) Tháng Mười Hai 2015  (6) Tháng Mười Một 2015  (5) Tháng Mười 2015  (6) Tháng Chín 2015  (11) Tháng Tám 2015  (16) Tháng Bảy 2015  (25) Tháng Sáu 2015  (4) Thống kê 360,709 hits Tháng Mười 2017 H B T N S B C « Th9       1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31   Data Science Lập trình Kiến thức Chia sẻ Dự án About Tạo một website miễn phí hoặc 1 blog với WordPress.com. Post to Hủy bỏ %d  bloggers like this:",
          "relevance": "1",
          "title": "Menu",
          "url": "https://ongxuanhong.wordpress.com/category/kien-thuc/xu-ly-ngon-ngu-tu-nhien-natural-language-processing-nlp/"
        },
        {
          "content": "Project name Trang chủ Tra cứu Tài liệu Đóng góp Giới thiệu Đăng ký Đăng nhập Đăng nhập \n                      Ghi nhớ\n                   Quên mật khẩu? Đăng nhập Bạn chưa có tài khoản? Hãy đăng ký. Tên đăng nhập hoặc mật khẩu chưa đúng Tài liệu Xử lý ngôn ngữ tự nhiên Social Sciences 0 XỬ LÝ NGÔN NGỮ TỰ NHIÊN VÀ TRÍ TUỆ NHÂN TẠO  Sự tiến hóa của ngôn ngữ  Vấn đề để hiểu được lời nói hành động giống như việc hiểu các vấn đề khác, tương tự như việc hiểu hình ảnh hoặc chẩn đoán y học. Chúng ta đưa ra một tập các đầu vào đa nghĩa và từ đó chúng ta làm ngược lại để quyết định trạng thái nào của thế giới có thể được tạo ra đầu vào. Hiểu được vấn đề của lời nói hành động là phần đặc tả của ngôn ngữ. Một phần của hiểu vấn đề có thể giải thích bằng các lí do logic. Chúng ta nhận thấy rằng các chuỗi logic liên kết lại là cách tốt để mô tả cách mà các từ và các cụm từ phối hợp để tạo ra một cụm từ lớn. Phần khác của việc hiểu vấn đề có thể chỉ được giải thích bởi các lí do kĩ thuật không rõ ràng. Thông thường có nhiều trạng thái của thế giới mà tất cả đều hướng dẫn đến một lời nói hành động tương tự, vì vậy người hiểu phải quyết định cái mà nó dễ xảy ra hơn.  Cơ sở của ngôn ngữ  Một ngôn ngữ hình thức được định nghĩa như một tập các chuỗi kí tự, trong đó mỗi chuỗi kí tự là một chuỗi các biểu tượng được lấy ra từ một tập hữu hạn được gọi là biểu tượng terminal. Một trong những phiền toái khi làm việc với cả ngôn ngữ tự nhiên và ngôn ngữ hình thức là có quá nhiều sự khác biệt hình thức và kí hiệu cho việc viết ngữ pháp. Tuy nhiên, hầu hết chúng đều tương tự như cách mà chúng căn cứ vào ý tưởng của cấu trúc cụm từ - các chuỗi kí tự được soạn thảo của các chuỗi kí tự cơ sở được gọi là cụm từ, dẫn đến các phạm trù khác nhau. Các phạm trù như cụm danh từ, cụm động từ, câu được gọi là biểu tượng nonterminal. Trong kí pháp BNF quy luật viết phù hợp của biểu tượng nonterminal đơn ở bên trái và liên kết của đầu cuối hoặc không đầu cuối viết ở bên phải. Quy luật được viết như trong ví dụ sau:  S → NP VP  Có nghĩa là chúng ta có thể đem bất kì cụm từ NP thêm vào sau bất kì cụm từ VP và kết quả là một cụm từ dạng câu.  Các bước thành phần của giao tiếp:  Một đoạn giao tiếp điển hình, trong đó người nói S muốn truyền đạt lời thông báo P đến người nghe H sử dụng từ W, được sắp xếp trong 7 tiến trình. Ba bước đối với người nói:  Mục đích: S muốn H tin P (trong đó S đặc biệt tin P) Phát sinh: S chọn từ W (bởi vì chúng nhấn mạnh nghĩa của P) Tổng hợp: S phát âm từ W ( thường chuyển thẳng chúng đến H) Bốn bước đối với người nghe Nhận thức: H nhận thức W’( W’=W, nhưng mất nhận thức là có thể) Phân tích : H suy luận W’ có thể mang các nghĩa P1…,Pn (từ và cụm từ có thể cónhiều nghĩa) Ý nghĩa hóa: H suy luận rằng S có ý định truyền đạt Pi (trong đó ý nghĩa Pi=P, nhưngsự mất giải thích là có thể) Hợp nhất : H quyết định tin tưởng vào Pi, (hoặc loại bỏ nó nếu nó không được Hchắc chắn tin tưởng) Khả năng phát sinh  Ngữ pháp hình thức có thể được phân loại bởi khả năng phát sinh của chúng: tập các ngôn ngữ mà chúng có thể trình bày. Chomsky (1957) mô tả bốn lớp của ngữ pháp hình thức suy luận. Các lớp này được sắp xếp trong một trật tự thứ bậc, trong đó mỗi lớp có thể được mô tả bởi ít nhất một lớp có quyền, giống như việc có thể thêm vài ngôn ngữ vào. Dưới đây là danh sách các lớp theo cấp bậc từ trên xuống dưới:  Ngữ pháp đệ quy liệt kê sử dụng quy luật không giới hạn: kích thước của quy luật viết lại có thể chứa số lượng bất kì biểu tượng terminal và không đầu cuối. Ngữ pháp này là tương đương với máy Turing.  Ngữ pháp nhạy ngữ cảnh được giới hạn chỉ ở bên phải và phải chứa ít nhất một số biểu tượng ở phía bên trái. Tên “nhạy ngữ cảnh” xuất phát từ một thực tế là một quy luật tương tự như ASB→AXB có nghĩa là một S có thể được viết lại như là một X trong ngữ cảnh của một A có trước và một sự kéo theo B.  Trong ngữ pháp phi ngữ cảnh ở phía bên phải chứa một biểu tượng nonterminal đơn. Vì vậy mỗi quy luật cho phép viết lại không đầu cuối ở bên phải trong bất kì ngữ cảnh nào.  Ngữ pháp thông thường là lớp được giới hạn nhất. Ngữ pháp thông thường là tương đương trong máy có số trạng thái hạn chế. Chúng không phù hợp lắm cho ngôn ngữ lập trình, vì chúng không thể xây dựng được cách trình bày giống như sự cân bằng của dầu mở và đóng ngoặc đơn. Để đưa ra cho các bạn một ý tưởng ngôn ngữ nào được điều khiển bởi lớp nào, ngôn ngữ anbn (một chuỗi n bản sao của a kéo theo bởi một số lượng tương tự của b) có thể được phát sinh bởi ngữ pháp phi ngữ cảnh, nhưng không phải là ngữ pháp thông thường. Ngôn ngữ đó yêu cầu một ngữ pháp nhạy ngữ cảnh, trong khi ngôn ngữ a*b* (một sự phối hợp của bất kì một số của a theo sau bởi một số bất kì của b) có thể được mô tả bởi một trong 4 lớp trên. Một bảng tóm tắt của 4 lớp:  Lớp Quy luật ví dụ Ngôn ngữ ví dụ Ngữ pháp liệt kê đệ quy AB →C Bất kì Nhạy ngữ cảnh AB→BA anbncn Ngữ cảnh tự do S→ a S b anbn Thông thường S→ a S a*b* Vấn đề nói và nghe.  Mục đích : Bằng cách này hay cách khác người nói quyết định rằng sẽ có một số cái mà nó đáng để nói với người nghe. Điều này thường bao gồm cả đức tin và mục đích của người nghe vì thế khi nói sẽ có sự tác dụng ao ước. Trong ví dụ của chúng ta người nói có mục đích cho người nghe biết wumpus không còn sống nữa.  Sự phát sinh: Người nói sử dụng kiến thức về ngôn ngữ để quyết định xem nói như thế nào. Trong nhiều cách sẽ khó khăn hơn việc lật ngược vấn đề của sự hiểu biết (ví dụ như phân tích và chuyển thành nghĩa). Sự phát sinh này không bị ép nhiều như sự hiểu biết trong trí tuệ nhân tạo, chủ yếu bởi vì con người chúng ta hay băn khoăn khi nói với máy, nhưng lại không bị kích động khi chúng nói lại. Bâygiờ chúng ta chỉ cho rằng người nghe có thể chọn từ “the wumpus is dead”.  Tổng hợp:  Hầu hết các ngôn ngữ đều căn cứ vào dạng hệ thống phân tích đầu ra của trí tuệ nhân tạo trên màn hình hoặc trên giấy. Tổng hợp lời nói đang được phát triển rộng rãi và một vài hệ thống đã bắt đầu nghe tiếng người. Chi tiết của kí pháp không quan trọng, điều này có nghĩa là âm thanh được phân tích rất khác với từ được nhân vật phát sinh. Mặc dù các từ phải đi liền với nhau, đây là một đặc điểm của việc nói nhanh.  Nhận thức.  Bình thường là lời nói, bước nhận thức được gọi là nhận dạng lời nói, khi nó được đưa ra máy in, nó được gọi là nhận dạng đặc điểm quang học. Cả hai đều chuyển đến người quan tâm. Chẳng hạn, chúng ta cho rằng người nghe nhận thức được âm thanh và thu lại hoàn toàn lời nói.  Phân tích.  Chúng ta phân tích chúng thành hai phần chính: cách hiểu về cú pháp (hay phân tích cú pháp) và sự giải thích về ngữ nghĩa. Sự giải thích về ngữ nghĩa bao gồm cả việc hiểu nghĩa của từ và hợp nhất kiến thức của tình huống hiện tại (cũng được gọi là sự giải thích thực tế).  Phân tích cú pháp từ . Xuất phát từ cụm từ Latin par orationis, hoặc “part of speech” và ám chỉ sự chuyển nhượng một phần của lời nói (danh từ, động từ) đến mỗi từ trong câu và nhóm các từ trong cụm từ.  Một cây phân tích từ loại  là một cây mà bên trong các nút tương ứng với các cụm từ, liên kết với các ứng dụng của quy luật ngữ pháp, và các nút lá tương úng với các từ. Nếu chúng ta định nghĩa số lượng của một nút như là một danh sách tất cả các lá ở bên dưới của nút đó theo thứ tự từ trái sang phải. Khi đó, chúng ta có thể nói rằng ý nghĩa của một cây phân tích từ loại là mỗi nút, với nhãn X xác định số lượng của nút đó là một cụm từ của phạm trù X.  Giải thích ngữ nghĩa  là quá trình rút ra ý nghĩa của một lời nói của một sự diễn đạt trong một sự trình diễn ngôn ngữ. Chúng ta sử dụng logic như sự trình diễn ngôn ngữ, nhưng sự trình diễn ngôn ngữ khác không được sử dụng.  Giải thích thực tế  là một phần của sự giải thích về ngữ nghĩa mà nó mang tình huống hiện thời vào bảng mô tả.  Chuyển thành ý nghĩa . Hầu hết các người nói đều không cố ý nói đa nghĩa, nhưng hầu hết lời nói đều có nhiều sự giải thích hợp lí. Giao tiếp làm việc bởi vì người nghe đã làm việc định hình một nghĩa mà người nghe hầu như chắc chắn truyền đạt. chú ý rằng đây là lần đầu tiên chúng sử dụng từ hầu như chắc chắn và việc chuyển thành ý nghĩa này là tiến trình đầu tiên mà nó phụ thuộc rất nhiều vào lý do không chắc chắn. Phân tích sự giải thích có thể: nếu có nhiều hơn một sự giải thích được tìm thấy, khi đó việc chuyển thành ý nghĩa sẽ chọn lấy một ý nghĩa tốt nhất.  Hợp nhất.  Về tổng thể, một nhân vật có thể tin vào mọi thứ mà anh ta nghe thấy, nhưng một người thông minh sẽ xem xét từ W và xuất phát từ sự giải thích P i  như là một phần thêm vào của các bằng chứng được cân nhắc kỹ lưỡng với tất cả các bằng chứng khác chống lại P i .  Nó chỉ làm nên câu để sử dụng ngôn ngữ khi các nhân vật giao tiếp với người (a) hiểu được ngôn ngữ thông thường, người (b) có một ngữ cảnh mà nó căn cứ vào cuộc hội thoại đó, và người (c) ít nhất có một phần lý trí. Giao tiếp không làm việc khi các nhân vật hoàn toàn không hợp lí,bởi vì không có cách nào để dự báo một nhân vật không hợp lí sẽ phản ứng lại một lời nói hành động. Hai mô hình của giao tiếp  Nghiên cứu của chúng ta về trung tâm giao tiếp là cách mà một niềm tin của nhân vật thay đổi vào từ và trở lại với niềm tin và kiến thức cơ bản của một nhân vật khác. Có hai cách để xem xét quá trình này:  Mô hình bản tin mã hóa  Mô hình bản tin mã hóa nói rằng người nói xác định một nhận định P trong ý nghĩ và mã hóa gợi ý này vào trong từ (hoặc kí hiệu) W. Người nghe sau đó sẽ cố gắng mã hóa bản tin W để lấy lại nguyên bản P (ví dụ như mã Morse). Dưới mô hình này ý nghĩa ở trong đầu người nói, bản tin mà nó được chuyển đi mà người nghe nhận được tất cả ý nghĩ có số lượng tương tự. Khi chúng không giống nhau thì nguyên nhân là do tiếng ồn trong khi giao tiếp hoặc một lỗi trong khi mã hay giải mã.  Mô hình tình huống giao tiếp  Hạn chế của bản tin mã hóa dẫn đến mô hình tình huống giao tiếp, là mô hình cho rằng ý nghĩa của một bản tin phụ thuộc vào cả từ ngữ và cả tình huống mà trong đó các từ được phát âm. Trong mô hình này, chỉ cần trong một phép tính tình huống, các hàm mã và giải mã đã thêm vào một đối số điển hình cho một tình huống mới. Bản mô tả cho sự việc mà những từ tương tự có thể có rất nhiều nghĩa cho những tình huống khác nhau.  Mô hình tình huống ngôn ngữ chỉ ra một nguồn của giao tiếp không thành công: nếu như người nói và người nghe có những ý tưởng khác nhau của tình huống hiện thời có thể, khi đó bản tin có thể không được thông qua như ý định.  Giao tiếp sử dụng ngôn ngữ hình thức  Hầu hết các đối tượng giao tiếp thông qua ngôn ngữ hơn là thông qua truy cập trực tiếp đến kiến thức cơ sở. Hình 1 cho một sơ đồ giao tiếp kiểu này. Đối tượng có thể thực hiện hành động mà nó sinh ra ngôn ngữ, với đối tượng khác có thể nhận biết được. Ngôn ngữ giao tiếp bên ngoài có thể khác so với ngôn ngữ mô tả bên trong, mỗi đối tượng có thể có ngôn ngữ bên trong khác nhau. Chúng không cần thiết phải đồng ý trên bất kì một kí hiệu bên trong nào miễn là mỗi một đối tượng có thể vẽ một bản đồ đáng tin cậy từ ngôn ngữ bên ngoài đến kí hiệu bên trong của chính nó.  Hai đối tượng giao tiếp với ngôn ngữ Một ngôn ngữ giao tiếp bên ngoài mang theo vấn đề tổng hợp và giao tiếp, và nhiều nỗ lực trong xử lý ngôn ngữ tự nhiên dẫn đến việc quyết định một thuật toán cho hai bước trên. Nhưng vấn đề khó khăn nhất của giao tiếp với ngôn ngữ vẫn là vấn đề: sự phù hợp kiến thức cơ bản của những đối tượng khác nhau. Đối tượng A nói như thế nào và làm sao đối tượng B dịch được trạng thái phụ thuộc chủ yếu trên những gì mà A và B thực sự tin tưởng (bao gồm những gì mà chúng tin và niềm tin lẫn nhau của chúng). Điều này có nghĩa là những đối tượng mà chúng có cùng ngôn ngữ bên trong và bên ngoài sẽ có một thời gian dễ dàng để tổng hợp và phân tích, nhưng chúng vẫn phải tìm hiểu để quyết định phải nói với nhau như thế nào.  Trong phần này chúng ta xem xét việc phát triển từ lĩnh vực trò chơi sang các hệ thống thực có hiệu quả trong các công việc về ngôn ngữ. Chúng ta cũng đã thấy một vài kĩ thuật dịch các câu từ một tập hợp tiếng Anh đơn giản các vấn đề đó là:  Các ứng dụng thực tế: các công việc về ngôn ngữ tự nhiên được chứng minh có hiệu quả. Xử lý bài luận: vấn đề nắm bắt đoạn văn có nhiều câu.  Hiệu quả của việc phân tích ngữ pháp: các thuật toán phân tích cú pháp và dịch các câu nhanh. Tăng cường về thuật ngữ: quan tâm tới các từ không thường dùng hoặc không biết. Tăng cường về ngữ pháp: quan tâm tới các ngữ pháp phức tạp.  Dịch theo nghĩa: một số vấn đề cần dịch theo nghĩa hơn là dịch theo các hàm đơn giản. Ngữ nghĩa: cách chọn phép dịch đúng.  Chúng ta bắt đầu xem xét các hệ thống đã thành công trong việc đưa ngôn ngữ tự nhiên vào ứng dụng thực tế. Các hệ thống này đều có chung hai tính chất: một là chúng đều tập chung vào một lĩnh vực nhất định chứ không cho phải là tất cả, hai là chúng chỉ tập chung vào một nhiện vụ cụ thể chứ không đòi hỏi hiểu toàn bộ ngôn ngữ.  XỬ LÝ VÀ HIỂU VĂN BẢN  Truy nhập cơ sở dữ liệu  Lĩnh vực đầu tiên thành công đối với việc xử lí ngôn ngữ tự nhiên là truy cấp CSDL. Vào năm 1970, nhiều CSDL trong các máy tính KHUNG CHÍNH (mainframe), nhưng chỉ truy cập được bằng cách viết các chương trình hoàn thiện bằng các ngôn ngữ khó hiểu. Nhân viên phục vụ trong các máy mainframe không thể đáp ứng tất cả các đòi hỏi của người sử dụng, còn người sử dụng không muốn học cách lập trình. Giao diện ngôn ngữ tự nhiên được đưa ra để giải quyết vấn đề này. Đầu tiên là giao diện của hệ thống LUNAR, một phương thức được xây dựng bởi William Woods (1973) và nhóm của mình cho trung tâm NASA. Nó cho phép, ví dụ, một nhà địa chất hỏi về dữ liệu hoá học của các mẫu đất đá trên mặt trăng được mang về từ tầu Apollo. Hệ thống này không thể sử dụng ở thế giới thực, nhưng trong một kiểm nghiệm nó thành công 78% các câu hỏi như là:  What is the average model plagioclase concentration for lunar samples that contain rubidium?  Hệ thống sơ đồ Fernando Pereira’s (Pereira, 1983) là một hệ thông tương đương. Nó trả lời như sau về các câu hỏi về CSDL địa lý như: Q: Which countries are bounded by two seas? A: Egypt, Iran, Israel, Saudi Arabia and Turkey.  Q: Whats are the counties from which a river flows into Black sea? A: Romania, Soviet Union.  Thuận lợi của các hệ thống như vậy mang lại rõ ràng. Nhưng có bất lợi là người sử dụng không biết khi nào thành công và những từ nào nằm ngoài hệ thống. Vào cuối thế kỷ trước, một vài hệ thống thương mại đã xây dựng một số lượng đủ lớn các từ, ngữ pháp đáp ứng diện rộng các văn bản. Cảnh báo chính trong các hệ thống hiện tại là sự tác động qua lại lẫn nhau. Người sử dụng sẽ hỏi một dãy các câu hỏi mà ở đó có một số câu hỏi lại liên quan đến các câu hỏi hoặc trả lời truớc đó.  What countries are north of the equator? How about south?  Show only the ones outside Australasia? What is their total area?  Một số hệ thống coi vấn đề đó như giới hạn.  Trong những năm 1990, nhiều công ty như Natural Language Inc. và Symatec vẫn bán các công cụ truy cập dữ liệu sử dụng ngôn ngữ tự nhiên, nhưng những khách hàng không thích mua sản phẩm dựa trên ngôn ngữ tự nhiên hơn là các giao diện đồ hoạ. Ngôn ngữ tự nhiên không phải là con đường tự nhiên nhất (ví dụ chuột và click). Thu thập thông tin  Thu thập thông tin là lấy từ một văn bản ra một số dữ liệu phù hợp với một câu hỏi. Một số tài liệu được miêu tả bởi đại diện, như tiêu đề, danh sách từ khoá, hoặc tóm tắt. Hiện nay có quá nhiều thông tin trực tuyến, tốt nhất là sử dụng toàn bộ văn bản, có thể chia thành các đoạn, mỗi đoạn coi như một tài liệu riêng biệt cho việc mục đích thu thập thông tin. Các câu hỏi thường là danh sách các từ khoá. Trong các hệ thống thu thập thông tin ban đầu, các câu hỏi là sự kết hợp logic các từ khoá. Khi một câu hỏi không tìm thấy tài liệu, ví dụ, nó không đủ rộng để tìm được một vài tài liệu. Chuyển một “and” thành một “or” là một khả năng; thêm vào một ngăn cách là một khả năng nữa, nhưng có khi lại tìm thấy quá nhiều và không đủ hướng dẫn.  Hầu hết các hệ thống hiện đại đều chuyển từ kiểu logic sang kiểu không gian vector, trong đó danh sách các từ (cả trong tài liệu, trong câu hỏi) đều được coi như một vettor trong không gian n-chiều, ở đó n là số dấu hiệu phân biệt của tập hợp tài liệu. Nó sẽ được coi như một vector. Khi đó việc tìm các tài liệu chính là việc so sánh vector này với tập hợp các vector khác và đưa ra những véc tơ gần nhất với nó. Kiểu véctơ linh động hơn kiểu logic bởi vì có thể sắp xếp các tài liệu bởi khoảng cách tới câu hỏi, và tài liệu nào gần nhất được báo cáo trước.  Kiểu này có nhiều dạng. Một vài hệ thống cho phép các câu hỏi phát biểu rằng hai từ phải xuất hiện gần nhau mới được đếm như một lần, một vài hệ thống khác sử dụng từ điển đồng nghĩa làm tăng thêm các từ trong câu hỏi bằng các từ đồng nghĩa với nó. Chỉ những hệ thống tồi nhất mới đếm tất cả các số hạng trong vector tương đương. Nhiều hệ thống đánh giá trọng lượng các số hạng khác nhau. Cách tốt nhất là cho số hạng trọng lượng lớn nếu nó là từ đặc trưng: nếu nó xuất hiện trong một số ít các văn bản hơn là trong nhiều văn bản.  Phân loại văn bản  Kỹ thuật xử lý ngôn ngữ tự nhiên (NLP: Natural Language Processing) đã thành công trong một công việc liên quan: sắp xếp văn bản theo các chủ đề xác định. Một số hệ thống thương mại truy cập thông tin của các bức điện báo theo cách này. Một người thuê bao có thể hỏi tất cả các thông tin trong các lĩnh vực công nghiệp, thương mại, hoặc địa lí. Các nhà cung cấp đã sử dụng kiến thức của các chuyên gia để xác định các lớp. Trong vài năm gần đây, các hệ thống NLP đã được chứng minh tính đúng đắn, phân lớp chính xác trên 90% các thông tin thời sự. Chúng cũng nhanh hơn và thích hợp hơn, và đã có sự chuyển đổi từ thủ công sang các hệ thống tự động.  Phân loại văn bản tuân theo các kĩ thuật NLP không phải là gọi lại (IR : Information Retrieval) bởi vì sự phân lớp là cố định, và những người xây dựng các hệ thống đó đã tập trung kết hợp các chương trình của họ với vấn đề đó.  Lấy dữ liệu vào văn bản  Lấy dữ liệu từ một văn bản là lấy ra một vài thông tin yêu cầu để có thể đưa vào một cấu trúc dữ liệu.  Hiệu quả phân tích từ  Trong phần này, chúng ta xem xét tính hiệu quả của thuật toán phân tích từ. ở mức broadest, có ba vấn đề chính làm tăng hiệu quả: Không làm hai lần cái gì có thể làm một lần. Không làm nếu có thể tránh được. Không trình bày riêng lẻ nếu không cần.  Đặc biệt, chúng ta sẽ thiết kế một thuật toán phân tích từ thực hiện như sau:  Chúng ta đã nhận thấy rằng “the students in section 2 of Computer Science 101” là danh từ NP (Noun Phrase), đó là một ý tưởng để thấy rằng kết quả trong một cấu trúc dữ liệu đã biết là một sơ đồ. Các thuật toán này được gọi là phân tích từ loại theo sơ đồ. Bởi vì chúng ta đang quan tâm tới các ngữ pháp ngữ cảnh tự do (context-free), mọi mệnh đề tìm thấy trong ngữ cảnh của một nhánh trong không gian tìm kiếm cũng có thể phải làm việc như vậy trong nhánh khác của không gian tìm kiếm. Ghi nhận các kết quả trong sơ đồ là mẫu cho việc lập trình tránh việc lặp lại.  Chúng ta thấy rằng thuật toán phân tích sơ đồ kết hợp việc xử lý trên xuống (top-down) và dưới lên (bottom-up). Sơ đồ phân tích câu “The agent feels a breeze”  Kết quả của thuật toán là một rừng được đóng gói (packed forest) của các cây phân tích hợp thành không chỉ là việc đếm mọi khả năng có thể.  Sơ đồ là một cấu trúc dữ liệu mô tả các kết quả thành phần của quá trình phân tích được dùng lại. Một sơ đồ cho một câu n từ gồm n+1 đỉnh và một số cạnh nối với các vector. Hình 2 biểu diễn một sơ đồ với 6 đỉnh và 3 cạnh. Ví dụ, cạnh có nhãn [0,5, S - NP VP*] có nghĩa là danh từ NP (Noun Phrase) đi theo bởi động từ VP (Verb Phrase) để tạo ra một mệnh đề S (S: sentnce) mà trải theo chuỗi từ 0 đến 5. Dấu * trong một cạnh tách được tìm thấy từ các phần còn lại. Các cạnh với dấu * ở cuối được gọi là cạnh hoàn thiện; ví dụ cạnh [0, 2 S - NP*VP]  Ta nói rằng một NP trải chuỗi từ 0 đến 2, và nếu có thể tìm một VP theo sau nó, sẽ có một S. Các cạnh với dấu chấm trước dấu kết thúc gọi là cạnh không hoàn thiện, và nó đang tìm một VP. Chúng ta đã biết hai cách xem xét quá trình xử lý. Trong cách phân tích Bottom - Up trong trang sau, chúng ta miêu tả xử lý như một quá trình xây dựng các từ vào cây, quay lui khi cần thiết. Với ngữ pháp mệnh đề nhất định (Definite Clause Grammar), chúng ta miêu tả việc xử lý như một mẫu của suy luận logic trong các chuỗi (string). Việc quay lui đã được sử dụng khi một vài qui tắc có thể điều khiển cùng một dự đoán. Bây giờ chúng ta sẽ xem cách tiếp cận thứ ba. Dưới cách nhìn này, quá trình phân tích một câu n - từ gồm một sơ đồ mẫu với n + 1 đỉnh và thêm vào một số cạnh để biểu diễn, cố gắng tạo ra một cạnh hoàn thiện mà trải ra từ đỉnh 0 tới đỉnh n và là sự phân lớp S. Không có việc quay lui: tất cả mọi thứ được đặt trong sơ đồ này.  Phân tích mở rộng từ sơ đồ : Đóng gói  Khi thuật toán phân tích sơ đồ kết thúc, nó trả về toàn bộ sơ đồ, nhưng chúng ta thực sự cần là một cây (tree) (hoặc một số cây). Phụ thuộc việc phân tích được sử dụng, chúng ta muốn chọn một hoặc toàn bộ cây phân tích mà trải toàn bộ đầu vào, hoặc chúng ta muốn xem xét một số cây con mà không trải ra trên toàn bộ đầu vào. Nếu có một ngữ pháp bổ sung, có thể chúng ta chỉ muốn tìm mở rộng ngữ nghĩa, bỏ qua các cấu trúc cú pháp. Trong mọi trường hợp, chúng ta cần khả năng phân tích mở rộng từ một sơ đồ.  Cách dễ nhất để làm việc đó là sửa bộ hòan thiện (Completer) sao cho khi nó kết hợp hai cạnh con tạo thành cạnh cha. Nó chứa trong cạnh cha danh sách các cạnh con mà cấu thành nó. Sau đó, khi phân tích chỉ cần tìm trong chart[n] cho một cạnh bắt đầo tại 0, đệ quy tại danh sách các cạnh con để tạo ra cây phân tích hoàn thiện. Chỉ phép biện chứng quyết định cái gì thực hiện phân tích mở rộng trên.  Chúng ta kết thúc vấn đề bằng việc phân tích độ phức tạp thuật toán đó là O(n 3 ) trong trường hợp xấu nhất (ở đó n là số từ đầu vào). Trường hợp tốt nhất có thể đạt được là ngữ pháp ngữ cảng tự do (context-free grammar). Chú ý, nếu thiếu rừng đóng gói, thuật toán sẽ bùng nổ trong trường hợp xấu nhất, bởi vì nó là khả năng có O(2n) cây phân tích khác nhau. Trong thực tế, có thể thực hiện thuật toán để phân tích với yêu cầu 100 từ một giây, với sự biến đổi phụ thuộc vào độ phức tạp của ngữ pháp và đầu vào.  Dấu hiệu cú pháp  Sự thay đổi như động từ, giới từ sinh ra nhiều sự nhập nhằng, bởi vì chúng có thể dẫn tới một vài sự khác biệt. Ví dụ:  Lee asked Kim to tell Toby to leave on Saturday.  Phó từ “Saturday” có thể được hỏi cho tell hoặc leave.  Dấu hiệu từ vựng Có nhiều từ nhập nhằng, nhưng tất cả không giống nhau. Khi hỏi nghĩa của từ “pen”, hầu hết mọi người đều trả lời là một công cụ để viết. Mặc dù nó còn các nghĩa khác như hàng rào, nhà lao, con thiên nga đực.  CÁC HỆ THỐNG DỊCH TỰ ĐỘNG  Vào những năm 60 của thế kỉ trước, người ta hi vọng máy tính có thể dịch từ một ngôn ngữ tự nhiên này sang một ngôn ngữ tự nhiên khác, đơn giản như máy Turing “dịch” các bản văn mã thành các bản văn rõ. Nhưng vào năm 1966, người ta nhận thấy rằng việc dịch đòi hỏi một sự hiểu biết về nghĩa của văn bản (và hơn nữa là những hiểu biết chi tiết về thế giới), trong khi đó việc giải mã chỉ phụ thuộc vào các tính chất ngữ pháp của văn bản.  Điều đó không làm mất ý nghĩ về việc dịch máy. Thực tế đã có nhiều hệ thống dịch máy, hàng ngày đã tiết kiệm rất nhiều so với việc xử lý hoàn toàn thủ công. Một trong hệ thống thành công nhất là hệ thống TAUM METEO, được phát triển bởi trường đại học Montral. Nó đã dịch các báo cáo thời tiết từ tiếng Anh sang tiếng Pháp. Nó làm được việc này bởi vì ngôn ngữ được sử dụng trong các báo cáo thời tiết của chính phủ có mẫu và quy tắc chặt chẽ.  Một lĩnh vực khác rộng hơn, mà kết quả gây ấn tượng không kém, đó là hệ thống SPANAM (Vascocellos và Leon, 1985). Nó có thẻ dịch một đoạn văn bản tiếng Tây Ban Nha sang tiếng Anh với chất lượng hầu như hiểu được tất cả, nhưng không đúng ngữ pháp và hiếm khi trôi chảy. Việc dịch máy là không đầy đủ. Nhưng khi người dịch có được văn bản như vậy, người dịch có thể dịch nhanh gấp bốn lần. Một số người có thể dịch thẳng từ văn bản đó không cần đọc bản gốc. Giá phải trả đối với hiệu quả của việc dịch máy là để có các thông tin rộng rãi, hệ thống dịch máy phải có lượng từ vựng từ 20.000 đến 100.000 từ và 100 đến 10.000 quy tắc ngữ pháp. Các con số đó phụ thuộc vào việc chọn hình thức dịch.  Việc dịch là khó vì, trong trường hợp tổng quát, nó đòi hỏi hiểu biết sâu sắc về văn bản, và tình huống trong giao tiếp. Thực vậy, ngay cả đối với các văn bản rất đơn giản - thậm chí chỉ có một từ. Xét từ “Open” trên cửa ra vào của một cửa hàng. Nó có nghĩa là đang đón khách. Cũng từ “Open” trên một biển quảng cáo lớn của một nhà hàng mới khánh thành, nó có nghĩa là nhà hàng đang trong những ngày làm việc, nhưng người đọc không cảm thấy bị lừa dối khi nhà hàng đóng cửa vào ban đêm mà không tháo biển quảng cáo. Một từ có thể mang nhiều nghĩa khác nhau. Trong một số ngôn ngữ khác, có một từ hoặc cụm từ như vậy sẽ được sử dụng trong cả hai trường hợp.  Vấn đề đó cho thấy trong các ngôn ngữ khác nhau sự phân loại từ là khác nhau. Để dịch tốt, người dịch (người, máy) phải đọc văn bản gốc hiểu được nghĩa mà nó mô tả, và tìm một văn bản tương ứng trong ngôn ngữ đích có một nghĩa tương đương. Ở đây có nhiều lựa chọn. Người dịch (cả máy và người) đôi khi khó có một sự lựa chọn.  XỬ LÝ VÀ HIỂU TIẾNG NÓI  Tổng quan về tiếng nói  Nhận dạng tiếng nói là một hệ thống tạo khả năng để máy nhận biết ngữ nghĩa của lời nói. Về bản chất, đây là quá trình biến đổi tín hiệu âm thanh thu được của người nói qua Micro, đường dây điện thoại hoặc các thiết bị khác thành một chuỗi các từ. Kết quả của quá trình nhận dạng có thể được ứng dụng trong điều khiển thiết bị, nhập dữ liệu, soạn thảo văn bản bằng lời, quay số điện thoại tự động hoặc đưa tới một quá trình xử lý ngôn ngữ ở mức cao hơn.  Các phần tử cơ bản của một hệ thống nhận dạng tiếng nói  Các hệ thống nhận dạng tiếng nói có thể được phân loại như sau:  • Nhận dạng từ phát âm rời rạc/liên tục;  • Nhận dạng tiếng nói phụ thuộc người nói/không phụ thuộc người nói;  • Hệ thống nhận dạng từ điển cớ nhỏ (dưới 20 từ)/từ điển cỡ lớn (hàng nghìn từ);  • Nhận dạng tiếng nói trong môi trường có nhiễu thấp/cao;  • Nhận dạng người nói.  Trong hệ nhận dạng tiếng nói với cách phát âm rời rạc có khoảng lặng giữa các từ trong câu. Trong hệ nhận dạng tiếng nói liên tục không đòi hỏi điều này. Tùy thuộc vào quy mô và phương pháp nhận dạng, ta có các mô hình nhận dạng tiếng nói khác nhau. Hình 2 là mô hình tổng quát của một hệ nhận dạng tiếng nói điển hình . Tín hiệu tiếng nói sau khi thu nhận được lượng tử hóa sẽ biến đổi thành một tập các vector tham số đặc trưng với các phân đoạn có độ dài trong khoảng 10-30 ms. Các đặc trưng này được dùng cho đối sánh hoặc tìm kiếm các từ gần nhất với một số ràng buộc về âm học, từ vựng và ngữ pháp. Cơ sở dữ liệu tiếng nói được sử dụng trong quá trình huấn luyện (mô hình hóa/phân lớp) để xác định các tham số hệ thống.  Các phương pháp tiếp cận trong nhận dạng tiếng nói  Có ba phương pháp phổ biến được sử dụng trong nhận dạng tiếng nói hiện nay là:  • Phương pháp Âm học-Ngữ âm học;  • Phương pháp nhận dạng mẫu;  • Phương pháp ứng dụng trí tuệ nhân tạo.  Các phương pháp được trình bày tóm tắt như dưới đây.  Phương pháp Âm học-Ngữ âm học Phương pháp này dựa trên lý thuyết về Âm học-Ngữ âm học. Lý thuyết đó cho biết: tồn tại các đơn vị ngữ âm xác định, có tính phân biệt trong lời nói và các đơn vị ngữ âm đó được đặc trưng bởi một tập các tín hiệu tiếng nói. Các bước nhận dang của phương pháp gồm:  Bước 1: phân đoạn và gán nhãn. Bước này chia tín hiệu tiếng nói thành các đoạn có đặc tính âm học đặc trưng cho một (hoặc một vài) đơn vị ngữ âm, đồng thời gán cho mỗi đoạn âm thanh đó một hay nhiều nhãn ngữ âm phù hợp.  Bước 2: nhận dạng. Bước này dựa trên một số điều kiện ràng buộc về từ vựng, ngữ pháp v.v… để xác định một hoặc một chuỗi từ đúng trong các chuỗi nhãn ngữ âm được tạo ra sau bước 1. Sơ đồ khối của phương pháp này được biểu diễn ở Hình 2. Nguyên lý hoạt động của phương pháp có thể mô tả như sau:  Trích chọn đặc trưng. Tín hiệu tiếng sau khi số hóa được đưa tới khối trích chọn đặc trưng nhằm xác định các phổ tín hiệu. Các kỹ thuật trích chọn đặc trưng tiếng nói phổ biến là sử dụng băng lọc (filter bank), mã hóa dự đoán tuyến tính (LPC)…  Tách tín hiệu tiếng nói nhằm biến đổi phổ tín hiệu thành một tập các đặc tính mô tả các tính chất âm học của các đơn vị ngữ âm khác nhau. Các đặc tính đó có thể là: tính chất các âm mũi, âm xát; vị trí các formant; âm hữu thanh, vô thanh; tỷ số mức năng lượng tín hiệu…  Phân đoạn và gán nhãn. Ở bước này hệ thống nhận dạng tiếng xác định các vùng âm thanh ổn định (vùng có đặc tính thay đổi rất ít) và gán cho mỗi vùng này một nhãn phù hợp với đặc tính của đơn vị ngữ âm. Đây là bước quan trọng của hệ nhận dạng tiếng nói theo khuynh hướng Âm học-Ngữ âm học và là bước khó đảm bảo độ tin cậy nhất.  Nhận dạng. Chọn lựa để kết hợp chính xác các khối ngữ âm tạo thành các từ nhận dạng. Đặc điểm của phương pháp nhận dạng tiếng nói theo hướng tiếp cận Âm học-Ngữ âm học:  • Người thiết kế phải có kiến thức khá sâu rộng về Âm học-Ngữ âm học;  • Phân tích các khối ngữ âm mang tính trực giác, thiếu chính xác;  • Phân loại tiếng nói theo các khối ngữ âm thường không tối ưu do khó sử dụng các công cụ toán học để phân tích.   Sơ đồ khối nhận dạng tiếng nói theo Âm học-Ngữ âm học  Phương pháp nhận dạng mẫu Sơ đồ khối hệ nhận dạng tiếng nói theo phương pháp mẫu Phương pháp nhận dạng mẫu không cần xác định đặc tính âm học hay phân đoạn tiếng nói mà sử dụng trực tiếp các mẫu tín hiệu tiếng nói trong quá trình nhận dạng. Các hệ thống nhận dạng tiếng nói theo phương pháp này được phát triển theo hai bước (Hình 4,5), cụ thể là.  Bước 1: Sử dụng tập mẫu tiếng nói (cơ sở dữ liệu mẫu tiếng nói) để đào tạo các mẫu tiếng nói đặc trưng (mẫu tham chiếu) hoặc các tham số hệ thống.  Bước 2: Đối sánh mẫu tiếng nói từ ngoài với các mẫu đặc trưng để ra quyết định.  Trong phương pháp này, nếu cơ sở dữ liệu tiếng nói cho đào tạo có đủ các phiên bản mẫu cấn nhận dạng thì quá trình đào tạo có thể xác định chính xác các đặc tính âm học của mẫu (các mẫu ở đây có thể là âm vị, từ, cụm từ…). Hiện nay, một số kỹ thuật nhận dạng mẫu được áp dụng thành công trong nhận dạng tiếng nói là lượng tử hóa vector, so sánh thời gian động (DTW), mô hình Markov ẩn (HMM), mạng nơron nhân tạo (ANN).  Hệ thống bao gồm các hoạt động sau: Trích chọn đặc trung: Tín hiệu tiếng nói được phân tích thành chuỗi các số đo để xác định mẫu nhận dạng. Các số đo đặc trưng là kết quả xử lý của các kỹ thuật phân tích phổ như: lọc thông dải, phân tích mã hóa dự đoán tuyến tính (LPC), biến đổi Fourier rời rạc (DFT).  Huấn luyện mẫu: Nhiều mẫu tiếng nói ứng với các đơn vị âm thanh cùng loại dùng để đào tạo các mẫu hoặc các mô hình đại diện, được gọi là mẫu tham chiếu hay mẫu chuẩn.  Nhận dạng: Các mẫu tiếng nói được đưa tới khối phân loại mẫu. Khối này đối sánh mẫu đầu vào với các mẫu tham chiếu. Kối nhận dạng căn cứ vào các tiêu chuẩn đánh giá để quyết định mẫu tham chiếu nào giống mẫu đầu vào.  Một số đặc điểm của phương pháp nhận dạng mẫu:  • Hiệu năng của hệ phụ thuộc vào số mẫu đưa vào. Nếu số lượng mẫu càng nhiều thì độ chính xác của hệ càng cao; tuy nhiên, dung lượng nhớ và thời gian luyện mẫu tăng. • Các mẫu tham chiếu phụ thuộc vào môi trường thu âm và môi trường truyền dẫn. • Không đòi hỏi kiến thức sâu về ngôn ngữ. • Phương pháp ứng dụng trí tuệ nhân tạo Phương pháp ứng dụng trí tuệ nhân tạo kết hợp các phương pháp trên nhằm tận dụng tối đa các ưu điểm của chúng, đồng thời bắt chước các khả năng của con người trong phân tích và cảm nhận các sự kiện bên ngoài để áp dụng vào nhận dạng tiếng nói. Sơ đồ khối của phương pháp trí tuệ nhân tạo theo mô hình từ dưới lên (bottom-up) (Hình 4.4).  Đặc điểm của các hệ thống nhận dạng theo phương pháp này là:  Sử dụng hệ chuyên gia để phân đoạn, gán nhãn ngữ âm. Điều này làm đơn giản hóa hệ thống so với phương pháp nhận dạng ngữ âm.  Sử dụng mạng nơron nhân tạo để học mối quan hệ giữa các ngữ âm, sau đó dùng nó để nhận dạng tiếng nói. Sơ đồ khối hệ nhận dạng tiếng nói theo phương pháp từ dưới lên  Việc sử dụng hệ chuyên gia nhằm tận dụng kiến thức con người vào hệ nhận dạng:  Kiến thức về âm học: để phân tích phổ và xác định đặc tính âm học của các mẫu tiếng nói. Kiến thức về từ vựng: sử dụng để kết hợp các khối ngữ âm thành các từ cần nhận dạng. Kiến thức về cú pháp: nhằm kết hợp các từ thành các câu cần nhận dạng.  Kiến thức về ngữ nghĩa: nhằm xác định tính logic của các câu đã được nhận dạng.  Có nhiều cách khác nhau để tổng hợp các nguồn kiến thức vào bộ nhận dạng tiếng nói. Phương pháp thông dụng nhất là xử lý ”từ dưới lên”. Theo cách này, tiến trình xử lý của hệ thống được triển khai tuần tự từ thấp lên cao. Trong Hình 6, các bước xử lý ở mức thấp (phân tích tín hiệu, tìm đặc tính, phân đoạn, gán nhãn) được triển khai trước khi thực hiện các bước xử lý ở mức cao (phân lớp âm thanh, xác định từ, xác định câu). Mỗi bước xử lý đòi hỏi một hoặc một số nguồn kiến thức nhất định. Ví dụ: bước phân đoạn tiếng nói cần hiểu biết sâu sắc về đặc tính Âm học-Ngữ âm học của các đơn vị ngữ âm; bước xác định từ đòi hỏi kiến thức về từ vựng; bước xác định câu đòi hỏi kiến thức về mô hình ngôn ngữ (nguyên tắc ngữ pháp).  Phương pháp này đã và đang được áp dụng thành công trong các ứng dụng nhận dạng tiếng nói thực tế. Đề tài sẽ sử dụng phương pháp nhận dạng mẫu cho bài toán nhận dạng một số từ tiếng Việt. Bước đầu tiên của quá trình nhận dạng là trích chọn các tham số tín hiệu tiếng nói. Phần tiếp theo sẽ trình bày chi tiết về phương pháp này.  Phân tích tham số tiếng nói  Trong nhận dạng, tổng hợp, mã hóa tiếng nói đều cần phân tích các tham số. Dưới đây, mô tả phương pháp phân tích cepstral theo thang đo mel để tính các hệ số MFCC thông qua việc sử dụng dãy các băng lọc.  Khái niệm cơ bản trong phân tích tín hiệu tiếng nói là phân tích thời gian ngắn (Short-Time Analysis). Trong khoảng thời gian dài, tín hiệu tiếng nói là không dừng, nhưng trong khoảng thời gian đủ ngắn (10-30 ms) tiếng nói được coi là dừng. Do đó, trong các ứng dụng xử lý tiếng nói người ta thường chia tiếng nói thành nhiều đoạn có thời gian bằng nhau được gọi là khung (frame), mỗi khung có độ dài từ 10 đến 30 ms.  Phát hiện tiếng nói  Phát hiện thời điểm bắt đầu, điểm kết thúc của tiếng nói (tách tiếng nói ra khỏi khoảng lặng) là phần cần thiết trong chương trình nhận dạng tiếng nói, đặc biệt trong chế độ thời gian thực. Phần này trình bày ba phương pháp phát hiện tiếng nói dựa trên hàm năng lượng thời gian ngắn SE (Short Energy) và tỷ lệ vượt quá điểm không ZCR (Zero Crossing).  Phát hiện tiếng nói dựa trên hàm năng lượng thời gian ngắn. Hàm năng lượng thời gian ngắn của tín hiệu tiếng nói được tính bằng cách chia tín hiệu tiếng nói thành các khung, mỗi khung dài N mẫu. Mỗi khung được nhân với một hàm cửa sổ W(n).Nếu hàm cửa sổ bắt đầu xét ở mẫu thứ m thì hàm năng lượng thời gian ngắn \n E m  size 12{ { size 24{E} }  rSub { size 8{m} } } {}  được xác định như sau: Trong đó: n: biểu thức rời rạc; m:số mẫu thử thứ m; N: là tổng số mẫu tiếng nói Hàm cửa sổ W(n) thường dùng là hàm cửa sổ chữ nhật được xác định như sau: Thuật toán xác định điểm đầu và điểm cuối tiếng nói theo phương pháp này: Phát hiện tiếng nói dựa trên hàm giả năng lượng và tỷ lệ vượt quá điểm không  Thuật toán này xác định điểm bắt đầu, điểm kết thúc của tín hiệu tiếng nói dựa trên hai đại lượng tĩnh của tín hiệu tiếng nói là: hàm giả năng lượng E (Pseudo-Energy) và tỷ lệ vượt quá điểm không ZCR (Zero Crossing Rate) .  Trong một dãy giá trị tín hiệu tiếng nói được rời rạc hóa, điểm không là điểm tại đó diễn ra sự đổi dấu cường độ tín hiệu và được mô tả bởi:  sgn[x(n+1)] ≠ sgn[x(n)]  trong đó, sgn(.) là hàm dấu  Năng lượng là đại lượng được dùng để xác định vùng chứa âm hữu thanh, vô thanh. Nhưng hàm năng lượng thường rất nhạy cảm với nhiễu. Do vậy, người ta thường sử dụng hàm giả năng lượng trong tính toán. Hàm giả năng lượng được xác định bởi: trong đó; E∧(n) : là hàm giả năng lượng, N: là kích thước khung cửa sổ. Tỷ lệ vượt quá điểm không ZCR  Ta thấy, khung có năng lượng càng cao thì tỷ lệ vượt quá điểm không càng thấp và ngược lại. Như vậy, tỷ lệ vượt quá điểm không là đại lượng đặc trưng cho tần số tín hiệu tiếng nói. Ở đây, chúng ta cần xác định các tham số ngưỡng cho hàm giả năng lượng với hai ngưỡng trên và dưới và một ngưỡng tỷ lệ vượt quá điểm không.  Kí hiệu:  E Up : ngưỡng năng lượng trên (cao); Edown : ngưỡng năng lượng dưới (thấp); ZCR _ T : ngưỡng tỷ lệ vượt quá điểm không. Thuật toán này được mô tả như sau : Phát hiện tiếng nói dựa trên năng lượng phổ ngắn hạn  Ý tưởng chính của phương pháp này là sử dụng bộ điều khiển dò biên tiếng nói VAD (Voice Activity Detector) dựa trên việc xác định năng lượng phổ ngắn hạn  f E trên các khung tín hiệu tiếng nói. VAD dùng để xác định một khung chứa tín hiệu tiếng nói hay nhiễu. Hàm đầu ra của VAD trên khung thứ m là v [m]. Với khung chứa tiếng nói (có thể cả nhiễu)  v  [m]=1, ngược lại khung chỉ chứa nhiễu v [m]=0.  Thuật toán được mô tả như sau: Phương pháp này ngăn việc phân loại sai của phụ âm sát và tiếng nói ở cuối tín hiệu tiếng nói.  Các phương pháp trích chọn tham số đặc trưng của tín hiệu tiếng nói  Trích chọn các tham số đặc trưng là bước có ý nghĩa quyết định tới kết quả của các chương trình nhận dạng tiếng nói. Có nhiều phương pháp trích chọn các tham số đặc trưng nhưng nhìn chung các phương pháp này dựa trên hai cơ chế:  Mô phỏng lại quá trình cảm nhận âm thanh của tai người. Mô phỏng lại quá trình tạo âm của cơ quan phát âm.  Phân tích cepstral theo thang đo mel  Phương pháp tính các hệ số MFCC là phương pháp trích chọn tham số tiếng nói được sử dụng rộng rãi bởi tính hiệu quả của nó thông qua phân tích cepstral theo thang đo mel.  Phương pháp được xây dựng dựa trên sự cảm nhận của tai người đối với các dải tần số khác nhau. Với các tần số thấp (dưới 1000 Hz), độ cảm nhận của tai người là tuyến tính. Đối với các tần số cao, độ biến thiên tuân theo hàm logarit. Các băng lọc tuyến tính ở tần số thấp và biến thiên theo hàm logarit ở tần số cao được sử dụng để trích chọn các đặc trưng âm học quan trọng của tiếng nói. Mô hình tính toán các hệ số MFCC được mô tả như Hình 7. Sơ đồ tính toán các hệ số MFCC  Ý nghĩa và phương pháp xác định tham số ở các khối trong sơ đồ trên mô tả như sau: Khối 1:  Bộ lọc hiệu chỉnh (Preemphasis)  Tín hiệu tiếng nói s(n) được đưa qua bộ lọc số bậc thấp để phổ đồng đều hơn, giảm ảnh hưởng gây ra cho các xử lý tín hiệu sau này. Thường bộ lọc này cố định bậc một, có dạng:  H(z) = 1- az 1−  0.9≤ a≤ 1.0  Quan hệ giữa tín hiệu ra với tín hiệu vào tuân theo phương trình  Giá trị a thường được chọn là 0.97.  Khối 2 : Phân khung (Frame Blocking)  Trong khối này tín hiệu hiệu chỉnh s(n) được phân thành các khung, mỗi khung có N mẫu; hai khung kề lệch nhau M mẫu. Khung đầu tiên chứa N mẫu, khung thứ hai bắt đầu chậm hơn khung thứ nhất M mẫu và chồng lên khung thứ nhất N-M mẫu. Tương tự, khung thứ ba chậm hơn khung thứ nhất 2M mẫu (chậm hơn khung thứ hai M mẫu) và chờm lên khung thứ nhất N-2M mẫu. Quá trình này tiếp tục cho đến khi tất cả các mẫu tiếng nói cần phân tích thuộc về một hoặc nhiều khung.  Khối 3:  Lấy cửa sổ (Windowing)  Bước tiếp theo là lấy cửa sổ cho mỗi khung riêng rẽ nhằm giảm sự gián đoạn của tín hiệu tiếng nói tại đầu và cuối mỗi khung. Nếu w(n), 0 ≤ n ≤ N-1, sau khi lấy cửa sổ được:  Thông thường, của sổ Hamming được sử dụng. Cửa sổ này có dạng:  Khối 4 : Biến đổi Fourier rời rạc (FFT) Tác dụng của FFT là chuyển đổi mỗi khung với N mẫu từ miền thời gian sang miền tần số. FFT là thuật toán tính DFT nhanh. DFT được xác định Khối 5 : Biến đổi sang thang đo Mel trên miền tần số  Như đã nói ở trên, tai người không cảm nhận sự thay đổi tần số của tiếng nói tuyến tính mà theo thang Mel. Người ta chọn tấn số 1kHz, 40 dB trên ngưỡng nghe là 1000 Mel. Do đó, công thức gần đúng biểu diễn quan hệ tần số ở thang mel và thang tuyến tính như sau: Các băng lọc tam giác theo thang tần số Mel  Một phương pháp để chuyển đổi sang thang mel là sử dụng băng lọc (Hình 4.6), trong đó mỗi bộ lọc có đáp ứng tần số dạng tam giác. Số băng lọc sử dụng thường trên 20 băng. Thông thường, người ta chọn tần số từ 0 dến Fs/2 (Fs là tần số lấy mẫu tiếng nói). Nhưng cũng có thể một dải tần giới hạn từ LOFREQ đến HIFREQ sẽ được dùng để lọc đi các tần số không cần thiết cho xử lý. Chẳng hạn, trong xử lý tiếng nói qua đường điện thoại có thể lấy giới hạn dải tần từ LOFREQ=300 đến HIFREQ=3400.  Sau khi tính FFT ta thu được phổ tín hiệu (fn). Thực chất đây là một dãy năng lượng  . Cho W(n) qua một dãy K băng lọc dạng tam giác, ta được một dãy các  . Tính tổng của các dãy   trong từng băng lọc, ta thu được một dãy các hệ số  Khối 6:  Biến đổi Cosine rời rạc (DCT) Trong bước này ta sẽ chuyển log của các giá trị   về miền thời gian bằng cách biến đổi Cosine rời rạc (DCT). Kết quả của phép biến đổi này ta thu được các hệ số MFCC. Thông thường, chỉ có một số giá trị đầu tiên của ic được sử dụng. Trong các ứng dụng nhận dạng tiếng nói, người ta thường lấy 12 hệ số MFCC và thêm 1 hệ số năng lượng của khung sau khi đã được chuẩn hóa làm tham số đặc trưng cho tín hiệu tiếng nói (như vậy tổng cộng có Q=13 hệ số).  Khối 7:  Cepstral có trọng số  Vì độ nhạy của các hệ số cepstral bậc thấp làm cho phổ toàn bộ bị đổ dốc, độ nhạy của các cepstral bậc cao gây ra nhiễu nên người ta thường sử dụng cửa sổ cepstral để cực tiểu hóa độ nhạy này. Công thức biểu diễn các hệ số cepstral có trọng số: Khối 8 : Lấy đạo hàm các hệ số MFCC theo thời gian Để nâng cao chất lượng nhận dạng, người ta đưa thêm các giá trị đạo hàm theo thời gian của cácgiá trị hệ số MFCC vào vector hệ số tiếng nói. Các giá trị đó được tính theo: trong đó; θ: là độ dài cửa sổ tính delta (thường chọn là 2 hoặc 3).  Kết thúc các bước trên với mỗi khung ta thu được một vector có 2Q thành phần biểu diễn tham số đặc trưng của tiếng nói. Phương pháp mã dự đoán tuyến tính LPC  Mô hình LPC được sử dụng để trích lọc các tham số đặc trưng của tín hiệu tiếng nói. Kết quả của quá trình phân tích tín hiệu thu được một chuỗi gồm các khung tiếng nói. Các khung này được biến đổi nhằm sử dụng cho việc phân tích âm học.  Nội dung phân tích dự báo tuyến tính là: một mẫu tiếng nói được xấp xỉ bởi tổ hợp tuyến tính của các mẫu trước đó. Thông qua việc tối thiểu hóa tổng bình phương sai số giữa các mẫu hiện tại với các mẫu dự đoán có thể xác định được một tập duy nhất các hệ số dự báo. Các hệ số s(n) dự báo này là các trọng số được sử dụng trong tổ hợp tuyến tính. Với dãy tín hiệu tiếng nói,giá trị dự báo được xác định bởi: trong đó;αk: là các hệ số đặc trưng cho hệ thống. Sơ đồ bộ xử lý LPC dùng trích chọn đặc trưng tiếng nói Sơ đồ khối bộ phân tích LPC dùng cho trích chọn các tham số đặc trưng của tín hiệu tiếngnói (Hình 4.7). Hàm sai số dự báo được tính theo công thức: Để cực tiểu hóa lỗi cần tìm tập giá trị {αk} phù hợp nhất. Do tín hiệu tiếng nói thay đổi theo thời gian nên các hệ số dự báo phải được ước lượng từ các đoạn tín hiệu ngắn. Vấn đề đặt ra là tìm một tập các hệ số dự báo để tối thiểu hóa sai số trung bình trên một đoạn ngắn.  Hàm lỗi dự báo trong một thời gian ngắn xác định bởi: trong đó; sn(m) : là một đoạn tín hiệu tiếng nói lân cận mẫu thứ n; Tìm tập giá trị α k để tối thiểu hóa E bằng cách   với I =1,2,…,p từ đó nhận được phương trình: Đặt: Phương trình trên có thể viết: Phương pháp PLP  Phương pháp này là sự kết hợp của hai phương pháp đã trình bày ở trên. Hình 10 mô tả các bước xác định hệ số PLP. Sơ đồ các bước xác định hệ số PLP Các khối xử lý  ♦ Khối 1:  Biến đổi Fourier nhanh (FFT)  Tương tự như phương pháp MFCC, tín hiệu tiếng nói được chia thành các khung và được chuyển sang miền tần số bằng thuật toán FFT. ♦ Khối 2:  Lọc theo thang tần số Bark  Tín hiệu tiếng nói được lọc qua các bộ lọc phân bố theo thang tần số phi tuyến, trong trường hợp này là thang tần số Bark: ♦ Khối 3 : Nhấn mạnh tín hiệu dùng hàm cân bằng đường xong cân bằng độ ồn (equal-loudnes) bằng độ ồn (Equal-Loudnes).Bước này tương tự nhấn mạnh (preemphasis) của phương pháp MFCC.Hàm này mô phỏng: ♦ Khối 4:  Dùng luật cường độ nghe (Power Law of Hearing) Bước xử lý này giống như bước lấy giá trị logarit trong phương pháp MFCC. Hàm căn lập phương được dùng có dạng: ♦ Khối 5:  Biến đổi Fourier ngược (Inverse DFT)  Các hệ số tự tương quan được biến đổi Fourier ngược là giá trị đầu vào cho LPC.  ♦ Khối 6:  Thuật toán Durbin  Thuật toán Durbin được sử dụng để tính các hệ số dự báo tuyến tính như phương pháp LPC . ♦ Khối 7 : Tính các giá trị delta  Phương pháp tính tương tự như phương pháp hệ số MFCC.  Phương pháp ứng dụng trí tuệ nhân tạo cho xử lý và nhận dạng tiếng nói có thể tham khảo thêm các tai liệu trích dẫn trong tài liêu..  CÁC HỆ THỐNG HỘI THOẠI  Chúng ta quan tâm đến những gì xảy ra bên trong của một đối tượng - từ khi nó nhận được một kết quả của tri thức đến khi đối tượng này quyết định một hành động. Trong phần này chúng ta tập trung vào giao diện giữa đối tượng và môi trường. Kết quả chúng ta có được sự nhận thức: thị giác, thính giác và có thể nhiều giác quan khác, ở một kết quả khác chúng ta có hành động: sự cử động của một cánh tay robot chẳng hạn.  Mặc dù bao trùm lên phần này là đối thoại. Một nhóm đối tượng có thể thành công hơn, cá thể hay tập thể nếu họ đối thoại với nhau về mục tiêu và sự hiểu biết của mình. Chúng ta sẽ xem xét một cách chặt chẽ ngôn ngữ nhân loại và ngôn ngữ này được sử dụng như là một công cụ đối thoại. Con người sử dụng một số hữu hạn các ký hiệu quy ước (mỉm cười, bắt tay) để giao tiếp tương tự như hầu hết các động vật khác. Con người cũng có thể phát triển một hệ thống các kí hiệu có kiến trúc phức tạp được biết như là ngôn ngữ mà có thể sử dụng chúng để đối thoại trong hầu hết những gì mà họ biết về thế giới.  Trong các hệ cơ sở tri thức, đặc biệt hẹ chuyên gia, các hệ thống đối thoai giữa ngươi và máy đươc thiết lập và là một khâu rất cần thiết để xử lý thông tin, Học viên có thể tham khảo thêm phần này ở các tài liệu trích dẫn kèm theo  TỪ ĐIỂN ĐIỆN TỬ  Bước đầu tiên trong việc định nghĩa ngữ pháp là định nghĩa một từ điển ngôn ngữ, hoặc danh sách các từ vựng có thể cho phép. Các từ được nhóm lại vào những phạm trù hoặc những phần của lời nói quen thuộc đến từ điển người dùng: danh từ, đại từ, và tên để biểu thị chúng, động từ để biểu thị một sự kiện, tính từ để bổ nghĩa cho danh từ, trạng từ bổ nghĩa cho động từ. Hình 11 cho một từ điển ngôn ngữ nhỏ.  Từ điển ngôn ngữ  Mỗi một phạm trù đều kết thúc để biểu thị rằng có những từ khác ở trong phạm trù này. Tuy nhiên chú ý rằng có hai lý do khác biệt cho việc mất từ. Đối với danh từ, động từ, tính từ và trạng từ, nó là nguyên tắc cơ bản bất khả thi để hiển thị tất cả chúng. Không những có hàng ngàn hoặc hàng chục ngàn thành viên trong mỗi lớp, mà mỗi một loại mới luôn luôn được bổ sung thêm vào. Ví dụ, ngày nay “fax” là một danh từ và động từ phổ biến nhưng nó chỉ được đặt ra trong vài nănm trước. Có bốn phạm trù được gọi là lớp mở. Những phạm trù khác (đại từ, quán từ, giới từ, và liên từ) được gọi là lớp đóng. Chúng thường có một số lượng nhỏ các từ (một vài từ đến một vài nhóm từ) mà nó có thể được liệt kê theo quy tắc. sự thay đổi lớp đóng diễn ra trong hàng thế kỷ , không phải hàng tháng. Ví dụ “thee” và “thou” thường được sử dụng làm ại từ trong thế kỷ XVII, bị suy tàn vào thế kỷ XIX, và ngày nay nó chỉ được thấy trong thơ ca và ngôn ngữ địa phuơng.  Ngữ pháp  Bước tiếp theo là phối hợp các từ trong cụm từ. chúng ta vẫn sử dụng năm biểu tượng nonterminal để định nghĩa sự khác nhau của cụm từ: câu (S), cụm danh từ (NP), cụm động từ (VP), cụm giới từ (PP), và mệnh đề quan hệ (Rel Clause)(4). Hình 12 xét một ngữ pháp cho ε0 với một ví dụ cho mỗi một quy luật viết lại.  Ngữ pháp cho từđiển, với cụm từ ví dụ cho mỗi quy luật  CÂU HỎI VÀ BÀI TẬP  Không xem lại bài trả lời các câu hỏi sau: bốn lớp được nhắc đến của ngữ pháp hình thức? Thực hiện một phiên bản của giải thuật biểu đồ phân tích cú pháp mà kết quả là một cây cho tất cả cạnh mà mở rộng cho toàn bộ đầu vào.  Trình bày phương pháp phát hiện tiếng nói dựa trên năng lượng phổ ngắn hạn . Trình bày phương pháp tính các hệ số MFCC: phương pháp trích chọn tham số tiếng nói được sử dụng rộng rãi bởi tính hiệu quả của nó thông qua phân tích cepstral theo thang đo mel.  Trình bày phương pháp mã dự đoán tuyến tính LPC. 0  Tải về   Tái sử dụng  Tài liệu PDF Tài liệu EPUB Học Viện Công Nghệ Bưu Chính Viễn Thông 0 Giáo trình\n                  | \n                 12 Tài liệu\n                 Đánh giá: 0  dựa trên\n         0  đánh giá\n\n        \n     Nội dung cùng tác giả   Nội dung tương tự   × VOER message Thư viện Học liệu Mở Việt Nam (VOER) được tài trợ bởi  Vietnam Foundation  và vận hành trên nền tảng  Hanoi Spring . Các tài liệu đều tuân thủ giấy phép Creative Commons Attribution 3.0 trừ khi ghi chú rõ ngoại lệ.",
          "relevance": "1",
          "title": "Xử lý ngôn ngữ tự nhiên",
          "url": "https://voer.edu.vn/m/xu-ly-ngon-ngu-tu-nhien/b687e267"
        },
        {
          "content": "Ông Xuân Hồng Chia sẻ kiến thức và thông tin về Machine learning Menu Skip to content Data Science Big Data Deep learning Exploratory Data Analysis Getting and cleaning data Machine learning Lập trình Python Spark R Weka Kiến thức Toán Statistical Inference Xử lý ngôn ngữ tự nhiên – Natural Language Processing (NLP) Chia sẻ Dự án About Xử lý ngôn ngữ tự nhiên (Natural Language Processing) là gì? Tháng Tám 13, 2015 Tháng Sáu 19, 2017 Ông Xuân Hồng Bạn nghĩ gì về bài viết này? natural langage processing Facebook Twitter Google LinkedIn Like this: Số lượt thích Đang tải... Liên quan Điều hướng bài viết ←  Scikit-learn: Naive Bayes Classifier Những ứng dụng thương mại của Deep learning  → Nhập bình luận của bạn tại đây... Mời bạn điền thông tin vào ô dưới đây hoặc kích vào một biểu tượng để đăng nhập: Thư điện tử  (bắt buộc) (Địa chỉ của bạn được giấu kín) Tên  (bắt buộc) Trang web  Bạn đang bình luận bằng tài khoản WordPress.com  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Twitter  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Facebook  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Google+  (  Đăng xuất  /  Thay đổi  ) Hủy bỏ Connecting to %s Notify me of new comments via email. Thông báo cho tôi bằng email khi có bài đăng mới. Bạn Không Thể Sống Một Mình Nếu ngày mai tớ trở thành người nổi tiếng. Gặp tớ giữa đường bạn đừng ngại tớ nghe. Cứ nhìn tớ như ngày xưa bạn thấy. Đừng nên nghĩ tớ quá xa vời.  Nếu một ngày tớ là kẻ ăn xin. Thì bố thí cho tớ vài kỷ niệm. Dù rất nhỏ nhưng đừng nên khinh tớ. Vì tớ vẫn là tớ của ngày xưa.  Nếu ngày mai tớ chẳng biết đường về. Hãy chỉ giùm tớ con đường đi đúng đắn. Đừng để tớ sa chân nơi bùn nhớp. Lúc rút chân lên thì đã bẩn mất rồi.  Nếu ngày mai tớ gặp bạn giữa đường. Thì ta sẽ gặp nhau và cười nhé. Đừng nên trách và đừng nên đánh mất. Đôi khi có một số người lướt qua cuộc đời bạn và ngay tức khắc bạn nhận ra rằng sự có mặt của họ ý nghĩa như thế nào. Họ đã dạy bạn những bài học, đã giúp bạn nhận ra giá trị của chính mình hoặc trở thành con người mà bạn từng mơ ước. Có lẽ bạn sẽ không biết được những con người này từ đâu đến ( bạn cùng phòng, người hàng xóm, vị giáo sư, người bạn mất liên lạc từ lâu hay thậm chí là một người hoàn toàn xa lạ ). Nhưng khi bạn thờ ơ với họ, hãy nhớ rằng trong từng khoảnh khắc họ sẽ ảnh hưởng rất sâu sắc đến cuộc đời bạn.  …Cuộc sống không hề làm khó bạn mà chính bạn đã tự làm khó mình bằng cách đặt ra những câu hỏi đại loại như: “Tại sao lại thế này?… Tại sao lại thế kia?…”. Không ai có thể trả lời giúp bạn mà chính bạn phải tự đi tìm lời giải đáp cho mình. Hãy luôn nghĩ về người khác trước khi nghĩ đến bản thân, chắc chắn lúc đó bạn sẽ hài lòng với câu trả lời của chính mình. Và đừng quên luôn đặt câu hỏi: “Mình đã làm được gì?” trước khi tự hỏi: “Mình đã nhận được gì?” nhé! Tôi tin là bạn sẽ thành công!   Thật ra trên đời, con người luôn cần có nhau để giải quyết rất nhiều vấn đề. Nhờ những lúc qua lại giúp đỡ đó mà hoá giải bao muộn phiền, nảy sinh các tình cảm, cùng trải nghiệm biết bao điều thú vị, hiểu biết thêm nhiều tri thức mới. Đây chính là cuộc sống! Vì thế hãy trân trọng những người đang “làm phiền” bên cạnh bạn nhé! Đừng để đến khi mất đi rồi mới nghẹn ngào lại bảo “giá như” … ĐĐ. GS. Thích Phước Tiến \n__(())__ Namo Bụt Sakyamuni Nhận Email khi có bài viết mới Liên hệ Email (bắt buộc) Comment (bắt buộc) Top bài viết Bài viết mới FAQ: Big data cho người mới bắt đầu Tháng Mười 2, 2017 Làm việc với Spark DataFrames – Truy vấn nâng cao (Subqueries, Cube, Rollup) Tháng Chín 15, 2017 Thống kê ứng dụng 3: Các vấn đề trong thống kê Tháng Chín 14, 2017 Thống kê ứng dụng 2: Suy luận thống kê Tháng Chín 12, 2017 Thống kê ứng dụng 1: Quan sát dữ liệu Tháng Chín 7, 2017 AI, Machine Learning, Deep Learning phân biệt như thế nào cho đúng Tháng Chín 4, 2017 Information extraction – Bài toán rút trích thông tin trong văn bản Tháng Tám 28, 2017 Bắt đầu nghiên cứu big data từ đâu và như thế nào Tháng Tám 3, 2017 Data Science – Mỏ vàng của Kỉ nguyên số Tháng Tám 3, 2017 SMA 2017 – Lý thuyết ra quyết định Tháng Sáu 17, 2017 Big Data Chia sẻ Data Science Deep learning Dự án Exploratory Data Analysis Getting and cleaning data Kiến thức Lập trình Machine learning Python R Spark Statistical Inference Toán Weka Xử lý ngôn ngữ tự nhiên - Natural Language Processing (NLP) This slideshow requires JavaScript. Thư viện Tháng Mười 2017  (1) Tháng Chín 2017  (5) Tháng Tám 2017  (3) Tháng Sáu 2017  (3) Tháng Năm 2017  (3) Tháng Ba 2017  (1) Tháng Một 2017  (3) Tháng Mười Hai 2016  (1) Tháng Mười Một 2016  (2) Tháng Mười 2016  (2) Tháng Chín 2016  (1) Tháng Tám 2016  (2) Tháng Bảy 2016  (2) Tháng Năm 2016  (1) Tháng Tư 2016  (1) Tháng Ba 2016  (2) Tháng Hai 2016  (2) Tháng Một 2016  (1) Tháng Mười Hai 2015  (6) Tháng Mười Một 2015  (5) Tháng Mười 2015  (6) Tháng Chín 2015  (11) Tháng Tám 2015  (16) Tháng Bảy 2015  (25) Tháng Sáu 2015  (4) Thống kê 360,709 hits Tháng Tám 2015 H B T N S B C « Th7   Th9 »   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31   Data Science Lập trình Kiến thức Chia sẻ Dự án About Blog tại WordPress.com. Post to Hủy bỏ %d  bloggers like this:",
          "relevance": "1",
          "title": "Menu",
          "url": "https://ongxuanhong.wordpress.com/2015/08/13/xu-ly-ngon-ngu-tu-nhien-la-gi/"
        },
        {
          "content": "Skip to content \n                  Features\n \n                  Business\n \n                  Explore\n \n                      Marketplace\n \n                  Pricing\n This repository Sign in or Sign up \n    Watch\n   \n    21\n   \n    Star\n   \n      62\n     \n        Fork\n       \n      47\n     minhpqn / nlp_100_drill_exercises Code Issues 0 Pull requests 0 \n      Projects\n       0 \n    Insights\n \n              Dismiss\n             Join GitHub today GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. Sign up \n            100 bài luyện tập xử lý ngôn ngữ tự nhiên\n           \n                63\n               \n              commits\n           \n              1\n             \n            branch\n           \n              0\n             \n            releases\n           \n      1\n     \n    contributor\n Python 99.1% Shell 0.9% Python Shell \n    Clone or download\n     \n          Clone with HTTPS\n           \n          Use Git or checkout with SVN using the web URL.\n         \n  Download ZIP\n \n      Find file\n     Branch: master Switch branches/tags Branches Tags \n                master\n               Nothing to show Nothing to show \n          New pull request\n         \n      Latest commit\n       \n        219240a\n       Nov 16, 2016 minhpqn Add data Permalink Failed to load latest commit information. data Add data Nov 16, 2016 python Revised Sep 22, 2016 Learning_note.md  version Feb 17, 2016 README.md Revised Sep 22, 2016 environment.yml Revised Sep 22, 2016 markdown_toclify.py  version Feb 17, 2016 tohoku_100_exercises_draft.md Revised Sep 22, 2016 \n      README.md\n     Table of Contents Chương 1: Bài tập khởi động Chương 2: Các lệnh cơ bản trên môi trường Unix Chương 3: Biểu thức chính quy (regular expressions) Chương 4: Morphological Analysis trong tiếng Nhật (形態素解析) Chương 5: Dependency parsing (係り受け解析) Chương 6: Xử lý văn bản tiếng Anh Chương 7: Database Chương 8: Machine Learning Chương 9: Không gian vector (I) Chương 10: Không gian vector (II) Phụ lục: Corpus, data sử dụng trong 100 bài luyện tập NLP 100 bài luyện tập xử lý ngôn ngữ tự nhiên Dịch từ tài liệu  言語処理100本ノック  của lab Inui-Okazaki, đại học Tohoku, Nhật Bản. Người dịch: Phạm Quang Nhật Minh\n(minhpqn). Tham khảo thêm phiên bản cũ của tài liệu tại  NLP 100 Drill\nExercises Chú ý: Khi sử dụng tài liệu 100 bài luyện tập xử lý ngôn ngữ tự nhiên, cần trích dẫn các nguồn sau: Tài liệu \"言語処理100本ノック\" của lab Inui-Okazaki, đại\nhọc Tohoku, Nhật Bản. URL:  http://www.cl.ecei.tohoku.ac.jp/nlp100 Đường link tới bản dịch hiện tại:  https://github.com/minhpqn/nlp_100_drill_exercises . Người dịch: Phạm Quang Nhật Minh. Chương 1: Bài tập khởi động 00. Đảo ngược xâu ký tự Hãy đảo ngược xâu ký tự \"stressed\" (theo thứ tự từ cuối xâu đến đầu xâu ký tự). 01. Trích xuất ký tự từ xâu ký tự Từ xâu ký tự \"MPyaktQrBoilk RCSahr\", hãy trích xuất các ký tự ở vị trí\n2,4,6,8,10,12,14,16,18,20 và kết hợp theo thứ tự đó để tạo thành 1 xâu ký tự mới\n(ký tự space cũng được tính, các ký tự được đánh số từ 1). 02. Kết hợp hai xâu ký tự Hãy kết hợp hai xâu ký tự \"Partrol\" và \"Car\" để tạo thành xâu mới \"PatrolCar\". 03. Tokenize và thống kê số lượng ký tự của mỗi từ Tokenize câu sau: \"Now I need a drink, alcoholic of course, after the heavy\nlectures involving quantum mechanics.\" Đưa ra danh sách gồm số ký tự alphabet trong mỗi từ theo thứ tự xuất hiện\ncủa từ đó trong câu. 04. Ký tự thành phần Tokenize câu sau: \"Hi He Lied Because Boron Could Not Oxidize Fluorine. New\nNations Might Also Sign Peace Security Clause. Arthur King Can.\" Lấy ra ký tự đầu tiên của các từ ở vị trí 1, 5, 6, 7, 8, 9, 15, 16, 19; với\ncác từ còn lại lấy ra 2 ký tự đầu tiên. Tạo ra một map từ các xâu ký tự được\ntrích ra tới vị trí của từ trong câu. 05. n-gram Viết hàm sinh ra tất cả các n-gram từ một dãy cho trước (xâu ký tự hoặc danh\nsách). Sử dụng hàm đã viết, sinh ra word bi-gram và character bi-gram từ câu sau:\n\"I am an NLPer\" 06. Tập hợp Sinh ra tập X và Y tương ứng là tập các character bi-gram từ hai xâu ký tự\n\"paraparaparadise\" và \"paragraph\". Sinh ra các tập hợp union, intersection và difference của X và Y Kiểm tra xem bi-gram 'se' có thuộc tập X (Y) hay không? 07. Sinh ra câu từ template Viết hàm số nhận vào 3 biến x, y, z và trả về xâu ký tự \"y vào lúc x giờ là z\"\nSinh ra kết quả với các giá trị x, y, z sau đây x=\"12\" y=\"Nhiệt độ\" z=22.4 08. Xâu mật mã Từ các ký tự của một xâu cho trước, cài đặt hàm có tên cipher để mã hoá xâu như\nsau: Nếu là ký tự tiếng Anh ở dạng thường (lower-case characters) thì chuyển\nthành ký tự có mã là (219 - mã ký tự). Các ký tự khác giữ nguyên. Sử dụng hàm đã viết để mã hoá và giải mã các xâu ký tự tiếng Anh. 09.  Typoglycemia Cho đầu vào là một câu tiếng Anh bao gồm các word ngăn cách nhau bằng ký tự\nspace. Viết chương trình thực hiện việc sau: Với mỗi word, giữ nguyên ký tự\nđầu và ký tự cuối, đảo thứ tự một cách ngẫu nhiên các ký tự còn lại của (tất\nnhiên các word có ít hơn 4 ký tự thì không cần làm gì) Cho trước một câu tiếng\nAnh hợp lệ, ví dụ \"I couldn't believe that I could actually understand what I\nwas reading : the phenomenal power of the human mind .\", chạy chương trình đã\nviết để đưa ra kết quả. Chương 2: Các lệnh cơ bản trên môi trường Unix Các bài tập trong chương này sử dụng dữ liệu trong file\n hightemp.txt . File\nnày bao gồm các bản ghi nhiệt độ cao nhất ở Nhật Bản. Mỗi bản ghi bao gồm 3 cột\n\"Tỉnh\", \"Địa điểm\", \"Nhiệt độ\" (độ C), và ngày tháng đo. Các cột dữ liệu được\nphân cách bằng ký tự tab. Viết các chương trình thực hiện các nhiệm vụ trong các\nbài tập dưới đây. Sau đó, chỉ sử dụng các lệnh trong unix để thực hiện các nhiệm\nvụ đó và xác nhận xem kết quả của chương trình bạn viết có giống với kết quả các\nlệnh unix đưa ra hay không. 10. Đếm số dòng trong file Đếm số dòng trong file. Xác nhận kết quả bằng lệnh wc trong unix. 11. Biến đổi các ký tự tab thành space Chuyễn mỗi ký tự tab thành ký tự space. Xác nhận kết quả lần lượt bằng các lệnh\nsed, tr, và expand. 12. Lưu cột 1 vào file col1.txt, cột 2 vào file col2.txt Trích xuất nội dung trong cột 1, cột 2 và lưu vào các file tương ứng: col1.txt\nvà col2.txt. Thử thực hiện công việc chỉ dùng lệnh cut trong unix. 13. Trộn hai file col1.txt và col2.txt Kết hợp nội dung trong 2 file col1.txt và col2.txt để tạo thành một file mới có\nnội dung giống với cột 1 và cột 2 trong file ban đầu (các cột cách nhau bởi ký\ntự tab). Sử dụng lệnh paste để thực hiện bài tập và xác nhận kết quả của chương\ntrình bạn viết. 14. Trích xuất ra N hàng đầu tiên của file Viết chương trình trích xuất ra N hàng đầu tiên của file. Biến số dạng dòng lệnh\nlà số tự nhiên N. Sử dụng lệnh head trong unix để thực hiện công việc. 15. Trích xuất ra N hàng cuối cùng của file Viết chương trình trích xuất ra N hàng cuối cùng của file. Chương trình nhận đầu\nvào từ dòng lệnh là số tự nhiên N. Sử dụng lệnh tail trong unix để thực hiện\ncông việc. 16. Chia file thành N phần Chia file thành các files nhỏ với N lines mỗi file (đơn vị là các hàng trong\nfile). Chương trình nhận đầu vào từ dòng lệnh là số tự nhiên N. Sử dụng lệnh\nsplit để thực hiện công việc (split -l N). Sau đó, cải tiến chương trình để chia file thành thành N phần bằng nhau (thay vì\nN lines mỗi file). 17. Đưa ra các các xâu ký tự duy nhất (unique) trong cột 1 Đưa ra các xâu ký tự duy nhất trong cột 1 của file. Sử dụng lệnh cut, sort, uniq\nđể thực hiện nhiệm vụ. 18. Sắp xếp các hàng theo thứ tự giảm dần của giá trị (numeric value) của cột thứ 3 Viết chương trình thực hiện nhiệm vụ trên. Dùng lệnh sort để xác nhận (trong bài\ntập này, kết quả của chương trình của bạn với lệnh sort có thể khác nhau do có\nthể có các giá trị giống nhau trong cột thứ 3). 19. Sắp xếp theo tần suất xuất hiện Đưa ra tần suất xuất hiện của các giá trị trong cột 1; sắp xếp các giá trị trong\ncột 1 theo thứ tự từ cao đến thấp của tần suất xuất hiện. Chỉ dùng lệnh cut,\nuniq, sort để thực hiện nhiệm vụ. Chương 3: Biểu thức chính quy (regular expressions) Bài tập trong chương 3 sử dụng file\n jawiki-country.json.gz .\nFile này lưu trữ các tài liệu Wikipedia và có định dạng sau đây. - Dòng thứ nhất\nlưu trữ thông tin về tài liệu dưới định dạng JSON. - Ở các dòng tiếp theo, dòng\ntiêu đề của văn bản được lưu trữ tại khoá \"title\"; nội dung của tài liệu được\nlưu trữ tại khoá \"text\". Các dòng này được lưu trữ ở định dạng JSON. Lập trình để xử lý các việc sau đây. 20. Đọc vào dữ liệu JSON Đọc dữ liệu từ file JSON chứa các tài liệu Wikipedia, trích xuất & hiển thị nội\ndung của tài liệu (trường text của JSON object) có liên quan đến \"イギリス\" (có\nnghĩa là nước Anh). Sử dụng các nội dung của tài liệu được trích xuất này để\nthực thi các nhiệm vụ trong các bài tập từ 21-29. 21. Trích xuất các dòng có chứa tên đề mục Trong các tài liệu, trích xuất các dòng có chứa tên đề mục (category name hay\nカテゴリ名). 22. Trích xuất các tên đề mục (Category name) Trích xuất tên đề mục của trong các tài liệu. Trong bài tập này, cần trích xuất\nchính xác các tên đề mục chứ không phải dòng chứa tên đề mục. 23. Cấu trúc của các Section Hiển thị tên của các section và level của các section trong các tài liệu\nWikipedia (Ví dụ với section == Section Name ==\" thì level bằng 1) 24. Trích xuất các liên kết file Trích xuất toàn bộ các liên kết đến các media files trong tài liệu. 25. Trích xuất templates Trích xuất vị trí và tên các folder có template \"基礎情報\" trong tài liệu. Lưu\nkết quả trong các đối tượng dictionary. Tham khảo về templates tại\n đây . 26. Loại bỏ các emphasis markups Trong khi làm các xử lý ở bài tập 25, xoá các emphasis markup (italic, bold,\nboth) từ vị trí của các templates và biến đổi thành plain text. (Tham khảo về\ncác loại markup tại  Wiki\nmarkup , bảng tham khảo bằng\ntiếng Nhật tại\n マークアップ早見表 ). 27. Xoá các link đến các trang Wikipedia khác Nhiệm vụ giống như bài 26 và thêm vào xử lý sau. Xoá các markups của liên kết\nđến các trang Wikipedia khác từ các templates được trích xuất và biến đổi thành\ndạng text. (Tham khảo về các loại markup tại  Wiki\nmarkup , bảng tham khảo bằng\ntiếng Nhật tại\n マークアップ早見表 ). 28. Xoá các markup trong văn bản Thêm vào xử lý ở bài 27. Xoá các markup trong các templates càng nhiều càng tốt\nvà in ra các thông tin cơ bản về quốc gia. 29. Lấy ra các URL của quốc kỳ Sử dụng nội dung của các template và lấy ra URl đến quốc kỳ (国旗画像のURL).\nHint: Gọi API  imageinfo  của\n MediaWWiki , biến đổi các file\nreferences thành URL. Chương 4: Morphological Analysis trong tiếng Nhật (形態素解析) Download file  neko.txt \nlà nội dung bằng plain text của cuốn tiểu thuyết \"吾輩は猫である\" của nhà văn\n夏目漱石 (Soseki Natsume). Sử dụng package  MeCab  để thực hiện\n\"morphological analysis\" (Tham khảo trang tiếng Nhật tại\n đây ).\nLưu kết quả vào file neko.txt.mecab. Sử dụng file kết quả để thực hiện các công\nviệc ở các bài tập dưới đây. Chú ý: Trong các bài tập 37, 38, 39 có thể sử dụng các phần mềm\n matplitlib  hoặc\n Gnuplot . @All: Với các bạn không quen xử lý text tiếng Nhật, có thể dùng một file text\ntiếng Anh và thực hiện POS tagging, sau đó làm các bài tập dưới đây. 30. Đọc vào kết quả morphological analysis Viết chương trình đọc vào kết quả morphological analysis (file neko.txt.mecab). Yêu cầu: Với mỗi morpheme, lưu các thông tin: 表層形 (surface form), 基本形\n(base form), 品詞 (pos), 品詞細分類1 (pos1) bằng cấu trúc dữ liệu hash map với\ncác key tương ứng là: surface, base, pos, pos1. Lưu trữ mỗi câu bằng danh sách\ncủa các morpheme. Trong các bài tập còn lại trong chương 4, hãy sử dụng cách tổ\nchức dữ liệu trong bài 30 này. 31. Động từ Trích xuất tất cả các surface forms của động từ (pos=動詞). 32. Dạng nguyên thể của động từ (動詞の原形) Trích xuất tất cả dạng nguyên thể của động từ (base form). 33. Danh từ dạng サ (サ変名詞) Trích xuất toàn bộ các danh từ dạng サ (サ変名詞). Tham khảo trang Wikipedia\ntiếng Nhật về\n サ行変格活用 . 34. 「AのB」 Trích xuất tất cả các danh từ ghép (compound nouns) gồm 2 danh từ kết nối bằng\nの. 35. Trích xuất các kết nối danh từ (noun connections hay 名詞の連接) Trích xuất tất cả các noun connections (các danh từ đứng cạnh nhau liên tiếp).\nKhi trích xuất, chú ý trích xuất chuỗi danh từ matching dài nhất có thể. Ví dụ\nABC trong đó A, B, C là danh từ thì phải trích xuất ABC thay vì AB. 36. Tần suất xuất hiện của từ Lập trình tính tần suất xuất hiện của từ trong văn bản. Đưa ra các từ theo thứ\ntự giảm dần của tần suất xuất hiện. 37. Top 10 từ xuất hiện nhiều nhất Vẽ đồ thị (ví dụ bar graph) của tần suất xuất hiện của 10 từ xuất hiện nhiều\nnhất trong văn bản. 38. Histogram Vẽ đồ thị histogram tần suất xuất hiện của các từ. Trục ngang là tần suất xuất\nhiện. Trục dọc là các từ. 39. Luật Zipf Vẽ đồ thị với trục ngang là rank của các từ theo tần suất xuất hiện (cao đến\nthấp), trục dọc là tần suất xuất hiện của các từ. Chương 5: Dependency parsing (係り受け解析) Thực hiện phân tích cấu trúc ngữ pháp (dependency parsing) bằng công cụ\n CaboCha  cho file\n neko.txt  và lưu kết\nquả vào file neko.txt.cabocha. Sử dụng file kết quả này làm đầu vào cho các bài\ntập dưới đây. 40. Đọc vào kết quả dependency parsing (theo morphemes) Cài đặt lớp Morph cho các morphemes. Lớp này có các biến thành phần (member\nvariables) là surface (cho surface forms của morphems), base (cho base form),\npos (cho POS tag), pos1 (cho detailed POS tag 1 品詞細分類1). Sau đó đọc vào kết\nquả phân tích dependency parsing trong file neko.txt.cabocha. Mỗi câu sẽ bao gồm\nmột danh sách các Morph objects. Hiển thị danh sách các morphemes cho câu thứ 3\ntrong văn bản. 41. Đọc vào kết quả dependency parsing (theo chunks và depedency relations) Tiếp theo bài 40, cài đặt lớp Chunk để lưu trữ các chunk (hay bunsetsu (文節)).\nLớp này có các biến thành phần là: morphs (để lưu trữ danh sách các Morph\nobjects) dst để lưu trữ index của chunk mà chunk hiện tại trỏ đến (chunk đích - destination) srcs để lưu trữ danh sách các indexes của các chunk trỏ đến chunk hiện tại. Sau đó, đọc vào kết quả dependency parsing. Mỗi câu sẽ bao gồm danh sách của các\nChunk objects. Hiển thị xâu ký tự và giá trị của biến dst của các chunk trong câu\nthứ 8 của file đầu vào. Các bài tập còn lại trong chương 5 sẽ sử dụng các chương trình được tạo ra\ntrong bài tập 40 và 41. 42. Hiển thị chunk nguồn (head) và chunk đích (modifier) trong các depedency relations Hiển thị nội dung (dạng text) các chunk nguồn (head) và chunk đích (modifier)\ntrên mỗi dòng và cách nhau bởi ký tự tab. Chú ý không hiển thị các dấu\n(punctuation marks) trong các chunk. 43. Trích xuất các dependency relations giữa các chunk chứa danh từ và các chunk chứa động từ Trích xuất các dependency relations giữa các chunk chứa danh từ và các chunk\nchứa động từ. In ra theo định dạng tab. Tương tự như bài 42, không hiển thị các\ndấu (punctuation marks) trong các chunk. 44. Visualize cây dependency Visualize dependency trees của một câu cho trước. Khi visualize, biến các\ndependency trees theo  ngôn ngữ\nDOT , hay\nsử dụng  Graphviz . Nếu sử dụng Python, có thể hiển\nthị đồ thị có hướng bằng thư viện/package\n pydot . 45. Trích xuất các verb case patterns Yêu cầu của bài tập này là tìm hiểu (investigate) về case frame trong tiếng Nhật\nsử dụng dữ liệu trong file đầu vào neko.txt. Coi các động từ là vị ngữ\n(predicate), các trợ từ (như が,を,...) của chunk liên với với động từ là các case, hãy\nin ra các động từ và các \"case\" theo định dạng cách nhau bởi ký tự tab. Output\ncủa chương trình cần thoả mãn các điều kiện sau: Các vị ngữ là dạng nguyên thể\n(base form) của động từ, tính từ bên trái nhất (lef-most) của các chunk\n(bunsetsu). Coi các trợ từ liên kết với các vị ngữ là các \"case\" trong case\nframe. Nếu một vị ngữ được liên kết bởi nhiều trợ từ, in tất cả các trợ từ\ntheo thứ tự từ điển. Các trợ từ cách nhau bởi dấu cách Xem xét ví dụ sau: 「吾輩はここで始めて人間というものを見た」(câu thứ 8 trong\nfile neko.txt.cabocha). Câu này gồm hai động từ 始める và 見る. Nếu trong kết\nquả phân tích cú pháp, động từ 始める liên kết với chunk ここで, động từ 見る\nliên kết với với chunk 吾輩は và ものを, chương trình sẽ in ra: 始める で\n見る は を\n Lưu output của chương trình ra file, xác nhận các mục sau chỉ với các lệnh của Unix. Kết hợp các động từ xuất hiện trong corpus và các case patterns Các case patterns của các động từ する, 見る, 与える (theo thứ tự từ cao đến thấp của tần suất xuất hiện trong corpus). Tham khảo về case frame structures trong tiếng Nhật tại\n đây  (Kawahara et al., 2010). 46. Trích xuất thông tin của case frames của các động từ Chỉnh sửa bài tập 45, trích xuất thêm các chunks mà các vị ngữ (predicate) liên\nkết tới. In ra theo định dạng tab. Ngoài các điều kiện đưa ra ở bài tập 45,\noutput phải thoả mãn các điều kiện sau. In ra các chunk (bunsetsu) ở dạng dãy các từ (không cần phải xoá đuôi và các\ntrợ từ). Trong trường hợp một predicates liên kết với nhiều chunk (bunsetsu), in ra\ncác chunk này theo cùng thứ tự với các trợ từ. Dùng ký tự space giữa các\nchunk. Xem xét ví dụ sau: 「吾輩はここで始めて人間というものを見た」(câu thứ 8 trong\nfile neko.txt.cabocha). Câu này gồm hai động từ 始める và 見る. Nếu trong kết\nquả phân tích cú pháp, động từ 始める liên kết với chunk ここで, động từ 見る\nliên kết với với chunk 吾輩は và ものを, chương trình sẽ in ra: 始める で ここで\n見る は を 吾輩 ものを\n 47. Mining các cấu trúc câu có động từ chức năng (cấu trúc này có tên tiếng Nhật là 機能動詞構文) Bài tập này tập trung vào các case frame を của các động từ, trong đó động từ có dạng liên kết サ変接続名詞. Cải tiến bài tập 46 để thoả mãn các yêu cầu sau đây. Bài tập này tập trung vào các trường hợp một bunsetsu có dạng sau đây liên kết với động từ. 「サ変接続名詞+を（助詞）」 Biến đổi các vị ngữ (predicates) về dạng 「サ変接続名詞+を+動詞の基本形」.\nNếu trong 1 chunk có nhiều động từ, sử dụng động từ bên trái nhất. Trong trường hợp một vĩ ngữ có liên kết với nhiều trợ từ (chunk), in tất cả\ncác trợ từ này theo thứ tự từ điển. Các trợ từ cách nhau bởi dấu cách. Trong trường hợp có nhiều chunks liên kết với một vị ngữ (predicate), in tất\ncả các chunk này đồng nhất với thứ tự in của các trợ từ mà nó bao gồm. Các\nchunk được cách nhau bởi ký tự space. Ví dụ, cho câu sau. 「別段くるにも及ばんさと、主人は手紙に返事をする。」. Chương\ntrình sẽ in ra kết quả sau. 返事をする と に は 及ばんさと 手紙に 主人は\n Lưu kết quả của bài tập 47 ra file, chỉ sử dụng lệnh unix để xác nhận: Các vị ngữ (predicates) xuất hiện trong file. Các vị ngữ và các case patterns 48. Trích xuất ra dependency path từ các danh từ đến gốc Chương trình yêu cầu trích xuất ra depedency path từ các chunk có chứa\ndanh từ đến root của cây depedency. Các dependency path phải\nthoả mãn yêu cầu sau đây. Biểu diễn các chunk (bunsetsu) dưới dạng chuỗi của các morpheme (surface\nform) Biểu diễn liên kết giữa các bunsetsu bằng ký tự mũi tên  -> . Ví dụ, đầu ra cho câu ví dụ 「吾輩はここで始めて人間というものを見た」(câu thứ 8\ntrong file neko.txt.cabocha) như sau: 吾輩は -> 見た\nここで -> 始めて -> 人間という -> ものを -> 見た\n人間という -> ものを -> 見た\nものを -> 見た\n 49. Trích xuất ra chuỗi liên kết giữa các danh từ Trích xuất dependency path ngắn nhất liên kết giữa các noun chunk. Đối với cặp\nnoun chunk với index tương ứng là  i  và  j  ( i  <  j ), các dependency paths\nthoả mãn các yêu cầu sau. Giống như bài 48, biểu diễn liên kết giữa các\nbunsetsu bằng ký tự mũi tên (->). Thay các noun chunk  i , và  j  tương ứng\nthành X và Y. Thêm nữa, các dependency path trong bài tập này có thể được diễn dịch như sau. Trên đường đi của noun chunk  i  tới gốc của cây, nếu tồn tại noun chunk  j :\ntrích xuất dependency path giữa noun chunk  i  và noun chunk  j . Ngoài trường hợp nói trên, nếu đường đi của noun chunk  i  và noun chunk  j  tới gốc của cây cắt nhau ở bunsetsu  k : In ra đường đi từ  i  tới bunsetsu ngay trước  k  và\nđường đi từ bunsetsu  j  tới bunsetsu ngay trước  k . Biểu diễn liên kết với\nbunsetsu  k  bằng ký tự |. Ví dụ, kết quả đưa ra cho câu ví dụ\n「吾輩はここで始めて人間というものを見た」(câu thứ 8 trong file\nneko.txt.cabocha) như sau: Xは | Yで -> 始めて -> 人間という -> ものを | 見た\nXは | Yという -> ものを | 見た\nXは | Yを | 見た\nXで -> 始めて -> Y\nXで -> 始めて -> 人間という -> Yを\nXという -> Y\n Chương 6: Xử lý văn bản tiếng Anh Cài đặt các chương trình xử lý văn bản tiếng Anh\n( nlp.txt ). 50. Tách câu (Sentence segmentation) Sử dụng patterns (. or ; or : or ? or !)  ->  ký tự space  ->  chữ cái tiếng Anh\nviết hoa (captial letter) để tách các câu trong văn bản. Đầu vào là văn bản\n nlp.txt , in ra mỗi câu\ntrong văn bản trên 1 dòng. 51. Tách từ Coi ký tự trắng (space) là ký tự phân tách các từ. Lấy đầu ra của bài 50 làm đầu\nvào, trích xuất các từ trong các câu và in ra theo định dạng: mỗi dòng 1 từ. In\nra dòng trắng để đánh dấu kết thúc câu. 52. Stemming Đầu vào là đầu ra của bài tập 51, áp dụng thuật toán Porter stemming để lấy ra\ngốc của các từ (stem). In ra các từ và stem cách nhau bởi ký tự tab. Nếu bạn sử\ndụng Python, bạn có thể sử dụng module\n stemming . 53. Tokenization Sử dụng tool  Stanford Core\nNLP  để phân tích văn bản đầu\nvào và lấy ra output theo định dạng XML. Sau đó đọc vào đầu ra XML và trích xuất\nra các token (word) theo định dạng mỗi word trên 1 dòng. 54. POS Tag Đọc vào kết quả phân tích dạng XML của Stanford Core NLP. Trích xuất ra word,\nlemma, POS tag và in ra các thuộc tính của mỗi word trên một dòng; các thuộc\ntính cách nhau bởi dấu tab. 55. Trích xuất named entities Trích xuất tất cả các named entities trong văn bản đầu vào. 56. Phân tích coreference Dựa trên kết quả phân tích coreference của Stanford Core NLP, thay thế các\nmention bằng representative mention. Chú ý khi thay thế các mention bằng\nrepresentative mention, lưu lại các mention gốc trong dấu ngoặc theo định dạng\nrepresentative mention (mention). 57. Phân tích cấu trúc dependency Từ kết quả phân tích dependency của Stanford Core NLP (collapsed-dependencies),\nvisualize câu đầu vào bằng đồ thị có hướng. Khi visualize các dependency trees,\ncó thể sử dụng  ngôn ngữ\nDOT , hay\nsử dụng  Graphviz . Nếu sử dụng Python, có thể hiển\nthị đồ thị có hướng bằng thư viện/package\n pydot . 58. Trích xuất tuples Từ kết quả phân tích dependency của Stanford Core NLP (collapsed-dependencies),\ntrích xuất các bộ 3 [Subject, Predicate, Purpose] và in ra các bộ 3 này (các\nthành phần cách nhau bởi ký tự tab). Subject, Predicate, Purpose được xác định\ndựa vào các tiêu chuẩn sau: Predicate: Là word ở các có các node con (dependant) trong các dependency relations: nsubj, dobj Subject: Các node con (dependant) trong các quan hệ nsubj từ predicate Purpose: Các node con (dependant) trong các quan hệ dobj từ predicate 59. Phrase structure analysis Từ kết quả phân tích cây cú pháp phrase structure (theo định dạng\n S-expression ), in ra tất cả các\nnoun phrases trong văn bản. Chú ý, cần in ra cả các noun phrases nằm trong các\nnoun phrases khác (nested NP). Chương 7: Database File\n artist.json.gz \nlà file nén lưu trữ thông tin về các artist trong cơ sở dữ liệu mở\n MusicBrainz . Các thông tin về các artist được lưu\ntrữ ở định dạng JSON, mỗi dòng lưu thông tin về một artist. Các trường thông tin (field) của file JSON như sau. Field Nội dung Format Ví dụ id id của artist Integer 20660 gid global id String \"ecf9f3a3-35e9-4c58-acaa-e707fba45060\" name Tên artist String \"Oasis\" sort_name Teen artist (dùng để sắp xếp) String \"Oasis\" area Khu vực hoạt động String \"United Kingdom\" aliases Các Tên khác Dictionary aliases[].name Tên khác String \"Oasis\" aliases[].sort_name Tên khác (dùng để sắp xếp) String \"Oasis\" begin Ngày bắt đầu hoạt động Dictionary begin.year Năm bắt đầu hoạt động Integer 1991 begin.month Tháng bắt đầu hoạt động Integer begin.date Ngày bắt đầu hoạt động Integer end Ngày dừng hoạt động Dictionary begin.year Năm dừng hoạt động Integer 2009 begin.month Tháng dừng đầu hoạt động Integer begin.date Ngày dừng hoạt động Integer tags Tag List các Dictionary objects tags[].count Số tag Integer 1 tags[].value Nội dung tag String \"rock\" rating Rating Dictionary rating.count Số phiếu bình chọn Integer 13 rating.value Giá trị bình chọn (trung bình) Integer 86 Hãy suy nghĩ phương pháp lưu trữ, tìm kiếm dữ liệu trong file artist.json.gz\nbằng các database  Key-Value-Store\n(KVS)  hay\n document-oriented database . Với\nKVS database, có thể sử dụng  LevelDB ,\n Redis ,  KyotoCabinet .\nVới document-oriented database, có thể sử dụng\n MongoDB ,  CouchDB \nhoặc  RethinkDB . 60. Tạo KVS database Để giúp cho việc tìm kiếm các trường (fields) từ name đến area của dữ liệu, hãy\nsử dụng Key-Value-Store (KVS) để lưu trữ dữ liệu. 61. Tìm kiếm với KVS Sử dụng cơ sở dữ liệu đã tạo ra trong bài tập 60, tìm kiếm khu vực hoạt động\n(area) của một artist cho trước. 62. Xử lý kiểu vòng lặp trong KVS Sử dụng cơ sở dữ liệu đã tạo ra trong bài tập 60, hãy đưa ra số artist có khu\nvực hoạt động (area) là Japan. 63. Lưu trữ các objects (đối tượng) trong KVS Sử dụng KVS, hãy tạo ra database cho việc tìm kiếm các trường từ name đến tag và\nsố lượng tag. Thử tìm kiếm các trường từ name đến tag và số lượng tag với\ndatabase đã tạo ra. 64. Tạo MongoDB Hãy lưu thông tin của artist (artist.json.gz) vào cơ sở dữ liệu MongoDB. Thêm\nnữa, hãy tạo indexes với các trường sau: name, aliases.name, tags.value,\nrating.value. 65. Tìm kiếm trong cơ sở dữ liệu MongoDB Sử dụng interactive shell của MongoDB, đưa ra các thông tin liên quan đến artist\ncó tên \"Queen\". Tiếp theo, cài đặt chương trình thực hiện chức năng đó. 66. Lấy số kết quả tìm kiếm Sử dụng interactive shell của MongoDB, tính số lượng các artist có khu vực hoạt\nđộng (area) là \"Japan.\" 67. Đưa ra multiple documents Tìm kiếm các artist có aliases cho trước. 68. Sắp xếp Trong số các artist có tag \"dance\", lấy ra top 10 artist có số phiếu bình chọn\ncao nhất. 69. Tạo Web application Tạo ứng dụng Web cho phép người dùng nhập vào các điều kiện tìm kiếm và hiển thị\ncác artist phù hợp với điều kiện tìm kiếm. Các điều kiện tìm kiếm bao gồm: tên\nartist (name), aliases, tag, etc. Hiển thị thông tin các artist (theo dòng) theo\nthứ tự từ cao tới thấp của lượng rating. Chương 8: Machine Learning Chương này yêu cầu bạn thực hiện bài toán sentiment analysis trên corpus\n sentence polarity dataset\nv1.0 \ntrong  Moview Review\nData  của tác giả Bo\nPang và Lillian Lee. Yêu cầu của bài toán sentiment analysis là phân loại các\ncâu thành positive và negative sentiments. 70. Download và tiền xử lý dữ liệu Sử dụng dữ liệu liên quan đến sentiment polarity của các câu (download tại\n đây ),\ntạo dữ liệu chuẩn hoá (sentiment.txt) theo hướng dẫn dưới đây. Thêm vào '+1' ở bắt đầu các dòng trong file rt-polarity.pos (giữa +1 và nội\ndung của câu cách nhau bởi ký tự trắng). Thêm vào '-1' ở bắt đầu các dòng trong file rt-polarity.neg (giữa -1 và nội\ndung của câu cách nhau bởi ký tự trắng). Kết hợp nội dung thu được trong phần 1 và 2 để tạo thành file sentiment.txt Sau khi đã thu được file sentiment.txt, xác nhận số lượng các câu với positive\npolarity và các câu với negative polarity. 71. Stopwords Tạo ra danh sách các stopwords trong tiếng Anh. Sau đó viết 1 hàm để kiểm tra\nmột từ có thuộc danh sách stopwords hay không. Hàm sẽ trả về giá trị TRUE nếu từ\ncho trước thuộc danh sách stopwords. Ngược lại hàm sẽ trả về giá trị FALSE. Sau\nđó viết mô tả về các test cho hàm đã viết. 72. Trích xuất đặc trưng Tự thiết kế các đặc trưng cho bài toán sentiment analysis. Sau đó trích xuất đặc\ntrưng từ dữ liệu training. Hint: phương pháp trích xuất đặc trưng đơn giản nhất là sử dụng từ gốc (stem)\ncác từ không trong danh sách các stopwords. Phương pháp này có thể sử dụng để\nlàm hệ thống baseline. 73. Training Training model bằng phương pháp logistics regressions sử dụng các đặc trưng tạo\nra trong bài 72. 74. Prediction Sử dụng mô hình logistics regressions đã huấn luyện trong bài 73, hãy viết\nchương trình dự đoán polarity cho một câu đầu vào và tính xác suất cho các nhãn\n(+1, -1). 75. Trọng số của các features (Feature weights) Trong mô hình logistics regression đã huấn luyện trong bài 73, đưa ra top 10 các\nfeatures có trọng số cao nhất và top 10 các features có trọng số thấp nhất. 76. Dự đoán trên dữ liệu training Sử dụng mô hình đã học trong bài 73 để đưa ra dự đoán trên dữ liệu training. Đưa\nra nhãn gốc, nhãn dự đoán, và xác suất của nhãn dự đoán cho mỗi câu trong dữ\nliệu (cách nhau bởi ký tự tab). 77. Tính độ chính xác của mô hình Sử dụng đầu ra của bài 76, tính accuracy cho toàn bộ dữ liệu; precision, recall,\nF1 cho nhãn +1. 78. 5-fold cross validation Vì các thực nghiệm trong bài 76, 77 đánh giá model trên dữ liệu huấn luyện nên\nkhó có thể nói đó là các đánh giá hợp lý. Các đánh giá này chỉ đánh giá khả năng\nmô hình \"fit\" với dữ liệu training chứ không đánh giá khả năng khái quát\n(generalization) của mô hình. Vì thế bài tập 78 yêu cầu bạn đánh giá mô hình sử\ndụng 5-fold cross validation. Đưa ra accuracy, precision, recall, F1 score cho\n5-fold cross validation (tính trung bình của 5 folds). 79. Vẽ đồ thị precision-recall Vẽ đồ thị precision-recall theo sự thay đổi của giá trị threshold trong mô hình\nlogistic regression. Chương 9: Không gian vector (I) File\n enwiki-20150112-400-r10-105752.txt.bz2 \nlà file nén dạng bzip2 của 105752 file text được lấy mẫu ngẫu nhiên (tỷ lệ 1/10)\ntừ các bài báo trên Wikipedia có trên 400 từ. Các bài báo trên Wikipedia được\nlấy vào ngày 12 tháng 1 năm 2015. Sử dụng dữ liệu file này làm corpus để học các\nvector thể hiện ý nghĩa của các từ. Trong nửa đầu của chương 9, bạn được yêu cầu\ntrích xuất các context của các từ, trích xuất đặc trưng, và dùng phương pháp PCA\nđể giảm bớt số chiều của dữ liệu. Nửa sau của chương 9 yêu cầu bạn tính độ tương\ntự của các từ sử dụng các word vectors đã học từ corpus. Chú ý, bài 83 yêu cầu 7GB memory. Trong trường hợp lượng memory của bạn không\nđủ, bạn cần có các phương pháp xử lý thích hợp hoặc sử dụng sample 1/100 của dữ\nliệu trong file\n enwiki-20150112-400-r10-105752.txt.bz2 . 80. Tiền xử lý dữ liệu Sử dụng khoảng trắng là ký tự ngăn cách để tokenize các từ trong các câu. Phương\npháp này có nhược điểm là các từ thu được sẽ còn các ký tự đặc biệt như dấu câu,\nhoặc dấu ngoặc. Vì thế sau khi tokenize các từ trong corpus, tiến hành các xử lý\nsau đây. Xoá các ký tự đặc biệt xuất hiện ở đầu và cuối các từ: .,!?;:()[]'\" Xoá các từ chỉ gồm ký tự trắng Sau khi tiền xử lý dữ liệu, lưu file dữ liệu gồm danh sách các từ cách nhau bởi\nkhoảng trắng. 81. Xử lý tên các nước tạo thành từ các compound words Trong tiếng Anh, nhiều từ cạnh nhau có thể tạo thành một từ có ý nghĩa. Ví dụ,\nhợp chủng quốc Hoa Kỳ là \"United States\", vương quốc Anh là \"United Kingdom\".\nNếu chỉ dùng các \"United\", \"State\", hay \"Kingdom\" như các từ riêng lẻ, ý nghĩa\ncủa các từ này sẽ nhập nhằng. Vì thế trong khi tiền xử lý dữ liệu, ta cần xác\nđịnh các từ ghép này. Đoán nhận các từ ghép là một bài toán khó, nên ở đây ta\nchỉ đoán nhận các từ ghép là tên của các nước. Trước hết, download danh sách tên của các nước trên Internet. Dùng danh sách tên\ncác nước này để đoán nhận các từ ghép là tên nước trong dữ liệu sử dụng ở bài\n80, sau đó biến đổi các ký tự spaces thành ký tự underscore (_) để nối các từ\nthành phần. Ví dụ \"United States\" sẽ trở thành \"United_States\", \"Isle of Man\"\nsẽ trở thành \"Isle_of_Man.\" 82. Trích xuất context Sử dụng corpus được tạo ra trong bài tập 81, trích xuất context của tất cả các\ntừ xuất hiện trong corpus. Context  c  của mỗi từ  t  trong dữ liệu sẽ cặp với từ  t  và xuất ra theo định dạng: các thông tin cách nhau bởi ký tự tab. Context của mỗi từ  t  được định nghĩa như sau: Trích xuất các từ ở trước và sau của\n t  với kích thước cửa sổ là  d  (chú ý context words của  t  sẽ không bao gồm\nbản thân của từ  t ) Với mỗi từ  t , kích thước của context (window size)  d \nsẽ được chọn ngẫu nhiên trong tập {1, 2, 3, 4, 5}. 83. Tính tần xuất xuất hiện của từ/context Sử dụng đầu ra của bài 82, tính phân bố xuất hiện và các hằng số sau: f ( t , c ): là số lần đồng xuất hiện của từ  t  và context word  c . f ( t ,*): số lần xuất hiện của từ  t . f (*, c ): số lần xuất hiện của context word  c . N : Tổng số lần xuất hiện của từ và các context word (hằng số). 84. Tạo Matrix của các từ và context words Sử dụng đầu ra của bài 83, tạo ma trận word/context  X . Các thành phần X_tc\ntrong ma trận  X  được định nghĩa như sau. Nếu  f ( t , c ) >= 10, X_tc =\nPPMI(t,c) = max{log N*f(t,c)/f(t,*) x f(*,c),0} Nếu f(t,c) < 10, X_tc = 0. Ở đây PPMI(t,c) là ký hiệu của Pointwise Mutual Information. Chú ý vì kích thước\nma trận X là rất lớn, nên lưu tất cả các giá trị của ma trận vào bộ nhớ là không\nthể. Bạn có thể sử dụng kỹ thuật lưu trữ ma trận thưa với chú ý rằng, phần lớn\ngiá trị của các phần tử trong X bằng 0. 85. Sử dụng PCA để giảm số chiều dữ liệu Sử dụng thuật toán PCA cho ma trận thu được trong bài tập 84 để giảm số chiều dữ\nliệu sao cho các word vector thu được có số chiều là 300. 86. Hiển thị word vectors Đọc vào các word vectors trong bài tập 85, hiển thị vector cho từ \"United\nStates\". Chú ý là từ \"United States\" trong corpus đã được biến đổi thành\n\"United_States.\" 87. Tính word similarity Sử dụng word vectors thu được trong bài tập 85, tính cosine similarity cho hai\ntừ \"United States\" và \"U.S.\" Chú ý là từ \"U.S.\" trong corpus được lưu trữ là\n\"U.S\" 88. Hiển thị top 10 có giá trị similarity cao nhất Sử dụng word vectors trong bài 85, hiển thị top 10 từ với cosine similarity cao\nnhất với từ \"England\" và các giá trị cosine similarity tương ứng. 89. Các thao tác cộng/trừ word vectors Đọc vào các word vectors thu được trong bài 85, tính vec(\"Spain\") -\nvec(\"Madrid\") + vec(\"Athens\") sau đó hiển thị top 10 từ có cosine similarity gần\nnhất với vector thu được cùng với các giá trị cosine similarity tương ứng. Chương 10: Không gian vector (II) Trong chương 10, chúng ta sẽ tiếp tục nội dung của chương 9 về không gian\nvector. 90. Sử dụng word2vec để học word vectors Áp dụng  word2vec  trên corpus đã tạo ra\nở bài 81 để học word vectors. Sau đó, sử dụng các word vectors đã học với\nword2vec để áp dụng cho các bài tập 86-89. 91. Chuẩn bị dữ liệu analogy Download dữ liệu  analogy\nevaluation .\nTrong dữ liệu, các dòng bắt đầu bằng \":\" thể hiện tên của section. Ví dụ dòng \":\ncapital-common-countries\" bắt đầu cho section \"capital-common-countries.\" Hãy\ntrích xuất các dòng của section \"family\" trong file đã download và lưu ra file. 92. Vận dụng dữ liệu analogy data Với các dòng trong dữ liệu analogy tạo ra trong bài 91, tính vector sau:\nvec(word ở cột 2) - vec(word ở cột 1) + vec(word ở cột 3) sau đó tìm word với\nword vector với độ tương tự cao nhất với word vector đã tính đồng thời tính độ\ntương tự (cosine similarity). Thêm vào cuối của các dòng từ tìm được và độ tương\ntự. Trong bài tập này, hãy thử sử dụng word vector đã học được sau bài 85 và bài\n90. 93. Tính độ chính xác của mô hình trên dữ liệu analogy Sử dụng dữ liệu của bài 92, tính độ chính xác của các mô hình với mô hình\nanalogy. 94. Tính word similarity trên dữ liệu WordSimilarity-353 Sử dụng đầu vào là dữ liệu  The WordSimilarity-353 Test\nCollection ,\ntính độ tương tự của các từ ở cột 1 và cột 2 và thêm vào cuối các dòng giá trị\nđộ tương tự này. Hãy áp dụng các mô hình word vectors đã học ở bài 85 và bài 90. 95. Đánh giá trên dữ liệu WordSimilarity-353 Sử dụng dữ liệu trong bài 94, sử dụng ranking với các giá trị độ tương tự đã\ntính với các mô hình và ranking do con người đưa ra để tính  Spearman\ncorrelation . 96. Trích xuất vectors liên quan đến tên nước Sử dụng mô hình đã học với word2vec, trích xuất các vectors của các từ liên quan\nđến tên các nước. 97. k-means clustering Lấy đầu vào là các word vectors từ bài tập 96, thực hiện clustering bằng thuật\ntoán k-means với số lượng clusters  k  = 5. 98. Clustering với phương pháp Ward Lấy đầu vào là các word vectors từ bài tập 96 (các word vectors của tên các\nnước), thực hiện hierarchical clustering bằng  phương pháp\nWard . Sau đó, visualize kết quả\nclustering bằng  dendrogram . 99. Visualize word vectors bằng phương pháp t-SNE Với các word vectors thu được từ bài tập 96, visualize không gian vectors bằng\n phương pháp t-SNE . Phụ lục: Corpus, data sử dụng trong 100 bài luyện tập NLP hightemp.txt :\nDữ liệu nhiệt độ cao nhất ở các địa phương qua các thời kỳ do nha khí tượng\nNhật Bản cung cấp. jawiki-country.json.gz :\nDữ liệu Wikipedia tiếng Nhật gồm các bài báo (trong ngày 18/10/2014) về các\nquốc gia, được trích xuất từ  Wikipedia Dump data (tiếng\nNhật) .\nDữ liệu được lưu trữ bằng định dạng JSON. neko.txt : là nội\ndung bằng plain text của cuốn tiểu thuyết \"吾輩は猫である\" của nhà văn\n夏目漱石 (Soseki Natsume) được cung cấp miễn phí tại trang Web:\n 青空文庫 . nlp.txt : nội dung\ncủa trang Wikipedia nói về  Natural Language\nProcessing  với\nđịnh dạng 1 dòng 1 câu. artist.json.gz :\nlà file nén lưu trữ thông tin về các artist trong cơ sở dữ liệu mở\n MusicBrainz . Các thông tin về các artist được\nlưu trữ ở định dạng JSON, mỗi dòng lưu thông tin về một artist. enwiki-20150112-400-r10-105752.txt.bz2 \nlà file nén dạng bzip2 của 105752 file text được lấy mẫu ngẫu nhiên (tỷ lệ\n1/10) từ các bài báo trên Wikipedia có trên 400 từ. Các bài báo trên\nWikipedia được lấy vào ngày 12 tháng 1 năm 2015. © 2017  GitHub , Inc. Terms Privacy Security Status Help Contact GitHub API Training Shop Blog About",
          "relevance": "0",
          "title": "Chương 1: Bài tập khởi động",
          "url": "https://github.com/minhpqn/nlp_100_drill_exercises"
        },
        {
          "content": "Posts Questions Discussions Announcements  No announcement yet. All Announcements  Sign In/Sign up\n     +5 bienhoang  Follow Published Aug 25th, 9:23 pm Xử Lý Ngôn Ngữ Tự Nhiên với Python - P1 Python NLP NLTK  Aug 25th, 9:23 pm\n          866  3  1  Report\n     \n            Marked as Trending on 2017-09-06 15:20:04\n         This post is in the series:\n                     Xử Lý Ngôn Ngữ Tự Nhiên với Python Xử Lý Ngôn Ngữ Tự Nhiên với Python - P1 (current post) Xử Lý Ngôn Ngữ Tự Nhiên với Python - P2\n                     Xử Lý Ngôn Ngữ Tự Nhiên với Python - P3\n                     Xử Lý Ngôn Ngữ Tự Nhiên với Python - P4\n                     Xử Lý Ngôn Ngữ Tự Nhiên với Python - P5\n                     Xử Lý Ngôn Ngữ Tự Nhiên với Python - P6\n                     Xử Lý Ngôn Ngữ Tự Nhiên với Python - P7\n                     Xin chào anh em, đợt này tôi có tham gia một dự án khá thú vị về AI. Vai trò của tôi trong dự án và thiết kế các thành phần \"biên\", hiểu đơn giản là những thứ râu ria bên ngoài hệ thống Trí tuệ nhân tạo kia. Ví dụ viết Mobile App, Web quảng bá, xử lí truy cập API, xử lí dữ liệu đầu vào... Cũng là cơ may được làm việc với ngôn ngữ Python và đặc biệt là xử lí ngôn ngữ tự nhiên với thư viện NLTK. Sau đây tôi sẽ chia sẻ với các bạn những trải nghiệm của tôi với việc \"Xử Lý Ngôn Ngữ Tự Nhiên - NLP\" cũng như Python và NLTK trong thời gian qua. Đây hầu hết là những kiến thức cơ bản về NLP dành cho Developer, không cần các bạn phải giỏi những kỹ thuật chuyên sâu hay các thuật toán phức tạp. Vì chúng ta cũng biết, NLP là một nhánh của Trí Tuệ Nhân Tạo và phải nói là khó nhất. Hy vọng bài viết sẽ mang lại những kiến thức hữu ích, giúp bạn tự tin hơn trong việc tìm hiểu AI nói chung và NLP nói riêng. Nào chúng ta cùng bắt đầu! 1. Ngôn ngữ tự nhiên là gì? Ngôn ngữ tự nhiên là ngôn ngữ mà các loài động vật sáng tạo ra để giao tiếp với đồng loại. Con người cũng là một loại động vật sử dụng ngôn ngữ để giao tiếp. Thế giới ngôn ngữ của con người rất phong phú, theo thông kê của các nhà khoa học thì có tới hàng ngàn ngôn ngữ tồn tại trên trái đất. Ngôn ngữ tự nhiên có 2 dạng là chữ viết và âm thanh (tức tiếng nói).  Ngôn ngữ của mỗi dân tộc, quốc gia lại khác nhau bao gồm khác nhau cả về cách viết cũng như cách phát âm. 2. Tại sao cần phải \"Xử Lý Ngôn Ngữ Tự Nhiên\". Xử Lý Ngôn Ngữ Tự Nhiên có vai trò hết sức quan trọng trong ngành Khoa Học Máy Tính. Nó có vô vàn ứng dụng hữu ích trong cuộc sống cũng như nghiên cứu. Chúng ta có thể điểm qua một vài ứng dụng của xử lý ngôn ngữ tự nhiên như: Nhận dạng chữ viết: Có hai kiểu nhận dạng, thứ nhất là nhận dạng chữ in, ví dụ nhận dạng chữ trên sách giáo khoa rồi chuyển nó thành dạng văn bản điện tử như dưới định dạng doc của Microsoft Word chẳng hạn. Phức tạp hơn là nhận dạng chữ viết tay, có khó khăn bởi vì chữ viết tay không có khuôn dạng rõ ràng và thay đổi từ người này sang người khác. Với chương trình nhận dạng chữ viết in có thể chuyển hàng ngàn đầu sách trong thư viện thành văn bản điện tử trong thời gian ngắn. Nhận dạng chữ viết của con người có ứng dụng trong khoa học hình sự và bảo mật thông tin (nhận dạng chữ ký điện tử). Nhận dạng tiếng nói: Nhận dạng tiếng nói rồi chuyển chúng thành văn bản tương ứng. Giúp thao tác của con người trên các thiết bị nhanh hơn và đơn giản hơn, chẳng hạn thay vì gõ một tài liệu nào đó bạn đọc nó lên và trình soạn thảo sẽ tự ghi nó ra. Đây cũng là bước đầu tiên cần phải thực hiện trong ước mơ thực hiện giao tiếp giữa con người với robot. Nhận dạng tiếng nói có khả năng trợ giúp người khiếm thị rất nhiều. Tổng hợp tiếng nói: Từ một văn bản tự động tổng hợp thành tiếng nói. Thay vì phải tự đọc một cuốn sách hay nội dung một trang web, nó tự động đọc cho chúng ta. Giống như nhận dạng tiếng nói, tổng hợp tiếng nói là sự trợ giúp tốt cho người khiếm thị, nhưng ngược lại nó là bước cuối cùng trong giao tiếp giữa robot với người. Dịch tự động (Machine translate): Như tên gọi đây là chương trình dịch tự động từ ngôn ngữ này sang ngôn ngữ khác. Một phần mềm điển hình về tiếng Việt của chương trình này là Evtrans của Softex, dịch tự động từ tiếng Anh sang tiếng Việt và ngược lại, phần mềm từng được trang web vdict.com mua bản quyền, đây cũng là trang đầu tiên đưa ứng dụng này lên mạng. Tháng 10 năm 2008 có hai công ty tham gia vào lĩnh vực này cho ngôn ngữ tiếng Việt là công ty Lạc Việt (công ty phát hành từ điển Lạc Việt) và Google, một thời gian sau đó Xalo.vn cũng đưa ra dịch vụ tương tự. Tìm kiếm thông tin (Information retrieval): Đặt câu hỏi và chương trình tự tìm ra nội dung phù hợp nhất. Thông tin ngày càng đầy lên theo cấp số nhân, đặc biệt với sự trợ giúp của Internet việc tiếp cận thông tin trở lên dễ dàng hơn bao giờ hết. Việc khó khăn lúc này là tìm đúng nhất thông tin mình cần giữa bề bộn tri thức và đặc biệt thông tin đó phải đáng tin cậy. Các máy tìm kiếm dựa trên giao diện web như Google hay Yahoo hiện nay chỉ phân tích nội dung rất đơn giản dựa trên tần suất của từ khoá và thứ hạng của trang và một số tiêu chí đánh giá khác để đưa ra kết luận, kết quả là rất nhiều tìm kiếm không nhận được câu trả lời phù hợp, thậm chí bị dẫn tới một liên kết không liên quan gì do thủ thuật đánh lừa của các trang web nhằm giới thiệu sản phẩm (có tên tiếng Anh là SEO viết tắt của từ Search Engine Optimization). Thực tế cho đến bây giờ chưa có máy tìm kiếm nào hiểu được ngôn ngữ tự nhiên của con người trừ trang  www.ask.com  được đánh giá là \"hiểu\" được những câu hỏi có cấu trúc ở dạng đơn giản nhất. Mới đây cộng đồng mạng đang xôn xao về trang Wolfram Alpha, được hứa hẹn là có khả năng hiểu ngôn ngữ tự nhiên của con người và đưa ra câu trả lời chính xác. Lĩnh vực này hứa hẹn tạo ra bước nhảy trong cách thức tiếp nhận tri thức của cả cộng đồng. Tóm tắt văn bản: Từ một văn bản dài tóm tắt thành một văn bản ngắn hơn theo mong muốn nhưng vẫn chứa những nội dung thiết yếu nhất. Khai phá dữ liệu (Data mining) và phát hiện tri thức: Từ rất nhiều tài liệu khác nhau phát hiện ra tri thức mới. Thực tế để làm được điều này rất khó, nó gần như là mô phỏng quá trình học tập, khám phá khoa học của con người, đây là lĩnh vực đang trong giai đoạn đầu phát triển. Ở mức độ đơn giản khi kết hợp với máy tìm kiếm nó cho phép đặt câu hỏi để từ đó công cụ tự tìm ra câu trả lời dựa trên các thông tin trên web mặc cho việc trước đó có câu trả lời lưu trên web hay không (giống như trang Yahoo! hỏi và đáp, nơi chuyên đặt các câu hỏi để người khác trả lời), nói một cách nôm na là nó đã biết xử lý dữ liệu để trả lời câu hỏi của người sử dụng, thay vì máy móc đáp trả những gì chỉ có sẵn trong bộ nhớ. \n(Nguồn: Wikipedia) 3. Tại sao lại sử dụng Python trong xử lý ngôn ngữ tự nhiên. Python ra đời năm 1991, và là một ngôn ngữ thông dịch. Trải qua hơn 20 năm phát triển, Python là một trong những ngôn ngữ được sử dụng nhiều nhất trong dậy lập trình và nghiên cứu khoa học.  Rất nhiều trường đại học sử dụng Python để dậy về lập trình cho các sinh viên ngành Khoa Học Máy Tính. Rất nhiều công ty lớn sử dụng Python để xây dựng hệ thống như Google, Youtube, Instagram, Dropbox, Atlassian... Python là một ngữ sử dụng được cho nhiều mô hình lập trình, đơn giản khi học và sử dụng. Tôi sử dụng Python chưa lâu nhưng khi so sánh việc Code sử dụng Pythong thì nó ngắn hơn rất nhiều so với khi viết bằng PHP hoặc Java. Bạn có thể bay bổng tự do với Python hoặc cũng có thể bắt nó trở lên vững chắc và mạnh mẽ như Java. Theo những thông tin mà tôi được biết thì Python cũng là một ngôn ngữ rất phát triển trong lĩnh vực Data Science và Machine Learning. Python cũng cung cấp những hàm và thư viện xử lý ngôn ngữ tuyệt vời. Scikit-learn và Tensor-flow là 2 thư viện Machine Learning nổi tiếng được viêt bằng Python. Đứng ở góc độ người tiếp cận sau, cá nhân tôi thấy Python là một lựa chọn hợp lý khi làm Xử Lý Ngôn Ngữ Tự Nhiên. 4. Giới thiệu về NLTK. NLTK hay Natural Language Toolkit - Bộ công cụ ngôn ngữ tự nhiên, là một thư viện được viết bằng Python hỗ trợ xử lý ngôn ngữ tự nhiên. Bằng cách cung cấp các cơ chế và kỹ thuật xử lý ngôn ngữ phổ biến, nó giúp cho việc xử lý ngôn ngữ tự nhiên trở lên dễ dàng và nhanh chóng hơn. Được viết bởi Steven Bird và Edward Loper, làm việc tại Khoa Máy Tính, Đại Học Pennsylvania, Hoa Kỳ và năm 2001. Ngoài việc hỗ trợ xử lý ngôn ngữ, NLTK còn có các mô phỏng đồ hoạ và dữ liệu mẫu hữu ích. NLTK cung cấp các xử lý như classification, tokenization, stemming, tagging, parsing, và semantic reasoning... Những ứng dụng này chúng ta sẽ dần được tìm hiểu ở những bài viết sau. Ngoài việc phục vụ xử lý ngôn ngữ tự nhiên, NLTK còn được sử dụng trong Machine Learning với tác dụng làm sạch dữ liệu, xử lý dữ liệu đầu vào cho các thuật toán Machine Learning. 5. Tổng kết. Trên đây, tôi đã giới thiếu cho các bạn sơ lược về NLP và những thứ chúng ta cần để bắt đầu việc xử lý ngôn ngữ tự nhiên bằng Pyhthon và NLTK. Ở bài viết sau, tôi sẽ hướng dẫn các bạn cách cài đặt Python 3 và NLTK. 6. Tài liệu tham khảo Indurkhya, Nitin and Fred Damerau (eds., 2010) Handbook of Natural Language Processing (second edition), Chapman & Hall/CRC. Jurafsky, Daniel and James Martin (2008) Speech and Language Processing (second edition), Prentice Hall. Mitkov, Ruslan (ed., 2002) The Oxford Handbook of Computational Linguistics. Oxford University Press. (second edition expected in 2010). Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit Next post  bienhoang @bienhoang Follow  963\n          50\n          13\n         \n                            Clip this post\n                         Have problems with  Python, NLP or NLTK ?  Ask on Viblo » Comments  No comments yet.\n         +5 • • • Facebook Viblo Hot authors Resources Posts Questions Videos Tags Authors Help Terms  RSS Feed Browser extension Atom plugin Feedback FAQ \n                    © 2017  Viblo . All rights reserved.\n                 \n        Viblo - Free service for technical knowledge sharing\n     Feedback \n                        Your feedback will be shared with our product delivery teams, and taken into consideration\n                        for future development.\n                     Name Email Send \n                Sign In\n             \n                Sign Up\n             Join us to find useful information required to improve your skills \n            Forgot your password?\n         Sign In Free  service for technical knowledge  sharing Join us to find useful information required to improve your skills \n                    I agree to  Viblo Terms of Service Sign Up or",
          "relevance": "1",
          "title": "Xử Lý Ngôn Ngữ Tự Nhiên với Python - P1",
          "url": "https://viblo.asia/p/xu-ly-ngon-ngu-tu-nhien-voi-python-p1-GrLZDbXw5k0"
        },
        {
          "content": "Latest FundaML.com 33. Đánh giá hệ thống phân lớp (1/2) 32. Naive Bayes Classifier Viết và nhận xét các bài báo khoa học 31. Maximum Likelihood và Maximum A Posteriori Con đường học Toán của tôi 30. Ôn tập Xác Suất Q2. Transfer Learning 29. Linear Discriminant Analysis Q1. Quick Notes 1 28. Principal Component Analysis (2/2) 27. Principal Component Analysis (1/2) 26. Singular Value Decomposition 25. Matrix Factorization Collaborative Filtering 24. Neighborhood-Based Collaborative Filtering 23. Content-based Recommendation Systems 22. Multi-class SVM 21. Kernel SVM 20. Soft Margin SVM 19. Support Vector Machine 18. Duality 17. Convex Optimization Problems 16. Convex sets và convex functions 15. Overfitting 14. Multi-layer Perceptron và Backpropagation 13. Softmax Regression 12. Binary Classifiers 11. Feature Engineering 10. Logistic Regression 9. Perceptron Learning Algorithm 8. Gradient Descent (2/2) 7. Gradient Descent (1/2) 6. K-nearest neighbors 5. K-means Clustering - Applications 4. K-means Clustering 3. Linear Regression 2. Phân nhóm các thuật toán Machine Learning 1. Giới thiệu về Machine Learning Machine Learning cơ bản  About Index Tags Categories Archive Math Copyrights ebook Search Có hai cách phổ biến phân nhóm các thuật toán Machine learning. Một là dựa trên phương thức học (learning style), hai là dựa trên chức năng (function) (của mỗi thuật toán). Trong trang này: 1. Phân nhóm dựa trên phương thức học Supervised Learning (Học có giám sát) Classification (Phân loại) Regression (Hồi quy) Unsupervised Learning (Học không giám sát) Clustering (phân nhóm) Association Semi-Supervised Learning (Học bán giám sát) Reinforcement Learning (Học Củng Cố) 2. Phân nhóm dựa trên chức năng Regression Algorithms Classification Algorithms Instance-based Algorithms Regularization Algorithms Bayesian Algorithms Clustering Algorithms Artificial Neural Network Algorithms Dimensionality Reduction Algorithms Ensemble Algorithms 3. Tài liệu tham khảo 1. Phân nhóm dựa trên phương thức học Theo phương thức học, các thuật toán Machine Learning thường được chia làm 4 nhóm: Supervise learning, Unsupervised learning, Semi-supervised lerning và Reinforcement learning.  Có một số cách phân nhóm không có Semi-supervised learning hoặc Reinforcement learning. Supervised Learning (Học có giám sát) Supervised learning là thuật toán dự đoán đầu ra (outcome) của một dữ liệu mới (new input) dựa trên các cặp ( input, outcome ) đã biết từ trước. Cặp dữ liệu này còn được gọi là ( data, label ), tức ( dữ liệu, nhãn ). Supervised learning là nhóm phổ biến nhất trong các thuật toán Machine Learning. Một cách toán học, Supervised learning là khi chúng ra có một tập hợp biến đầu vào \\( \\mathcal{X} = \\{\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_N\\} \\) và một tập hợp nhãn tương ứng \\( \\mathcal{Y} = \\{\\mathbf{y}_1, \\mathbf{y}_2, \\dots, \\mathbf{y}_N\\} \\), trong đó \\( \\mathbf{x}_i, \\mathbf{y}_i \\) là các vector. \nCác cặp dữ liệu biết trước \\( (\\mathbf{x}_i, \\mathbf{y}_i) \\in \\mathcal{X} \\times \\mathcal{Y} \\) \nđược gọi là tập  training data  (dữ liệu huấn luyện). Từ tập traing data này, chúng ta cần tạo ra một hàm số ánh xạ mỗi phần tử từ tập \\(\\mathcal{X}\\) sang một phần tử (xấp xỉ) tương ứng của tập \\(\\mathcal{Y}\\): \\[ \\mathbf{y}_i \\approx f(\\mathbf{x}_i), ~~ \\forall i = 1, 2, \\dots, N\\] \nMục đích là xấp xỉ hàm số \\(f\\) thật tốt để khi có một dữ liệu \\(\\mathbf{x}\\) mới, chúng ta có thể tính được nhãn tương ứng của nó \\( \\mathbf{y} = f(\\mathbf{x}) \\). Ví dụ 1:  trong nhận dạng chữ viết tay, ta có ảnh của hàng nghìn ví dụ của mỗi chữ số được viết bởi nhiều người khác nhau. Chúng ta đưa các bức ảnh này vào trong một thuật toán và chỉ cho nó biết mỗi bức ảnh tương ứng với chữ số nào. Sau khi thuật toán tạo ra (sau khi  học ) một mô hình, tức một hàm số mà đầu vào là một bức ảnh và đầu ra là một chữ số, khi nhận được một bức ảnh mới mà mô hình  chưa nhìn thấy bao giờ , nó sẽ dự đoán bức ảnh đó chứa chữ số nào. MNIST : bộ cơ sở dữ liệu của chữ số viết tay.   (Nguồn:  Simple Neural Network implementation in Ruby) Ví dụ này khá giống với cách học của con người khi còn nhỏ. Ta đưa bảng chữ cái cho một đứa trẻ và chỉ cho chúng đây là chữ A, đây là chữ B. Sau một vài lần được dạy thì trẻ có thể nhận biết được đâu là chữ A, đâu là chữ B trong một cuốn sách mà chúng chưa nhìn thấy bao giờ. Ví dụ 2:  Thuật toán dò các khuôn mặt trong một bức ảnh đã được phát triển từ rất lâu. Thời gian đầu, facebook sử dụng thuật toán này để chỉ ra các khuôn mặt trong một bức ảnh và yêu cầu người dùng  tag friends  - tức gán nhãn cho mỗi khuôn mặt. Số lượng cặp dữ liệu ( khuôn mặt, tên người ) càng lớn, độ chính xác ở những lần tự động  tag  tiếp theo sẽ càng lớn. Ví dụ 3:  Bản thân thuật toán dò tìm các khuôn mặt trong 1 bức ảnh cũng là một thuật toán Supervised learning với training data (dữ liệu học) là hàng ngàn cặp ( ảnh, mặt người ) và ( ảnh, không phải mặt người ) được đưa vào. Chú ý là dữ liệu này chỉ phân biệt  mặt người  và  không phải mặt người  mà không phân biệt khuôn mặt của những người khác nhau. Thuật toán supervised learning còn được tiếp tục chia nhỏ ra thành hai loại chính: Classification (Phân loại) Một bài toán được gọi là  classification  nếu các  label  của  input data  được chia thành một số hữu hạn nhóm. Ví dụ: Gmail xác định xem một email có phải là spam hay không; các hãng tín dụng xác định xem một khách hàng có khả năng thanh toán nợ hay không. Ba ví dụ phía trên được chia vào loại này. Regression (Hồi quy) (tiếng Việt dịch là  Hồi quy , tôi không thích cách dịch này vì bản thân không hiểu nó nghĩa là gì) Nếu  label  không được chia thành các nhóm mà là một giá trị thực cụ thể. Ví dụ: một căn nhà rộng \\(x ~ \\text{m}^2\\), có \\(y\\) phòng ngủ và cách trung tâm thành phố \\(z~ \\text{km}\\) sẽ có giá là bao nhiêu? Gần đây  Microsoft có một ứng dụng dự đoán giới tính và tuổi dựa trên khuôn mặt . Phần dự đoán giới tính có thể coi là thuật toán  Classification , phần dự đoán tuổi có thể coi là thuật toán  Regression .  Chú ý rằng phần dự đoán tuổi cũng có thể coi là  Classification  nếu ta coi tuổi là một số nguyên dương không lớn hơn 150, chúng ta sẽ có 150 class (lớp) khác nhau. Unsupervised Learning (Học không giám sát) Trong thuật toán này, chúng ta không biết được  outcome  hay  nhãn  mà chỉ có dữ liệu đầu vào. Thuật toán unsupervised learning sẽ dựa vào cấu trúc của dữ liệu để thực hiện một công việc nào đó, ví dụ như phân nhóm (clustering) hoặc giảm số chiều của dữ liệu (dimension reduction) để thuận tiện trong việc lưu trữ và tính toán. Một cách toán học, Unsupervised learning là khi chúng ta chỉ có dữ liệu vào \\(\\mathcal{X} \\) mà không biết  nhãn  \\(\\mathcal{Y}\\) tương ứng. Những thuật toán loại này được gọi là Unsupervised learning vì không giống như Supervised learning, chúng ta không biết câu trả lời chính xác cho mỗi dữ liệu đầu vào. Giống như khi ta học, không có thầy cô giáo nào chỉ cho ta biết đó là chữ A hay chữ B. Cụm  không giám sát  được đặt tên theo nghĩa này. Các bài toán Unsupervised learning được tiếp tục chia nhỏ thành hai loại: Clustering (phân nhóm) Một bài toán phân nhóm toàn bộ dữ liệu \\(\\mathcal{X}\\) thành các nhóm nhỏ dựa trên sự liên quan giữa các dữ liệu trong mỗi nhóm. Ví dụ: phân nhóm khách hàng dựa trên hành vi mua hàng. Điều này cũng giống như việc ta đưa cho một đứa trẻ rất nhiều mảnh ghép với các hình thù và màu sắc khác nhau, ví dụ tam giác, vuông, tròn với màu xanh và đỏ, sau đó yêu cầu trẻ phân chúng thành từng nhóm. Mặc dù không cho trẻ biết mảnh nào tương ứng với hình nào hoặc màu nào, nhiều khả năng chúng vẫn có thể phân loại các mảnh ghép theo màu hoặc hình dạng. Association Là bài toán khi chúng ta muốn khám phá ra một quy luật dựa trên nhiều dữ liệu cho trước. Ví dụ: những khách hàng nam mua quần áo thường có xu hướng mua thêm đồng hồ hoặc thắt lưng; những khán giả xem phim Spider Man thường có xu hướng xem thêm phim Bat Man, dựa vào đó tạo ra một hệ thống gợi ý khách hàng (Recommendation System), thúc đẩy nhu cầu mua sắm. Semi-Supervised Learning (Học bán giám sát) Các bài toán khi chúng ta có một lượng lớn dữ liệu \\(\\mathcal{X}\\) nhưng chỉ một phần trong chúng được gán nhãn được gọi là Semi-Supervised Learning. Những bài toán thuộc nhóm này nằm giữa hai nhóm được nêu bên trên. Một ví dụ điển hình của nhóm này là chỉ có một phần ảnh hoặc văn bản được gán nhãn (ví dụ bức ảnh về người, động vật hoặc các văn bản khoa học, chính trị) và phần lớn các bức ảnh/văn bản khác chưa được gán nhãn được thu thập từ internet. Thực tế cho thấy rất nhiều các bài toán Machine Learning thuộc vào nhóm này vì việc thu thập dữ liệu có nhãn tốn rất nhiều thời gian và có chi phí cao. Rất nhiều loại dữ liệu thậm chí cần phải có chuyên gia mới gán nhãn được (ảnh y học chẳng hạn). Ngược lại, dữ liệu chưa có nhãn có thể được thu thập với chi phí thấp từ internet. Reinforcement Learning (Học Củng Cố) Reinforcement learning là các bài toán giúp cho một hệ thống tự động xác định hành vi dựa trên hoàn cảnh để đạt được lợi ích cao nhất (maximizing the performance). Hiện tại, Reinforcement learning chủ yếu được áp dụng vào Lý Thuyết Trò Chơi (Game Theory), các thuật toán cần xác định nước đi tiếp theo để đạt được điểm số cao nhất. AlphaGo chơi cờ vây với Lee Sedol. AlphaGo là một ví dụ của Reinforcement learning.   (Nguồn:  AlphaGo AI Defeats Sedol Again, With 'Near Perfect Game') Ví dụ 1: AlphaGo gần đây nổi tiếng với việc chơi cờ vây thắng cả con người .  Cờ vây được xem là có độ phức tạp cực kỳ cao  với tổng số nước đi là xấp xỉ \\(10^{761} \\), so với cờ vua là \\(10^{120} \\) và tổng số nguyên tử trong toàn vũ trụ là khoảng \\(10^{80}\\)!! Vì vậy, thuật toán phải chọn ra 1 nước đi tối ưu trong số hàng nhiều tỉ tỉ lựa chọn, và tất nhiên, không thể áp dụng thuật toán tương tự như  IBM Deep Blue  (IBM Deep Blue đã thắng con người trong môn cờ vua 20 năm trước). Về cơ bản, AlphaGo bao gồm các thuật toán thuộc cả Supervised learning và Reinforcement learning. Trong phần Supervised learning, dữ liệu từ các ván cờ do con người chơi với nhau được đưa vào để huấn luyện. Tuy nhiên, mục đích cuối cùng của AlphaGo không phải là chơi như con người mà phải thậm chí thắng cả con người. Vì vậy, sau khi  học  xong các ván cờ của con người, AlphaGo tự chơi với chính nó với hàng triệu ván chơi để tìm ra các nước đi mới tối ưu hơn. Thuật toán trong phần tự chơi này được xếp vào loại Reinforcement learning. (Xem thêm tại  Google DeepMind’s AlphaGo: How it works ). Ví dụ 2: Huấn luyện cho máy tính chơi game Mario . Đây là một chương trình thú vị dạy máy tính chơi game Mario. Game này đơn giản hơn cờ vây vì tại một thời điểm, người chơi chỉ phải bấm một số lượng nhỏ các nút (di chuyển, nhảy, bắn đạn) hoặc không cần bấm nút nào. Đồng thời, phản ứng của máy cũng đơn giản hơn và lặp lại ở mỗi lần chơi (tại thời điểm cụ thể sẽ xuất hiện một chướng ngại vật cố định ở một vị trí cố định). Đầu vào của thuật toán là sơ đồ của màn hình tại thời điểm hiện tại, nhiệm vụ của thuật toán là với đầu vào đó, tổ hợp phím nào nên được bấm. Việc huấn luyện này được dựa trên điểm số cho việc di chuyển được bao xa trong thời gian bao lâu trong game, càng xa và càng nhanh thì được điểm thưởng càng cao (điểm thưởng này không phải là điểm của trò chơi mà là điểm do chính người lập trình tạo ra). Thông qua huấn luyện, thuật toán sẽ tìm ra một cách tối ưu để tối đa số điểm trên, qua đó đạt được mục đích cuối cùng là cứu công chúa. Huấn luyện cho máy tính chơi game Mario 2. Phân nhóm dựa trên chức năng Có một cách phân nhóm thứ hai dựa trên chức năng của các thuật toán. Trong phần này, tôi xin chỉ liệt kê các thuật toán. Thông tin cụ thể sẽ được trình bày trong các bài viết khác tại blog này. Trong quá trình viết, tôi có thể sẽ thêm bớt một số thuật toán. Regression Algorithms Linear Regression Logistic Regression Stepwise Regression Classification Algorithms Linear Classifier Support Vector Machine (SVM) Kernel SVM Sparse Representation-based classification (SRC) Instance-based Algorithms k-Nearest Neighbor (kNN) Learning Vector Quantization (LVQ) Regularization Algorithms Ridge Regression Least Absolute Shrinkage and Selection Operator (LASSO) Least-Angle Regression (LARS) Bayesian Algorithms Naive Bayes Gaussian Naive Bayes Clustering Algorithms k-Means clustering k-Medians Expectation Maximization (EM) Artificial Neural Network Algorithms Perceptron Softmax Regression Multi-layer Perceptron Back-Propagation  Dimensionality Reduction Algorithms Principal Component Analysis (PCA) Linear Discriminant Analysis (LDA) Ensemble Algorithms Boosting AdaBoost Random Forest Và còn rất nhiều các thuật toán khác. 3. Tài liệu tham khảo A Tour of Machine Learning Algorithms Điểm qua các thuật toán Machine Learning hiện đại Share Share Interactive Learning Facebook page Machine Learning cơ bản Forum Recommended books \"Pattern recognition and Machine Learning.\", C. Bishop  \"The Elements of Statistical Learning\", T. Hastie et al.   \"Computer Vision:  Models, Learning, and Inference\", Simon J.D. Prince  \"Convex Optimization\", Boyd and Vandenberghe Recommended courses \"Machine Learning\", Andrew Ng  CS224n: Natural Language Processing with Deep Learning CS231n: Convolutional Neural Networks for Visual Recognition CS246: Mining Massive Data Sets CS20SI: Tensorflow for Deep Learning Research  Introduction to Computer Science and Programming Using Python Others Top-down learning path: Machine Learning for Software Engineers Blog này được tạo như thế nào? Chúng tôi đã apply và học tiến sỹ như thế nào? (1/2) Chúng tôi đã apply và học tiến sỹ như thế nào? (2/2) 8 Inspirational Applications of Deep Learning Matrix calculus TensorFlow-Examples Eight Easy Steps To Get Started Learning Artificial Intelligence The 9 Deep Learning Papers You Need To Know About",
          "relevance": "1",
          "title": "Bài 2: Phân nhóm các thuật toán Machine Learning",
          "url": "https://machinelearningcoban.com/2016/12/27/categories/"
        },
        {
          "content": "Bách khoa toàn thư mở Wikipedia Bạn có  tin nhắn mới  ( thay đổi gần đây ). \n\t\t\t\t\tBước tới:\t\t\t\t\t menu , \t\t\t\t\t tìm kiếm Xử lý ngôn ngữ tự nhiên  ( natural language processing  - NLP) là một nhánh của  trí tuệ nhân tạo  tập trung vào các ứng dụng trên  ngôn ngữ  của con người. Trong trí tuệ nhân tạo thì xử lý ngôn ngữ tự nhiên là một trong những phần khó nhất vì nó liên quan đến việc phải hiểu ý nghĩa ngôn ngữ-công cụ hoàn hảo nhất của  tư duy  và  giao tiếp . Mục lục 1 Các bước xử lý 2 Các bài toán và ứng dụng 3 Các bài toán trong xử lý tiếng Việt 4 Xem thêm 5 Tham khảo 6 Liên kết ngoài 7 Chú thích Các bước xử lý [ sửa  |  sửa mã nguồn ] Phân tích hình thái  - Trong bước này từng từ sẽ được phân tích và các ký tự không phải chữ (như các dấu câu) sẽ được tách ra khỏi các từ. Trong  tiếng Anh  và nhiều ngôn ngữ khác, các từ được phân tách với nhau bằng dấu cách. Tuy nhiên trong  tiếng Việt , dấu cách được dùng để phân tách các tiếng (âm tiết) chứ không phải từ. Cùng với các ngôn ngữ như  tiếng Trung ,  tiếng Hàn , tiếng Nhật ,  phân tách từ  trong tiếng Việt là một công việc không hề đơn giản. Phân tích cú pháp  - Dãy các từ sẽ được biến đổi thành các cấu trúc thể hiện sự liên kết giữa các từ này. Sẽ có những dãy từ bị loại do vi phạm các luật văn phạm. Phân tích ngữ nghĩa  - Thêm ngữ nghĩa vào các cấu trúc được tạo ra bởi bộ phân tích cú pháp. Tích hợp văn bản  - Ngữ nghĩa của một câu riêng biệt có thể phụ thuộc vào những câu đứng trước, đồng thời nó cũng có thể ảnh hưởng đến các câu phía sau. Phân tích thực nghĩa  - Cấu trúc thể hiện điều được phát ngôn sẽ được thông dịch lại để xác định nó thật sự có nghĩa là gì. Tuy nhiên, ranh giới giữa 5 bước xử lý này cũng rất mong manh. Chúng có thể được tiến hành từng bước một, hoặc tiến hành cùng lúc - tùy thuộc vào giải thuật và ngữ cảnh cụ thể. Các bài toán và ứng dụng [ sửa  |  sửa mã nguồn ] Nhận dạng chữ viết : Có hai kiểu nhận dạng, thứ nhất là nhận dạng chữ in, ví dụ nhận dạng chữ trên sách giáo khoa rồi chuyển nó thành dạng văn bản điện tử như dưới định dạng doc của  Microsoft Word  chẳng hạn. Phức tạp hơn là  nhận dạng chữ viết tay , có khó khăn bởi vì chữ viết tay không có khuôn dạng rõ ràng và thay đổi từ người này sang người khác. Với chương trình nhận dạng chữ viết in có thể chuyển hàng ngàn đầu sách trong thư viện thành văn bản điện tử trong thời gian ngắn. Nhận dạng chữ viết của con người có ứng dụng trong khoa học hình sự và bảo mật thông tin (nhận dạng chữ ký điện tử). Nhận dạng tiếng nói : Nhận dạng tiếng nói rồi chuyển chúng thành văn bản tương ứng. Giúp thao tác của con người trên các thiết bị nhanh hơn và đơn giản hơn, chẳng hạn thay vì gõ một tài liệu nào đó bạn đọc nó lên và trình soạn thảo sẽ tự ghi nó ra. Đây cũng là bước đầu tiên cần phải thực hiện trong ước mơ thực hiện giao tiếp giữa con người với robot. Nhận dạng tiếng nói có khả năng trợ giúp người khiếm thị rất nhiều. Tổng hợp tiếng nói : Từ một văn bản tự động tổng hợp thành tiếng nói. Thay vì phải tự đọc một cuốn sách hay nội dung một trang  web , nó tự động đọc cho chúng ta. Giống như nhận dạng tiếng nói, tổng hợp tiếng nói là sự trợ giúp tốt cho  người khiếm thị , nhưng ngược lại nó là bước cuối cùng trong giao tiếp giữa  robot  với người. Dịch tự động  ( machine translate ): Như tên gọi đây là chương trình dịch tự động từ ngôn ngữ này sang ngôn ngữ khác. Một phần mềm điển hình về tiếng Việt của chương trình này là  Evtrans  của Softex, dịch tự động từ tiếng Anh sang tiếng Việt và ngược lại, phần mềm từng được trang web vdict.com mua bản quyền, đây cũng là trang đầu tiên đưa ứng dụng này lên mạng. Tháng 10 năm 2008 có hai công ty tham gia vào lĩnh vực này cho ngôn ngữ tiếng Việt là công ty Lạc Việt (công ty phát hành từ điển Lạc Việt) và  Google , một thời gian sau đó Xalo_vn cũng đưa ra dịch vụ tương tự. Tìm kiếm thông tin  ( information retrieval ): Đặt câu hỏi và chương trình tự tìm ra nội dung phù hợp nhất. Thông tin ngày càng đầy lên theo cấp số nhân, đặc biệt với sự trợ giúp của  internet  việc tiếp cận thông tin trở lên dễ dàng hơn bao giờ hết. Việc khó khăn lúc này là tìm đúng nhất thông tin mình cần giữa bề bộn tri thức và đặc biệt thông tin đó phải đáng tin cậy. Các  máy tìm kiếm  dựa trên giao diện web như Google hay  Yahoo  hiện nay chỉ phân tích nội dung rất đơn giản dựa trên tần suất của từ khoá và thứ hạng của trang và một số tiêu chí đánh giá khác để đưa ra kết luận, kết quả là rất nhiều tìm kiếm không nhận được câu trả lời phù hợp, thậm chí bị dẫn tới một liên kết không liên quan gì do thủ thuật đánh lừa của các trang web nhằm giới thiệu sản phẩm (có tên tiếng Anh là  SEO  viết tắt của từ  search engine optimization ). Thực tế cho đến bây giờ chưa có máy tìm kiếm nào hiểu được ngôn ngữ tự nhiên của con người trừ trang www.ask.com được đánh giá là \"hiểu\" được những câu hỏi có cấu trúc ở dạng đơn giản nhất. Mới đây cộng đồng mạng đang xôn xao về trang  Wolfram Alpha , được hứa hẹn là có khả năng hiểu ngôn ngữ tự nhiên của con người và đưa ra câu trả lời chính xác [1] . Lĩnh vực này hứa hẹn tạo ra bước nhảy trong cách thức tiếp nhận tri thức của cả cộng đồng. Tóm tắt văn bản : Từ một văn bản dài tóm tắt thành một văn bản ngắn hơn theo mong muốn nhưng vẫn chứa những nội dung thiết yếu nhất. Khai phá dữ liệu  ( data mining ) và  phát hiện tri thức : Từ rất nhiều tài liệu khác nhau phát hiện ra tri thức mới. Thực tế để làm được điều này rất khó, nó gần như là mô phỏng quá trình  học tập , khám phá khoa học của con người, đây là lĩnh vực đang trong giai đoạn đầu phát triển. Ở mức độ đơn giản khi kết hợp với  máy tìm kiếm  nó cho phép đặt câu hỏi để từ đó công cụ tự tìm ra câu trả lời dựa trên các thông tin trên web mặc cho việc trước đó có câu trả lời lưu trên web hay không (giống như trang Yahoo! hỏi và đáp, nơi chuyên đặt các câu hỏi để người khác trả lời), nói một cách nôm na là nó đã biết xử lý dữ liệu để trả lời câu hỏi của người sử dụng, thay vì máy móc đáp trả những gì chỉ có sẵn trong bộ nhớ. Các bài toán trong xử lý tiếng Việt [ sửa  |  sửa mã nguồn ] Phân tách câu Phân tách từ Tự động thêm dấu : Chữ viết tiếng Việt là chữ viết có dấu thanh. Trong các văn bản chính thống như sách, báo chí, văn bản hành chính, các dấu thanh được viết chính xác. Tuy nhiên trong cách tình huống không chính thống như chat, gõ tìm kiếm, người dùng thông thường không gõ các dấu thanh, dẫn tới khó khăn nhất định cho máy tính trong việc hiểu ý nghĩa của văn bản. Xem thêm [ sửa  |  sửa mã nguồn ] Ngôn ngữ Xử lý tiếng nói Trí tuệ nhân tạo Dịch tự động Máy tìm kiếm Nhập nhằng Học máy Ngữ liệu Xử lý tiếng Việt Tham khảo [ sửa  |  sửa mã nguồn ] Nghiên cứu xử lý ngôn ngữ tự nhiên ứng dụng vào dịch tự động Anh Việt Nửa thế kỷ của trí tuệ nhân tạo Ứng dụng ngôn ngữ tự nhiên vào tìm kiếm thông tin Artificial Intelligence .  Elaine Rich ,  Kevin Knight . McGraw-Hill, Inc.  ISBN 0-07-052263-4 . Phát triển một Hệ thống S.E Hỗ trợ Tìm kiếm Thông tin, thuộc lãnh vực CNTT trên Internet qua từ khóa bằng tiếng Việt Liên kết ngoài [ sửa  |  sửa mã nguồn ] Trang dịch tự động của Google Trang dịch tự động của Bing Trang chủ của máy tìm kiếm Ask Chú thích [ sửa  |  sửa mã nguồn ] ^ Cỗ máy tìm kiếm có thể thay đổi thế giới Internet x t s Những lĩnh vực chính của  khoa học máy tính Các nền tảng toán học Logic toán học   · Lý thuyết tập hợp   · Lý thuyết số   · Lý thuyết đồ thị   · Lý thuyết kiểu   · Lý thuyết thể loại   · Giải tích số   · Lý thuyết thông tin   · Đại số   · Nhận dạng mẫu   · Nhận dạng tiếng nói   · Toán học tổ hợp   · Đại số Boole   · Toán rời rạc Lý thuyết phép tính Độ phức tạp Kolmogorov   · Lý thuyết Automat   · Lý thuyết tính được   · Lý thuyết độ phức tạp tính toán   · Lý thuyết điện toán lượng tử Các cấu trúc dữ liệu \nvà  các giải thuật Phân tích giải thuật   · Thiết kế giải thuật   · Hình học tính toán   · Tối ưu hóa tổ hợp Các ngôn ngữ lập trình \nvà  Các trình biên dịch Các bộ phân tích cú pháp   · Các trình thông dịch   · Lập trình cấu trúc   · Lập trình thủ tục   · Lập trình hướng đối tượng   · Lập trình hướng khía cạnh   · Lập trình hàm   · Lập trình logic   · Lập trình máy tính   · Lập trình mệnh lệnh   · Lập trình song song   · Lập trình tương tranh   · Các mô hình lập trình   · Prolog   · Tối ưu hóa trình biên dịch Tính song hành , Song song , \nvà các hệ thống  phân tán Đa xử lý   · Điện toán lưới   · Kiểm soát song hành   · Hiệu năng hệ thống   · Tính toán phân tán Công nghệ phần mềm Phân tích yêu cầu   · Thiết kế phần mềm   · Các phương pháp hình thức   · Kiểm thử phần mềm   · Quy trình phát triển phần mềm   · Các phép đo phần mềm   · Đặc tả chương trình   · LISP   · Mẫu thiết kế   · Tối ưu hóa phần mềm Kiến trúc hệ thống Kiến trúc máy tính   · Tổ chức máy tính   · Các hệ điều hành   · Các cấu trúc điều khiển   · Cấu trúc bộ nhớ lưu trữ   · Vi mạch   · Thiết kế ASIC   · Vi lập trình   · Vào/ra dữ liệu   · VLSI design   · Xử lý tín hiệu số Viễn thông \nvà  Mạng máy tính Audio máy tính   · Chọn tuyến   · Cấu trúc liên kết mạng   · Mật mã học Các cơ sở dữ liệu \nvà  Các hệ thống thông tin Hệ quản trị cơ sở dữ liệu   · Cơ sở dữ liệu quan hệ   · SQL   · Các giao dịch   · Các chỉ số cơ sở dữ liệu   · Khai phá dữ liệu   · Biểu diễn và giao diện thông tin   · Các hệ thống thông tin   · Khôi phục dữ liệu   · Lưu trữ thông tin   · Lý thuyết thông tin   · Mã hóa dữ liệu   · Nén dữ liệu   · Thu thập thông tin Trí tuệ nhân tạo Lập luận tự động   · Ngôn ngữ học tính toán   · Thị giác máy tính   · Tính toán tiến hóa   · Các hệ chuyên gia    · Học máy   · Xử lý ngôn ngữ tự nhiên   · Robot học Đồ họa máy tính Trực quan hóa   · Hoạt họa máy tính   · Xử lý ảnh Giao diện người-máy tính Khả năng truy cập máy tính   · Giao diện người dùng   · Điện toán mang được   · Điện toán khắp mọi nơi   · Thực tế ảo Khoa học tính toán Cuộc sống nhân tạo   · Tin sinh học   · Khoa học nhận thức   · Hóa học tính toán   · Khoa học thần kinh tính toán   · Vật Lý học tính toán   · Các giải thuật số   · Toán học kí hiệu Chú ý: khoa học máy tính còn có thể được chia thành nhiều chủ đề hay nhiều lĩnh vực khác dựa theo  Hệ thống xếp loại điện toán ACM . x t s Những Chuyên ngành chính của  Tin học  •   Phần cứng  •   Phần mềm Công nghệ thông tin Cuộc sống nhân tạo Đa xử lý Điện toán lưới Đồ họa máy tính Hệ chuyên gia Hệ thống thông tin quản lý Hoạt họa máy tính Khoa học nhận thức Khoa học tính toán Khoa học thần kinh tính toán Khoa học thông tin Kiểm soát song hành Kiến trúc hệ thống Lập luận tự động Ngôn ngữ hình thức Ngôn ngữ học tính toán Người máy Robot học Thực tế ảo Tính toán song song Tối ưu hóa trình biên dịch Tổ chức máy tính Trí tuệ nhân tạo Từ điển học Tương tranh Vật lý học tính toán Hệ thống thông tin An toàn thông tin Cơ sở dữ liệu đa phương tiện Cơ sở dữ liệu thông minh Dữ liệu lớn Hệ cơ sở tri thức Hệ dựa trên logic Hệ gợi ý Hệ thích nghi dựa trên ngữ cảnh Hệ thống hướng tác tử Hệ thống thông minh Hệ thống thông tin địa lý Hệ trợ giúp quyết định Kỹ nghệ dữ liệu Kỹ nghệ tri thức Logic mờ Phân tích dữ liệu Phân tích và thiết kế hệ thống Quản trị dự án Quản trị tri thức Thiết kế và quản trị dữ liệu Tích hợp dữ liệu Tính toán hiệu năng cao Web ngữ nghĩa Xử lý thông tin mờ Khoa học máy tính Cơ sở dữ liệu phân tán Hệ quản trị cơ sở dữ liệu Hệ thống đa lõi Hệ thống truyền thông Hình học tính toán Hóa học tính toán Học máy Khai phá dữ liệu Lập trình song song Lý thuyết mã hóa Lý thuyết tính toán Ngôn ngữ và phương pháp dịch Nguyên lý ngôn ngữ lập trình Quy hoạch ràng buộc Sinh học tính toán  ( Tin sinh học ) Thiết kế và phân tích thuật toán Tìm kiếm thông tin Tính toán khoa học Tính toán kí hiệu Tính toán phân tán Tính toán tiến hóa Tính toán tự nhiên Tối ưu hoá tổ hợp Xử lý song song Kỹ thuật máy tính Đa phương tiện Định vị vệ tinh  ( GNSS ) Giao diện người dùng Ghép nối máy tính Hệ nhúng Hệ thống thời gian thực Hiệu năng hệ thống Kiến trúc máy tính Lập trình đôi Lập trình đồ họa Lập trình hệ thống Lý thuyết nhận dạng Mạng nơ-ron Nhận dạng tiếng nói Phân tích tín hiệu Thị giác máy tính Thiết kế IC Thoại IP Tổng hợp giọng nói Tương tác người–máy tính Vi xử lý Xử lý ảnh Xử lý dữ liệu đa phương tiện Xử lý ngôn ngữ tự nhiên Xử lý tiếng nói Xử lý tín hiệu số Kỹ nghệ phần mềm Bảo trì phần mềm Các phương pháp hình thức Chất lượng phần mềm Đảm bảo chất lượng phần mềm Đánh giá phần mềm Đo lường và quản trị phần mềm Độ tin cậy và chịu lỗi phần mềm Kiểm thử phần mềm Kiến trúc doanh nghiệp Kiến trúc phần mềm Kinh tế công nghệ phần mềm Kỹ nghệ hướng dịch vụ Lập trình linh hoạt Mẫu thiết kế Mô hình hóa phần mềm Phân tích hệ thống Phân tích thiết kế hướng đối tượng  ( UML ) Phân tích yêu cầu phần mềm Phát triển phần mềm Quản lý cấu hình phần mềm Quản lý dự án phần mềm Quản lý kỹ thuật phần mềm Quy trình phát triển phần mềm  ( Vòng đời phát hành phần mềm ) Thiết kế phần mềm Triển khai phần mềm Tối ưu hóa phần mềm Mạng máy tính An ninh mạng An ninh trong giao dịch điện tử Đánh giá hiệu năng mạng  ( QoS ) Điện toán đám mây Định tuyến Hệ phân tán Kỹ thuật truyền thông Lý thuyết thông tin Mạng không dây Mạng thế hệ mới Mạng thiết bị di động Mạng thông tin quang Mật mã học Mô phỏng mạng Nhận dạng Quản trị mạng Thiết bị truyền thông và mạng Thiết kế mạng Tính toán khắp nơi và di động Trung tâm dữ liệu Truyền thông di động Truyền thông đa phương tiện Truyền thông số Vệ tinh thông tin Viễn thông  ( Mạng viễn thông ) Ước lượng tín hiệu và hệ thống Web thế hệ mới Tin học kinh tế x t s Giám đốc công nghệ thông tin  ·  Tin học kinh tế  ·  Quản lý công nghệ thông tin Quản lý ITIL  &  ITSM Định hướng phát triển Phát triển nhân lực Quản lý bảo mật Quản lý chất lượng Quản lý công nghệ Quản lý dự án Quản lý mua sắm Quản lý ngân sách Quản lý nguồn lực Quản lý phát hành Quản lý rủi ro Quản lý tài sản Quản lý thay đổi Quản lý tích hợp Quản lý tổ chức Quản lý truyền thông Quản lý tuân thủ Quản lý vấn đề Thiết kế giải pháp Xây dựng chiến lược Xây dựng chính sách Quản lý mạng Ảo hóa Mạng campus Mạng diện rộng Mạng nội bộ Mạng riêng ảo STP VLAN IVR VTP Quản trị hệ thống Hoạt động vận hành Bảo trì thiết bị Bảo vệ hệ thống Đối phó sự cố Kế hoạch dự phòng Hoạt động kỹ thuật Hỗ trợ kỹ thuật Kiểm soát truy cập Kiểm tra hệ thống Xác thực người dùng Hoạt động an toàn An ninh nhân sự An ninh hệ thống Nhận thức an toàn Rủi ro hệ thống Quản lý hệ thống Bàn dịch vụ Quản lý cấu hình Quản lý công suất Quản lý dịch vụ Quản lý hạ tầng Quản lý khôi phục Quản lý người dùng Quản lý sự cố Quản lý tính liên tục Quản lý tính sẵn sàng Tổ chức công việc Tổ chức hỗ trợ Kỹ năng lãnh đạo Kỹ năng cộng tác nhóm Kỹ năng đàm phán Kỹ năng giải quyết vấn đề Kỹ năng giao tiếp Kỹ năng gọi thoại Kỹ năng huấn luyện Kỹ năng lắng nghe Kỹ năng phân công ủy thác Kỹ năng phỏng vấn tuyển dụng Kỹ năng quản lý thời gian Kỹ năng tạo động lực Kỹ năng tư duy Kỹ năng thiết kế quy trình Kỹ năng thuyết trình Kỹ năng viết tài liệu kỹ thuật Ứng dụng Chính phủ điện tử Giáo dục trực tuyến Hoạch định tài nguyên doanh nghiệp Kinh doanh điện tử  ( Mua sắm trực tuyến    · Thương mại điện tử    · Tiếp thị trực tuyến ) Kinh doanh thông minh Quản lý quan hệ khách hàng Quản lý tri thức Các lĩnh vực liên quan Kinh tế Luật pháp Tài chính Kế toán Kinh doanh Tổ chức Xã hội Quản lý Quản trị kinh doanh Bài viết này vẫn còn  sơ khai . Bạn có thể giúp Wikipedia bằng cách  mở rộng nội dung  để bài được hoàn chỉnh hơn. x t s \n\t\t\t\t\t\tLấy từ “ https://vi.wikipedia.org/w/index.php?title=Xử_lý_ngôn_ngữ_tự_nhiên&oldid=24204090 ”\t\t\t\t\t Thể loại :  Sơ khai Xử lý ngôn ngữ tự nhiên Trí tuệ nhân tạo Ngôn ngữ học Thể loại ẩn:  Trang sử dụng liên kết tự động ISBN Trình đơn chuyển hướng Công cụ cá nhân Chưa đăng nhập Thảo luận cho địa chỉ IP này Đóng góp Mở tài khoản Đăng nhập Không gian tên Bài viết Thảo luận Biến thể Các hiển thị Đọc Sửa đổi Sửa mã nguồn Xem lịch sử Khác Tìm kiếm Xem nhanh Trang Chính Bài viết chọn lọc Tin tức Bài viết ngẫu nhiên Thay đổi gần đây Phản hồi lỗi Quyên góp Tương tác Hướng dẫn Giới thiệu Wikipedia Cộng đồng Thảo luận chung Giúp sử dụng Liên lạc Công cụ Các liên kết đến đây Thay đổi liên quan Các trang đặc biệt Liên kết thường trực Thông tin trang Khoản mục Wikidata Trích dẫn trang này In/xuất ra Tạo một quyển sách Tải về dưới dạng PDF Bản để in ra Tại dự án khác Wikimedia Commons Ngôn ngữ khác Afrikaans العربية Azərbaycanca Bahasa Indonesia বাংলা Bân-lâm-gú Беларуская Беларуская (тарашкевіца)‎ Български Català Čeština Dansk Eesti Ελληνικά English Español Euskara فارسی Français Galego 한국어 हिन्दी Íslenska Italiano עברית ಕನ್ನಡ ქართული Lietuvių Македонски मराठी Монгол မြန်မာဘာသာ 日本語 Polski Português Română Русский Simple English کوردی Српски / srpski தமிழ் ไทย Türkçe Українська 中文 Sửa liên kết  Trang này được sửa đổi lần cuối lúc 07:11 ngày 13 tháng 9 năm 2016. Văn bản được phát hành theo  Giấy phép Creative Commons Ghi công–Chia sẻ tương tự ; có thể áp dụng điều khoản bổ sung. Với việc sử dụng trang web này, bạn chấp nhận  Điều khoản Sử dụng  và  Quy định quyền riêng tư . \nWikipedia® là thương hiệu đã đăng ký của  Wikimedia Foundation, Inc. , một tổ chức phi lợi nhuận. Quy định quyền riêng tư Giới thiệu Wikipedia Lời phủ nhận Nhà phát triển Tuyên bố về cookie Phiên bản di động",
          "relevance": "1",
          "title": "Xử lý ngôn ngữ tự nhiên",
          "url": "https://vi.wikipedia.org/wiki/X%E1%BB%AD_l%C3%BD_ng%C3%B4n_ng%E1%BB%AF_t%E1%BB%B1_nhi%C3%AAn"
        },
        {
          "content": "  Trang chủ   Giới thiệu Công ty Sứ mệnh, tầm nhìn Tổ chức Đối tác Công nghệ  Giải pháp Quảng cáo trực tuyến Lọc và tổng hợp thông tin An toàn, bảo mật thông tin Giải trí  - Giáo dục  Dịch vụ Tối ưu hóa công cụ tìm kiếm - NORON Quảng cáo theo ngữ cảnh Kiểm toán website Lọc web theo ngữ nghĩa - REMPARO Quản lý danh tiếng Người đối thoại ảo Hệ thống chống rò rỉ thông tin Thiết kế website Đăng ký tên miền Web Hosting Tổng hợp thông tin Cho thuê Website Quảng cáo Google Adwords  Đại lý Đại lý Chính sách đại lý  Tin tức Tin CSA Tin công nghệ Điểm tin Tuyển dụng Vị trí đang tuyển Qui trình tuyển dụng Biểu mẫu  Liên hệ Tiếng Việt English Quảng cáo trực tuyến Tổng hợp  giám sát thông tin An toàn bảo mật thông tin Giải trí & giáo dục Điểm tin                     \r\n                    Người chuyên 'xử lý ngôn ngữ tự nhiên' trong tin học\r\n                     \r\n                    TP - “Tôi đã làm ứng dụng xử lý ngôn ngữ tự nhiên cho nhiều thứ tiếng như Nhật, Hàn Quốc, Arập, Trung Quốc. Về nước lần này cũng vì muốn nghiên cứu ứng dụng cho ngôn ngữ nước mình”, TS Phạm Bảo Sơn tâm sự. \r\n\t  \r\n\t  \r\n\t\t\t\t\t\tPhạm Bảo Sơn \r\n\t\t\tLà một trong 10 gương mặt thanh niên xuất sắc trong lĩnh vực công nghệ thông tin đạt giải Quả Cầu Vàng 2007 do T.Ư Đoàn TNCS Hồ Chí Minh và Bộ KHCN tổ chức từng có hai bằng sáng chế công nghệ thông tin, Tiến sĩ trẻ Phạm Bảo Sơn có mong muốn giản dị là có một công trình khoa học bằng tiếng Việt và truyền cho những sinh viên Việt ngọn lửa khám phá, sáng tạo từ những kiến thức mình được học và nghiên cứu về tin học nơi đất bạn. \r\n\t\t\tĐó cũng là lý do anh trở về làm giảng viên tại trường ĐH Công nghệ sau 10 năm học tập và nghiên cứu tại trường Đại học Tổng hợp New South Wales (Úc). \r\n\t\t\tThành quả của 10 năm đam mê \r\n\t\t\tĐam mê ngôn ngữ tin học từ khi còn là học sinh phổ thông, suốt 10 năm học tập và nghiên cứu tại Úc, Phạm Bảo Sơn dồn toàn bộ thời gian và tâm huyết để nghiên cứu về Xử lý ngôn ngữ tự nhiên trong tin học. \r\n\t\t\tTài năng và công sức của Sơn với thành quả của 10 năm là 2 bằng phát minh sáng chế công nghệ thông tin năm 2006 và bảo vệ xuất sắc luận án Tiến sĩ năm 2007. “Tôi có duyên trong nghiên cứu vấn đề này”. \r\n\t\t\tLàm thế nào để biết được các thông tin về tác giả của một email? Câu trả lời sẽ được tìm thấy trong sáng chế: “Xử lý ngôn ngữ email và thuộc tính người viết” và “Xử lý ngôn ngữ văn bản” mà anh Sơn và các bạn trong Công ty Appen (Công ty công nghệ ngôn ngữ của Úc) đã dày công nghiên cứu. Ngôn ngữ tự nhiên chính là ngôn ngữ chúng ta vẫn dùng. Nó có thể được lưu trữ trên máy tính dưới dạng văn bản (email, blog, news...), hay ghi âm lại giọng nói. Nghiên cứu “Xử lý ngôn ngữ tự nhiên” nhằm mục đích để máy tính có thể tự xử lý được ngôn ngữ tự nhiên, hiểu được ngôn ngữ tự nhiên… “Theo sáng chế này, bạn có thể xác định tên tuổi, tính cách, sở thích, và những thuộc tính khác của người gửi email chỉ qua cách viết của họ nhờ việc khai thác hoạt động tìm kiếm thông minh của máy tính”, anh Sơn chia sẻ. \r\n\t\t\tAnh đã mất 1 năm để nghiên cứu email của hơn 100 người ở các nước sử dụng ngôn ngữ tiếng Anh và tiếng Ả rập, mỗi người 10 email, từ đó gửi những câu hỏi tâm lý học, phân tích thói quen, tính cách, sở thích, đặc điểm chung nhất của người viết. \r\n\t\t\tCòn đối với văn bản, có thể xác định được đoạn nào người gửi email viết đoạn nào của người khác. “Sáng chế này sẽ được áp dụng với mục đích an ninh như: tìm chủ nhân của thư lạc danh, thư khủng bố, chat, blog,... và hiện tại Cty Appen đã triển khai ứng dụng nó”, anh Sơn cho biết. \r\n\t\t\tCùng với thành công trong những sáng chế, anh Sơn đã bảo vệ xuất sắc luận án tiến sĩ Luận văn nghiên cứu của anh là: “Incremental Knowledge Acquisition for Natural Language Processing” (“Thu thập tri thức từng bước cho xử lý ngôn ngữ tự nhiên”) sau 3,5 năm nghiên cứu. \r\n\t\t\tĐề tài của anh Sơn đề cập tới một vấn khó và không mới nhưng được đánh giá cao bởi đã đưa ra được những giải pháp. “Lập trình hệ thống luật để máy tính tự hỏi người sử dụng khi đặt lệnh xử lý, qua đó đưa trí thức của người sử dụng máy tính vào trong máy, để máy tính có thể tự đánh giá được vấn đề nào tốt hay xấu...”, anh Sơn chia sẻ. \r\n\t\t\t“Muốn khảo sát thị trường, bạn có thể đặt lệnh, máy tính sẽ tự khảo sát trên các trang web và đưa ra kết quả. Ví dụ, muốn tìm tên sản phẩm của Cty trên các web, máy tính sẽ tự động search khi được nhận lệnh và đưa ra kết quả thống kê chính xác từ các trang web”, anh Sơn nói về ứng dụng của đề tài này. \r\n\t\t\tBảo vệ thành công luận án Tiến sĩ, Phạm Bảo Sơn là người duy nhất của Khoa học máy tính trường ĐH New South đạt Huy chương của trường dành cho sinh viên tốt nghiệp xuất sắc năm 2001. \r\n\t\t\t\t\t\tSơn cùng gia đình trong ngày bảo vệ luận án Tiến sĩ tại trường ĐH Tổng hợp New South Wales (Úc) \r\n\t\t\tHai lần vô địch robot bóng đá thế giới \r\n\t\t\tLà học sinh giỏi quốc tế (lớp 11 và 12 đạt Huy chương bạc quốc tế môn Tin học), được tuyển thẳng vào Đại học Công nghệ - ĐH Quốc gia Hà Nội. Sang Úc du học, Sơn tiếp tục theo đuổi môn Tin học. \r\n\t\t\tNăm thứ hai Đại học Sơn nhận được học bổng hè của trường dành cho những sinh viên có thành tích học tập tốt trong năm 2000. May mắn đã đến với anh khi người trao học bổng đó là vị giáo sư đầu ngành trong lĩnh vực trí tuệ nhân tạo. Ông ngỏ ý mời Sơn tham gia đội tuyển Robot bóng đá của trường. \r\n\t\t\tSơn chia sẻ: “Tôi đồng ý trong tâm trạng vừa mừng, vừa lo. Mừng vì đây là cơ hội tốt cho tôi được cọ xát, nghiên cứu; lo vì  mình vừa sang được 1 năm sợ còn non kinh nghiệm”. \r\n\t\t\tRobot bóng đá quốc tế là cuộc thi thường niên dành cho các trường ĐH công nghệ thông tin trên thế giới. Với hình thức là các chú chó robot của các đội sẽ thi đá bóng với nhau trong một khoảng thời gian nhất định. \r\n\t\t\t“Mỗi con chó robot đều có phần cứng giống nhau, mỗi đội sẽ phải lập trình phần mềm cho con chó để thi đấu đạt kết quả cao nhất”- Sơn cho biết: \r\n\t\t\tĐội của Sơn (gồm 3 người) đã dành quyết tâm cao độ, miệt mài viết chương trình phần mềm trong cả 1 năm trời. Bước vào thi đấu đội robot của Sơn đi từ chiến thắng này đến chiến thắng khác. Đây là giải đấu bóng đá robot đầu tiên trường của Sơn có chiến thắng dòn giã như thế, bởi chiến thuật đã có sự khác biệt và nổi trội. \r\n\t\t\tNgay cả trong trận gặp đội của trường CMU của Mỹ - trường hàng đầu thế giới về tin học, đội tuyển của Sơn cũng đã thắng với tỷ số 13 - 10, thắng đội của Trung tâm nghiên cứu robot của Paris tỷ số 10 - 0. Năm 2000, cũng là năm đầu tiên, trường của Sơn đoạt chức vô địch robot bóng đá thế giới. \r\n\t\t\tĐến năm 2001, Sơn làm huấn luyện cho đội tuyển robot bóng đá của trường. Truyền lại những kinh nghiệm, đồng thời cùng mọi người viết những chương trình mới, một lần nữa, Sơn đã đưa cúp vô địch robot bóng đá thế giới về cho trường New South. \r\n\t\t\tHiện, Sơn về giảng dạy tại Khoa Công nghệ thông tin- Đại học Công nghệ Hà Nội với mong muốn rất giản dị là làm một công trình khoa học bằng tiếng Việt, điều mà 10 năm qua Sơn chưa làm được. \r\n\t\t\t“Tôi đã làm ứng dụng xử lý ngôn ngữ tự nhiên cho nhiều thứ tiếng như Nhật, Hàn Quốc, Arập, Trung Quốc. Về nước lần này cũng vì muốn nghiên cứu ứng dụng cho ngôn ngữ nước mình”, anh Sơn tâm sự. \r\n\t\t\tHải Yến \r\n\t\t\t  Trở về trang chủ \r\n    Các tin khác Top VỀ CSA Công ty Sứ mệnh, tầm nhìn Tổ chức Đối tác DỊCH VỤ CSA Quảng cáo trực tuyến Lọc và tổng hợp thông tin An toàn, bảo mật thông tin Giải trí  - Giáo dục TIN TỨC Tin CSA Tin công nghệ Điểm tin SiteMap \r\nCopyright © 2011 CSA. All rights reserved.  Designed by HD Net",
          "relevance": "0",
          "title": "Người chuyên 'xử lý ngôn ngữ tự nhiên' trong tin học",
          "url": "http://www.csa.vn/3/25/Nguoi-chuyen-xu-ly-ngon-ngu-tu-nhien-trong-tin-hoc.aspx"
        },
        {
          "content": "Đăng nhập bằng Facebook Đăng nhập Đăng ký Khám phá Giải đáp thắc mắc Liên hệ hợp tác Tiếng Việt English Tạo sự kiện Đăng nhập | Đăng ký Tháng 10 28 Thứ 6 Techtalk #1: Giao lưu với kỹ sư Google Translate về công nghệ xử lý ngôn ngữ tự nhiên và máy học Chia sẻ  Thêm lịch Google Calendar Outlook Calendar iCal Thứ 6, 28 Tháng 10 2016 (06:30 PM - 09:00 PM)\r\n                                         UP Coworking Space Tầng 8 toà nhà Creative City, số 1 Lương Yên, Quận Hai Bà Trưng, Hà Nội Từ  50.000 VND Sự kiện đã kết thúc Chia sẻ  Thêm lịch Google Calendar Outlook Calendar iCal Từ  50.000 VND Sự kiện đã kết thúc Chọn thời gian diễn Đóng iCalendar GOOGLE Calendar Outlook Calendar Đóng Giới thiệu Thông tin vé Nhà tổ chức Tin tức Giới thiệu \r\n\tCông nghệ xử lý ngôn ngữ tự nhiên và công nghệ máy học đang phát triển nhanh chóng. Ngày nay, có nhiều công ty khởi nghiệp trên thế giới xây dựng rô-bốt trả lời tự động và ứng dụng chúng vào nhiều lĩnh vực trong cuộc sống. Chỉ tính riêng 7 tháng đầu năm 2016, có 15 công ty khởi nghiệp xây dựng công nghệ rô-bốt trả lời tự động đã nhận được đầu tư từ các quỹ. Còn tại Việt Nam, nhiều kỹ sư nghiên cứu và công ty khởi nghiệp cũng đang quan tâm tới ứng dụng của công nghệ này. \r\n\t____________________________________________________________ \r\n\tTuy nhiên nền tảng công nghệ tại Việt Nam còn cần nhiều đầu tư và học hỏi để thực sự đưa công nghệ trí tuệ nhân tạo lại gần hơn cuộc sống. Chính vì vậy, Cinnamon AI Labs đã hợp tác cùng Airpoli và UP Coworking Space để đưa một buổi chia sẻ công nghệ vô cùng thú vị tới cộng đồng nghiên cứu công nghệ trí tuệ nhân tạo. \r\n\tBuổi chia sẻ công nghệ #1 có sự góp mặt của Keith Stevens, kỹ sư trưởng của Google Translates, một trong những công ty hàng đầu về công nghệ xử lý ngôn ngữ tự nhiên và máy học. Keith đã tốt nghiệp Tiến sĩ tại ĐH California, chuyên ngành Computer Sciences và Computational Linguistics. Trong 6 năm tại Google, Keith cùng với đội ngũ xây dựng Google Translate như ta thấy ngày nay. Ngoài ra, Keith từng là giảng viên tại ĐH California trong 3 năm. Nội dung buổi chia sẻ: \r\n\t1. Quá trình phát triển của công nghệ dịch (Translation) \r\n\t2. Ứng dụng công nghệ mạng Nơ-ron vào xử lý ngôn ngữ tự nhiên \r\n\t3. Một số công nghệ khác để phát triển xử lý ngôn ngữ tự nhiên \r\n\t4. Hỏi/đáp cùng diễn giả \r\n\t*Ghi chú: \r\n\t- Có giá vé ưu đãi đặc biệt dành cho học sinh/ sinh viên. Xin vui lòng mang theo thẻ SV khi làm thủ tục check-in trước sự kiện. \r\n\t- Nếu là thành viên của UP Coworking Space, xin vui lòng kiểm tra địa chỉ email hoặc gửi tin nhắn tới fanpage của UP (https://www.facebook.com/up.coworkingspace) để nhận mã giảm giá 100%. Xin vui lòng mang theo thẻ thành viên của UP khi làm thủ tục check-in trước sự kiện. \r\n\t- Nếu là thành viên của Intelligence Program, xin vui lòng kiểm tra tin nhắn nội bộ trên Slack để nhận mã giảm giá 100% \r\n\t- Toàn bộ tiền bán vé sẽ được sử dụng vào chi trả các chi phí tổ chức sự kiện cho cộng đồng \r\n\t  Thông tin vé \r\n                                                            Tiêu Chuẩn\r\n                                                         100.000 VND Vé ngừng bán online \r\n                                                            Dành cho Sinh Viên (Yêu cầu thẻ SV)\r\n                                                         50.000 VND Vé ngừng bán online \r\n                                                            Xin vui lòng mang theo thẻ SV khi làm thủ tục check-in. Nếu anh/chị không đưa được thẻ SV khi làm thủ tục, BTC xin vui lòng phụ thu thêm 50.000 VNĐ/vé\r\n                                                         Nhà tổ chức Cinnamon AI Labs, Airpoli và UP Coworking Space \r\n                \t\tCinnamon AI Labs là vườn ươm các chuyên viên nghiên cứu tài năng trong lĩnh vực ứng dụng công nghệ trí tuệ nhân tạo. Cinnamon AI Labs phát triển các chương trình đào tạo, dự án nghiên cứu và các hoạt động xây dựng cộng đồng nhằm mang lại nhiều lộ trình sự nghiệp khác nhau cho các kỹ sư nghiên cứu. Airpoli là ứng dụng học ngoại ngữ dành cho học viên trung cấp. Bằng công nghệ xử lý ngôn ngữ tự nhiên, cá nhân hoá trải nghiệm học một cách thông minh và thiết kế theo dạng trò chơi, Airpoli mong muốn giúp hàng triệu học viên thông thạo ngoại ngữ. UP là không gian làm việc chung với sứ mệnh nâng cánh cho StartUp Việt phát triển khỏe mạnh và đạt mục tiêu một cách nhanh nhất qua gói hỗ trợ giá tốt nhất thị trường và môi trường kết nối với mạng lưới trong giới đầu tư, kinh doanh và công nghệ.\r\n                \t \r\n                                    Liên hệ nhà tổ chức\r\n                                 Liên hệ nhà tổ chức Chọn chủ đề Thông tin vé Hợp tác kinh doanh Nội dung sự kiện Chương trình khuyến mãi Soát vé Câu hỏi chung Gửi lời nhắn Đóng News Techtalk #1: Giao lưu với kỹ sư Google Translate về công nghệ xử lý ngôn ngữ tự nhiên và máy học 06:30 PM - 09:00 PM UP Coworking Space Tầng 8 toà nhà Creative City, số 1 Lương Yên, Quận Hai Bà Trưng, Hà Nội Từ  50.000 VND Sự kiện đã kết thúc Chia sẻ  Thêm lịch Google Calendar Outlook Calendar iCal Sự kiện của bạn đã được tạo thành công Chúng tôi sẽ thông báo cho bạn qua email nếu sự kiện này được đưa lên danh sách sự kiện của TicketBox. \r\n                    Một số cách để quảng bá cho sự kiện của bạn :\r\n                 Đây là đường link đến sự kiện của bạn https://ticketbox.vn/event/trao-doi-voi-ky-su-google-translate-62851/37487 Bạn có thể: \r\n                                        Quảng bá đường link này trên các kênh truyền thông và mạng xã hội.\r\n                                     \r\n                                        Gửi email giới thiệu sự kiện này cho bạn bè kèm theo đường link.\r\n                                     Chia sẻ lên Facebook\r\n                                     Cám ơn bạn đã tạo sự kiện trên TicketBox.vn, sự kiện của bạn đã sẵn sàng để bán vé. OK! Nhấn vào đây  để tìm hiểu cách bán vé trực tiếp từ website của bạn. Hotline hỗ trợ \r\n                            Hồ Chí Minh:  Thứ 2 - Thứ 6 (8:30 - 19:30) 028.7300.7998 \r\n                            Hà Nội:  Thứ 2 - Thứ 6 (9:30 - 18:30) 024.7300.1235 Email hỗ trợ\r\n  support@ticketbox.vn Chúng tôi giúp gì được cho bạn? Dễ dàng - Tiện lợi - Bảo mật cao Văn phòng Hồ Chí Minh\r\n Tầng 1, Cao ốc văn phòng H3, 384 Hoàng Diệu, Q.4 Tầng 3A, 159 Nguyễn Du, Phường Bến Thành, Q.1 Văn phòng Hà Nội Phòng 608, Lầu 6, Cao ốc 142 Lê Duẩn, phường Khâm Thiên, Quận Đống Đa, Hà Nội. Hướng dẫn mua vé Chỉ với vài thao tác đơn giản Về công ty chúng tôi Thông tin về TicketBox Thông tin tuyển dụng Quy chế hoạt động sàn TMĐT Cơ chế giải quyết tranh chấp/ khiếu nại FAQ Câu hỏi thường gặp Đăng ký nhận email Start time must be less than end time Ticketbox Châu Á Vietnam Thailand Singapore Ứng dụng Ticketbox available ON App store ANDROID APP ON Google play Ứng dụng check-in Desktop APP Multi platform MOBILE APP Android and iOS Follow Us Ngôn ngữ \r\n                    Hệ thống quản lý và phân phối vé sự kiện hàng đầu châu Á.   TicketBox Co. Ltd. © 2016\r\n                 \r\n                    Giấy phép Kinh doanh số 0313605444\r\n\r\n                     \r\n                    do Sở Kế hoạch & Đầu tư TPHCM cấp ngày 07/01/2016\r\n                 Đăng nhập Cần tạo tài khoản? Đăng ký Đăng nhập bằng Facebook hoặc Oops! \r\n                                Ghi nhớ\r\n                             Quên mật khẩu Đăng nhập Đóng Đăng ký Bạn đã có tài khoản? Đăng nhập Đăng ký bằng Facebook  Hoặc đăng ký tài khoản trên TicketBox Opps! Bằng việc đăng ký và sử dụng trang web, bạn đã đồng ý với các  Điều khoản & Quy chế Hoàn tất Cám ơn bạn đã đăng ký! Cám ơn bạn đã đăng ký tài khoản trên TicketBox! Chúng tôi đã gửi email xác nhận đến địa chỉ email  Quay lại Đăng nhập Đóng Quên mật khẩu Or Đăng nhập Opps! Hoàn tất \r\n                        Mật khẩu mới đã được gửi đến email của bạn.\r\n                     Quay lại Đăng nhập Đóng Tạo sự kiện Opps! Nhạc sống Văn hóa nghệ thuật Sân khấu Nightlife Ngoài trời Hội thảo Khóa học Hội chợ Hội họp Thể thao Cộng đồng Vui chơi giải trí Tạo sự kiện  Đóng",
          "relevance": "0",
          "title": "Techtalk #1: Giao lưu với kỹ sư Google Translate về công nghệ xử lý ngôn ngữ tự nhiên và máy học",
          "url": "https://ticketbox.vn/event/trao-doi-voi-ky-su-google-translate-62851/37487"
        }
      ]
    },
    {
      "query": "Mô hình không gian vector để biểu diễn văn bản",
      "description": "Tìm hiểu cách biểu diễn văn bản bằng mô hình không gina vector, ưu nhược điểm, ứng dụng của nó.",
      "sites": [
        {
          "title": "Cơ sở dữ liệu GIS",
          "content": "TRANG CHỦ THIẾT BỊ TRẮC ĐỊA DỊCH VỤ ĐO ĐẠC Kiến thức Trắc Địa Trắc địa công trình Trắc địa địa hình Trắc địa cao cấp Trắc địa địa chính Hệ thông tin địa lý và GIS Hướng dẫn sử dụng Liên hệ Hỗ Trợ Bản đồ đường đi Chính sách bảo hành Chính sách vận chuyển Đăng ký mua máy đo đạc trắc địa Hướng dẫn thanh toán Sửa máy toàn đạc điện tử Kiểm định máy toàn đạc điện tử uy tín chất lượng Search for: Log in / Sign up \n\t\t\t\t\t\tMy cart\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t0\t\t\t\t\t\t\t Giao hàng Toàn Quốc Phone: 0435 627450 HotLine: 08888 11 666 Share  Log in  / Sign up Username Password * Required Forgot Your Password? New Customer ? Sign up All Categories Xe Đo Khoảng Cách Thiết Bị Định Vị GPS Phụ Kiện Trắc Địa Máy Toàn Đạc Điện Tử Máy Thủy Bình Laser Máy Thủy Bình Máy Kinh Vĩ Máy Đo Sâu Hồi Âm Máy Đo Khoảng Cách Máy dò tìm đường ống ngầm Máy Bộ Đàm Search for: Categories Xe Đo Khoảng Cách Thiết Bị Định Vị GPS Phụ Kiện Trắc Địa Máy Toàn Đạc Điện Tử Máy Thủy Bình Laser Máy Thủy Bình Máy Kinh Vĩ Máy Đo Sâu Hồi Âm Máy Đo Khoảng Cách Máy dò tìm đường ống ngầm Máy Bộ Đàm TRANG CHỦ THIẾT BỊ TRẮC ĐỊA Máy đo đạc Máy thủy bình Máy thủy bình laser bắn cốt Máy kinh vĩ Máy toàn đạc điện tử Máy đo khoảng cách cầm tay Máy đo sâu hồi âm Thiết bị cầm tay Thiết bị định vị GPS Máy bộ đàm cầm tay Phụ kiện trắc địa Thước thép Bánh xe đo khoảng cách DỊCH VỤ ĐO ĐẠC Kiến thức Trắc Địa Trắc địa công trình Trắc địa địa hình Trắc địa cao cấp Trắc địa địa chính Hệ thông tin địa lý và GIS Hướng dẫn sử dụng Liên hệ Hỗ Trợ Bản đồ đường đi Chính sách bảo hành Chính sách vận chuyển Đăng ký mua máy đo đạc trắc địa Hướng dẫn thanh toán Sửa máy toàn đạc điện tử Kiểm định máy toàn đạc điện tử uy tín chất lượng Cơ sở dữ liệu GIS May do dac Le Linh » Hệ thông tin địa lý và GIS » Cơ sở dữ liệu GIS Previous Next Nguyễn Thùy Linh \r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t31 Jul, 2015\t\t\t\t\t\t\t\t\t\t\t\t \r\n\t\t\t\t\t\t\t\t\t\t\t\t\t0\t\t\t\t\t\t\t\t\t\t\t\t\tcomment(s)\t\t\t\t\t\t\t\t\t\t\t\t  Categories: Hệ thông tin địa lý và GIS ,  Kiến thức Trắc địa Cơ sở dữ liệu : là tập hợp các thông tin được thu thập theo mục đích sử dụng nào đó, được lưu trữ trong máy tính theo những quy tắc nhất định. Đó là tập hợp dữ liệu mà ta có thể điều khiển, lưu trữ một số lượng lớn dữ liệu và dữ liệu có thể chia sẻ giữa các ứng dụng khác nhau. Nội dung bài viết 1  Cấu trúc cơ sở dữ liệu GIS 1.1  1. Dữ liệu không gian 1.2  2. Dữ liệu thuộc tính 2  Tổ chức cơ sở dữ liệu GIS 2.1  Bài viết mới Cấu trúc cơ sở dữ liệu GIS Cấu trúc dữ liệu  GIS  gồm 2 phần cơ bản là dữ liệu không gian (dữ liệu bản đồ) và dữ liệu thuộc tính (dữ liệu phi không gian). Cấu trúc dữ liệu GIS 1. Dữ liệu không gian Cơ sở dữ liệu không gian chứa đựng những thông tin định vị của các đối tượng, cho biết vị trí, kích thước, hình dạng, sự phân bố… của các đối tượng. Các đối tượng không gian được định dạng về 3 loại: đối tượng dạng điểm, dạng đường và dạng vùng. Dữ liệu không gian có hai mô hình lưu trữ:  mô hình dữ liệu raster  và  mô hình dữ liệu vector . Cấu trúc dữ liệu Raster và Vector Mô hình dữ liệu Vector : thông tin về điểm, đường, vùng được mã hóa và lưu dưới dạng tập hợp các tọa độ x,y. Đối tượng dạng điểm lưu dưới dạng tọa độ (x,y). Đối tượng dạng đường như đường giao thông, sông suối… được lưu dưới dạng tập hợp các toạ độ điểm x 1 y 1 ,x 2 y 2 , …, x n y n  hoặc là một hàm toán học, tính được chiều dài. Đối t­ượng dạng vùng như­ khu vực buôn bán, nhà cửa, thủy hệ… được lư­u như­ một vòng khép kín của các điểm tọa độ, tính được chu vi và diện tích vùng. Biểu diễn thông tin điểm, đường, vùng theo cấu trúc vector Mô hình dữ liệu Raster : Trong cấu trúc dữ liệu Raster, đối tượng được biểu diễn thông qua các ô (cell) hay ô ảnh (pixel) của một lưới các ô. Trong máy tính, các ô lưới này được lưu trữ dưới dạng ma trận trong đó mỗi ô lưới là giao điểm của một hàng và một cột trong ma trận. Điểm được xác định bởi một pixel (giá trị nhỏ nhất trong cấu trúc Raster), đường được xác định bởi một chuỗi các ô có cùng thuộc tính kề nhau có hướng nào đó, còn vùng được xác định bởi một số các pixel cùng thuộc tính phủ lên trên một diện tích nào đó. Cấu trúc dữ liệu Raster Mối quan hệ logic giữa vị trí của các đối tượng trong cấu trúc dữ liệu được gọi là  TOLOGY . Cấu trúc dữ liệu thuộc topology cung cấp một cách tự động hóa để xử lý việc số hóa, xử lý lỗi; giảm dung lượng lưu trữ dữ liệu. 2. Dữ liệu thuộc tính Cơ sở dữ liệu thuộc tính lưu trữ các số liệu mô tả các đặc trưng, tính chất, … của đối tượng nghiên cứu. Các thông tin này có thể là định tính hay định lượng, được lưu trữ trong máy tính như là tập hợp các con số hay ký tự; ở dạng văn bản và bảng biểu. Thông thường, dữ liệu thuộc tính là các thông tin chi tiết cho đối tượng hoặc các số liệu thống kê cho đối tượng. Các dữ liệu thuộc tính chủ yếu được tổ chức thành các bảng dữ liệu, gồm có các cột dữ liệu (trường dữ liệu): mỗi cột diễn đạt một trong nhiều thuộc tính của đối tượng; và các hàng tương ứng với một bản ghi: gồm toàn bộ nội dung thuộc tính của một đối tượng quản lý. Tổ chức cơ sở dữ liệu GIS Cơ sở dữ liệu là một gói dữ liệu được tổ chức dưới dạng các Layer. Các Layer có thể được tạo ra từ nhiều khuôn dạng dữ liệu khác nhau như: Shape files, personal geodatabase, ArcInfo cover datasets, CAD drawings, SDE databases, photo, image. Hiện nay, theo các chuẩn dữ liệu ISO-TC 211 và chuẩn dữ liệu của Bộ Tài nguyên và Môi trường, dữ liệu được tổ chức theo khuôn dạng chuẩn là  GeoDatabase .   Tác giả bài viết:  Nguyễn Thùy Linh Nguồn tin:  http://tracdiapro.com Bài viết mới Phương pháp xây dựng mô hình số độ cao  (0) Phương pháp xây dựng mô hình số độ cao\r\nPhương pháp chụp ảnh lập thể:\r\nPhương pháp này dùng một dụng cụ chụp ảnh chuyên dùng để chụp một […] Posted in  Trắc địa địa hình ,  Hệ thông tin địa lý và GIS Ứng dụng của DEM trong GIS  (0) Một số ứng dụng của DEM trong GIS\r\nNgày nay với sự phát triển của công nghệ thông tin và sự ra đời của hệ thống thông tin địa lý toàn cầu […] Posted in  Kiến thức Trắc địa ,  Hệ thông tin địa lý và GIS Phương pháp biểu thị mô hình số độ cao  (0) Phương pháp biểu thị mô hình số độ cao\r\nSự biến đổi giá trị độ cao địa hình trên một vùng đất có thể được mô hình hóa theo nhiều cách. DEM […] Posted in  Kiến thức Trắc địa ,  Trắc địa địa hình ,  Hệ thông tin địa lý và GIS Đo GPS tuyệt đối  (0) Posted in  Kiến thức Trắc địa ,  Trắc địa địa chính ,  Hệ thông tin địa lý và GIS ,  Trắc địa cao cấp Phương pháp giải đoán ảnh viễn thám và lập bản đồ chuyên đề  (0) Công nghệ viễn thám và GIS là một công cụ hữu hiệu giúp cho các nhà khoa học, đặc biệt là các nhà địa lý, nghiên cứu, điều tra tài nguyên […] Posted in  Kiến thức Trắc địa ,  Trắc địa địa chính ,  Hệ thông tin địa lý và GIS Cơ sở dữ liệu bản đồ số địa chính  (0) Cơ sở dữ liệu bản đồ số địa chính\r\nCơ sở dữ liệu bản đồ số địa chính là một tập hợp số liệu được chọn và phân chia bởi người sử dụng. Đó […] Posted in  Kiến thức Trắc địa ,  Trắc địa địa chính Cơ sở dữ liệu GIS 1  (20%)  1  vote     Bạn đang ở: Trang chủ » Hệ thông tin địa lý và GIS »  Cơ sở dữ liệu GIS Bài viết mới Các loại máy thủy bình Oct 21, 2017 Đo khoảng cách bằng máy thủy bình Oct 19, 2017 Phần mềm Geosurvey, Phần mềm tìm điểm theo VN2000 Oct 11, 2017 Lấy tọa độ trên Google Map Sep 8, 2017 < > Be first to comment  Cancel reply Your name Your email Website Your message Sent Chuyên mục Hot Kiến thức chung Hướng dẫn sử dụng Phần mềm xử lý số liệu Trắc Địa Bài viết quan tâm nhiều \n                  Đo diện tích đất bằng máy định vị GPS cầm tay                 \n                Bạn đang chuẩn bị mua một quả đồi hay một...                                   Xem chi tiết → \n                  Phần mềm Geosurvey, Phần mềm tìm điểm theo VN2000                 Xem chi tiết → \n                  Cách bố trí nhà xưởng bằng máy toàn đạc điện tử                 \n                Khi đã có bảng tọa độ điểm trong tay và...                                   Xem chi tiết → \n                  Số hóa bản đồ bằng Microstation                 \n                Nội dung bài viết1 Hướng dẫn số hóa bản đồ...                                   Xem chi tiết → \n                  Tạo Topology trong Famis                 \n                Hướng dẫn tạo Topology trong Famis Topology là một mô...                                   Xem chi tiết → \n                  Kiểm tra và hiệu chỉnh máy kinh vĩ điện tử                 \n                Bạn là người lần đầu tiên sử dụng máy kinh...                                   Xem chi tiết → 21 Oct \n\t\t\t\t\t\t\t\t\t\tCác loại máy thủy bình\t\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t [...]\t\t\t\t\t\t\t\t 20 Oct \n\t\t\t\t\t\t\t\t\t\tĐo diện tích đất bằng máy định vị GPS cầm tay\t\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\tBạn đang chuẩn bị mua một quả đồi hay một... [...]\t\t\t\t\t\t\t\t 19 Oct \n\t\t\t\t\t\t\t\t\t\tĐo khoảng cách bằng máy thủy bình\t\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\tĐo khoảng cách bằng máy thủy bình là dựa vào... [...]\t\t\t\t\t\t\t\t 13 Oct \n\t\t\t\t\t\t\t\t\t\tHướng dẫn trút dữ liệu từ máy toàn đạc Geomax\t\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\tHiện tại các dòng máy toàn đạc đều được trang... [...]\t\t\t\t\t\t\t\t < > Bình luận mới Máy trắc địa Lê Linh  on  Dịch vụ đo đạc Máy trắc địa Lê Linh  on  Máy thủy bình Hồng Minh  on  Máy thủy bình Phái  on  Máy thủy bình Nam  on  Máy thủy bình tintucadmin  on  Máy toàn đạc điện tử chính hãng Lee Cong  on  Máy thủy bình DỊCH VỤ ĐO ĐẠC DỊCH VỤ SỬA CHỮA Dịch vụ sửa máy Trắc Địa  Máy thủy bình Máy thủy chuẩn Nikon Máy thủy chuẩn Topcon Máy thủy bình Sokkia Máy đo chênh cao Leica Auto Level Pentax Bản đồ Trắc Địa Lê Linh Trắc địa Lê Linh mang đến cho quý khách hàng dịch vụ mua máy đo đạc trắc địa thuận lợi hỗ trợ sản phẩm dài lâu cùng với những giải pháp đo đạc hợp lý nhất hiện nay Máy toàn đạc điện tử Máy toàn đạc Topcon Máy toàn đạc điện tử Nikon Máy toàn đạc điện tử Sokkia Máy toàn đạc Leica Total Station Geomax Thông tin Công Ty CÔNG TY TRẮC ĐỊA LÊ LINH Giám đốc: Nguyễn Văn Doanh Trụ sở chính: Số 125 phố Láng Hạ, P. Láng Hạ, Q. Đống Đa, Tp. Hà Nội. Điện thoại: (024) 3 562 7450 Hotline: 0985.565.789 Email: lelinh203@gmail.com Gọi điện thoại VP Đại diện 120D Trần Phú, Q.Ninh Kiều, TP.Cần Thơ Máy kinh vĩ điện tử Máy kinh vĩ điện tử Nikon Máy kinh vĩ điện tử Sokkia Máy kinh vĩ điện tử Leica Máy kinh vĩ điện tử Pentax Máy kinh vĩ điện tử Geomax Tracdiapro.com Tracdiapro.com là chuyên trang giới thiệu về các thiết bị máy trắc địa cùng với   kiến thức trắc địa   của  Công ty đo đạc  Lê Linh, là đơn vị hàng đầu về cung cấp các thiết bị máy đo đạc, máy trắc địa mang đến trải nghiệm mới về các dịch vụ trắc địa cho người dùng, cung cấp kiến thức trắc địa cho người sử dụng Chính sách & Dịch vụ Giới thiệu về chúng tôi Hướng dẫn mua hàng Hình thức thanh toán Chính sách bảo hành Chính sách vận chuyển Bảo mật thông tin Hình thức đổi trả Thuê máy thủy chuẩn Từ khóa hot máy toàn đạc điện tử Topcon ,  máy toàn đạc điện tử Leica ,  Máy toàn đạc điện tử Nikon ,  máy đo cao độ tại hải phòng Công ty TNHH Trắc Địa LÊ LINH Số 125 phố Láng Hạ, P. Láng Hạ, Q. Đống Đa, Tp. Hà Nội. Đại diện: Ông Nguyễn Văn Doanh Mã số thuế: 0105390653 . Cấp ngày:01/07/2011. tại Phòng đăng ký kinh doanh Sở Kế hoạch và Đầu tư Thành phố Hà Nội Email: Lelinh203@gmail.com . Điện thoại: (04) 3 562 7450 \n\t\t\t\t\t\t\t\t\t\t\t\t\t\tCopyright © 2014 Trac dia Le Linh .    125 Lang Ha Street- Dong Da- Ha Noi -Viet Nam Họ và Tên Địa chỉ Số điện thoại Yêu cầu đặt hàng",
          "url": "http://tracdiapro.com/co-so-du-lieu-gis/",
          "relevance": "0"
        },
        {
          "title": "Đồ án Text categorization phân loại văn bản",
          "content": "Đăng ký Đăng nhập Liên hệ Đồ án, luận văn, do an, luan van Thư viện đồ án, luận văn, tiểu luận, luận án tốt nghiệp, thạc sĩ, tiến sĩ, cao học Trang Chủ Tài Liệu Upload Đồ án Text categorization phân loại văn bản \r\n                    Phân loại văn bản là một vấn đề quan trọng trong lĩnh vực xử lý ngôn ngữ. Nhiệm vụ của bài toán này là gán các tài liệu văn bản vào nhóm các chủ đề cho trước. Đây là một bài toán rất thường gặp trong thực tế điển hình như : một nhà chuyên phân tích thị thường chứng khoán, anh ta cần phải tổng hợp rất nhiềutài liệu, bài viết về thị trường chứng khoán để đọc và đưa ra phán đoán của mình. Tuy nhiên, anh ta không thể đọc tất cả các bài viết, bài báo hay các tài liệu để rồi phân loại chúng đâu là tài liệu chứng khoán sau đó anh ta mới đọc kỹ chúng cho mục đích của anh ta.\r\n                 38 trang  |  Chia sẻ:  vietpd  | Ngày: 30/08/2013  | Lượt xem: 689  | Lượt tải: 0 Bạn đang xem nội dung tài liệu  Đồ án Text categorization phân loại văn bản , để tải tài liệu về máy bạn click vào nút DOWNLOAD ở trên  ĐẠI HỌC QUỐC GIA TP. HỒ CHÍ MINH \r\nTRƯỜNG ĐẠI HỌC KHOA HỌC TỰ NHIÊN \r\nKHOA CÔNG NGHỆ THÔNG TIN \r\nNGUYỄN MINH THÀNH – 10 12 042 \r\nĐỒ ÁN MÔN HỌC \r\nXỬ LÝ NGÔN NGỮ TỰ NHIÊN \r\nĐề tài: \r\nText Categorization \r\nPhân Loại Văn Bản (Chương 16) \r\nDựa trên tài liệu: \r\nFoundations Of Statistical Natural Language \r\nProcessing \r\nChristopher D.Manning, Hinrich Schutze \r\nTP.HCM – 01/2011 \r\nMỤC LỤC \r\n1. Tóm tắt đồ án ................................................................................................... 1 \r\n2. Bài toán phân loại văn bản............................................................................... 2 \r\n2.1 Giới thiệu ................................................................................................... 2 \r\n2.2 Phát biểu bài toán ...................................................................................... 2 \r\n2.3 Mô hình tổng quát ...................................................................................... 3 \r\n2.3.1 Giai đoạn huấn luyện ........................................................................... 4 \r\n2.3.2 Giai đoạn phân lớp .............................................................................. 5 \r\n2.4 Tiền xử lý văn bản ..................................................................................... 6 \r\n2.5 Phương pháp biểu diễn văn bản ................................................................ 7 \r\n2.5.1 Mô hình không gian vector .................................................................. 7 \r\n2.5.2 Khái niệm trọng số ............................................................................... 7 \r\n2.6 Đánh giá bộ phân lớp ................................................................................. 9 \r\n2.6.1 Macro-Averaging ............................................................................... 11 \r\n2.6.2 Micro-Averaging ................................................................................ 11 \r\n3. Các phương pháp phân loại văn bản ............................................................. 12 \r\n3.1 Thuật toán Naïve Bayes ........................................................................... 12 \r\n3.1.1 Định lý ............................................................................................... 12 \r\n3.1.2 Thuật toán ......................................................................................... 13 \r\n3.1.3 Áp dụng trong phân loại văn bản ....................................................... 15 \r\n3.2 Cây quyết định (Decision Tree) ................................................................ 18 \r\n3.2.1 Khái niệm .......................................................................................... 18 \r\n3.2.2 Thuật toán xây dựng cây ................................................................... 19 \r\n3.2.2.1 Thuật toán ID3 ............................................................................ 19 \r\n3.2.2.2 Các độ đo trong thuật toán : ........................................................ 20 \r\n3.2.2.3 Ví dụ ........................................................................................... 20 \r\n3.2.3 Áp dụng vào phân loại văn bản ......................................................... 23 \r\n3.2.3.1 Biểu diễn văn bản ....................................................................... 23 \r\n3.2.3.2 Giai đoạn huấn luyện .................................................................. 24 \r\n3.2.3.3 Cross-validation .......................................................................... 28 \r\n3.2.3.4 Giai đoạn phân lớp ..................................................................... 29 \r\n3.3 Mô hình xác xuất Entropy tối đại (Maximum Entropy Modeling) .............. 29 \r\n3.3.1 Entropy .............................................................................................. 29 \r\n3.3.1.1 Khái niệm .................................................................................... 29 \r\n3.3.1.2 Entropy của biến ngẫu nhiên ...................................................... 30 \r\n3.3.2 Áp dụng vào phân loại văn bản ......................................................... 30 \r\n3.3.2.1 Biểu diễn văn bản ....................................................................... 30 \r\n3.3.2.2 Hàm đặc trưng và ràng buộc ...................................................... 31 \r\n3.3.2.3 Một số kí hiệu : ............................................................................ 31 \r\n3.3.2.4 Mô hình ....................................................................................... 31 \r\n3.3.2.5 Thủ tục huấn luyện Generalized iterative scaling ........................ 32 \r\n3.3.2.6 Giai đoạn phân lớp ..................................................................... 34 \r\n5. Tài liệu tham khảo .......................................................................................... 35 \r\n1 \r\n1. Tóm tắt đồ án \r\n Phần này trình bày sơ lược về bài toán “Phân loại văn bản” được đề \r\ncập đến trong cuốn sách “Foundations Of Statistical Natural \r\nLanguage Processing” và các phương pháp để thực thi bài toán phân \r\nloại văn bản theo phương pháp thống kê. \r\nPhân loại văn bản là một vấn đề quan trọng trong lĩnh vực xử lý ngôn ngữ. \r\nNhiệm vụ của bài toán này là gán các tài liệu văn bản vào nhóm các chủ đề cho \r\ntrước. Đây là một bài toán rất thường gặp trong thực tế điển hình như : một nhà \r\nchuyên phân tích thị thường chứng khoán, anh ta cần phải tổng hợp rất nhiều tài \r\nliệu, bài viết về thị trường chứng khoán để đọc và đưa ra phán đoán của mình. \r\nTuy nhiên, anh ta không thể đọc tất cả các bài viết, bài báo hay các tài liệu để rồi \r\nphân loại chúng đâu là tài liệu chứng khoán sau đó anh ta mới đọc kỹ chúng cho \r\nmục đích của anh ta. Lý do của vấn đề này là bởi ví số lượng bào viết, bài báo \r\nhiện nay rất nhiều, đặc biệt là trên internet, nếu để đọc hết được tất cả tài liệu đó \r\nthì sẽ mất rất nhiều thời gian. Một ví dụ khác trong thực tế là việc phân loại spam \r\nmail. Khi một mail được gửi đến hộp thư, nếu để người dùng phải đọc tất cả các \r\nmail thì sẽ tốn rất nhiều thời gian vì spam mail rất nhiều. Vì vậy, cần có một hệ \r\nthống phân loại đâu là spam mail và đâu là mail tốt. \r\nĐể giải bài toán này đã có rất nhiều phương pháp được đưa ra như : thuật \r\ntoán Naïve Bayes, K-NN (K-Nearest-Neighbor), Cây quyết định (Decision Tree), \r\nMạng Neuron nhân tạo (Artificial Neural Network) và SVM (Support Vector \r\nMachine). Mỗi phương pháp đều cho kết quả khá tốt cho bài toán này, tuy nhiên \r\nđể có được sự so sánh đầy đủ, ở các phân sau chúng ta sẽ đi vào chi tiết từng \r\nphương pháp. \r\nĐồ án nêu ra chi tiết các bước thực hiện bài toán “Phân Loại Văn Bản” trong \r\nlĩnh vực xử lý ngôn ngữ tự nhiên và một số cách tiếp cận để giải quyết bài toán \r\ncũng những kết quả đã đạt được dựa trên một số những ví dụ thử nghiệm của tác \r\ngiả trong cuốn sách này. \r\n\r\n2 \r\n2. Bài toán phân loại văn bản \r\n Phần này trình bày về chi tiết các bước thực hiện bài toán phân loại \r\nvăn bản như mô hình biểu diễn, các độ đo cũng như các phương pháp \r\nđánh giá kết quả thực hiện bài toán phân loại văn bản. \r\n2.1 Giới thiệu \r\nNhư đã trình bày ở trên, bài toán phân loại văn bản là một bài toán quan trọng \r\ntrong xử lý ngôn ngữ. Có khá nhiều bài toán phân loại trong lĩnh vực xử lý ngôn \r\nngữ tự nhiên như : gán nhãn từ loại (POS tagging), khử nhập nhằng nghĩa từ \r\nvựng (Word Sense Disambiguation) và gán nhãn ngữ tính từ (Prepositional \r\nPhrase Attachment)… \r\nMỗi bài toán phân loại đều có các đối tượng thao tác khác nhau và mục tiêu \r\nphân loại khác nhau. Trong bài toán gán nhãn từ loại (POS tagging) và khử nhập \r\nnhằng nghĩa từ vựng (Word Sense Disambiguation), thì từ được xem là đối tượng \r\nnội dung cần thao tác (mức độ từ). Trong gán nhãn ngữ tính từ (Prepositional \r\nPhrase Attachment) thì một ngữ là đối tượng nội dung cần thao tác. Còn trong bài \r\ntoán phân loại văn bản thì một văn bản (document hay text) là đối tượng nội dung \r\ncần thao tác. \r\nHình 2.1: Các bài toán phân loại trong xử lý ngôn ngữ tự nhiên \r\n2.2 Phát biểu bài toán \r\nBài toán phân loại văn bản có thể được phát biểu như sau : Cho trước một tập \r\nvăn bản D={d1,d2,…,dn} và tập chủ đề được định nghĩa C={c1,c2,…,cn}. \r\nNhiệm vụ của bài toán là gán lớp di thuộc về cj đã được định nghĩa. Hay nói \r\ncách khác, mục tiêu của bài toán là đi tìm hàm  : \r\n\r\n3 \r\n  : DxC  Boolean \r\n ( ) {\r\n ( ) nếu d thuộc về lớp c \r\n ( ) nếu d không thuộc về lớp c \r\nTrong các thử nghiệm của tác giả, ông đã sử dụng các bài viết tin tức của \r\nhãng tin Reuters, cũng như danh sách các chủ đề tin tức của hãng này. \r\nHình 2.2: Ví dụ về một bản tin tức của Reuters \r\nCác chủ đề tin tức của Hãng Reuters với hơn 100 chủ đề và số lượng bài viết \r\n(văn bản) tác giả sử dụng trong các thử nghiệm này là 12,902 bài viết. \r\n2.3 Mô hình tổng quát \r\nCó rất nhiều hướng tiếp cận bài toán phân loại văn bản đã được nghiên cứu \r\nnhư: tiếp cận bài toán phân loại dựa trên lý thuyết đồ thị, cách tiếp cận sử dụng lý \r\nthuyết tập thô, cách tiếp cận thống kê… Tuy nhiên, tất cả các phương pháp trên \r\nđều dựa vào các phương pháp chung là máy học đó là : học có giám sát, học \r\nkhông giám sát và học tăng cường. \r\n4 \r\nVấn đề phân loại văn bản theo phương pháp thống kê dựa trên kiểu học có \r\ngiám sát được đặc tả bao gồm 2 giai đoạn : giai đoạn huấn luyện và giai đoạn \r\nphân lớp. \r\n2.3.1 Giai đoạn huấn luyện \r\nChúng ta có một tập huấn luyện, mỗi phần tử trong tập huấn luyện được gán \r\nvào một hoặc nhiều lớp mà chúng ta sẽ thể hiện chúng bằng một mô hình mã hoá \r\n(được trình bày chi tiết ở Phương pháp biểu diễn văn bản). Thông thường, mỗi \r\nphần tử trong tập huấn luyện được thể hiện theo dạng ( ⃗⃗⃗ ). Trong đó, là vector \r\nbiểu diễn cho văn bản trong tập huấn luyện. \r\nSau đó, chúng ta định nghĩa một lớp mô hình và một thủ tục huấn luyện. Lớp \r\nmô hình là họ các tham số của bộ phân loại, thủ tục huấn luyện là một giải thuật \r\n(hay thuật toán) để chọn ra một họ các tham số tối ưu cho bộ phân loại. Nhưng \r\nlàm thế nào để đánh giá được họ các tham số là tối ưu ? Câu hỏi này sẽ được \r\ntrình bày trong phần Đánh giá bộ phân lớp. \r\nHình 2.3: Mô hình giai đoạn huấn luyện \r\nĐầu vào : ngữ liệu huấn luyện và thuật toán huấn luyện \r\nĐầu ra : mô hình phân lớp (bộ phân lớp – classifier) \r\nMột ví dụ về một họ các tham số cho bộ phân loại nhị phân : ( ) ⃗⃗ . \r\nỞ đây, bộ phân loại nhị phân chỉ phân loại cho 2 lớp. Chúng ta gọi lớp c1 là lớp \r\nvới các văn bản có ( ) và lớp c2 là lớp với các văn bản có ( ) .Họ các \r\ntham số cần xác định là vector ⃗⃗ và ngưỡng . \r\nChi tiết giai đoạn huấn luyện bộ phân lớp \r\n5 \r\nHình 2.4: Chi tiết giai đoạn huấn luyện \r\nTrong đó : \r\n Ngữ liệu huấn luyện : kho ngữ liệu thu thập từ nhiều nguồn khác \r\nnhau. \r\n Tiền xử lý : chuyển đổi tài liệu trong kho ngữ liệu thành một hình thức \r\nphù hợp để phân loại. \r\n Vector hoá : mã hoá văn bản bởi một mô hình trọng số (chi tiết ở phần \r\nPhương pháp biểu diễn văn bản). \r\n Trích chọn đặc trưng : loại bỏ những từ (đặc trưng) không mang \r\nthông tin khỏi tài liệu nhằm nâng cao hiệu suất phân loại và giảm độ \r\nphức tạp của thuật toán huấn luyện. \r\n Thuật toán huấn luyện : Thủ tục huấn luyện bộ phân lớp để tìm ra họ \r\ncác tham số tối ưu. \r\n Đánh giá : bước đánh giá hiệu suất (chất lượng) của bộ phân lớp (chi \r\ntiết trong phần Đánh giá bộ phân lớp). \r\nThủ tục huấn luyện sẽ được thực thi lặp nhiều lần để tìm họ các tham số tối \r\nưu sau mỗi lần lặp. Tuy nhiên, do ban đầu họ các tham số được gán với một giá \r\ntrị khởi tạo, do đó nếu giá trị khởi tạo ban đầu được gán sai thì kết quả tối ưu của \r\nhọ các tham số có thể chỉ là tối ưu cục bộ. \r\n2.3.2 Giai đoạn phân lớp \r\nSau khi đã hoàn thành giai đoạn huấn luyện, mô hình phân lớp sẽ được áp \r\ndụng cho các văn bản mới cần phân loại. \r\n6 \r\nHình 2.5: Mô hình giai đoạn phân lớp \r\nChi tiết giai đoạn phân lớp \r\nHình 2.6: Mô hình giai đoạn phân lớp \r\n2.4 Tiền xử lý văn bản \r\nVăn bản trước khi được vector hoá, tức là trước khi sử dụng, cần phải được \r\ntiền xử lý. Quá trình tiền xử lý sẽ giúp nâng cao hiệu suất phân loại và giảm độ \r\nphức tạp của thuật toán huấn luyện. \r\nTuỳ vào mục đích bộ phân loại mà chúng ta sẽ có những phương pháp tiền \r\nxử lý văn bản khác nhau, như : \r\n Chuyển vẳn bản về chữ thường \r\n Loại bỏ dấu câu (nếu không thực hiện tách câu) \r\n Loại bỏ các kí tự đặc biệt biệt([ ],[.], [,], [:], [“], [”], [;], [/], [[]], [~], [`], [!], \r\n[@], [#], [$],[%],[^],[&],[*],[(],[)]), các chữ số, phép tính toán số học \r\n Loại bỏ các stopword (những từ xuất hiện hầu hết trong các văn bản) \r\nkhông có ý nghĩa khi tham gia vào phân loại văn bản. \r\n … \r\n7 \r\n2.5 Phương pháp biểu diễn văn bản \r\nMột trong những nhiệm vụ đầu tiền trong việc xử lý phân loại văn bản là chọn \r\nđược một mô hình biểu diễn văn bản thích hợp. Một văn bản ở dạng thô (dạng \r\nchuỗi) cần được chuyển sang một mô hình khác để tạo thuận lợi cho việc biểu \r\ndiễn và tính toán. Tuỳ thuộc vào từng thuật toán phân loại khác nhau mà chúng ta \r\ncó mô hình biểu diễn riêng. Một trong những mô hình đơn giản và thường được \r\nsử dụng trong nhiệm vụ này là mô hình không gian vector. Một văn bản trong \r\nnhiệm vụ này được biểu diễn theo dạng , với là một vector n chiều để đo \r\nlường giá trị của phần tử văn bản. \r\n2.5.1 Mô hình không gian vector \r\nMô hình không gian vector là một trong những mô hình được sử dụng rộng \r\nrãi nhất cho việc tìm kiếm (truy hồi) thông tin. Nguyên nhân chính là bởi vì sự đơn \r\ngiản của nó. \r\nTrong mô hình này, các văn bản được thể hiện trong một không gian có số \r\nchiều lớn, trong đó mỗi chiều của không gian tương ứng với một từ trong văn bản. \r\nPhương pháp này có thể biểu diễn một cách hình tượng như sau : mỗi văn bản D \r\nđược biểu diễn dưới dạng (vector đặc trưng cho văn bản D). Trong đó, \r\n( ), và n là số lượng đặc trưng hay số chiều của vector văn bản, là \r\ntrọng số của đặc trưng thứ i (với 1 i  n). \r\nNhư vậy, nếu trong kho ngữ liệu của quá trình huấn luyện nhiều văn bản, ta kí \r\nhiệu Dj, là văn bản thứ j trong tập ngữ liệu, và vector ⃗⃗ ⃗ ( ) là \r\nvector đặc trưng cho văn bản Dj, và xij là trọng số thứ i của vector văn bản . \r\n2.5.2 Khái niệm trọng số \r\nMột vấn đề quan trọng nữa trong việc biểu diễn một văn bản đó là tính trọng \r\nsố cho vector đặc trưng của văn bản. Có nhiều cách khác nhau để tính trọng số \r\nnày như : \r\n Word frequency weighting \r\n Boolean weighting \r\n tf*idf weighting \r\n8 \r\n Entropy weighting \r\nTuy nhiên, để đơn giản cho vấn đề này, chúng ta sẽ chỉ xem xét cách tính \r\nWord frequency weighting (trọng số tần suất từ) và tf*idf, một cách đơn giản đó là \r\nđếm số từ đó trong văn bản. Tuy nhiên vẫn có nhiều cách khác nhau để tính trọng \r\nsố dạng này. \r\nHình 2.7: Ba giá trị trong cách tính trọng số thuật ngữ (từ) thường dùng \r\nCó ba thông tin được sử dụng trong cách tính trọng số bằng tần suất từ là : \r\nterm frequency (tfij số lần suất hiện của từ wi trong văn bản dj), document \r\nfrequency (dfi số văn bản có chứa từ wi), collection frequency (cfi số lần suất hiện \r\ncủa từ wi trong cả tập ngữ liệu). Trong đó, và ∑ . \r\nThông tin được nắm bắt bởi term frequency là sự nổi bật của thông tin (hay \r\ntừ) trong một văn bản. Term frequency càng cao (số lần xuất hiện càng nhiều \r\ntrong văn bản) thì đó là từ miêu tả tốt cho nội dung văn bản. Giá trị thứ hai, \r\ndocument frequency, có thể giải thích như là một bộ chỉ định nội dung thông tin. \r\nMột từ được tập trung ngữ nghĩa thường xảy ra nhiều lần trong một văn bản nếu \r\nnó cũng xuất hiện trong tất cả các văn bản khác. Nhưng từ không được tập trung \r\nngữ nghĩa trải đều đồng nhất trong tất cả các văn bản. \r\nHãy xem xét một ví dụ sau, kho ngữ liệu của báo New York Times, và hai từ \r\ntry và insurance được thống kế như sau : \r\nHai từ try và insurance có giá trị gần như nhau. Nhưng ngược lại, với giá trị \r\n , từ insurance chỉ xuất hiện trong hẫu như chỉ một nửa kho ngữ liệu. Điều này \r\ngiải thích là bởi vì, từ try có thể được sử dụng trong hầu hết các chủ đề, nhưng từ \r\ninsurance chỉ được dùng để ám chỉ đến một khái niệm nhỏ mà chỉ liên quan đến \r\nmột số lượng nhỏ các chủ đề. Một tính chất nữa của từ được tập trung ngữ nghĩa \r\nđó là, nếu chúng xuất hiện trong một văn bản thì chúng sẽ xuất hiện vài lần. \r\n9 \r\nĐể thể hiện trọng số phản ánh hết thông tin của từ, thường ta sẽ kết hợp cả \r\nhai loại trọng số là và trong một đơn vị chung. Dạng biểu diễn trọng số này \r\nđược gọi là . Công thức kết hợp hai giá trị trọng số : \r\n ( ) {\r\n( ( )) (\r\n) \r\nTrong đó, N là tổng số văn bản. Biểu thức thứ nhất áp dụng cho các từ có \r\nxuất hiện trong văn bản, còn biểu thức thứ hai cho các từ không xuất hiện trong \r\nvăn bản. \r\n2.6 Đánh giá bộ phân lớp \r\nSau khi đã tìm được họ các tham số tối ưu cho bộ phân lớp (hay có thể nói là \r\nbộ phân lớp đã được huấn luyện xong), nhiệm vụ tiếp theo là cần phải đánh giá \r\n(kiểm tra) bộ phân lớp đó cho kết quả như thế nào? Tuy nhiên, quá trình kiểm tra \r\nphải được thực hiện trên một tập ngữ liệu khác với tập ngữ liệu huấn luyện, còn \r\nđược gọi với cái tên là tập ngữ liệu kiểm tra (a test set). Việc kiểm tra bộ phân lớp \r\nlà một sự đánh giá trên một tập ngữ liệu chưa được biết vì thế đó là sự đo lường, \r\nđánh giá duy nhất cho biết khả năng thực sự của một bộ phân lớp. \r\nĐể đơn giản, ta sẽ xem xét một bộ phân lớp nhị phân (phân hai lớp). Những \r\nbộ phân lớp thường được đánh giá bằng cách lập một bảng thống kê sau : \r\nTrong đó, \r\n a : là số lượng đối tượng thuộc về lớp đang xét và được bộ phân lớp \r\ngán vào lớp. \r\n b : là số lượng đối tượng không thuộc về lớp đang xét nhưng được bộ \r\nphân lớp gán vào lớp. \r\n c : là số lượng đối tượng thuộc về lớp đang xét nhưng được bộ phân \r\nlớp loại khỏi lớp. \r\n10 \r\n d : là số lượng đối tượng không thuộc về lớp đang xét và được bộ phân \r\nlớp loại khỏi lớp. \r\nĐể đánh giá chất lượng bộ phân lớp, hai đơn vị đo lường quan trọng đó là độ \r\nđúng đắn (accuracy) được đo bằng công thức \r\n và độ sai lỗi (Error) được \r\ntính bằng công thức \r\n. Độ đo này phản ánh đầy đủ chất lượng của bộ phân \r\nlớp. Tuy nhiên, khi đánh giá bộ phân lớp, thường người ta chỉ xem xét những đối \r\ntượng thuộc về lớp và được phân lớp đúng, còn những đối tượng không thuộc về \r\nlớp thường sẽ ít được quan tâm. Do đó, một số độ đo khác đã được định nghĩa. \r\nCác độ đo bao gồm : \r\n Precision (độ chính xác) : \r\n Recall (Độ bao phủ, độ đầy đủ) : \r\n Fallout (Độ loại bỏ) : \r\nTuy nhiên, trong một số trường hợp thực tế, nếu tính độ precision và độ recall \r\nriêng rẽ sẽ cho kết quả không cân đối. Do đó, để thuận tiện, người ta kết hợp hai \r\nđộ đo này vào một đơn vị đo tổng quát duy nhất. Để làm điều này, người ta sử \r\ndụng đơn vị đo lường F được định nghĩa như sau : \r\n( )\r\nTrong đó: \r\n P là độ chính xác Precision \r\n R là độ bao phủ Recall \r\n  là một hệ số xác định sự cân bằng của độ quyết định và độ bao phủ. \r\nGiá trị =5 thường được chọn cho sự cân bằng giữa P và R. Với giá trị \r\nnày độ đo được tính đơn giản là ( ). \r\nNhững độ đo trên được dùng để đánh giá cho những bộ phân lớp nhị phân \r\n(phân hai lớp). Tuy nhiên, trong thực tế, thường các bộ phân lớp phải phân chia \r\nnhiều lớp, chính vì vậy để đánh giá tổng thể toàn bộ các lớp phân loại, sau khi lập \r\n11 \r\nbảng thống kê cho từng lớp, hai phương pháp nữa đã được áp dụng để đánh giá \r\nđó là micro-averaging và macro-averaging. \r\n2.6.1 Macro-Averaging \r\nĐây là phương pháp tính trung bình các độ đo precious và recall của từng lớp. \r\nCác lớp sau khi đã lập bảng thống kê và tính các độ đo precious và recall cho \r\ntừng lớp. Các độ đo này sẽ được tính trung bình lại. \r\nCông thức tính macro-averaging : \r\n| |\r\n∑\r\n| |\r\n| |\r\n∑\r\n| |\r\nTrong đó : |C| là số lớp cần phân loại. \r\n2.6.2 Micro-Averaging \r\nĐây là phương pháp tính trung bình các kết quả thống kê của từng lớp. Các \r\nlớp sau khi đã lập bảng thống kê. Các bảng này sẽ được cộng này lại tương ứng \r\ntheo từng ô. Sau đó, sẽ tính độ đo Precision và Recall cho bảng thống kê lớn đó. \r\nCông thức micro-averaging : \r\n∑ \r\n| |\r\n∑ ( )\r\n| |\r\n∑ \r\n| |\r\n∑ ( )\r\n| |\r\n12 \r\n3. Các phương pháp phân loại văn bản \r\n Phần này trình bày một số phương pháp phân loại văn bản phổ biến hiện \r\nnay theo phương pháp thống kê : thuật toán Naïve Bayes, Cây quyết định, \r\nMaximun Entropy Modeling và KNN. Phần này cũng trì Tài liệu liên quan Đề tài Quản lý điểm cho sinh viên 24 trang | Lượt xem: 1819 | Lượt tải: 9 Bài giảng Thiết kế biểu mẫu dùng các điều khiển trong Visual Basic 114 trang | Lượt xem: 965 | Lượt tải: 0 Luận văn Tổ chức và xây dựng bài giảng cho chương trình đào tạo từ xa 81 trang | Lượt xem: 434 | Lượt tải: 0 Thủ thuật sử dụng máy tính 55 trang | Lượt xem: 535 | Lượt tải: 0 Bài giảng Điều khiển cạnh tranh 23 trang | Lượt xem: 569 | Lượt tải: 0 CakePHP Framework: Kỹ thuật sử dụng layout 16 trang | Lượt xem: 1070 | Lượt tải: 0 Hướng dẫn sử dụng Pro/Engineer 2000i 163 trang | Lượt xem: 803 | Lượt tải: 2 Lập trình các ngắt 19 trang | Lượt xem: 730 | Lượt tải: 0 Đề tài Quản lí bệnh viện tỉnh Khánh Hòa 48 trang | Lượt xem: 813 | Lượt tải: 1 Bài giảng Tổng quan về visual basic 6.0 158 trang | Lượt xem: 741 | Lượt tải: 1 \r\n            Copyright © 2016  DoAn.edu.vn \r\n            Thư viện  tài liệu ,  luận văn  tham khảo cho sinh viên.\r\n         \r\n            Chia sẻ: ",
          "url": "http://doan.edu.vn/do-an/do-an-text-categorization-phan-loai-van-ban-23397/",
          "relevance": "1"
        },
        {
          "title": "Seo Website",
          "content": "SEO Website Tư vấn SEO website Dịch vụ SEO bài viết Seo Copywriting SEO Website toàn diện Thiết kế Website Thiết kế website cao cấp Thiết kế website theo công nghệ SEO Thiết kế website theo phong thủy Mẫu thiết kế web Dịch vụ ECVN Tư vấn phát triển website Dịch vụ quản trị website Đăng ký tên miền Bảng giá Hosting Marketing online Giải Pháp SEO Thương mại điện tử Dự án SEO Blog Video Giải thuật SVM có thể áp dụng tạo phần mềm SEO WEBSITE \n\t09 Dec 12 by @dmin in  Blog Bài viết này là nghiên cứu mới của PBS trong việc áp dụng giải thuật SVM đẻ xây dựng phần mềm nhận dạng văn bản hỗ trợ cho SEO website . Chỉ mang giá trị tham khảo cho các bạn nào đang nghiêm cứu về SVM. 1.       Giới thiệu: Bài toán phân loại là một trong những bài toán kinh điển trong lĩnh vực xử lý văn bản. Đã có nhiều công trình nghiên cứu về vấn đề này. Tuy nhiên việc ứng dụng đối với các văn bản Tiếng Việt còn rất hạn chế. Một trong những khó khăn trong việc áp dụng những thuật toán phân loại văn bản vào tiếng Việt là việc phân tách một câu thành các từ một cách chính xác. Một trong những phương pháp được nghiên cứu và áp dụng trong thực tế là phương pháp sử dụng bộ phân loại vector SVM. 2.       Bộ phân loại vector SVM (Support Vector Machine): 2.1.   Tổng quan:  SVM là tập hợp các phương pháp học có giám sát bao gồm phân tích dữ liệu và nhận dạng mẫu. Ý tưởng của phương pháp này là cho trước tập huấn luyện được biểu diễn trong không gian vector, trong đó mỗi văn bản được xem như một điểm trong không gian này. Phương pháp này sẽ tìm ra một siêu phẳng tốt nhất chia các điểm trong không gian thành hai lớp riêng biệt tương ứng. Chất lượng của siêu phẳng được quyết định bởi khoảng cách \n(gọi là biên) của điểm dữ liệu gần nhất của mỗi lớp đến mặt phẳng này. Tức là khoảng cách biên càng lớn thì kết quả phân loại đạt được càng cao. Mục tiêu của thuật toán SVM là tìm được khoảng cách biên lớn nhất để đạt kết quả phân loại cao. Việc tính toán để tìm ra siêu phẳng tối ưu rất phức tạp và khó khăn. Hiện nay đã có những bộ thư viện hỗ trợ việc tính toán trên như: LIBSVM, SVMlight, jSVM,… 2.2.   Phân loại văn bản và SVM: Phân loại văn bản là tiến trình đưa văn bản chưa biết chủ đề vào các lớp văn bản đã biết. Mỗi lĩnh vực được xác định bởi một số tài liệu mẫu của lĩnh vực đó. Trong quá trình phân loại, c ác văn bản được biểu diễn dưới dạng vector với các thành phần (chiều) của vector này là  trọng số của các từ . Một số phương pháp xác định trọng số từ: –          Tần suất từ (TF): trọng số từ là tần suất xuất hiện của từ đó trong tài liệu. Tức một từ là quan trọng trong tài liệu đó khi nó xuất hiện nhiều lần. –          TFIDF: trọng số từ là tích của tần suất từ TF và tần suất nghịch đảo của từ đó và được xác định bằng công thức: IDF = log(N/DF) + 1 Trong đó: N là kích thước tập huấn luyện, DF là số tài liệu mà một từ xuất hiện trong đó. Để tiến hành phân loại văn bản nói chung, chúng ta cần thực hiện các bước sau: –          Bước 1: Rút trích đặc trưng văn bản và biểu diễn văn bản bằng mô hình vector. Để rút trích đặc trưng văn bản, cần thực hiện thao tác tách từ trong văn bản, xác định từ loại của từ và sau đó tiến hành biểu diễn các văn bản bằng mô hình vector. –          Bước 2: Áp dụng thuật toán phân loại. Trong bước này chúng ta sử dụng SVM. 3.       Làm việc với libSVM: 3.1.   Giới thiệu: LIBSVM là bộ phần mềm tích hợp được xây dựng để hiện thực hóa thuật toán SVM. Mục tiêu giúp người dùng có thể sử dụng SVM một cách dễ dàng. LBSVM cung cấp một giao diện đơn giản mà người dùng có thể dễ dàng liên kết nó với chương trình do họ tạo ra. Một số tính năng của LIBSVM: –          Tích hợp các công thức tính toán trong SVM –          Khả năng phân loại đa lớp –          Cho phép lựa chọn mô hình phân loại –          Ước lượng xác suất –          Các kernel –          Hỗ trợ hầu hết các ngôn ngữ phổ biến hiện nay như: C++, Java, C#.Net, Python… –          Tự động lựa chọn mô hình nhằm tạo ra độ chính xác nhất định. 3.2.   Sử dụng LibSVM cho .NET: SVM.Net là phiên bản được chuyển đổi từ phiên bản libSVM dành cho Java. Mang đầy đủ tính năng và hiệu quả giống như phiên bản gốc nhưng cấu trúc được thay đổi cho phù hợp với nền tảng .NET. Việc sử dụng thư viện này khá đơn giản, bạn cần có bộ dữ liệu mẫu và thực hiện các bước sau đây: Các bước căn bản để training và test data với libSVM. //Đầu tiên, đọc file training dữ liệu Problem train = Problem.Read(“a1a.train”); Problem test = Problem.Read(“a1a.test”); //Sử dụng một số tham số mặc định. Parameter parameters = new Parameter(); double C; double Gamma; //Tối ưu hóa để tìm được các giá trị tham số thích hợp nhất //các tham số này được lưu trữ trong C và Gamma. ParameterSelection.Grid(train, parameters, “params.txt”, out C, out Gamma); parameters.C = C; parameters.Gamma = Gamma; //Train model sử dụng các tham số tối ưu. Model model = Training.Train(train, parameters); //Thực hiện phân loại với dữ liệu test,đưa kết quả ra //results.txt Prediction.Predict(test, “results.txt”, model, false); 3.2.1.       Các thành phần trong lớp Problem . Chứa tập hợp các vector được phân loại. Các thuộc tính: –          Count: số lượng vector –          Y: nhãn của lớp vector –          X: dữ liệu dạng vector –          maxIndex: chỉ số lớn nhất trong 1 vector. Các phương thức: –          Phương thức Read(): đọc tập dữ liệu được biểu diễn dưới dạng vector. –          Phương thức Write(): ghi tập dữ liệu dưới dạng vector. 3.2.2.       Các thành phần trong lớp Parameter:  bao gồm các tham số khác nhau có thể ảnh hưởng đến SVM. Có thể sử dụng các giá trị mặc định đã được cung cấp. Các loại SVM model. Có thể lựa chọn model phù hợp để làm giảm thiểu lỗi quá trình phân lớp. Mặc định sử dụng C-SVC. –          C_SVC (C-SVC) –          NU_SVC (nu-SVC) –          ONE_CLASS (one-class SVM) –          EPSILON_SVR (epsilon-SVR) –          NU_SVR (nu-SVR) Kernel là khái niệm được đưa ra để tính độ tương đồng giữa các dữ liệu mẫu, chuyển đổi không gian dữ liệu mẫu để tạo ra siêu phẳng phân lớp. Các loại kernel mà thư viện SVM sử dụng. Có thể chọn lựa các kernel phù hợp. Mặc định là Polynomial. –          LINEAR (Linear: u’*v): được minh họa như sau –          POLY (Polynomial: (gamma*u’*v + coef0)^degree) –          RBF (Radial basis function: exp(-gamma*|u-v|^2)) –          SIGMOID (Sigmoid: tanh(gamma*u’*v + coef0)) –          PRECOMPUTED (Precomputed kernel) Các tham số khác: –          _degree: giá trị degree bậc của kernel (mặc định 3) –          _gamma: sử dụng trong kernel (mặc định 1/k) –          _coef0: sử dụng trong kernel (mặc định 0) –          _cacheSize: kích thước bộ nhớ đệm tính theo MB (mặc định 100) –          _C: tham số trong C-SVC, epsilon-SVR, và nu-SVR (mặc định 1). Giới hạn lỗi ràng buộc. –          _eps: mặc định 0.001. Dung sai của tiêu chí dừng. –          _weights: mặc định là 1 –          _nu: tham số trong nu-SVC, one-class SVM, và nu-SVR (mặc định 5) –          _p: giá trị epsilon trong hàm loss của epsilon-SVR (mặc định 0.1) –          _shrinking: sử dụng công nghệ tự động thu hẹp (mặc định là true) –          _probability: ước tính xác suất cho SVC và SVR (mặc định false). 3.2.3.       Lớp ParameterSelection: –          Phương thức Grid: thực hiện việc chọn lựa các giá trị tham số tối ưu 3.2.4.       Lớp Training: –          Phương thức Train: thực hiện việc training dữ liệu với các tham số đã được tối ưu hóa. 3.2.5.       Lớp Model: –          Chứa các thông tin được trả sau khi training dữ liệu 3.2.6.       Lớp Prediction: –          Phương thức Predict: thực hiện phân loại dữ liệu dựa vào việc so khớp giữa dữ liệu test và dữ liệu lấy từ lớp Model. 4.       Kết luận: Phân loại văn bản với SVM yêu cầu văn bản phải được biểu diễn dưới dạng vector đặc trưng. Vì vậy để có kết quả phân loại tốt cần: –          Dữ liệu huấn luyện chuẩn và đủ lớn. –          Phương pháp tách từ trong văn bản đóng vai trò quan trọng trong quá trình biểu diễn văn bản bằng vector. Share Lên trang đầu Tagged:  seo website ,  SVM Các bài viết liên quan SEO website và các phương pháp thu thập văn bản (SVM) SEO website  và việc thu thập văn bản là một trong những yếu tố ảnh hưởng trực tiếp tác động đến kết quả từ việc xây dựng nội dung cho website. Tài liệu này được trích dẫn từ việc thu thập […] Tâm sự seo website tư vấn sàn gỗ SEO website cho tuvansango.com mang lại cho mình nhiều thích thú, được PR cho sản phẩm mà mình yêu thích, nói thật sự mình vẫn mong muốn có được một căn nhà riêng, sàn nhà được lót bằng […] Giới thiệu về trang Blog… SEO WEBSITE là công việc mệt mỏi về trí óc, nhiều những đêm chúng tôi thức tới sáng, rồi lại mệt mài làm việc vào ngày hôm sau, có những khi nhấm nháp một đêm cũng xong cả bao thuốc cho xả […] Chia sẻ với khách hàng lần đầu tiên SEO web tại EC Viet Nam \r\nĐặt niềm tin vào công ty chịu trách nhiệm SEO web cho mình là điều khó khăn với nhiều khách hàng khi một vài lần thất bại trong việc triển khai dự án SEO website cho mình. Tuy nhiên, cần […] Seo website: Điểm lại một năm thành công của EC Viet Nam Công ty cổ phần Thương Mại Điện Tử  EC Viet Nam được biết đến trong lĩnh vực SEO website như một công ty tiên phong trong việc cung cấp các dịch vụ seo mới dựa trên công nghệ và kỹ thuật […] Seo Website SEO website là gì? Tư vấn SEO website Tư vấn phát triển website Seo Copywriting Dịch vụ SEO bài viết SEO Website toàn diện SEO website dịch vụ SEO website bán hàng Thiết kế web Thiết kế website cao cấp Thiết kế website theo công nghệ SEO Thiết kế website theo phong thủy Kiến thức SEO mới Dịch vụ SEO website toàn diện Thực hiện một chiến dịch seo website cấp tốc cần??? Seo website chiến lược lâu dài hiệu quả cao Seo website cấp tốc chất lượng tốt nhất Cách xếp hạng các Website của Search Engine. Một số hiểu biết về Seo website tổng quát 5 Nguyên tắc cơ bản seo website top 10 dễ dàng Hướng dẫn seo website hiệu quả nhất trong năm 2017 Các cách seo website hiệu quả trên google dễ dàng 6 bước đơn giản để có quy trình seo website hiệu quả nhất. Các cách cơ bản để seo webiste hiệu quả nhất hiện nay Làm thế nào để seo website hiệu quả với những website mới? Làm thế nào để seo website hiệu quả lên top Google? Những chia sẻ hay về seo website uy tín Những yếu tố cần có khi seo bài viết Thương mại điện tử Lưu trữ July 2017 May 2017 April 2017 March 2017 February 2017 May 2015 March 2015 January 2015 December 2014 April 2014 March 2014 January 2014 December 2013 August 2013 July 2013 June 2013 May 2013 April 2013 March 2013 February 2013 January 2013 December 2012 Dịch vụ Blog Dịch vụ ECVN Dự án SEO Giải Pháp SEO Marketing online Seo Copywriting SEO Website Seo Website toàn diện Thiết kế Web Thương mại điện tử Tuyển dụng Uncategorized DỊCH VỤ SEO Tư vấn SEO website SEO website toàn diện SEO Copywriting web Dịch vụ seo bài viết DỊCH VỤ Đăng ký tên miền Cung cấp Hosting Quản trị website Cho thuê website Công ty EC Việt Nam DÀNH CHO KHÁCH HÀNG Cong ty bao ve chuyen nghiep SEO Website congtybaove113.com SEO Website maylanhthienphat.com SEO Website cualuoivietthong.vn WEBSITE CỦA ECVN Thiết kế website SEO WEB VỀ CHÚNG TÔI Thông tin thanh toán Giới thiệu Liên hệ Thiết kế web Quy trình làm việc Hotline: 0908 176 020 Email: cuongnd.ecvn@gmail.com EC Viet Nam SEO Would   Copyright & copy EC Viet Nam 2012. All rights reserved   \n \n\t",
          "url": "http://seowebsite.site/blog/giai-thuat-svm-co-the-ap-dung-tao-phan-mem-seo-website/",
          "relevance": "0"
        },
        {
          "title": "Luận văn Tìm kiếm văn bản theo nội dung trong cơ sở dữ liệu đa phương tiện",
          "content": "Đăng ký Đăng nhập Trợ giúp Thư viện tài liệu, ebook tham khảo dành cho học sinh, sinh viên Thư viện tài liệu trực tuyến miễn phí dành cho các bạn học sinh, sinh viên Trang Chủ Tài Liệu Upload Luận văn Tìm kiếm văn bản theo nội dung trong cơ sở dữ liệu đa phương tiện \r\n                    Cùng với sự phát triển nhanh chóng của công nghệ tin học thì khối lượng dữ\r\nliệu đa phương tiện (Multimedia) được thu thập và lưu trữ dưới dạng số ngày càng\r\nnhiều dẫn tới việc tìm kiếm dữ liệu đa phương tiện trở nên khó khăn vì vậy cần có\r\ncác hệ thống tìm kiếm thông tin (Information Retrieval) hỗ trợ người dùng tìm kiếm\r\nmột cách chính xác và nhanh chóng các thông tin mà họ cần trên kho tư liệu khổng\r\nlồ này.\r\nHiện nay có một số hệ thống tìm kiếm như GoogleDesktop, DTSearch,\r\nLucene, tuy nhiên các hệ thống này sử dung các kỹ thuật tìm kiếm đơn giản nên\r\nhiệu quả còn chưa cao. Vì vậy mục tiêu của luận văn này nhằm tìm hiểu một số kỹ\r\nthuật nâng cao tìm kiếm thông tin, cụ thể ở đây là tìm kiếm văn bản theo nội dung\r\ntrong cơ sở dữ liệu đa phương tiện nhằm đáp ứng nhu cầu cấp thiết của thời đại\r\nbùng nổ thông tin điện tử hiện nay.\r\nBố cục của luận văn gồm các phần sau:\r\n+ CHƯƠNG 1: TỔNG QUAN VỀ HỆ QUẢN TRỊ CSDL ĐA PHƯƠNG TIỆN:\r\nPhần này sẽ giới thiệu tổng quan về hệ quản trị CSDL đa phương tiện.\r\n+ CHƯƠNG 2: MỘT SỐ KỸ THUẬT CHỈ MỤC VÀ TÌM KIẾM VĂN BẢN\r\n- Trình bày các v ấn đềvề hệ tìm kiếm thông tin.\r\n- Trình bày kỹ thuật cơ sở chỉ mục văn bản trên cơ sở mô hình Bool và mô\r\nhình vector.\r\n+ CHƯƠNG 3: MỘT SỐ KỸ THUẬT NÂNG CAO HIỆU NĂNG TÌM KIẾM\r\nVĂN\r\n- Trình bày cơ sở lý thuyết về một số kỹ thuật chỉ mục nâng cao.\r\n- Giới thiệu kỹ thuật chỉ mục nâng cao LSI.\r\n+ CHƯƠNG 4: PHÁT TRIỂN CHƯƠNG TRÌNH THỬ NGHIỆM: Chương này\r\nphát triển chương trình thử nghiệm áp dụng kỹ thuật chỉ mục và kỹ thuật tìm kiếm\r\nvăn bản theo nội dung trong cơ sở dữ liệu đa phương tiện.\r\n+ KẾT LUẬN VÀ HƯỚNG PHÁT TRIỂN: Trình bày các kết quả đạt được trong\r\nluận văn và nêu phương hướng phát triển của đề tài trong tương lai.\r\n+ TÀI LIỆU THAM KHẢO và PHỤ LỤC: Trình bày các thông tin liên quan đến\r\nluận văn.\r\n\r\n\r\nMỤC LỤC\r\nMỤC LỤC 2\r\nDANH MỤC CÁC TỪ TIẾNG ANH VÀ VIẾT TẮT . 5\r\nDANH MỤC CÁC BẢNG . 6\r\nDANH MỤC CÁC HÌNH, ĐỒ THỊ 6\r\nMỞ ĐẦU . 7\r\nCHƯƠNG 1: TỔNG QUAN HỆ QUẢN TRỊ CƠ SỞ DỮ LIỆU ĐA PHƯƠNG\r\nTIỆN (MDBMS) . 8\r\n1.1 Mục đích của MDBMS 8\r\n1.2 Các yêu cầu của một MDBMS 11\r\n1.2.1 Khả năng quản trị lưu trữ lớn 13\r\n1.2.2 Hỗ trợ truy vấn và khai thác dữ liệu 14\r\n1.2.3 Tích hợp các phương tiện, tổng hợp và thể hiện 14\r\n1.2.4 Giao diện và tương tác. 15\r\n1.2.5 Hiệu suất. .15\r\n1.3 Các vấn đề của MDBMS .16\r\n1.3.1 Mô hình hoá dữ liệu MULTIMEDIA 16\r\n1.3.2 Lưu trữ đối tượng MULTIMEDIA .17\r\n1.3.3 Tích hợp Multimedia, thể hiện và chất lượng của dịch vụ (QoS) 19\r\n1.3.4 Chỉ số hoá Multimedia 20\r\n1.3.5 Hỗ trợ truy vấn Multimedia, khai thác và duyệt qua. 21\r\n1.3.6 Quản trị CSDL Multimedia phân tán 22\r\n1.3.7 Sự hỗ trợ của hệ thống 23\r\n1.4 Kết luận 23\r\nCHƯƠNG 2: MỘT SỐ KỸ THUẬT CHỈ MỤC VÀ TÌM KIẾM VĂN BẢN THEO\r\nNỘI DUNG 25\r\n2.1 Giới thiệu hệ tìm kiếm thông tin 25\r\n2.1.1 Kỹ thuật tìm kiếm thông tin 25\r\n2.1.2 Một số vấn đề trong tìm kiếm thông tin 26\r\n2.1.3 Hệ thống tìm kiếm thông tin – IR 27\r\n2.1.4 Sự khác biệt giữa các hệ thống IR và các hệ thống thông tin khác .32\r\n2.1.5 Các hệ tìm kiếm văn bản thường được sử dụng hiện nay 34\r\n2.2 Một số kỹ thuật tìm kiếm văn bản theo nội dung 35\r\n2.2.1 Chỉ mục tự động văn bản và mô hình tìm kiếm Bool 35\r\n2.2.1.1. Mô hình tìm kiếm Bool cơ sở 35\r\n2.2.1.2 Tìm kiếm Bool mở rộng .37\r\n2.2.1.3 Các bước để xây dựng hệ thống tìm kiếm thông tin – IR 39\r\n2.2.1.4 Lập chỉ mục tài liệu 40\r\n2.2.2 Mô hình tìm kiếm không gian vector 51\r\n2.2.2.1 Mô hình tìm kiếm không gian vector cơ sở 51\r\n2.2.2.2. Kỹ thuật phản hồi phù hợp (Relevance Feedback Technique) .53\r\n2.2.3. Thước đo hiệu năng 55\r\n2.3 Ví dụ 56\r\n2.4 Kết luận .58\r\nCHƯƠNG 3: MỘT SỐ KỸ THUẬT NÂNG CAO HIỆU NĂNG TÌM KIẾM VĂN\r\nBẢN .59\r\n3.1 Giới thiệu .59\r\n3.2 Một số kỹ thuật nâng cao hiệu năng tìm kiếm đa phương tiện 60\r\n3.2.1 Lọc bằng phân lớp, thuộc tính có cấu trúc và các từ khóa .60\r\n3.2.2 Các phương pháp trên cơ sở tính không đều tam giác 61\r\n3.2.3 Mô hình tìm kiếm trên cơ sở cụm (cluster-based) .63\r\n3.2.3.1 Sinh cụm .63\r\n3.2.3.2 Tìm kiếm trên cơ sở cụm .64\r\n3.2.4 Chỉ mục ngữ nghĩa tiềm ẩn (LSI) để tìm kiếm thông tin trên cơ sở không\r\ngian vector 64\r\n3.3 Kỹ thuật LSI 66\r\n3.3.1 Giới thiệu LSI 66\r\n3.3.2 Phương pháp luận LSI .67\r\nCHƯƠNG 4: PHÁT TRIỂN CHƯƠNG TRÌNH THỬ NGHIỆM 79\r\n4.1 Giới thiệu bài toán .79\r\n4.2 Chức năng chương trình .79\r\n4.3 Quy trình phát triển ứng dụng 79\r\n4.3.1 Xây dựng ma trận Term – Doc .80\r\n4.3.2 Lập chỉ mục tài liệu 80\r\n4.3.3 Xây dựng ma trận trọng số 80\r\n4.3.4 Tìm kiếm theo mô hình vector 81\r\n4.3.5 Phương pháp LSI .81\r\n4.2 Cài đặt thử nghiệm .82\r\n4.2.1 Giao diện màn hình lập chỉ mục 82\r\n4.2.2 Giao diện màn hình cập nhập chỉ mục .83\r\n4.2.2 Tìm kiếm tài liệu theo mô hình vector 83\r\nKẾT LUẬN VÀ HƯỚNG PHÁT TRIỂN 84\r\nTÀI LIỆU THAM KHẢO 86\r\n                 Các file đính kèm theo tài liệu này: 1LV08_CNTTPhamThiNgoc.pdf Tài liệu liên quan Luận văn Tìm hiểu search engine và ứng dụng thuật toán đối sánh đa mẫu cho hệ thống tìm kiếm thông tin trên mạng 79 trang | Lượt xem: 991 | Lượt tải: 0 Luận văn Nghiên cứu tổng hợp bộ điều chỉnh lai sử dụng trong hệ thống tuỳ động 94 trang | Lượt xem: 568 | Lượt tải: 0 Luận văn Xây dựng công cụ mô hình hóa tiến trình hỗ trợ mẫu tiến trình 57 trang | Lượt xem: 672 | Lượt tải: 0 Luận văn Phân tích, thiết kế hệ thống quản lý nhà đất cấp quận/huyện theo cách tiếp cận hướng đối tượng 98 trang | Lượt xem: 767 | Lượt tải: 1 Luận văn Hệ thống điều khiển động cơ không đồng bộ làm việc ở bốn góc phần tư 121 trang | Lượt xem: 687 | Lượt tải: 0 Đồ án Nghiên cứu quy trình xử lý bã dâu làm phân hữu cơ - Vi sinh 43 trang | Lượt xem: 329 | Lượt tải: 0 Luận văn Điều khiển turbine thuỷ điện 101 trang | Lượt xem: 605 | Lượt tải: 0 Luận án Nén tín hiệu ảnh phục vụ truyền thông tin trên mạng trong điều kiện đường truyền xấu 26 trang | Lượt xem: 617 | Lượt tải: 0 Luận văn Điều tra đa dạng loài và quần xã thực vật của rừng phòng hộ Nam Hòn Khô, thành phố Nha Trang, tỉnh Khánh Hòa 182 trang | Lượt xem: 1690 | Lượt tải: 2 Luận văn Khảo sát thiết kế hệ thống điều khiển thổi bụi lò hơi cho nhà máy nhiệt điện trên nền simatic S7-300 75 trang | Lượt xem: 1199 | Lượt tải: 2 \r\n            Copyright © 2017 Tai-Lieu.com\r\n         \r\n            Chia sẻ: ",
          "url": "http://tai-lieu.com/tai-lieu/luan-van-tim-kiem-van-ban-theo-noi-dung-trong-co-so-du-lieu-da-phuong-tien-13743/",
          "relevance": "1"
        },
        {
          "title": "Tôi là Duyệt",
          "content": "Tôi là Duyệt \nTopic Modeling: Tìm chủ đề cho tập văn bản bài viết\n \nMột công ty A hoạt động trong lĩnh vực nghiên cứu thị trường đã tiến hành  thu thập dữ liệu  từ các trang  báo điện tử  Việt Nam để khảo sát xem thị hiếu của người dân về các chủ đề xã hội và đời sống như thế nào. Từ đó hỗ trợ cho các công ty bán hàng làm chiến lược marketing hiệu quả hơn. Dữ liệu được lấy về, lưu trên một cơ sở dữ liệu dưới định dạng file văn bản (.txt) mà chưa qua bất kỳ khâu xử lý nào. Do trong quá trình lấy dữ liệu, các kỹ thuật viên của công ty A đã sơ suất  quên ghi nhớ chủ đề cho từng bài viết  khi được tải về. Những gì công ty A hiện có là một thư mục chứa hơn  28,000 file văn bản  (text), mỗi file văn bản là nội dung một bài viết trên một trang báo nào đó. Câu hỏi:  Với số lượng bài viết lớn như vậy (hơn 28,000 bài viết), bạn hãy tìm cách nào đó để  nhóm các bài viết  theo những chủ đề khác nhau. Bạn hãy đề xuất một phương pháp để có thể  đặt tên cho từng chủ đề  một cách hợp lý nhất. Kết quả công ty A mong đợi sẽ là một file dạng csv gồm 2 cột: cột 1 là tên bài báo, cột 2 là tên chủ đề tương ứng. Download file dữ liệu 28,000 bài viết . -----------------------------------  \nVới lượng dữ liệu lớn như vậy, có 2 mục tiêu cần thực hiện: Nhóm các bài viết thành các chủ đề khác nhau Đặt tên chủ đề cho từng nhóm bài viết ấy.  \nViệc nhóm các văn bản thành các chủ đề khác nhau có thể sử dụng các thuật toán phân cụm như  K-means , khai phá chủ đề  LDA (Latent Dirichlet Allocation),  ... Nhưng với LDA và cả K-means đều yêu cầu phải biết trước giá trị k - số cụm để phân chia. \nSố cụm của đề bài không thể xác định chính xác. Quan sát dữ liệu, ta thấy được đa số các bài báo được lấy từ các trang tin lớn như VnExpress, Tuổi trẻ Online. \n1. Phân tích về tên chủ đề bài viết trên báo điện tử \nPhân tích một chút về danh mục chủ đề trên các trang báo này. Các mục tại VnExpress được phân thành từng mục chính, mỗi mục chính lại có các chuyên mục nhỏ. Một số trang báo có các chuyên mục riêng: như mục Video ( http://video.vnexpress.net ), iOne ( http://ione.vnexpress.net ), ... là một mục nhỏ của VnExpress, nhưng lại có các chủ đề con cùng tên với tất cả các mục lớn của VnExpress.  Khi thu thập bài viết từ nhiều nguồn, sẽ có trường hợp các bài viết trùng nhau (hoặc gần giống nhau), nhưng 2 báo lại có cách đặt tên chủ đề khác nhau.  Một số trang tin tổng hợp (Zing.vn, Báo mới, ...) sẽ tổng hợp các tin từ các trang chính thống, và sẽ sắp xếp các bài viết vào các mục tương đương.  \n2. Phân tích dữ liệu thô \nDữ liệu thu được ở dạng text, mỗi file là một bài viết. Mỗi bài viết đều bị nhiễu (do thu thập dư các liên kết bài viết liên quan, các bài xem nhiều nhất, ...). \nDữ liệu cũng thu thập các bài ở dạng bài viết ảnh (chỉ thu thập được Caption của ảnh), hoặc bài viết Video (thu thập được số giây của video, ...). \nCác dữ liệu này khá nhiễu, khó rút trích đặc trưng, cần phải trải qua trá trình tiền xử lý để xử lý các bài viết này.  \n3. Phương pháp thực hiện \n3.1. Đề xuất phương pháp  \nDo sự khó khăn trong dữ liệu, và không xác định được số chủ đề của bài viết, cách đặt tên chủ đề, ... có nhiều phương pháp khác nhau để tiến hành nhóm các bài viết.  \nTa thống nhất sẽ chỉ chia các bài viết thành các chủ đề chính (không chia thành các chủ đề phân cấp nhỏ hơn). Tổng quát lại chúng ta sẽ có các cách sau để tiến hành nhóm các bài viết cùng chủ đề lại với nhau: Sử dụng thuật toán  DBSCAN  ( Density-based spatial clustering of applications with noise ): đây là thuật toán được đề xuất để phát hiện các cụm trong tập dữ liệu (chấp nhận dữ liệu nhiễu), với DBSCAN ta không cần biết trước số cụm. Nhược điểm của DBSCAN là độ phức tạp cao, chạy chậm.  Sử dụng thuật toán K-means:  K-means sẽ phân cụm dữ liệu bài viết vào số cụm k xác định, số cụm có thể ước lượng từ các phân tích tên chủ đề ở trên. Công ty A thu thập dữ liệu từ các trang báo thuộc chủ đề xã hội và đời sống, tổng hợp lại ta sẽ có các chủ đề:  Thời sự, Thế giới, Kinh doanh, Giải trí, Thể thao, Pháp luật, Giáo dục, Sức khỏe, Gia đình, Du lịch, Khoa học, Số hóa, Xe, Cộng đồng và mục khác . Số cụm ước lượng sẽ từ 13 ~ 15 cụm. Nếu các bài viết ở mục \"Khác\" chênh lệch lớn thì tiến hành điều chỉnh tham số k cho phù hợp.  Sử dụng kỹ thuật phân lớp văn bản : kỹ thuật này có thể tốn thời gian nhưng hiệu quả và giải quyết được cả vấn đề gom nhóm và đặt tên chủ đề. Tiến hành thu thập rút trích lại một số bài viết từ tất cả các chủ đề trên báo điện tử, dữ liệu này thu thập sẽ bao gồm  bài viết  và  nhãn  (chủ đề) của bài viết đó. Sử dụng các phương pháp/công cụ thống kê hoặc máy học (Machine Learning) để tiến hành tạo ra mô hình, sử dụng mô hình để phân lớp cho 28.000 văn bản của công ty A.  Đặt tên chủ đề:  Với phương pháp 1 và 2, việc làm sau khi phân cụm được các bài viết là tìm cách đặt trên cho các chủ đề này. Từ mỗi nhóm bài viết, ta có thể tiến hành rút trích từ khóa đặc trưng sử dụng mô hình túi từ, tính tần số, chọn ra các từ khóa đặc trưng. Từ các từ khóa đặc trưng này có ta thể suy luận ra được chủ đề, bằng phương pháp thủ công hoặc tự động. Để có các kết quả chính xác thì trong tập bài viết các từ  stopwords , các ký hiệu đặc biệt, ... phải được lọc bỏ.  \n3.2. Thực nghiệm \nTrong giới hạn, mình không thể thực nghiệm hết được tất cả các phương pháp đã nêu, mà chỉ chọn một phương pháp truyền thống: sử dụng phương pháp thu thập lại và phân lớp chủ đề sử dụng SVM. Phương pháp gồm 4 bước cơ bản:  Bước 1: Thu thập, rút trích Tiến hành thu thập, rút trích lại một số bài viết từ VnExpress.net và các chủ đề tương ứng đi kèm cho mỗi bài viết. Các chủ đề này sẽ là các chủ đề chuẩn cho 28.000 bài viết của công ty A. Bước 2: Tiền xử lý dữ liệu Tập văn bản (bao gồm thu thập được và dữ liệu thô có sẵn) sẽ được xử lý tách câu, tách từ, loại bỏ các dấu câu và các stopword. Sau bước này, mỗi văn bản sẽ là tập hợp của các từ đã được sàng lọc trong văn bản đó.  Quá trình tách câu tách từ trong tiếng Việt được sử dụng công cụ  vnTokenizer  với độ chính xác được tác giả công bố 96% - 98%. Stopwords sẽ được xóa bỏ khỏi kết quả bằng cách sử dụng bộ  từ điển stopwords tiếng Việt .  Bước 3: Vector hóa văn bản Tập từ thu được từ bước tiền xử lý đang ở dạng không cấu trúc, do đó để xử lý phân lớp bằng các phương pháp máy học cần vector hóa chúng. Mô hình túi từ được áp dụng, theo mô hình này, dữ liệu văn bản không có cấu trúc (độ dài khác nhau) được biểu diễn thành dạng vector tần số xuất hiện của từ trong văn bản.  Từ tần số của từ, vector của từng văn bản sẽ được tính bằng công thức TF*IDF ( tham khảo ).  Đây là công thức giúp đánh giá mức độ quan trọng của một từ đối với văn bản trong bối cảnh của tập ngữ liệu.  TF (term frequency) là tần số xuất hiện của một từ trong một văn bản. IDF (inverse document frequency) là tần số nghịch của 1 từ trong tập ngữ liệu. Kết quả của bước này là  vector phân bố xác suất  của tập từ biểu diễn chủ đề của từng văn bản. Các từ có tần số TF*IDF dưới 1 ngưỡng quy định sẽ bị lọc bỏ. Việc lọc này nhằm lựa ra những từ đủ tính chất đặc trưng cho chủ đề, loại bỏ những từ quá hiếm xuất hiện hoặc xuất hiện quá phổ biến. Bước 4: Phân lớp văn bản Tiến hành phân lớp sử dụng phương pháp học máy SVM. Tập văn bản đầu vào sau khi trải qua các bước xử lý sẽ được đại diện bằng tập các vector Chúng sẽ là đầu vào của giải thuật SVM truyền thống. SVM là thuật toán phân lớp nhị phân, do đó ta phải tổ chức sử dụng các kết hợp các mô hình One-vs-All hoặc All-vs-All Classiﬁcation  Sau quá trình phân lớp sẽ cho ta kết quả gãn nhãn chủ đề cho từng văn bản dựa trên văn bản đã thu thập được. \nCó thể tổng quá quá trình thực hiện như sau \nTổng kết \nQua quá trình xử lý, ta được tập kết quả của 28,000 văn bản cùng với chủ đề. Với lượng dữ liệu lớn, mô hình xử lý đơn lẻ truyền thống không thể phân tích nhanh chóng và có hiệu quả. Có thể kết hợp thêm kỹ thuật phân tán hóa dữ liệu và song song các tác vụ để nâng cao tốc độ thực thi, cụ thể có thể sử dụng mô hình MapReduce của các framework  Apache Hadoop  hoặc  Apache Spark Đăng ký nhận bài viết mới qua email ❤  by  duyetdev",
          "url": "http://blog.duyet.net/2016/06/topic-modeling-tim-chu-de-cho-tap-van-ban-bai-viet.html",
          "relevance": "1"
        },
        {
          "title": "Dao động của ô tô hai cầu theo mô hình hệ không gian 7 bậc tự do",
          "content": "Đăng nhập Phiên bản Mobile Tin nóng Tin mới Đề xuất Xã hội Thời sự Giao thông Môi trường - Khí hậu Thế giới Văn hóa Nghệ thuật Ẩm thực Du lịch Kinh tế Kinh doanh Lao động - Việc làm Tài chính Chứng khoán Giáo dục Học bổng - Du học Đào tạo - Thi cử Thể thao Bóng đá Quần vợt Giải trí Âm nhạc Thời trang Điện ảnh - Truyền hình Pháp luật Hình sự - Dân sự An ninh - Trật tự KH - CN CNTT - Viễn thông Khoa học - Tự nhiên Thiết bị - Phần cứng Đời sống Dinh dưỡng - Làm đẹp Tình yêu - Hôn nhân Sức khỏe - Y tế Xe cộ Nhà đất Quản lý - Quy hoạch Không gian - Kiến trúc Xe cộ Dao động của ô tô hai cầu theo mô hình hệ không gian 7 bậc tự do GTVT 30/09/2015 06:22 GMT+7 zalo facebook 0 Gốc Từ khoá \nKhông Gian Vector \nVectơ \nHệ Thống Treo \nPhản Biện \nLiên Kết \nPhụ Thuộc \nĐề Xuất \nBài báo khảo sát dao động của ô tô hai cầu có hệ thống treo phụ thuộc theo mô hình hệ không gian tuyến tính, 7 bậc tự do.\n PGS. TS. Vũ Công Hàm  PGS. TS. Vũ Quốc Trụ ThS. NCS. Nguyễn Đình Dũng Học viện Kỹ thuật Quân sự Người phản biện:  TS. Vũ Minh Đức PGS. TS. Vũ Đức Lập Tóm tắt: Bài báo khảo sát dao động của ô tô hai cầu có hệ thống treo phụ thuộc theo mô hình hệ không gian tuyến tính, 7 bậc tự do. Hệ phương trình vi phân dao động và phương pháp tính được đề xuất trong bài báo cho phép khảo sát đầy đủ đáp ứng động lực học của ô tô. Từ khóa:  Dao động, ô tô hai cầu, không gian tuyến tính. Abstract:  This paper considers the response of a two-axled automobile with dependent suspensions in the model of a seven-DOF spatial vibration system. The system of differential equations of vibration and the method proposed for solving allow to investigate fully the dynamic response of the automobile. Keywords:  Oscillator, automotive seals, linear space. 1. Đặt vấn đề Bài toán khảo sát dao động của ô tô trong quá trình chuyển động trên đường không bằng phẳng nhận được sự quan tâm của nhiều tác giả trong và ngoài nước. Do tính chất phức tạp của việc khảo sát, tính toán, các mô hình không gian về dao động của ô tô mặc dù đã được đề cập nhưng còn khá khiêm tốn. Mô hình hệ dao động không gian 8 bậc tự do của ô tô hai cầu đã được khảo sát trong [3], ở đó tính cản của các lốp xe được bỏ qua, còn thân xe được lập mô hình theo hai khối lượng dao động trong các mặt phẳng vuông góc với trục dọc xe. Trong [5] đã sử dụng mô hình hệ không gian 7 bậc tự do để khảo sát dao động của ô tô hai cầu hệ thống treo độc lập với giảm chấn kiểu khí nén. Bài báo này sẽ khảo sát dao động của ô tô hai cầu có hệ thống treo phụ thuộc theo mô hình hệ không gian 7 bậc tự do. 2. Mô hình khảo sát dao động 2.1. Một số giả thiết được áp dụng Bài báo sử dụng các giả thiết sau: - Các bánh xe luôn tiếp xúc với mặt đường; - Trong mô hình dao động, thân xe, cầu trước và cầu sau được xem là vật rắn tuyệt đối; - Coi cản của lốp xe là đàn nhớt tuyến tính; - Đường tác dụng của lực lò xo và lực giảm chấn thuộc cùng một cụm treo trùng nhau trên một đường thẳng thẳng đứng. 2.2. Mô hình khảo sát Hình 2.1  biểu diễn một ô tô hai cầu với ba phần tử khối lượng: Thân xe 1, cầu trước 2 và cầu sau 3. Liên kết giữa cầu trước và cầu sau với thân xe được thực hiện thông qua các cụm lò xo, giảm chấn thuộc hệ thống treo của ô tô. Hình 2.1: Mô hình khảo sát dao động của ô tô Mô hình sử dụng hai hệ tọa độ; hệ cố định OXYZ gắn với mặt đường; hệ động O’X’Y’Z’ gắn với thân xe. Các ký hiệu S, S 1 , S 2 - Tương ứng là khối tâm của thân xe, cầu trước và cầu sau;  a  1 ,  a  2 - Khoảng cách từ khối tâm thân xe đến trục cầu trước và trục cầu sau;  2b  - Khoảng cách trung bình giữa hai vệt bánh xe phải và trái (chiều rộng cơ sở);  2c  - Khoảng cách giữa nhíp phải và trái; M B , M C1 , M C2 - Tương ứng là khối lượng phần treo, phần không treo cầu trước và cầu sau; J BX , J BY - Tương ứng là mô mem quán tính khối lượng của thân xe đối với trục O ’ X ’ và O ’ Y ’ ; J C1 , J C2 - Tương ứng là mô-men quán tính khối lượng của cầu trước và cầu sau với trục song song với trục OX; C TC1 , K TC1 , C TC2 , K TC2 - Tương ứng là hệ số đàn hồi và hệ số cản của hệ treo cầu trước và cầu sau; C LC1 , K LC1 , C LC2 , K LC2 - Tương ứng là hệ số đàn hồi và hệ số cản của lốp trước và lốp sau. z  D1 ,  z  D2 ,  z  D3 ,  z  D4 - Độ cao của biên dạng mặt đường tại điểm các lốp xe tương ứng tiếp xúc với mặt đường. Khi biên dạng mặt đường biết trước thì độ cao  z  Dk sẽ chỉ phụ thuộc vào vị trí của xe (tọa độ  x của khối tâm S )hay: (1) Trong các thí dụ tính toán, hàm  z  Dk được cho trước, tọa độ  x  được xác định qua vận tốc chuyển động  V  và thời gian chuyển động  t  của xe. 2.3. Các thông số mô tả dao động Dao động của thân xe và các cầu xe sẽ được khảo sát với các thành phần như sau: - Thân xe có 3 chuyển động (3 bậc tự do) là: Dịch chuyển thẳng đứng của khối tâm  z  B ; dịch chuyển góc j B (góc lắc dọc) quanh trục OY ’ ;dịchchuyển góc Ψ B ( góc lắc ngang) quanh trục OX ’ . - Các cầu xe (khối lượng không được treo) có 4 chuyển động là: Dịch chuyển thẳng đứng của khối tâm cầu trước và sau  z  C1 ,  z  C2 và dịch chuyển góc  Ψ  C1 ,  Ψ  C2 của cầu trước và sau quanh các trục song song với OX. Quy ước các chuyển vị  z  B,   z  C1­ ,  z  C2 , lấy từ vị trí cân bằng tĩnh, theo đó không cần đề cập đến trọng lượng bản thân các khối lượng dao động. 3. Hệ phương trình vi phân dao động 3.1. Chuyển vị của các điểm gắn lò xo, giảm chấn trên mô hình dao động Với các bậc tự do nêu trên, có thể thành lập vectơ các tọa độ suy rộng: (2) Trên  Hình 2.1  biểu diễn vị trí của 12 điểm gắn lò xo và giảm chấn, được đánh số từ 1 đến 8 cho các điểm liên kết thực và bốn điểm liên kết danh nghĩa 1’, 2’, 3’, 4’. Chuyển vị thẳng đứng của các điểm gắn lò xo và giảm chấn được ký hiệu tương ứng là: (3) Các chuyển vị thẳng đứng được biểu diễn qua hệ tọa độ suy rộng như sau: (4) 3.2. Thiết lập hệ phương trình vi phân dao động của cơ hệ Áp dụng định luật 2 Newton cho 3 khối lượng dao động ta thiết lập được hệ phương trình vi phân dao động của cơ hệ, gồm 7 phương trình vi phân cấp hai như sau: (5) (6) (7) (8) (9) (10) (11) Thay các biểu thức vào hệ phương trình trên rồi thực hiện một số phép biến đổi đơn giản ta có thể đưa hệ phương trình vi phân dao động ở trên về dạng ma trận, trong đó không có mặt chuyển vị thẳng đứng của các điểm gắn lò xo và giảm chấn, chỉ có mặt các tọa độ suy rộng: (12) Trong đó:  Ma trận khối lượng [ M ], ma trận cản [ C ], ma trận độ cứng [ K ] và vectơ lực kích thích hoàn toàn xác định theo các thông số hình học, động lực học của xe và biên dạng mặt đường. 4. Dao động của ô tô trong hai trường hợp kích thích tiêu biểu Khảo sát dao động của ô tô dẫn đến việc giải hệ phương trình vi phân dao động tuyến tính theo điều kiện đầu cho trước. Bài viết sử dụng phương pháp số (phương pháp Runge-Kutta) để giải hệ phương trình vi phân dao động bằng cách lập chương trình tính trong phần mềm MATLAB. Theo đó, đầu tiên cần biến đổi hệ về hệ phương trình vi phân cấp một tương ứng bằng cách đặt .Khi đó, hệ phương trình vi phân cấp một nhận được có dạng: (13) Trong đó, - Vectơ đại diện cho vế phải của 14 phương trình vi phân cấp một. Theo (13) có thể thấy 7 thành phần đầu của là `vecp` , 7 thành phần còn lại là biểu thức của `vecp` được rút ra từ hệ phương trình (5) `-:` (11) bằng cách chia các vế phải cho hệ số của gia tốc suy rộng tương ứng Dưới đây trình bày kết quả tính toán trong hai trường hợp tiêu biểu về kích thích. Giá trị của các thông số hình học và động lực học được lấy theo xe GAZ-66 [4]: M B =2200 kg; M C1 =660 kg; M C2 =580kg; J BX =756kg.m 2 ; J BY =2750kg.m 2 ; J C1 =1780kg.m 2 ; J C2 =1170kg.m 2 ; K T1 =246000 N/m;K T2 =196000N/m; C T1 =1500kg/s; C T2 =1500kg/s; K L1 =800000N/m; K L2 =800000N/m; C L1 =62000kg/s; C L2 =62000kg/s; a 1 =1,563m; a 2 =1,737m; 2b=1,8m; 2c=1,2m. Trường hợp 1 :  Xe chuyển động với vận tốc không đổi  V   trên mặt đường có biên dạng hình sin  (Hình 4.1) . Biên độ  h  và bước sóng  L  của biên dạng mặt đường cho hai bánh bên trái được ký hiệu là ( h  T ,  L  T ), cho hai bánh bên phải là ( h  P ,  L  P ). Phương trình mô tả chiều cao mấp mô tại điểm tiếp xúc của bốn bánh xe với mặt phẳng danh nghĩa của đường (trục Ox trên  Hình 2.1 ) trong trường hợp này là: (14) Hình 4.1: Biên dạng mặt đường hình sin Trong các công thức (14),  x  =  Vt  là tọa độ theo phương  x  của khối tâm S của thân ô tô tại thời điểm  t . Giá trị cụ thể của các thông số về biên dạng mặt đường được sử dụng để tính toán là: V = 20km/h;  h P = 15mm; h T = 25mm; L P = 5,0m; L T = 5,0m. Các điều kiện đầu là: Hình 4.2  là các đồ thị biểu diễn ba thành phần dao động thẳng đứng của thân xe ( z  B =  z  B ( t )), cầu trước ( z  C1 =  z  C1 ( t )) và cầu sau ( z  C2 =  z  C2 ( t )). Hình 4.2: Dao động thẳng đứng của thân xe và hai cầu khi biên dạng mặt đường hình sin (z B - nét liền, z C1 - nét đứt, z C2 - chấm chấm) Các đồ thị  Hình 4.2  cho thấy, với dạng kích thích đang xét, ba thành phần dao động thẳng đứng ở giai đoạn sau quá độ cũng có dạng hình sin và hoàn toàn phù hợp với lý thuyết. Đồ thị mô tả các thành phần dao động góc (không trình bày ở đây) cũng có dạng tương tự. Trường hợp 2 :  Xe đang chạy với vận tốc không đổi  V  trên đường bằng phẳng thì đụng phải một mô đất trên vệt bánh xe bên phải, nơi hai bánh xe số 1 và số 3 đi qua. Sau đó một khoảng bằng  d  theo hướng chuyển động, xe lại đụng phải một mô đất khác trên vệt bánh xe bên trái, nơi bánh xe số 2 và bánh xe số 4 đi qua. Sự thay đổi chiều cao của hai mô đất theo phương chuyển động được mô tả theo hàm sin với giá trị lớn nhất là h p và h T , chiều dài tương ứng của hai mô đất theo phương chuyển động là L p và L T  (Hình 4.3) . Ngoài hai mô đất đó, mặt đường lại hoàn toàn bằng phẳng. Trong trường hợp riêng, khi xe chỉ đụng phải một mô đất trên một vệt bánh xe, chỉ cần lấy h p = 0 hoặc h T = 0. Hình 4.3: Biểu diễn biên dạng và vị trí tương đối của hai mô đất trên hai vệt bánh xe Các giá trị số được sử dụng để tính toán là: V  = 25km/h, h p = 35mm, h T = 25mm L P = 65cm, L T = 80cm,  d  = 5m Các điều kiện đầu: Hình 4.4  là đồ thị mô tả sự thay đổi theo thời gian của hai thành phần dao động góc của thân xe ϕ B =ϕ B ( t ), Ψ B = Ψ B ( t ). Hình 4.4: Đồ thị mô tả hai thành phần dao động góc của thân xe (ϕ  B   - nét liền, Ψ  B   - nét đứt) Các đồ thị cho thấy, dạng kích thích đang xét có ảnh hưởng lớn hơn đối với thành phần dao động góc ngang xe, điều này là phù hợp với thực tế. Do có mặt của các giảm chấn nên hai dao động góc ở trên đều là dao động tắt dần. 5 thành phần dao động còn lại của cơ hệ cũng được xác định một cách đồng thời với hai thành phần dao động góc nêu trên và có thể được biểu diễn theo cách tương tự. 5. Kết luận Bài báo đã xây dựng mô hình dao động dạng không gian, 7 bậc tự do của ô tô hai cầu, có hệ thống treo phụ thuộc, chịu kích thích động học khi chuyển động thẳng trên mặt đường với vận tốc không đổi, đã thiết lập hệ phương trình vi phân dao động của cơ hệ và chỉ ra phương pháp giải hệ phương trình vi phân dao động bằng phương pháp số nhằm tìm đáp ứng dao động của cơ hệ theo tất cả các bậc tự do. Bài báo cũng mô tả hai dạng kích thích động học khá tiêu biểu thường được áp dụng trong tính toán dao động của ô tô, đã xây dựng chương trình tính toán và giới thiệu một vài kết quả tiêu biểu ở dạng đồ thị nhằm minh họa tính khả thi của phương pháp. Với hệ phương trình vi phân dao động nhận được, có thể xác định các tần số dao động riêng và các dạng dao động riêng của ô tô bằng phương pháp giải tích. Chương trình tính đã xây dựng được cho phép khảo sát kỹ lưỡng hơn dao động của ô tô khi chuyển động trên mặt đường không bằng phẳng. Tài liệu tham khảo [1]. Vũ Công Hàm, Trần Văn Bình (2014),  Lý thuyết dao động , NXB. Quân đội nhân dân, Hà Nội. [2]. Vũ Công Hàm, Trần Quang Dũng (2007),  Dao động cơ học , Học viện Kỹ thuật Quân sự. [3]. Vũ Đức Lập (2011),  Dao động ô tô , NXB. Quân đội nhân dân, Hà Nội. [4]. Vũ Đức Lập (2004),  Sổ tay tra cứu tính năng kỹ thuật ô tô , Học viện Kỹ thuật Quân sự. [5]. CHEN Ke, ZHANG Ming, TONG Xuefeng,  Vibration characteristic analysis of vehicle air suspension based on fuzzy control , The second International Conference on Electronic & Mechanical Engineering and Information Technology (EMEIT-2012), pp. 2196-2199, Published by Atlantis Press, Paris, France. Công hàm - Quóc Trụ - Đình Dũng zalo facebook Tin liên quan Tin đề xuất Tin nóng Tin mới Video Trước thủ tướng Áo, thế giới đã có những nhà lãnh đạo trẻ nổi tiếng nào? iOne Trước thủ tướng Áo, thế giới đã có những nhà lãnh đạo trẻ nổi tiếng nào? Cách làm món bánh mì chuối bổ dưỡng PNNews Cách làm món bánh mì chuối bổ dưỡng Tiêu chí chọn bạn trai dài 2 trang A4 của cô giáo tiếng Anh VietnamNet Tiêu chí chọn bạn trai dài 2 trang A4 của cô giáo tiếng Anh Clip: Phóng ngược chiều trên cầu vượt, xe máy suýt tông vào đầu ô tô Người Đưa Tin Clip: Phóng ngược chiều trên cầu vượt, xe máy suýt tông vào đầu ô tô Ngày 26/10, Quốc hội phê chuẩn bổ nhiệm Bộ trưởng Giao thông vận tải và Tổng Thanh tra Chính phủ VTC Ngày 26/10, Quốc hội phê chuẩn bổ nhiệm Bộ trưởng Giao thông vận tải và Tổng Thanh tra Chính phủ Putin trổ tài nói tiếng Anh trước thanh niên thế giới PNNews Putin trổ tài nói tiếng Anh trước thanh niên thế giới Ngoại trưởng Mỹ kêu gọi các nước Vùng Vịnh cô lập Iran Zing Ngoại trưởng Mỹ kêu gọi các nước Vùng Vịnh cô lập Iran Công Phượng lại muốn ra nước ngoài thi đấu VTC Công Phượng lại muốn ra nước ngoài thi đấu Xác ướp quấn dây thừng kì dị bất ngờ dạt bờ sông ở Anh Tiền Phong Xác ướp quấn dây thừng kì dị bất ngờ dạt bờ sông ở Anh Những màn mix&match dở tệ ở Hoa hậu Hoàn vũ Việt Nam tập 4 2Sao Những màn mix&match dở tệ ở Hoa hậu Hoàn vũ Việt Nam tập 4 Nóng trong ngày Trong tuần Theo dõi Báo Mới Báo Mới \nFanpage Báo Mới \nGiải Trí Gửi tin nóng \ncho bạn Google \nPlus Xu hướng Gordon Murray Santa Fe McLaren F1 Audi A7 2019 Super Cub honda crv porsche Honda Việt Nam Range Rover Evoque Convertible VTEC Turbo Pajero Sport 4x2 MT Pajero Sport VIMS 2017 Pajero Sport 4x2 AT volvo crossover Đào Tâm Nguyễn Chung Driver Attention Monitor Mazda CX-5 PHIÊN BẢN KHÁC Báo Mới APPS Báo Mới ENGLISH Báo Mới BLOG LIÊN HỆ Giới thiệu Điều khoản sử dụng Quảng cáo NGƯỜI DÙNG Nhúng tin vào trang web Thống kê & So sánh \nBÁO MỚI thực hiện việc tổng hợp\n \nvà sắp xếp các thông tin tự động\n \nbởi chương trình máy tính\n Giấy phép số 1818/GP-TTĐT do Sở Thông tin và Truyền thông Hà Nội cấp ngày 05/05/2017 Đơn vị chủ quản: Công ty Cổ phần Công nghệ EPI * Chịu trách nhiệm: Nguyễn Thanh Tùng Địa chỉ: Tầng 7, Tòa nhà Báo Sinh Viên VN, D29 Phạm Văn Bạch, Yên Hòa, Cầu Giấy, Hà Nội \nTel: (04) 3-212-3232 ext. 2947\n Cung cấp trong: 52.18 ms. Đăng nhập Đăng nhập tài khoản Facebook Đăng nhập tài khoản Google ×",
          "url": "http://www.baomoi.com/dao-dong-cua-o-to-hai-cau-theo-mo-hinh-he-khong-gian-7-bac-tu-do/c/17623353.epi",
          "relevance": "0"
        },
        {
          "title": "Giới thiệu về Feature Engineering",
          "content": "Công nghệ Lập trình Lập trình ứng dụng Lập trình web Tools & Tips Sự kiện Chuyên gia nói Tâm sự coder Devvui Tìm kiếm Sign in Đăng nhập tài khoản Tài khoản mật khẩu của bạn Forgot your password? Get help Password recovery Khởi tạo mật khẩu email của bạn Mật khẩu đã được gửi vào email của bạn. Tech Talk Điều gì sẽ xảy ra khi cập nhật phần mềm mỗi… Trí tuệ nhân tạo của Google tự đánh cờ vây với… 8 xu hướng công nghệ sẽ thống trị trong giai đoạn… Những kĩ năng cần có của một developer thành công Bản update Windows 10 Fall Creators không tương thích với laptop… Tất cả Lập trình ứng dụng Lập trình web Framework Microsoft .Net 4.7.1 có gì mới? 7 lí do để loại bỏ React’s Functional Components Tại sao C # là một trong những ngôn ngữ lập… Vì sao JavaScript là ngôn ngữ lập trình quyến rũ Getting Started with Entity Framework 6 Code First using MVC 5 Thử một lần code mà không dùng If xem nào? 5 cách giúp lập trình viên tăng năng suất làm việc Lập trình viên, liệu bạn đã đủ “hấp dẫn” trong mắt… [TÀI LIỆU] Beginning Mobile App Development with React Native “Ở Việt Nam, cơ hội để thực sự làm về Trí… [ Học bổng] Học tiếng Nhật miễn phí cùng ” Akira”-… VWS2017 – Xây dựng Decentralized web trên nền tảng Blockchain Họp báo ra mắt sự kiện Vietnam Web Summit 2017 [Miễn phí] Tham gia sự kiện App Analytics Tools: con đường… 4 sai lầm có thể khiến các lập trình viên rời… Lương kỹ sư IT đi Nhật có thể lên đến 165… Phỏng vấn chuyên gia Machine Learning từ AdAsia về ứng dụng… 5 điều phiền toái nhất của CSS Thương hiệu cá nhân Dev quan trọng hơn bạn tưởng Tâm sự một coder: hãy dũng cảm thành thật với con… Web Dev: Cố gắng hoàn hảo sẽ cản trở bạn Thay đổi code, thay đổi thế giới Thương hiệu cá nhân Dev quan trọng hơn bạn tưởng Những tình huống “đứng hình” trong JavaScript Trang Chủ Lập trình Giới thiệu về Feature Engineering Giới thiệu về Feature Engineering February 9, 2017 595 Chia sẻ Facebook Tweet 1. Giới thiệu Cho tới lúc này, tôi đã trình bày 5 thuật toán Machine Learning cơ bản:  Linear Regression ,  K-means Clusterning ,  K-nearest neighbors ,  Perceptron Learning Algorithm  và  Logistic Regression . Trong tất cả các thuật toán này, tôi đều giả sử các điểm dữ liệu được biểu diễn bằng các vector, được gọi là  feature vector  hay  vector đặc trưng , có độ dài bằng nhau, và cùng là vector cột hoặc vector hàng. Tuy nhiên, trong các bài toán thực tế, mọi chuyện không được tốt đẹp như vậy! Với các bài toán về Computer Vision, các bức ảnh là các ma trận có kích thước khác nhau. Thậm chí để nhận dạng vật thể trong ảnh, ta cần thêm một bước nữa là  object detection , tức là tìm cái khung chứa vật thể chúng ta cần dự đoán. Ví dụ, trong bài toán nhận dạng khuôn mặt, chúng ta cần tìm được vị trí các khuôn mặt trong ảnh và  crop  các khuôn mặt đó trước khi làm các bước tiếp theo. Ngay cả khi đã xác định được các khung chứa các khuôn mặt (và có thể resize các khung đó về cùng một kích thước), ta vẫn phải làm rất nhiều việc nữa vì hình ảnh của khuôn mặt còn phụ thưộc vào góc chụp, ánh sáng, … và rất nhiều yếu tố khác nữa. Các bài toán NLP (Natural Language Processing – Xử lý ngôn ngữ tự nhiên) cũng có khó khăn tương tự khi độ dài của các văn bản là khác nhau, thậm chí có những từ rất hiếm gặp hoặc không có trong từ điển. Cũng có khi thêm một vài từ vào văn bản mà nội dung của văn bản không đổi hoặc hoàn toàn mang nghĩa ngược lại. Hoặc cùng là một câu nói nhưng tốc độ, âm giọng của mỗi người là khác nhau, thậm chí của cùng một người nhưng lúc ốm lúc khỏe cũng khác nhau. Khi làm việc với các bài toán Machine Learning thực tế, nhìn chung chúng ta chỉ có được dữ liệu thô (raw) chưa qua chỉnh sửa, chọn lọc. Chúng ta cần phải tìm một phép biến đổi để loại ra những dữ liệu nhiễu (noise), và để đưa dữ liệu thô với số chiều khác nhau về cùng một chuẩn (cùng là các vector hoặc ma trận). Dữ liệu chuẩn mới này phải đảm bảo giữ được những thông tin đặc trưng (features) cho dữ liệu thô ban đầu. Không những thế, tùy vào từng bài toán, ta cần  thiết kế  những phép biến đổi để có những features phù hợp. Quá trình quan trọng này được gọi là  Feature Extraction , hoặc  Feature Engineering , một số tài liệu tiếng Việt gọi nó là  trích chọn đặc trưng . Tôi xin trích một câu nói của thầy Andrew Ng và xin phép thêm không dịch ra tiếng Việt (Nguồn  Feature Engineering – wiki ): Coming up with features is difficult, time-consuming, requires expert knowledge. “Applied machine learning” is basically feature engineering. Để giúp các bạn có cái nhìn tổng quan hơn, trong phần tiếp theo tôi xin đặt bước Feature Engineering này trong một bức tranh lớn hơn. 2. Mô hình chung cho các bài toán Machine Learning Phần lớn các bài toán Machine Learning có thể được thể hiện trong hình (tôi) vẽ dưới đây: --------------------  Thông tin cho Dev  -------------------- Hình 1: Mô hình chung cho các bài toán Machine Learning. Có hai phases lớn là Training phase và Testing phase. Xin nhắc lại là với các bài toán Supervised learning, ta có các cặp dữ liệu ( input, output ), với các bài toán Unsupervised learing, ta chỉ có  input  mà thôi. TRAINING PHASE Có hai khối có nền màu xanh lục chúng ta cần phải thiết kế: Feature Extractor ĐẦU RA  Tôi xin đề cập đầu ra của khối này trước vì mục đích của Feature Engineering là tạo ra một Feature Extractor biến dữ liệu thô ban đầu thành dữ liệu phù hợp với từng mục đích khác nhau. Cơ hội gặp gỡ, học hỏi kinh nghiệm với các chuyên gia Xamarin trong & ngoài nước! ĐẦU VÀO raw training input . Raw input là tất cả các thông tin ta biết về dữ liệu. Ví dụ: với ảnh thì là giá trị của từng pixel; với văn bản thì là từng từ, từng câu; với file âm thanh thì nó là một đoạn tín hiệu; với cơ sở dữ liệu  Iris  thì nó là độ dài các cánh hoa và đài hoa, … Dữ liệu thô này thường không ở dạng vector, không có số chiều như nhau. Thậm chí có thể có số chiều như nhau nhưng số chiều quá lớn, như một bức ảnh màu 1000 pixel x 1000 pixel thì số  elements  đã là  3 × 10 6 3×106  (3 vì ảnh màu thường có 3 channels: Red, Green, Blue). Đây là một con số quá lớn, không lợi cho lưu trữ và tính toán. (optional)  output  của  training set . Trong các bài toán Unsupervised learning, ta không biết  output  nên hiển nhiên sẽ không có đầu vào này. Trong các bài toán Supervised learning, có khi dữ liệu này cũng không được sử dụng. Ví dụ: nếu  raw input  đã có cùng số chiều rồi nhưng số chiều quá lớn, ta muốn giảm số chiều của nó thì cách đơn giản nhất là  chiếu  vector đó xuống một không gian có số chiều nhỏ hơn bằng cách lấy một ma trận ngẫu nhiên nhân với nó. Ma trận này thường là ma trận  béo  (số hàng ít hơn số cột, tiếng Anh – fat matrices) để đảm bảo số chiều thu được nhỏ hơn số chiều ban đầu. Việc làm này mặc dù làm mất đi thông tin, trong nhiều trường hợp vẫn mang lại hiệu quả vì đã giảm được lượng tính toán ở phần sau. Đôi khi  ma trận chiếu  không phải là ngẫu nhiên mà có thể được  học  dựa trên toàn bộ  raw input , ta sẽ có bài toán tìm ma trận chiếu để lượng thông tin mất đi là ít nhất. Trong nhiều trường hợp, dữ liệu  output  của  training set  cũng được sử dụng để tạo ra Feature Extractor. Ví dụ: trong bài toán classification, ta không quan tâm nhiều đến việc mất thông tin hay không, ta chỉ quan tâm đến việc những thông tin còn lại có đặc trưng cho từng class hay không. Ví dụ, dữ liệu thô là các hình vuông và hình tam giác có màu đỏ và xanh. Trong bài toán phân loại đa giác, các output là  tam giác  và  vuông , thì ta không quan tâm tới màu sắc mà chỉ quan tâm tới số cạnh của đa giác. Ngược lại, trong bài toán phân loại màu, các class là  xanh  và  đỏ , ta không quan tâm tới số cạnh mà chỉ quan tâm đến màu sắc thôi. (optional)  Prior knowledge about data : Đôi khi những giả thiết khác về dữ liệu cũng mang lại lợi ích. Ví dụ, trong bài toán classification, nếu ta biết dữ liệu là (gần như)  linearly separable  thì ta sẽ đi tìm một ma trận chiếu sao cho ở trong không gian mới, dữ liệu vẫn đảm bảo tính  linearly separable , việc này thuận tiện hơn cho phần classification vì các thuật toán linear, nhìn chung, đơn giản hơn. Sau khi  học  được feature extractor thì ta cũng sẽ thu được  extracted features  cho  raw input data . Những  extracted features  này được dùng để huấn luyện các thuật toán Classification, Clustering, Regression,… ở phía sau. Main Algorithms Khi có được  extracted features  rồi, chúng ta sử dụng những thông tin này cùng với (optional)  training output  và (optional)  prior knowledge  để tạo ra các mô hình phù hợp, điều mà chúng ta đã làm ở những bài trước. Chú ý:  Trong một số thuật toán cao cấp hơn, việc  huấn luyện  feature extractor và main algorithm được thực hiện cùng lúc với nhau chứ không phải từng bước như trên. Một điểm rất quan trọng: khi xây dựng bộ  feature extractor  và  main algorithms , chúng ta không được sử dụng bất kỳ thông tin nào trong tập  test data . Ta phải giả sử rằng những thông tin trong  test data  chưa được nhìn thấy bao giờ. Nếu sử dụng thêm thông tin về  test data  thì rõ ràng ta đã  ăn gian ! Tôi từng đánh giá các bài báo khoa học quốc tế, rất nhiều tác giả xây dựng mô hình dùng cả dữ liệu  test data , sau đó lại dùng chính mô hình đó để kiểm tra trên  test data  đó. Việc  ăn gian  này là lỗi rất nặng và hiển nhiên những bài báo đó bị từ chối (reject). TESTING PHASE Bước này đơn giản hơn nhiều. Với  raw input  mới, ta sử dụng feature extractor đã tạo được ở trên (tất nhiên không được sử dụng  output  của nó vì  output  là cái ta đang đi tìm) để tạo ra feature vector tương ứng. Feature vector được đưa vào  main algorithm  đã được học ở training phase để dự đoán  output . 3. Một số ví dụ về Feature Engineering Trực tiếp lấy raw data Với bài toán phân loại chữ số viết tay trong bộ cơ sở dữ liệu  MNIST , mỗi bức ảnh có số chiều là 28 pixel x 28 pixel (tất nhiên việc  crop  và chỉnh sửa mỗi bức ảnh đã được thực hiện từ trước rồi, đó đã là một phần của feature engineering rồi). Một cách đơn giản thường được dùng là  kéo dài  ma trận 28×28 này để được 1 vector có số chiều 784. Trong cách này, các cột (hoặc hàng) của ma trận ảnh được đặt chồng lên (hoặc cạnh nhau) để được 1 vector dài. Vector dài này được trực tiếp sử dụng làm feature đưa vào các bộ classifier/clustering/regression/… Lúc này, giá trị của mỗi pixel ảnh được coi là một feature. Rõ ràng việc làm đơn giản này đã làm mất thông tin về  không gian  (spatial information) giữa các điểm ảnh, tuy nhiên, trong nhiều trường hợp, nó vẫn mang lại kết quả khả quan. Feature selection Giả sử rằng các điểm dữ liệu có số features khác nhau (do kích thước dữ liệu khác nhau hay do một số feature mà điểm dữ liệu này có nhưng điểm dữ liệu kia lại không thu thập được), và số lượng features là cực lớn. Chúng ta cần  chọn  ra một số lượng nhỏ hơn các feature phù hợp với bài toán.  Chọn thế nào  và  thế nào là phù hợp  lại là một bài toán khác, tôi sẽ không bàn thêm ở đây. Dimensionality reduction Một phương pháp nữa tôi đã đề cập đó là làm giảm số chiều của dữ liệu để giảm bộ nhớ và khối lượng tính toán. Việc giảm số chiều này có thể được thực hiện bằng nhiều cách, trong đó  random projection  là cách đơn giản nhất. Tức chọn một  ma trận chiếu  (projection matrix) ngẫu nhiên (ma trận béo) rồi nhân nó với từng điểm dữ liệu (giả sử dữ liệu ở dạng vector cột) để được các vector có số chiều thấp hơn. Lúc này, có thể ta không có tên gọi cho mỗi feature nữa vì các feature ở vector ban đầu đã được trộn lẫn với nhau theo một tỉ lệ nào đó rồi lưu và vector mới này. Mỗi thành phần của vector mới này được coi là một feature (không tên). Việc chọn một ma trận chiếu ngẫu nhiên đôi khi mang lại kết quả tệ không mong muốn vì thông tin bị mất đi quá nhiều. Một phương pháp được sử dụng nhiều để hạn chế lượng thông tin mất đi có tên là  Principle Component Analysis  sẽ được tôi trình bày sau đây khoảng 1-2 tháng. Chú ý:  Feature learning không nhất thiết phải làm giảm số chiều dữ liệu, đôi khi feature vector còn có số chiều lớn hơn raw data. Random projection cũng có thể làm được việc này nếu ma trận chiếu là một ma trận  cao  (số cột ít hơn số hàng). Bag-of-words Hẳn rất nhiều bạn đã tự đặt câu hỏi: Với một văn bản thì feature vector sẽ có dạng như thế nào? Làm sao đưa các từ, các câu, đoạn văn ở dạng  text  trong các văn bản về một vector mà mỗi phần tử là một số? Có một phương pháp rất phổ biến giúp bạn trả lời những câu hỏi này. Phương pháp đó có tên là  Bag of Words (BoW)  ( Túi đựng Từ ). Vẫn theo thói quen, tôi bắt đầu bằng một ví dụ. Giả sử chúng ta có bài toán phân loại tin rác. Ta thấy rằng nếu một tin có chứa các từ  khuyến mại, giảm giá, trúng thưởng, miễn phí, quà tặng, tri ân, …  thì nhiều khả năng đó là một tin nhắn rác. Vậy phương pháp đơn giản nhất là  đếm  xem trong tin đó có bao nhiêu từ thuộc vào các từ trên, nếu nhiều hơn 1 ngưỡng nào đó thì ta quyết định đó là tin rác. (Tất nhiên bài toán thực tế phức tạp hơn nhiều khi các từ có thể được viết dưới dạng không dấu, hoặc bị cố tình viết sai chính tả, hoặc dùng ngôn ngữ teen). Với các loại văn bản khác nhau thì lượng từ liên quan tới từng chủ đề cũng khác nhau. Từ đó có thể dựa vào số lượng các từ trong từng loại để làm các vector đặc trưng cho từng văn bản. Tôi xin lấy ví dụ cụ thể hơn về cách tạo ra vector đặc trưng cho mỗi văn bản dựa trên BoW và xin được lấy tiếng Anh làm ví dụ (nguồn  Bag of Words wiki . Tiếng Việt khó hơn vì một từ có thể có nhều âm tiết, tiếng Anh thì thường cứ gặp dấu cách là kết thúc một từ). Giả sử chúng ta có hai văn bản đơn giản: \n(1) John likes to watch movies. Mary likes movies too.\r\n 1 2 ( 1 ) John  likes  to watch  movies . Mary  likes  movies  too .   và \n(2) John also likes to watch football games.\r\n 1 2 ( 2 ) John  also  likes  to watch  football  games .   Dựa trên hai văn bản này, ta có danh sách các từ được sử dụng, được gọi là  từ điển  với 10  từ  như sau: \n[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"also\", \"football\", \"games\", \"Mary\", \"too\"]\r\n 1 2 [ \"John\" , \"likes\" , \"to\" , \"watch\" , \"movies\" , \"also\" , \"football\" , \"games\" , \"Mary\" , \"too\" ]   Với mỗi văn bản, ta sẽ tạo ra một vector đặc trưng có số chiều bằng 10, mỗi phần tử đại diện cho số từ tương ứng xuất hiện trong văn bản đó. Với hai văn bản trên, ta sẽ có hai vector đặc trưng là: \n(1) [1, 2, 1, 1, 2, 0, 0, 0, 1, 1]\r\n(2) [1, 1, 1, 1, 0, 1, 1, 1, 0, 0]\r\n 1 2 3 ( 1 ) [ 1 , 2 , 1 , 1 , 2 , 0 , 0 , 0 , 1 , 1 ] ( 2 ) [ 1 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 0 ]   Văn bản (1) có 1 từ “John”, 2 từ “likes”, 0 từ “also”, 0 từ “football”, … nên ta thu được vector tương ứng như trên. Cơ hội gặp gỡ, học hỏi kinh nghiệm với các chuyên gia Xamarin trong & ngoài nước! Có một vài điều cần lưu ý trong BoW: Với những ứng dụng thực tế,  từ điền  có nhiều hơn 10 từ rất nhiều, có thể đến một trăm nghìn hoặc cả triệu, như vậy vector đặc trưng thu được sẽ rất  dài . Một văn bản chỉ có 1 câu, và 1 tiểu thuyết nghìn trang đều được biểu diễn bằng các vector có số chiều bằng 100 nghìn hoặc 1 triệu. Có rất nhiều từ trong từ điển không xuất hiện trong một văn bản. Như vậy các vector đặc trưng thu được thường có rất nhiều phần tử bằng 0. Các vector có nhiều phần tử bằng 0 được gọi là  sparse vector  (sparse hiểu theo nghĩa là  thưa thớt, rải rác , tôi xin phép chỉ sử dụng khái niệm này bằng tiếng Anh). Để việc lưu trữ được hiệu quả hơn, ta không lưu cả vector đó mà chỉ lưu  vị trí  của các phần tử khác 0 và  giá trị  tương ứng. Lưu ý: nếu có hơn 50% số phần tử khác 0, việc làm này lại phản tác dụng! Thi thoảng có những từ hiếm gặp không nằm trong từ điển, vậy ta sẽ làm gì? Một cách thường được dùng là  mở rộng  vector đặc trưng thêm 1 phần tử, gọi là phẩn tử  <Unknown> . Mọi từ không có trong từ điền đều được coi là  <Unknown> . Nghĩ kỹ một chút, những từ hiếm đôi khi lại mang những thông tin qua trọng nhất mà chỉ loại văn bản đó có. Đây là một nhược điểm của BoW. Có một phương pháp cải tiến khác giúp khắc phục nhược điểm này có tên là Term Frequency-Inverse Document Frequency (TF-IDF) dùng để xác định tầm quan trọng của một từ trong một văn bản dựa trên toàn bộ văn bản trong cơ sở dữ liệu (corpus). Bạn đọc muốn tìm hiểu thêm có thể xem  5 Algorithms Every Web Developer Can Use and Understand, section 5. Nhược điểm lớn nhất của BoW là nó không mang thông tin về thứ tự của các từ. Cũng như sự liên kết giữa các câu, các đoạn văn trong văn bản. Ví dụ, ba câu sau đây: “ Em yêu anh không? ”, “ Em không yêu anh ”, và “ Không, (nhưng) anh yêu em ” khi được trích chọn đặc trưng bằng BoW sẽ cho ra ba vector giống hệt nhau, mặc dù ý nghĩa khác hẳn nhau. Bonus:  hình dưới đay là tần suất sử dụng các từ (coi mỗi âm tiết là một từ) trong Truyện Kiều ( theo bản này ) nếu ta chỉ sử dụng 30 từ có tần suất cao nhất. : Hình 2: Bag of Words cho Truyện Kiều với 30 từ có tần suất cao nhất. Bag-of-Words trong Computer Vision Bags of Words cũng được áp dụng trong Computer Vision với cách định nghĩa  words  và từ điển khác. Xét các ví dụ sau: Ví dụ 1: Có hai class ảnh, một là ảnh các khu rừng, một là ảnh các sa mạc. Phân loại một bức ảnh là rừng hay sa mạc (giả sử ta biết rằng nó thuộc một trong hai loại này) một cách trực quan nhất là dựa vào màu sắc. Màu xanh nhiều thì là rừng, màu đỏ và vàng nhiều thì là sa mạc. Vậy chúng ta có thể có một mô hình đơn giản để trích chọn đặc trưng như sau: Với một bức ảnh, chuẩn bị một vector  x x  có số chiều bằng 3, đại diện cho 3 màu xanh ( x 1 x1 ), đỏ ( x 2 x2 ), và vàng ( x 3 x3 ). Với mỗi điểm ảnh trong bức ảnh đó, xem nó gần với màu xanh, đỏ hay vàng nhất dựa trên giá trị của pixel đó. Nếu nó gần điểm xanh nhất, tăng  x 1 x1  lên 1; gần đỏ nhất, tăng  x 2 x2  lên 1; gần vàng nhất, tăng  x 3 x3  lên 1. Sau khi xem xét tất cả các điểm ảnh, dù cho bức ảnh có kích thước thế nào, ta vẫn thu được một vector có độ dài bằng 3, mỗi phần tử thể hiện việc có bao nhiêu pixel trong bức ảnh có màu tương ứng. Vector cuối này còn được gọi là vector histogram của bức ảnh tương ứng với ba màu xanh, đỏ, vàng. Dựa vào vector này, ta có thể quyết định bức ảnh đó là ảnh rừng hay sa mạc. Ví dụ 2: Trên thực tế, các bài toán xử lý ảnh không đơn giản như ví dụ 1 trên đây. Mắt người thực ra nhạy với các đường nét, hình dáng hơn là màu sắc. Một cái (ảnh) cây dù không có màu vẫn là một cái (ảnh) cây! Vì vậy, xem xét gía trị từng điểm ảnh một không mang lại kết quả khả quan vì lượng thông tin bị mất quá nhiều. Có một cách khắc phục là thay vì xem xét một điểm ảnh, ta xem xét một  cửa sổ  nhỏ trong ảnh (trong Computer Vision, cửa sổ này được gọi là patch) là một hình chữa nhật chứa nhiều điểm ảnh gần nhau. Cửa sổ này đủ lớn để có thể chứa được các bộ phận có thể mô tả được vật thể trong ảnh. Ví dụ với mặt người, các patch nên đủ lớn để chứa được các phần của khuôn mặt như mắt, mũi, miệng như hình dưới đây. Hình 3: Bag of Words cho ảnh chứa mặt người. (Nguồn  Bag of visual words model: recognizing object categories ) Tương tự thế, với ảnh là ô tô, các patch thu được có thể là bánh xe, khung xe, cửa xe, … như hàng trên trong hình dưới đây. Hình 4: Bag of Words cho ảnh ô tô. (Nguồn: tôi cố gắng tìm nguồn cho hình này nhưng tất cả các tài liệu tôi tìm được đều ghi “Source: B. Leibe”, tôi cũng xin được trích nguồn tương tự) Có một câu hỏi đặt ra là, trong xử lý văn bản, hai từ được coi là như nhau nếu nó được biểu diễn bởi các ký tự giống nau. Vậy trong xử lý ảnh, hai patch được coi là như nhau khi nào? Khi mọi pixel bằng nhau sao? Cơ hội gặp gỡ, học hỏi kinh nghiệm với các chuyên gia Xamarin trong & ngoài nước! Câu trả lời là không. Xác suất để hai patches giống hệt nhau từng pixel là rất thấp vì có thể một phần của vật thể trong một patch bị lệch đi vài pixel so với phần đó trong patch kia; hoặc phần vật thể trong patch bị méo, hoặc có độ sáng khác nhau, mặc dù ta vẫn nhìn thấy hai patches đó  rất giống nhau . Vậy thì hai patch được coi là như nhau khi nào? Và  từ điển  ở đây được định nghĩa như thế nào? Câu trả lời ngắn: hi patches là gần giống nhau nếu khoảng cách Euclid giữa hai vector tạo bởi hai patches đó gần nhau. Từ điển (codebook) sẽ có số phần tử do ta tự chọn. Số phần tử càng cao thì độ sai lệch càng ít, nhưng sẽ nặng về tính toán hơn. Câu trả lời dài: chúng ta có thể áp dụng  K-means clustering  (thật may là tôi đã trình bày bài này trước). Với rất nhiều patches thu được, giả sử ta muốn xây dựng một codebook với chỉ khoảng 1000  words . Vậy thì ta cho  k = 1000 k=1000  rồi thực hiện K-means clustering trên toàn bộ số patches thu được (từ tập training). Sau khi thực hiện K-means clustering, ta thu được 1000 clusters và 1000 centers tương ứng. Mỗi centers này được coi là một  words , và tất cả những điểm rơi vào cùng một cluster được coi là cùng một bag. Với ảnh trong tập test data, ta cũng lấy các patches rồi xem chúng rơi vào những bags nào. Từ đó suy ra vector đặc trưng cho mỗi bức ảnh. Chú ý rằng với  k = 1000 k=1000 , mỗi bức ảnh sẽ được  mô tả  bởi một vector có số chiều 1000, tức là mỗi điểm dữ liệu bây giờ đã có số chiều bằng nhau, mặc dù ảnh thô đầu vào có thể có kích thước khác nhau. Feature Scaling and Normalization (Tham khảo  Feature Scaling wiki ). Các điểm dữ liệu đôi khi được đo đạc với những đơn vị khác nhau, m và feet chẳng hạn. Hoặc có hai thành phần (của vector dữ liệu) chênh lệch nhau quá lớn, một thành phần có khoảng giá trị từ 0 đến 1000, thành phần kia chỉ có khoảng giá trị từ 0 đến 1 chẳng hạn. Lúc này, chúng ta cần chuẩn hóa dữ liệu trước khi thực hiện các bước tiếp theo. Chú ý:  việc chuẩn hóa này chỉ được thực hiện khi vector dữ liệu đã có cùng chiều. Một vài phương pháp chuẩn hóa thường dùng: Rescaling Phương pháp đơn giản nhất là đưa tất cả các thành phần về cùng một khoảng,  [ 0 , 1 ] [0,1] hoặc  [ − 1 , 1 ] [−1,1]  chẳng hạn, tùy thuộc vào ứng dụng. Nếu muốn đưa một thành phần (feature) về khoảng  [ 0 , 1 ] [0,1] , công thức sẽ là: x ′ = x − min ( x ) max ( x ) − min ( x ) x′=x−min(x)max(x)−min(x) trong đó  x x  là giá trị ban đầu,  x ′ x′  là giá trị sau khi chuẩn hóa.  min ( x ) , max ( x ) min(x),max(x)  được tính trên toàn bộ dữ liệu training data ở cùng một thành phần. Việc này được thực hiện trên từng thành phần của vector dữ liệu  x x . Standardization Một phương pháp nữa cũng hay được sử dụng là giả sử mỗi thành phần đều có phân phối chuẩn với kỳ vọng là 0 và phương sai là 1. Khi đó, công thức chuẩn hóa sẽ là: x ′ = x − x ¯ σ x′=x−x¯σ với  x ¯ , σ x¯,σ  lần lượt là kỳ vọng và phương sai (standard deviation) của thành phần đó trên toàn bộ training data. Scaling to unit length Một lựa chọn khác nữa cũng được sử dụng rộng rãi là chuẩn hóa các thành phần của mỗi vector dữ liệu sao cho toàn bộ vector có độ lớn (Ecluid, tức  norm 2 ) bằng 1. Việc này có thể được thực hiện bằng: x ′ = x | | x | | 2 x′=x||x||2 4. Thảo luận Xem ra thế giới Machine Learning rất rộng lớn và có rất nhiều thứ chúng ta cần làm. Và vẫn có khá nhiều thứ tôi có thể viết được. Tuy nhiên, blog này sẽ không tập trung nhiều vào Feature Learning, mặc dù sẽ có một vài bài nói về Dimensionality Reduction. Tôi sẽ sử dụng các bộ dữ liệu có sẵn, và đã qua bước Feature Learning. 5. Tài liệu tham khảo Feature Enginieering – wiki Feature Scaling wiki Csurka, Gabriella, et al. “ Visual categorization with bags of keypoints. ” Workshop on statistical learning in computer vision, ECCV. Vol. 1. No. 1-22. 2004. Bag of Words model – wiki Bag of Words Meets Bags of Popcorn Techtalk via  tiepvupsu TAGS Feature Engineering Facebook Twitter Bài viết trước Microsoft và chương trình Surface Hub Try Bài kế tiếp Top 5 Javascript Frameworks năm 2017 Chủ tịch FPT Software Hoàng Nam Tiến: Làm phần mềm cần 10-14h/ngày, anh chị nào quen làm 5 – 6h ở những nơi khác... April 25, 2017 Làm thế nào để học lập trình nhanh hơn? February 16, 2017 Triết lý lập trình của NASA July 26, 2017 Mẫu Chuyện nhỏ đáng yêu tiếp động lực cho Nữ IT Việt Nam March 3, 2017 Ứng dụng di dộng – ngày tàn đang đến? June 9, 2017 Tâm sự của các nữ coder sống chết với nghề May 12, 2017 “Ở Việt Nam, cơ hội để thực sự làm về Trí tuệ nhân tạo còn quá ít, trong khi những thứ mang hình thù... October 21, 2017 [ Học bổng] Học tiếng Nhật miễn phí cùng ” Akira”- chắp cánh ước mơ October 19, 2017 VWS2017 – Xây dựng Decentralized web trên nền tảng Blockchain October 19, 2017 VỀ CHÚNG TÔI • Giấy phép thiết lập Mạng xã hội số 569/GP-BTTTT do Bộ Thông Tin và Truyền Thông cấp.\r\n• Cơ quan chủ quản: Công ty Cổ phần Applancer.\r\nTrụ sở: 179 Đường Nguyễn Đình Chính, Phường 11, Quận PN, TP.HCM  Liên hệ:  hieuld@applancer.net - Tel: 028 6264 5022 THEO CHÚNG TÔI",
          "url": "https://techtalk.vn/gioi-thieu-ve-feature-engineering.html",
          "relevance": "0"
        },
        {
          "title": "Phân loại văn bản bằng định lý Bayes",
          "content": "Tìm kiếm trang web này Trang chủ LY NAM PHONG Lưu Tuấn Anh Nguyễn Văn Hải Các công cụ xử lý Trích lọc tiếng Việt từ HTML DongDu Download dữ liệu Giới thiệu về các nghiên cứu mới [Máy học]Learning Combination Features with L1 Regularization [phân loại]Text Categorization with All Substring Features  [in Japanese] Automatic Tree and String Based Wrapper Generation for Semi-structured Documents Extracting Structured Data from Web Pages Online Feature Selection using Grafting Kiến thức cơ bản về xử lý ngôn ngữ tự nhiên Khởi đầu NLP với Python Liblinear-thư viện học máy Lựa chọn đặc trưng (Feature selection) Machine Learning trong NLP Mô hình ngôn ngữ NLP là gì ? Phân nhóm dữ liệu (Clustering) Thuật toán tách từ (Tokenizer) Xử lý tiếng Việt bằng Python (1) Ứng dụng Pointwise để tách từ Nghiên cứu của tác giả Bài toán thêm dấu cho tiếng Việt Việt hoá Mecab Nhập môn Linux SHELL là gì SHELL mạnh nhất : zsh Tài nguyên ngôn ngữ tiếng Việt Khái yếu về corpus Khái yếu về từ điển Kế hoạch xây dựng tự động corpus từ nguồn Web Đặc trưng của tiếng Việt Tạp đàm seminar là gì Sơ đồ trang web Hoạt động gần đây của trang web Tác giả trang anh tháng hai 6, 2012 Kiến thức cơ bản về xử lý ngôn ngữ tự nhiên ‎ > ‎ Machine Learning trong NLP ‎ > ‎\n   Phân loại văn bản bằng định lý Bayes Là 1 phương pháp phân loại có giám sát. Dù rất dễ hiểu và dễ cài đặt,\nnhưng kết quả thu được lại rất tốt, vì thế đây là 1 phương pháp rất quan trọng\ntrong NLP. Ứng dụng đầu tiên trong NLP của máy phân loại này là phân loại văn bản.\nGần đây, máy phân loại này còn được ứng dụng thành công vào phần mềm lọc spam tự\nđộng. Lý thuyết về định lý Bayes và máy phân loại Bayes đã được nhắc đến trong phần  Định lý Bayes . Bài viết này sẽ nói kĩ và sâu hơn về việc ứng dụng phân loại Bayes cho phân loại văn bản. Cuối cùng sẽ có 1 ví dụ về ứng dụng máy phân loại Bayes trong bộ lọc spam.  bài toán phân loại văn bản .  Vector\nđặc trưng x biểu diễn số lần xuất hiện các từ trong văn bản,  y\nlà các catalog (nhãn) mà văn bản thuộc về (ví dụ như thể thao, kinh tế, giải\ntrí, …) Cho 1 tập dữ liệu huấn luyện đã được gán\nnhãn D={ (x (i) ,y­ (i) ) }với i=1~N.  Ở đây x (i)  là vector đặc trưng thứ i\ntrong tập huấn luyện, y­ (i) thuộc  { 1,2,…,C }  là\ncác nhãn tương ứng với vector đó.  x (i) =(\nx (i) 1 ,x (i) 2 ,x (i) D )\nx (i) d  là số lần xuất hiện của từ thứ d trong từ điển (từ\ngiờ sẽ gắn số thứ tự với từ, nên sẽ gọi là từ d). Áp dụng công thứ Bayes, ta tính giá trị của p(y|x), nếu giá trị này lớn hơn 1 giá trị t cho trước, ta kết luận nhãn của vector x là y. Yêu cầu đặt ra là ngăn chặn spam bằng cách phân loại một email gửi đến là spam hay non-spam. Cần đạt được hiệu quả phân loại email thật khả quan. Tuy nhiên cần tuyệt đối tránh lỗi sai cho rằng email non-spam là spam vì có thể gây hậu quả nghiêm trọng hơn là khả năng lọc spam thấp. Do đó yêu cầu đối với hệ thống là phải nhận ra được email spam càng nhiều càng tốt và giảm thiểu lỗi nhận sai email non-spam là email spam. Bài toán bộ lọc spam Ý tưởng của phương pháp là tìm cách xây dựng một bộ phân loại nhằm phân loại cho một mẫu mới bằng cách huấn luyện từ những mẫu có sẵn. Ở đây mỗi mẫu mà ta xét đến chính là mỗi một email, tập các lớp mà mỗi email có thể thuộc về là y={spam, non-spam} Khi ta nhận được 1 email mới gửi đến, khi đó ta dựa vào một số đặc điểm hay thuộc tính nào đó của email để tăng khả năng phân loại chính xác email đó. Các đặc điểm của 1 email như: tiêu đề, nội dung, có tập tin đính kèm hay không… Càng nhiều những thông tin như vậy xác suất phân loại đúng càng lớn, tất nhiên còn phụ thuộc vào kích thước của tập mẫu huấn luyện. Việc tính toán xác suất sẽ dựa vào công thức Naïve Bayes, từ xác suất thu được ta đem so sánh với một giá trị ngưỡng t nào đó mà ta xem là ngưỡng để phân loại email spam hay non-spam. Nếu lớn hơn t thì email đó là spam, ngược lại là non-spam. Như ta đã biết khi phân loại email có hai lỗi : lỗi nhận 1 email non-spam thành spam và lỗi cho qua một email spam. Loại lỗi thứ nhất nghiêm trọng hơn, vì vậy ta xem mỗi một email non-spam như là λ email non-spam. Như vậy khi lỗi nhận 1 email non-spam thành spam xảy ra ta xem như là λ lỗi, và khi phân loại đúng xem như λ lần thành công. Ngưỡng để phân loại t sẽ phụ thuộc và chỉ số λ này. CƠ SỞ LÝ THUYẾT Công thức xác suất có điều kiện Xác suất điều kiện của biến cố A với điều kiện biến cố B đã xảy ra là một số không âm, ký hiệu là P( A/B ) nó biểu thị khả năng xảy ra biến cố A trong tình huống biến cố B đã xảy ra. P( A/B ) = (P( AB ))/(P( B )) Suy ra P( A/B ) . P( B ) = P( B/A ) . P( A ) = P( AB ) Công thức xác suất đầy đủ  Giả sử B1, B2, … Bn là 1 nhóm đầy đủ các biến cố. Xét biến cố A sao cho A xảy ra chỉ khi một trong các biến cố B1, B2, … Bn xảy ra. Khi đó : P(A) = ∑ P(Bi) . P(A/Bi) Công thức xác suất Bayes   Từ các công thức ở trên ta có công thức xác suất Bayes : P(Bk/A) = (P(ABk) )/(P(A) ) = (P(Bk) .P(A/Bk) )/(ΣP(Bi) .P(A/Bi)) Phương pháp phân loại Naïve-Bayesian Phân loại Bayesian là phương pháp phân loại sử dụng tri thức các xác suất đã qua huấn luyện. Phương pháp này thích hợp với những lớp bài toán đòi hỏi phải dự đoán chính xác lớp của mẫu cần kiểm tra dựa trên những thông tin từ tập huấn luyện ban đầu Giả thiết mỗi một email được đại diện bởi một vector thuộc tính đặc trưng là x = (x1, x2,…,xn) với x1, x2, …, xn là giá trị của các thuộc tính X1, X2,…,Xn tương ứng trong không gian vector đặc trưng X Dựa vào công thức xác suất Bayes và công thức xác suất đầy đủ ta có được xác suất 1 email với vector đặc trưng x thuộc về loại c là : P(C=c | X=x) = (P(C=c) .P(X=x | C=c) )/(∑P(C=k) .P(X=x | C=k))  với C là email được xét , c € {spam, non-spam} Xác suất P(C=c) được tính dễ dàng từ tập huấn luyện. Thực tế rất khó để tính được xác suất P(X=x | C=c) . Giả thiết rằng tất cả các biến cố X1, X2…Xn là độc lập với nhau do đó chúng ta có thể tính được xác suất P(X=x | C=c) dựa theo công thức: P(X=x | C=c) = ∏ P(Xi=xi | C=c) Như vậy công thức tính xác suất 1 email là spam sẽ được viết thành : P(C=c | X=x) = (P(C=c) . ∏ P(Xi=xi | C=c) )/(∑ P(C=k) .∏ P(Xi=xi | C=k)) Từ xác suất này ta so sánh với một giá trị ngưỡng t là ngưỡng để phân loại email là spam hay không, nếu xác suất này lớn hơn t, ta cho email đó là spam, ngược lại email đó là non-spam Trong phân loại email có 2 loại sai lầm, một là sai lầm nhận 1 email spam thành non-spam và sai lầm thứ 2 là nhận 1 email non-spam thành spam. Rõ ràng sai lầm thứ 2 là nghiêm trọng hơn vì người dùng có thể chấp nhận một email spam vượt qua bộ lọc nhưng không thể chấp nhận một email hợp lệ quan trọng lại bị bộ lọc chặn lại. Giả sử ta gọi S->N và N->S tương ứng với 2 loại lỗi ở trên. Để hạn chế loại lỗi thứ 2 ta giả sử rằng lỗi N->S có chi phí gấp λ lỗi S->N nghĩa là ta phân loại 1 email là spam dựa theo : (P(C=spam | X=x) )/(P(C=non-spam | X=x)) > λ Mặt khác P(C=spam | X=x) = 1 – P(C=non-spam | X=x) và P(C=spam | X=x) > t Như vậy ta giá trị ngưỡng t phụ thuộc vào λ, cụ thể : t = λ / (λ + 1) PHƯƠNG PHÁP THỰC HIỆN Để đánh giá 1 email ta phải chuyển mỗi một email sang một vector x = (x1,x2,…xn) với x1,x2,..xn là giá trị các thuộc tính X1,X2…Xn trong không gian vector đặc trưng X. Mỗi thuộc tính được thể hiện bởi một token đơn. Theo phương pháp đơn giản nhất ta có thể lập ra một từ điển chứa các token. Sau đó với mỗi token trong email nếu nó xuất hiện trong từ điển thì giá trị thuộc tính sẽ là 1, ngược lại thì là 0. Tuy nhiên trên thực tế, tập huấn luyện của ta không thường là một bộ từ điển như vậy. Thay vào đó tập huấn luyện lúc này sẽ gồm có 2 kho ngữ liệu. Kho ngữ liệu Spam sẽ chứa một list các email đã được xác định là spam trước đó, và tương tự với kho ngữ liệu Non-spam sẽ chứa các email hợp lệ. Như vậy nếu ta vẫn để giá trị các thuộc tính là 0 hoặc 1 thì sẽ rất khó đánh giá được 1 email là spam hay không. Đặc biệt nếu email nhận được là dài, khi đó nếu ta vẫn sử dụng giá trị thuộc tính là 0 hoặc 1 thì sự xuất hiện của 1 token 100 lần cũng tương đương với việc xuất hiện chỉ 1 lần. Để khắc phục vấn đề này giá trị thuộc tính bây giờ ta sẽ thay bằng xác suất spam của token đó. Xác suất này tương đương với xác suất spam của 1 email chỉ chứa token đó và là email spam. Việc tính xác suất này thì có nhiều phương pháp. Ta có thể tính dựa trên số lần xuất hiện của token này trong mỗi kho ngữ liệu học ban đầu. Ví dụ một token w có số lần xuất hiện trong kho ngữ liệu spam là s và non-spam là n, số email tổng cộng ở kho spam và non-spam tương ứng là Ns và Nn thì xác suất spam của token w này sẽ là : P(X=w | C=spam) = (s/Ns)/(s/Ns+n/Nn) Tuy nhiên nhược điểm của phương pháp này khả năng spam của một token xuất hiện 100 lần ở 100 email khác nhau là bằng với khả năng spam của một token xuất hiện 100 lần chỉ ở trong 1 email. Thay vào việc tính xác suất này dựa theo số lần xuất hiện của token trong từng kho ngữ liệu ta có thể dựa vào số email chứa token trong từng kho ngữ liệu. Ví dụ một token w có số email chứa nó trong kho ngữ liệu spam và non-spam là ns và nn thì xác suất spam của token w này sẽ là : P(X=w | C=spam) = (s/Ns)/(ns/Ns+nn/Nn) Nhược điểm của phương pháp này là khả năng spam của một token xuất hiện 1 lần trong 1 email là bằng với khả năng spam của một token xuất hiện 100 lần trong 1 email. Vì vậy chúng ta sử dụng cách thứ ba là tổng hợp của hai cách trên : P(X=w | C=spam) = ((s*ns)/Ns)/((ns*s)/Ns+(nn*n)/Nn)) Còn đối với các token chỉ xuất hiện trong kho ngữ liệu này mà không xuất hiện trong kho ngữ liệu kia thì không thể kết luận một token chỉ xuât hiện ở kho ngữ liệu spam thì không bao giờ xuất hiện trong kho ngữ liệu non-spam và ngược lại. Cách thích hợp thì ta sẽ gán cho chúng một giá trị phù hợp. Với những token chỉ xuất hiện trong kho ngữ liệu spam thì ta gán xác suất spam cho nó là giá trị N gần với 1 ( chẳng hạn 0,9999) và ngược lại thì gán xác suất spam là giá trị M gần với 0 ( chẳng hạn 0,0001). Như vậy ta có công thức tính xác suất spam của token dựa trên số lần xuất hiện và số email chứa nó là : P = Max ( M, Min ( N, ((ns*s)/Ns)/((ns*s)/Ns+(nn*n)/Nn) ) ) ns : số email chứa token trong kho spam nn : số email chứa token trong kho non-spam s : số lần token xuất hiện trong kho spam n : số lần token xuất hiện trong kho non-spam Ns : tổng số email trong kho spam Nn : tổng số email trong kho non-spam ================ Như thế, các bạn đã nắm được những khái niệm cơ bản nhất về định lý Bayes và bộ lọc Bayes. Tiếp theo, tôi xin giới thiệu 1 phần mềm mã nguồn mở sử dụng bộ lọc spam bằng máy phân loại Bayes. Các bạn có thể    tải về . Các tài liệu hướng dẫn có sẵn bên trong. Dù thuật toán rất đơn giản, nhưng với bộ test của chương trình, độ chính xác vẫn lên đến 96%.  Comments Sign in | Recent Site Activity | Report Abuse | Print Page | Powered By Google Sites",
          "url": "http://viet.jnlp.org/kien-thuc-co-ban-ve-xu-ly-ngon-ngu-tu-nhien/machine-learning-trong-nlp/phan-loai-van-ban-bang-dinh-ly-bayes",
          "relevance": "0"
        },
        {
          "title": "Tìm hiểu về mô hình không gian vector",
          "content": "Bút Chì Số About Category Tìm hiểu về mô hình không gian vector Oct 27, 2013 • hungnq1989 Hung Neo Full stack developer. GitHub Twitter Email Newest Posts SOLID - 5 nguyên tắc của thiết kế hướng đối tượng Laravel : Dependency Injection và IoC container Từ đồng nghĩa trong ElasticSearch Mã hoá bất đối xứng RSA Generator in ES6 and solving callback hell TOC \n© 2016 Hung Neo\n",
          "url": "http://butchiso.com/2013/10/tim-hieu-ve-mo-hinh-khong-gian-vector.html",
          "relevance": "1"
        },
        {
          "title": "Bài 11: Giới thiệu về Feature Engineering",
          "content": "Latest by category Q1. Quick Notes 1 15. Overfitting 11. Feature Engineering 2. Phân nhóm các thuật toán Machine Learning 1. Giới thiệu về Machine Learning Latest FundaML.com 33. Đánh giá hệ thống phân lớp (1/2) 32. Naive Bayes Classifier Viết và nhận xét các bài báo khoa học 31. Maximum Likelihood và Maximum A Posteriori Con đường học Toán của tôi 30. Ôn tập Xác Suất Q2. Transfer Learning 29. Linear Discriminant Analysis Q1. Quick Notes 1 28. Principal Component Analysis (2/2) 27. Principal Component Analysis (1/2) 26. Singular Value Decomposition 25. Matrix Factorization Collaborative Filtering 24. Neighborhood-Based Collaborative Filtering 23. Content-based Recommendation Systems 22. Multi-class SVM 21. Kernel SVM 20. Soft Margin SVM 19. Support Vector Machine 18. Duality 17. Convex Optimization Problems 16. Convex sets và convex functions 15. Overfitting 14. Multi-layer Perceptron và Backpropagation 13. Softmax Regression 12. Binary Classifiers 11. Feature Engineering 10. Logistic Regression 9. Perceptron Learning Algorithm 8. Gradient Descent (2/2) 7. Gradient Descent (1/2) 6. K-nearest neighbors 5. K-means Clustering - Applications 4. K-means Clustering 3. Linear Regression 2. Phân nhóm các thuật toán Machine Learning 1. Giới thiệu về Machine Learning Machine Learning cơ bản  About Index Tags Categories Archive Math Copyrights ebook Search Trong trang này: 1. Giới thiệu 2. Mô hình chung cho các bài toán Machine Learning TRAINING PHASE Feature Extractor Main Algorithms TESTING PHASE 3. Một số ví dụ về Feature Engineering Trực tiếp lấy raw data Feature selection Dimensionality reduction Bag-of-words Bag-of-Words trong Computer Vision Feature Scaling and Normalization Rescaling Standardization Scaling to unit length 4. Thảo luận 5. Tài liệu tham khảo 1. Giới thiệu Cho tới lúc này, tôi đã trình bày 5 thuật toán Machine Learning cơ bản:  Linear Regression ,  K-means Clusterning ,  K-nearest neighbors ,  Perceptron Learning Algorithm  và  Logistic Regression . Trong tất cả các thuật toán này, tôi đều giả sử các điểm dữ liệu được biểu diễn bằng các vector, được gọi là  feature vector  hay  vector đặc trưng , có độ dài bằng nhau, và cùng là vector cột hoặc vector hàng. Tuy nhiên, trong các bài toán thực tế, mọi chuyện không được tốt đẹp như vậy! Với các bài toán về Computer Vision, các bức ảnh là các ma trận có kích thước khác nhau. Thậm chí để nhận dạng vật thể trong ảnh, ta cần thêm một bước nữa là  object detection , tức là tìm cái khung chứa vật thể chúng ta cần dự đoán. Ví dụ, trong bài toán nhận dạng khuôn mặt, chúng ta cần tìm được vị trí các khuôn mặt trong ảnh và  crop  các khuôn mặt đó trước khi làm các bước tiếp theo. Ngay cả khi đã xác định được các khung chứa các khuôn mặt (và có thể resize các khung đó về cùng một kích thước), ta vẫn phải làm rất nhiều việc nữa vì hình ảnh của khuôn mặt còn phụ thưộc vào góc chụp, ánh sáng, … và rất nhiều yếu tố khác nữa. Các bài toán NLP (Natural Language Processing - Xử lý ngôn ngữ tự nhiên) cũng có khó khăn tương tự khi độ dài của các văn bản là khác nhau, thậm chí có những từ rất hiếm gặp hoặc không có trong từ điển. Cũng có khi thêm một vài từ vào văn bản mà nội dung của văn bản không đổi hoặc hoàn toàn mang nghĩa ngược lại. Hoặc cùng là một câu nói nhưng tốc độ, âm giọng của mỗi người là khác nhau, thậm chí của cùng một người nhưng lúc ốm lúc khỏe cũng khác nhau. Khi làm việc với các bài toán Machine Learning thực tế, nhìn chung chúng ta chỉ có được dữ liệu thô (raw) chưa qua chỉnh sửa, chọn lọc. Chúng ta cần phải tìm một phép biến đổi để loại ra những dữ liệu nhiễu (noise), và để đưa dữ liệu thô với số chiều khác nhau về cùng một chuẩn (cùng là các vector hoặc ma trận). Dữ liệu chuẩn mới này phải đảm bảo giữ được những thông tin đặc trưng (features) cho dữ liệu thô ban đầu. Không những thế, tùy vào từng bài toán, ta cần  thiết kế  những phép biến đổi để có những features phù hợp. Quá trình quan trọng này được gọi là  Feature Extraction , hoặc  Feature Engineering , một số tài liệu tiếng Việt gọi nó là  trích chọn đặc trưng . Tôi xin trích một câu nói của thầy Andrew Ng và xin phép thêm không dịch ra tiếng Việt (Nguồn  Feature Engineering - wiki ): Coming up with features is difficult, time-consuming, requires expert knowledge. “Applied machine learning” is basically feature engineering. Để giúp các bạn có cái nhìn tổng quan hơn, trong phần tiếp theo tôi xin đặt bước Feature Engineering này trong một bức tranh lớn hơn. 2. Mô hình chung cho các bài toán Machine Learning Phần lớn các bài toán Machine Learning có thể được thể hiện trong hình (tôi) vẽ dưới đây: Hình 1: Mô hình chung cho các bài toán Machine Learning. Có hai phases lớn là Training phase và Testing phase. Xin nhắc lại là với các bài toán Supervised learning, ta có các cặp dữ liệu ( input, output ), với các bài toán Unsupervised learing, ta chỉ có  input  mà thôi. TRAINING PHASE Có hai khối có nền màu xanh lục chúng ta cần phải thiết kế: Feature Extractor ĐẦU RA Tôi xin đề cập đầu ra của khối này trước vì mục đích của Feature Engineering là tạo ra một Feature Extractor biến dữ liệu thô ban đầu thành dữ liệu phù hợp với từng mục đích khác nhau. ĐẦU VÀO raw training input . Raw input là tất cả các thông tin ta biết về dữ liệu. Ví dụ: với ảnh thì là giá trị của từng pixel; với văn bản thì là từng từ, từng câu; với file âm thanh thì nó là một đoạn tín hiệu; với cơ sở dữ liệu  Iris  thì nó là độ dài các cánh hoa và đài hoa, … Dữ liệu thô này thường không ở dạng vector, không có số chiều như nhau. Thậm chí có thể có số chiều như nhau nhưng số chiều quá lớn, như một bức ảnh màu 1000 pixel x 1000 pixel thì số  elements  đã là \\(3 \\times 10^6\\) (3 vì ảnh màu thường có 3 channels: Red, Green, Blue). Đây là một con số quá lớn, không lợi cho lưu trữ và tính toán. (optional)  output  của  training set . Trong các bài toán Unsupervised learning, ta không biết  output  nên hiển nhiên sẽ không có đầu vào này. Trong các bài toán Supervised learning, có khi dữ liệu này cũng không được sử dụng. Ví dụ: nếu  raw input  đã có cùng số chiều rồi nhưng số chiều quá lớn,  ta muốn giảm số chiều của nó thì cách đơn giản nhất là  chiếu  vector đó xuống một không gian có số chiều nhỏ hơn bằng cách lấy một ma trận ngẫu nhiên nhân với nó. Ma trận này thường là ma trận  béo  (số hàng ít hơn số cột, tiếng Anh - fat matrices) để đảm bảo số chiều thu được nhỏ hơn số chiều ban đầu. Việc làm này mặc dù làm mất đi thông tin, trong nhiều trường hợp vẫn mang lại hiệu quả vì đã giảm được lượng tính toán ở phần sau. Đôi khi  ma trận chiếu  không phải là ngẫu nhiên mà có thể được  học  dựa trên toàn bộ  raw input , ta sẽ có bài toán tìm ma trận chiếu để lượng thông tin mất đi là ít nhất. Trong nhiều trường hợp, dữ liệu  output  của  training set  cũng được sử dụng để tạo ra Feature Extractor. Ví dụ: trong bài toán classification, ta không quan tâm nhiều đến việc mất thông tin hay không, ta chỉ quan tâm đến việc những thông tin còn lại có đặc trưng cho từng class hay không. Ví dụ, dữ liệu thô là các hình vuông và hình tam giác có màu đỏ và xanh. Trong bài toán phân loại đa giác, các output là  tam giác  và  vuông ,  thì ta không quan tâm tới màu sắc mà chỉ quan tâm tới số cạnh của đa giác. Ngược lại, trong bài toán phân loại màu, các class là  xanh  và  đỏ , ta không quan tâm tới số cạnh mà chỉ quan tâm đến màu sắc thôi. (optional)  Prior knowledge about data : Đôi khi những giả thiết khác về dữ liệu cũng mang lại lợi ích. Ví dụ, trong bài toán classification, nếu ta biết dữ liệu là (gần như)  linearly separable  thì ta sẽ đi tìm một ma trận chiếu sao cho ở trong không gian mới, dữ liệu vẫn đảm bảo tính  linearly separable , việc này thuận tiện hơn cho phần classification vì các thuật toán linear, nhìn chung, đơn giản hơn. Sau khi  học  được feature extractor thì ta cũng sẽ thu được  extracted features  cho  raw input data . Những  extracted features  này được dùng để huấn luyện các thuật toán Classification, Clustering, Regression,… ở phía sau. Main Algorithms Khi có được  extracted features  rồi, chúng ta sử dụng những thông tin này cùng với (optional)  training output  và (optional)  prior knowledge  để tạo ra các mô hình phù hợp, điều mà chúng ta đã làm ở những bài trước. Chú ý:  Trong một số thuật toán cao cấp hơn, việc  huấn luyện  feature extractor và main algorithm được thực hiện cùng lúc với nhau chứ không phải từng bước như trên. Một điểm rất quan trọng: khi xây dựng bộ  feature extractor  và  main algorithms , chúng ta không được sử dụng bất kỳ thông tin nào trong tập  test data . Ta phải giả sử rằng những thông tin trong  test data  chưa được nhìn thấy bao giờ. Nếu sử dụng thêm thông tin về  test data  thì rõ ràng ta đã  ăn gian ! Tôi từng đánh giá các bài báo khoa học quốc tế, rất nhiều tác giả xây dựng mô hình dùng cả dữ liệu  test data , sau đó lại dùng chính mô hình đó để kiểm tra trên  test data  đó. Việc  ăn gian  này là lỗi rất nặng và hiển nhiên những bài báo đó bị từ chối (reject). TESTING PHASE Bước này đơn giản hơn nhiều. Với  raw input  mới, ta sử dụng feature extractor đã tạo được ở trên (tất nhiên không được sử dụng  output  của nó vì  output  là cái ta đang đi tìm) để tạo ra feature vector tương ứng. Feature vector được đưa vào  main algorithm  đã được học ở training phase để dự đoán  output . 3. Một số ví dụ về Feature Engineering Trực tiếp lấy raw data Với bài toán phân loại chữ số viết tay trong bộ cơ sở dữ liệu  MNIST , mỗi bức ảnh có số chiều là 28 pixel x 28 pixel (tất nhiên việc  crop  và chỉnh sửa mỗi bức ảnh đã được thực hiện từ trước rồi, đó đã là một phần của feature engineering rồi). Một cách đơn giản thường được dùng là  kéo dài  ma trận 28x28 này để được 1 vector có số chiều 784. Trong cách này, các cột (hoặc hàng) của ma trận ảnh được đặt chồng lên (hoặc cạnh nhau) để được 1 vector dài. Vector dài này được trực tiếp sử dụng làm feature đưa vào các bộ classifier/clustering/regression/… Lúc này, giá trị của mỗi pixel ảnh được coi là một feature. Rõ ràng việc làm đơn giản này đã làm mất thông tin về  không gian  (spatial information) giữa các điểm ảnh, tuy nhiên, trong nhiều trường hợp, nó vẫn mang lại kết quả khả quan. \n Feature selection Giả sử rằng các điểm dữ liệu có số features khác nhau (do kích thước dữ liệu khác nhau hay do một số feature mà điểm dữ liệu này có nhưng điểm dữ liệu kia lại không thu thập được), và số lượng features là cực lớn. Chúng ta cần  chọn  ra một số lượng nhỏ hơn các feature phù hợp với bài toán.  Chọn thế nào  và  thế nào là phù hợp  lại là một bài toán khác, tôi sẽ không bàn thêm ở đây. Dimensionality reduction Một phương pháp nữa tôi đã đề cập đó là làm giảm số chiều của dữ liệu để giảm bộ nhớ và khối lượng tính toán. Việc giảm số chiều này có thể được thực hiện bằng nhiều cách, trong đó  random projection  là cách đơn giản nhất. Tức chọn một  ma trận chiếu  (projection matrix) ngẫu nhiên (ma trận béo) rồi nhân nó với từng điểm dữ liệu (giả sử dữ liệu ở dạng vector cột) để được các vector có số chiều thấp hơn. Ví dụ, vector ban đầu có số chiều là 784, chọn  ma trận chiếu  có kích thước (100x784), khi đó nếu nhân ma trận chéo này với vector ban đầu, ta sẽ được một vector mới có số chiều là 100, nhỏ hơn số chiều ban đầu rất nhiều. Lúc này, có thể ta không có tên gọi cho mỗi feature nữa vì các feature ở vector ban đầu đã được trộn lẫn với nhau theo một tỉ lệ nào đó rồi lưu và vector mới này. Mỗi thành phần của vector mới này được coi là một feature (không tên). Việc chọn một ma trận chiếu ngẫu nhiên đôi khi mang lại kết quả tệ không mong muốn vì thông tin bị mất đi quá nhiều. Một phương pháp được sử dụng nhiều để hạn chế lượng thông tin mất đi có tên là  Principle Component Analysis  sẽ được tôi trình bày sau đây khoảng 1-2 tháng. Chú ý:  Feature learning không nhất thiết phải làm giảm số chiều dữ liệu, đôi khi feature vector còn có số chiều lớn hơn raw data. Random projection cũng có thể làm được việc này nếu ma trận chiếu là một ma trận  cao  (số cột ít hơn số hàng). Bag-of-words Hẳn rất nhiều bạn đã tự đặt câu hỏi: Với một văn bản thì feature vector sẽ có dạng như thế nào? Làm sao đưa các từ, các câu, đoạn văn ở dạng  text  trong các văn bản về một vector mà mỗi phần tử là một số? Có một phương pháp rất phổ biến giúp bạn trả lời những câu hỏi này. Phương pháp đó có tên là  Bag of Words (BoW)  ( Túi đựng Từ ). Vẫn theo thói quen, tôi bắt đầu bằng một ví dụ. Giả sử chúng ta có bài toán phân loại tin rác. Ta thấy rằng nếu một tin có chứa các từ  khuyến mại, giảm giá, trúng thưởng, miễn phí, quà tặng, tri ân, …  thì nhiều khả năng đó là một tin nhắn rác. Vậy phương pháp đơn giản nhất là  đếm  xem trong tin đó có bao nhiêu từ thuộc vào các từ trên, nếu nhiều hơn 1 ngưỡng nào đó thì ta quyết định đó là tin rác. (Tất nhiên bài toán thực tế phức tạp hơn nhiều khi các từ có thể được viết dưới dạng không dấu, hoặc bị cố tình viết sai chính tả, hoặc dùng ngôn ngữ teen). Với các loại văn bản khác nhau thì lượng từ liên quan tới từng chủ đề cũng khác nhau. Từ đó có thể dựa vào số lượng các từ trong từng loại để làm các vector đặc trưng cho từng văn bản. Tôi xin lấy ví dụ cụ thể hơn về cách tạo ra vector đặc trưng cho mỗi văn bản dựa trên BoW và xin được lấy tiếng Anh làm ví dụ (nguồn  Bag of Words wiki . Tiếng Việt khó hơn vì một từ có thể có nhiều âm tiết, tiếng Anh thì thường cứ gặp dấu cách là kết thúc một từ). Giả sử chúng ta có hai văn bản đơn giản: (1) John likes to watch movies. Mary likes movies too.\n và (2) John also likes to watch football games.\n Dựa trên hai văn bản này, ta có danh sách các từ được sử dụng, được gọi là  từ điển  với 10  từ  như sau: [\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"also\", \"football\", \"games\", \"Mary\", \"too\"]\n Với mỗi văn bản, ta sẽ tạo ra một vector đặc trưng có số chiều bằng 10, mỗi phần tử đại diện cho số từ tương ứng xuất hiện trong văn bản đó. Với hai văn bản trên, ta sẽ có hai vector đặc trưng là: (1) [1, 2, 1, 1, 2, 0, 0, 0, 1, 1]\n(2) [1, 1, 1, 1, 0, 1, 1, 1, 0, 0]\n Văn bản (1) có 1 từ “John”, 2 từ “likes”, 0 từ “also”, 0 từ “football”, … nên ta thu được vector tương ứng như trên. Có một vài điều cần lưu ý trong BoW: Với những ứng dụng thực tế,  từ điền  có nhiều hơn 10 từ rất nhiều, có thể đến một trăm nghìn hoặc cả triệu, như vậy vector đặc trưng thu được sẽ rất  dài . Một văn bản chỉ có 1 câu, và 1 tiểu thuyết nghìn trang đều được biểu diễn bằng các vector có số chiều bằng 100 nghìn hoặc 1 triệu. Có rất nhiều từ trong từ điển không xuất hiện trong một văn bản. Như vậy các vector đặc trưng thu được thường có rất nhiều phần tử bằng 0. Các vector có nhiều phần tử bằng 0 được gọi là  sparse vector  (sparse hiểu theo nghĩa là  thưa thớt, rải rác , tôi xin phép chỉ sử dụng khái niệm này bằng tiếng Anh). Để việc lưu trữ được hiệu quả hơn, ta không lưu cả vector đó mà chỉ lưu  vị trí  của các phần tử khác 0 và  giá trị  tương ứng. Lưu ý: nếu có hơn 50% số phần tử khác 0, việc làm này lại phản tác dụng! Thi thoảng có những từ hiếm gặp không nằm trong từ điển, vậy ta sẽ làm gì? Một cách thường được dùng là  mở rộng  vector đặc trưng thêm 1 phần tử, gọi là phẩn tử  <Unknown> . Mọi từ không có trong từ điền đều được coi là  <Unknown> . Nghĩ kỹ một chút, những từ hiếm đôi khi lại mang những thông tin qua trọng nhất mà chỉ loại văn bản đó có. Đây là một nhược điểm của BoW. Có một phương pháp cải tiến khác giúp khắc phục nhược điểm này có tên là Term Frequency-Inverse Document Frequency (TF-IDF) dùng để xác định tầm quan trọng của một từ trong một văn bản dựa trên toàn bộ văn bản trong cơ sở dữ liệu (corpus). Bạn đọc muốn tìm hiểu thêm có thể xem  5 Algorithms Every Web Developer Can Use and Understand, section 5. Nhược điểm lớn nhất của BoW là nó không mang thông tin về thứ tự của các từ. Cũng như sự liên kết giữa các câu, các đoạn văn trong văn bản. Ví dụ, ba câu sau đây: “ Em yêu anh không? ”, “ Em không yêu anh ”, và “ Không, (nhưng) anh yêu em ” khi được trích chọn đặc trưng bằng BoW sẽ cho ra ba vector giống hệt nhau, mặc dù ý nghĩa khác hẳn nhau. Bonus:  hình dưới đay là tần suất sử dụng các từ (coi mỗi âm tiết là một từ) trong Truyện Kiều ( theo bản này ) nếu ta chỉ sử dụng 30 từ có tần suất cao nhất. : Hình 2: Bag of Words cho Truyện Kiều với 30 từ có tần suất cao nhất. Bag-of-Words trong Computer Vision Bags of Words cũng được áp dụng trong Computer Vision với cách định nghĩa  words  và từ điển khác. Xét các ví dụ sau: Ví dụ 1: Có hai class ảnh, một class là ảnh các khu rừng, một class là ảnh các sa mạc. Phân loại một bức ảnh là rừng hay sa mạc (giả sử ta biết rằng nó thuộc một trong hai loại này) một cách trực quan nhất là dựa vào màu sắc. Màu xanh nhiều thì là rừng, màu đỏ và vàng nhiều thì là sa mạc. Vậy chúng ta có thể có một mô hình đơn giản để trích chọn đặc trưng như sau: Với một bức ảnh, chuẩn bị một vector \\(\\mathbf{x}\\) có số chiều bằng 3, đại diện cho 3 màu xanh (\\(x_1\\)), đỏ (\\(x_2\\)), và vàng (\\(x_3\\)). Với mỗi điểm ảnh trong bức ảnh đó, xem nó gần với màu xanh, đỏ hay vàng nhất dựa trên giá trị của pixel đó. Nếu nó gần điểm xanh nhất, tăng \\(x_1\\) lên 1; gần đỏ nhất, tăng \\(x_2\\) lên 1; gần vàng nhất, tăng \\(x_3\\) lên 1. Sau khi xem xét tất cả các điểm ảnh, dù cho bức ảnh có kích thước thế nào, ta vẫn thu được một vector có độ dài bằng 3, mỗi phần tử thể hiện việc có bao nhiêu pixel trong bức ảnh có màu tương ứng. Vector cuối này còn được gọi là vector histogram của bức ảnh tương ứng với ba màu xanh, đỏ, vàng. Dựa vào vector này, ta có thể quyết định bức ảnh đó là ảnh rừng hay sa mạc. Ví dụ 2: Trên thực tế, các bài toán xử lý ảnh không đơn giản như ví dụ 1 trên đây. Mắt người thực ra nhạy với các đường nét, hình dáng hơn là màu sắc. Một cái (ảnh) cây dù không có màu vẫn là một cái (ảnh) cây! Vì vậy, xem xét giá trị từng điểm ảnh một không mang lại kết quả khả quan vì lượng thông tin bị mất quá nhiều. Có một cách khắc phục là thay vì xem xét một điểm ảnh, ta xem xét một  cửa sổ  nhỏ trong ảnh (trong Computer Vision, cửa sổ này được gọi là patch) là một hình chữ nhật chứa nhiều điểm ảnh gần nhau. Cửa sổ này đủ lớn để có thể chứa được các bộ phận có thể mô tả được vật thể trong ảnh. Ví dụ với mặt người, các patch nên đủ lớn để chứa được các phần của khuôn mặt như mắt, mũi, miệng như hình dưới đây. Hình 3: Bag of Words cho ảnh chứa mặt người. (Nguồn   Bag of visual words model: recognizing object categories ) Tương tự thế, với ảnh là ô tô, các patch thu được có thể là bánh xe, khung xe, cửa xe, … như hàng trên trong hình dưới đây. Hình 4: Bag of Words cho ảnh ô tô. (Nguồn: tôi cố gắng tìm nguồn cho hình này nhưng tất cả các tài liệu tôi tìm được đều ghi \"Source: B. Leibe\", tôi cũng xin được trích nguồn tương tự) Có một câu hỏi đặt ra là, trong xử lý văn bản, hai từ được coi là như nhau nếu nó được biểu diễn bởi các ký tự giống nhau. Vậy trong xử lý ảnh, hai patchés được coi là như nhau khi nào? Khi mọi pixel trong hai patches có giá trị bằng nhau sao? Câu trả lời là không. Xác suất để hai patches giống hệt nhau từng pixel là rất thấp vì có thể một phần của vật thể trong một patch bị lệch đi vài pixel so với phần đó trong patch kia; hoặc phần vật thể trong patch bị méo, hoặc có độ sáng khác nhau, mặc dù ta vẫn nhìn thấy hai patches đó  rất giống nhau . Vậy thì hai patch được coi là như nhau khi nào? Và  từ điển  ở đây được định nghĩa như thế nào? Câu trả lời ngắn: hai patches là gần giống nhau nếu khoảng cách Euclid giữa hai vector tạo bởi hai patches đó gần nhau. Từ điển (codebook) sẽ có số phần tử do ta tự chọn. Số phần tử càng cao thì độ sai lệch càng ít, nhưng sẽ nặng về tính toán hơn. Câu trả lời dài: chúng ta có thể áp dụng  K-means clustering . Với rất nhiều patches thu được, giả sử ta muốn xây dựng một  codebook  với chỉ khoảng 1000  words . Vậy thì ta cho \\(k = 1000\\) rồi thực hiện K-means clustering trên toàn bộ số patches thu được (từ tập training). Sau khi thực hiện K-means clustering, ta thu được 1000 clusters và 1000 centers tương ứng. Mỗi centers này được coi là một  words , và tất cả những điểm rơi vào cùng một cluster được coi là cùng một bag. Với ảnh trong tập test data, ta cũng lấy các patches rồi xem chúng rơi vào những bags nào. Từ đó suy ra vector đặc trưng cho mỗi bức ảnh. Chú ý rằng với \\(k = 1000\\), mỗi bức ảnh sẽ được  mô tả  bởi một vector có số chiều 1000, tức là mỗi điểm dữ liệu bây giờ đã có số chiều bằng nhau, mặc dù ảnh thô đầu vào có thể có kích thước khác nhau.\n Feature Scaling and Normalization (Tham khảo  Feature Scaling wiki ). Các điểm dữ liệu đôi khi được đo đạc với những đơn vị khác nhau, m và feet chẳng hạn. Hoặc có hai thành phần (của vector dữ liệu) chênh lệch nhau quá lớn, một thành phần có khoảng giá trị từ 0 đến 1000, thành phần kia chỉ có khoảng giá trị từ 0 đến 1 chẳng hạn. Lúc này, chúng ta cần chuẩn hóa dữ liệu trước khi thực hiện các bước tiếp theo. Chú ý:  việc chuẩn hóa này chỉ được thực hiện khi vector dữ liệu đã có cùng chiều. Một vài phương pháp chuẩn hóa thường dùng:\n Rescaling Phương pháp đơn giản nhất là đưa tất cả các thành phần về cùng một khoảng, \\([0, 1]\\) hoặc \\([-1, 1]\\) chẳng hạn, tùy thuộc vào ứng dụng. Nếu muốn đưa một thành phần (feature) về khoảng \\([0, 1]\\), công thức sẽ là: \n\\[\nx’ = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n\\]\ntrong đó \\(x\\) là giá trị ban đầu, \\(x’\\) là giá trị sau khi chuẩn hóa. \\(\\min(x), \\max(x)\\) được tính trên toàn bộ dữ liệu training data ở cùng một thành phần. Việc này được thực hiện trên từng thành phần của vector dữ liệu \\(\\mathbf{x}\\). Standardization Một phương pháp nữa cũng hay được sử dụng là giả sử mỗi thành phần đều có phân phối chuẩn với kỳ vọng là 0 và phương sai là 1. Khi đó, công thức chuẩn hóa sẽ là: \n\\[\nx’ = \\frac{x - \\bar{x}}{\\sigma}\n\\]\nvới \\(\\bar{x}, \\sigma\\) lần lượt là kỳ vọng và phương sai (standard deviation) của thành phần đó trên toàn bộ training data. \n Scaling to unit length Một lựa chọn khác nữa cũng được sử dụng rộng rãi là chuẩn hóa các thành phần của mỗi vector dữ liệu sao cho toàn bộ vector có độ lớn (Euclid, tức  norm 2 ) bằng 1. Việc này có thể được thực hiện bằng:\n\\[\n\\mathbf{x}’ = \\frac{\\mathbf{x}}{||\\mathbf{x}||_2}\n\\] 4. Thảo luận Xem ra thế giới Machine Learning rất rộng lớn và có rất nhiều thứ chúng ta cần làm. Và\nvẫn có khá nhiều thứ tôi có thể viết được. Tuy nhiên, blog này sẽ không tập trung nhiều vào Feature Learning, mặc dù sẽ có một vài bài nói về Dimensionality Reduction. Tôi sẽ sử dụng các bộ dữ liệu có sẵn, và đã qua bước Feature Learning. 5. Tài liệu tham khảo Feature Enginieering - wiki Feature Scaling wiki Csurka, Gabriella, et al. “ Visual categorization with bags of keypoints. ” Workshop on statistical learning in computer vision, ECCV. Vol. 1. No. 1-22. 2004. Bag of Words model - wiki Bag of Words Meets Bags of Popcorn Share Share Interactive Learning Facebook page Machine Learning cơ bản Forum Recommended books \"Pattern recognition and Machine Learning.\", C. Bishop  \"The Elements of Statistical Learning\", T. Hastie et al.   \"Computer Vision:  Models, Learning, and Inference\", Simon J.D. Prince  \"Convex Optimization\", Boyd and Vandenberghe Recommended courses \"Machine Learning\", Andrew Ng  CS224n: Natural Language Processing with Deep Learning CS231n: Convolutional Neural Networks for Visual Recognition CS246: Mining Massive Data Sets CS20SI: Tensorflow for Deep Learning Research  Introduction to Computer Science and Programming Using Python Others Top-down learning path: Machine Learning for Software Engineers Blog này được tạo như thế nào? Chúng tôi đã apply và học tiến sỹ như thế nào? (1/2) Chúng tôi đã apply và học tiến sỹ như thế nào? (2/2) 8 Inspirational Applications of Deep Learning Matrix calculus TensorFlow-Examples Eight Easy Steps To Get Started Learning Artificial Intelligence The 9 Deep Learning Papers You Need To Know About",
          "url": "https://machinelearningcoban.com/general/2017/02/06/featureengineering/",
          "relevance": "0"
        },
        {
          "title": "MỤC LỤC",
          "content": " Dammediachat Docs Giới thiệu MapInfo MỤC LỤC CHƯƠNG I GIỚI THIỆU CHUNG I.1 - Mô hình dữ liệu I.2 - Tổ chức dữ liệu I.3 - Xây dựng cơ sở dữ liệu I.4 - một số phần mềm xây dựng cơ sở dữ liệu CHƯƠNG II XÂY DỰNG CƠ SỞ DỮ LIỆU KHÔNG GIAN II.1 - Tổ chức thông tin không gian II.2 - Thao tác trên lớp dữ liệu và cửa sổ bản đồ II.3 - Nhập dữ liệu không gian II.4 - biên tập các đối tượng bản đồ II.5 - Phân tích không gian CHƯƠNG III XÂY DỰNG CƠ SỞ DỮ LIỆU THUỘC TÍNH III.1 - Tổ chức, thay đổi cấu trúc dữ liệu III.3 - Liên kết dữ liệu có sẵn III.4 - Cập nhật và bổ sung dữ liệu III.5 - chọn và kết hợp dữ liệu theo điều kiện CHƯƠNG IV XÂY DỰNG & KẾT XUẤT BẢN ĐỒ IV.1 - Bản đồ chuyên đề IV.2 - Khái quát về thành lập bản đồ chuyên đề IV.3 - Các phương pháp tạo bản đồ chuyên đề IV.4 - Tạo trang Layout để xuất bản đồ ArcGIS Giới thiệu ArcMap Tạo bản đồ Quản lý các Layer Soạn thảo dữ liệu đồ họa Autocad Microstation QGIS Surfer \n       \n     Dammediachat Docs Docs  » MapInfo MỤC LỤC ¶ MỤC LỤC CHƯƠNG I GIỚI THIỆU CHUNG I.1 - Mô hình dữ liệu I.1.1 - Khái niệm I.1.2 - Cấu trúc dữ liệu I.2 - Tổ chức dữ liệu I.2.1 - Khái niệm I.2.2 - Tổ chức cơ sở dữ liệu I.2.3 - Mô hình cơ sở dữ liệu không gian I.2.4 - Số hóa bản đồ I.3 - Xây dựng cơ sở dữ liệu I.3.1 - Khái niệm I.3.2 - Thu thập dữ liệu I.3.3 - Nhập & quản lý dữ liệu I.4 - một số phần mềm xây dựng cơ sở dữ liệu CHƯƠNG II XÂY DỰNG CƠ SỞ DỮ LIỆU KHÔNG GIAN II.1 - Tổ chức thông tin không gian II.2 - Thao tác trên lớp dữ liệu và cửa sổ bản đồ II.2.1 - Các thủ tục chung để tạo ra bản đồ máy tính: II.2.2 - Thao tác trên lớp dữ liệu II.2.3 - Thao tác trên cửa sổ bản đồ II.3 - Nhập dữ liệu không gian II.3.1 - Tạo đối tượng điểm trên trang bản đồ II.3.2 - Vectơ hóa các đối tượng bản đồ II.3.3 - Trao đổi dữ liệu với hệ thống khác II.4 - biên tập các đối tượng bản đồ II.4.1 - Vẽ đối tượng mới II.4.2 - Xóa đối tượng đã có II.4.3 - Sao chép và dán các đối tượng II.4.4 - Dịch chuyển vị trí đối tượng II.4.5 - Biên tập các đỉnh của đối tượng II.5 - Phân tích không gian II.5.1 - Tổng hợp và phân tách dữ liệu (Aggregating and disaggregating data) II.5.2 - Kết hợp các đối tượng địa lý với đối tượng xử lý II.5.3 - Phân tách đối tượng II.5.4 - Phân tách đối tượng sử dụng một đường (Polyline) II.5.5 - Xóa một phần của đối tượng II.5.6 - Tạo các điểm nút của các đối tượng giao nhau II.5.7 - Tạo vùng đệm bao quanh một (hay nhiều) đối tượng II.5.8 - Tạo vùng Voronoi (sơ đồ Voronoi) II.5.9 - Tổng hợp đối tượng thông qua các trường dữ liệu thuộc tính CHƯƠNG III XÂY DỰNG CƠ SỞ DỮ LIỆU THUỘC TÍNH III.1 - Tổ chức, thay đổi cấu trúc dữ liệu III.3 - Liên kết dữ liệu có sẵn III.3.1 - Cơ sở dữ liệu .dbf (Dbase III, Dbase IV, Foxbase, Foxpro) III.3.2 - Cơ sở dữ liệu Microsoft Excel III.4 - Cập nhật và bổ sung dữ liệu III.4.1 - Bổ sung số liệu thống kê III.4.2 - Bổ sung số liệu do Mapinfo tính toán được III.5 - chọn và kết hợp dữ liệu theo điều kiện III.5.1 - Chọn các đối tượng theo điều kiện trong một lớp dữ liệu III.5.2 - Kết hợp với một lớp dữ liệu khác CHƯƠNG IV XÂY DỰNG & KẾT XUẤT BẢN ĐỒ IV.1 - Bản đồ chuyên đề IV.2 - Khái quát về thành lập bản đồ chuyên đề IV.3 - Các phương pháp tạo bản đồ chuyên đề IV.3.1 - Tạo bản đồ chuyên đề theo phương pháp Range IV.3.2 - Tạo bản đồ chuyên đề bằng phương pháp Bar Chart IV.3.3 - Tạo bản đồ chuyên đề bằng phương pháp Pie Chart IV.3.4 - Tạo bản đồ chuyên đề bằng phương pháp Graduated IV.3.5 - Tạo bản đồ chuyên đề bằng phương pháp Dot Density IV.3.6 - Tạo bản đồ chuyên đề bằng phương pháp Individual IV.4 - Tạo trang Layout để xuất bản đồ IV.4.1 - Tạo lưới bản đồ IV.4.2 - Tạo thước tỷ lệ IV.4.3 - Thiết kế trang in (layout) CHƯƠNG I GIỚI THIỆU CHUNG ¶ I.1 - Mô hình dữ liệu ¶ I.1.1 - Khái niệm ¶ - Các biến địa lý trong thế giới thực rất phức tạp. Càng quan sát gần, càng\nnhiều chi tiết, nói chung là không giới hạn. - Điều đó sẽ cần một cơ sở dữ liệu (CSDL) xác định để thu thập các đặc điểm của\nthế giới thực. - Số liệu cần phải giảm đến một số lượng nhất định và quản lý được từ việc xử\nlý tạo ra hoặc trừu tượng hóa. - Biến địa lý cần được biểu diễn trong các thuật ngữ các phần tử hữu hạn hoặc\ncác đối tượng. - Các quy tắc được dùng để chuyển các biến địa lý sang các đối tượng là mô hình\ndữ liệu. - Mô hình dữ liệu như là một bộ các quy tắc để biểu diễn sự tổ chức logic của\ndữ liệu trong CSDL... bao gồm tên các đơn vị logic dữ liệu và các quan hệ giữa\nchúng. - Mô hình dữ liệu được chọn để cho một đối tượng đặc biệt hoặc ứng dụng bị ảnh\nhưởng bởi: Phần mềm phù hợp Đào tạo cán bộ chủ chốt Tiền lệ có tính lịch sử I.1.2 - Cấu trúc dữ liệu ¶ Khi nhập dữ liệu vào một hệ thống thông tin, có các cấu trúc dữ liệu như sau: - Cấu trúc của hiện tượng theo quan niệm người sử dụng - Cấu trúc của hiện tượng thể hiện trong một hệ GIS Topology và các đợn vị bản đồ Topo (topology) là tập hợp các tính chất của một thực thể hình học trong trạng\nthái biến dạng và biến vị. Các thuật ngữ dùng trong tọa độ hình học là vùng,\nmiềm kế cận, không gian bao quanh,... Các đơn vị bản đồ là: điểm, đường và vùng. Topo là một cấu trúc, trong đó các\nđiểm , đường và vùng là duy nhất và có liên quan với nhau. Ba đơn vị này được\nxác định bằng các vị trí không gian trong một hệ tọa độ thích hợp (thí dụ UTM\nhay Gauss) và bằng các thuộc tính của chúng. I.2 - Tổ chức dữ liệu ¶ I.2.1 - Khái niệm ¶ Dữ liệu được nhập vào và lưu trữ trên máy tính trong một không gian được gọi là\ntệp dữ liệu hay tệp tin. Tệp tin được ghi với độ dài có số lượng byte nhất định.\nCác số ghi này có thể là số thực hay số nguyên và được tổ chức theo một khuôn\ndạng đặc biệt. Mỗi một số ghi mô tả một yếu tố duy nhất và chứa các trường nhận\nbiết các thuộc tính của yếu tố đó. Các dữ liệu được lưu trữ trong các trường\nnày. Các tệp tin được chia theo 3 kiểu phổ biến: - Tệp tin đơn giản theo một chiều - Tệp tin sắp xếp theo dãy - Tệp tin theo chỉ số I.2.2 - Tổ chức cơ sở dữ liệu ¶ Các dữ liệu được nhập vào và lưu trữ nhờ các phần mềm quản lý CSDL. Một CSDL là\nmột tập hợp các cách biểu diễn thực dưới dạng các dữ liệu có liên kết qua lại ở\nmức tối đa. Những dữ liệu này được ghi nhớ theo chuỗi tính toán và theo một cấu\ntrúc hợp lý sao cho có thể khai thác dễ dàng, nhằm thỏa mãn các yêu cầu khi cung\ncấp thông tin và các chỉ dẫn cho người sử dụng. CSDL được tổ chức ở dạng một thư mục, trong đó dữ liệu được ghi nhớ trong nhiều\ntệp Phần mềm quản lý cho phép ghi nhớ các tệp dữ liệu trong tệp theo thứ tự, hoặc\ntheo chỉ số trực tiếp. Chúng quản lý các tệp độc lập, các tệp có cấu trúc thứ\nbậc, dạng mạng, hoặc dạng quan hệ.ý I.2.3 - Mô hình cơ sở dữ liệu không gian ¶ Bản đồ thực chất là sản phẩm thu được trong việc đơn giản hóa một thực thể. Nó\nphản ánh đồng thời những thông tin đặc trưng và các thông tin tổng hợp. Thông\ntin tổng hợp thường được thể hiện dưới dạng các ký hiệu, ngược lại, các đối\ntượng hình ảnh được biểu diễn theo tọa độ không gian. Dữ liệu không gian thường\nđược hiển thị theo 02 phương pháp. Phương pháp thứ nhất biểu diễn dưới dạng các\nđơn vị bản đồ. Phương pháp thứ hai biểu diễn dưới dạng các ô lưới hay ma trận.\nHai phương pháp này gọi là mô hình vectơ và mô hình rastơ tương ứng. 1. Mô hình dữ liệu Vectơ Mô hình vectơ thường được biểu diễn dưới dạng điểm, đường và vùng. Phương pháp\nthường dùng là biểu diễn dữ liệu dưới dạng vectơ. Bản đồ học dựa trên các đường,\nhoặc vectơ để biểu diễn thực thể như đường xá, sông ngòi và để xác định các\nđường biên giữa các thực thể không gian khác nhau. Các kỹ thuật đo đạc bản đồ\nđược hình thành trên cơ sở các nguyên tắc hình học và lượng giác sử dụng các\nvectơ. Trong cấu trúc vectơ các đường được xác định bởi độ dày và không thay đổi\nnếu như vùng được mở rộng. Trong cấu trúc vectơ, không gian 2D được coi như liên tục và biểu diễn chính xác\nvị trí, độ dài, khoảng cách và diện tích. Vị trí được mô tả bằng các cặp tọa độ. Trong cấu trúc vectơ, các điểm được biểu diễn bằng cặp tọa độ X,Y. Các đối tượng\nđường và vùng được tạo thành từ các đoạn thẳng nối hai cặp tọa độ (Xi,Yi),\nXi+1,Yi+1). Các quan hệ không gian quan trọng đối với hệ thống các đường và đa\ngiác là được tạo bởi các cung - nút (arc - node). Các giá trị thuộc tính đối với điểm, đường và đa giác được lưu trữ độc lập với\ncác quan hệ không gian của các thực thể. Các dữ liệu vectơ được số hóa với một độ chính xác theo yêu cầu. Độ chính xác\nvectơ, dù sao cũng chỉ đúng với một số nhóm dữ liệu. Trên thực tế để biểu diễn\nmột đường cong liên tục chúng ta thường xấp xỉ bằng đường gấp khúc. Trong trường\nhợp này độ chính xác biểu diễn đối tượng sẽ phụ thuộc vào mức độ rời rạc hóa các\nđiểm đường cong. 2. Mô hình dữ liệu Rastơ Cấu trúc dữ liệu rastơ 2D được xem như một ma trận các ô lưới (cells - pixels -\npicture element). Mỗi cell đặc trưng cho một ô vuông của bề mặt đất. Độ phân\ngiải của dữ liệu rastơ được xác định bởi kích thước của ô lưới, như vậy số liệu\nrastơ đặc trưng cho không gian rời rạc trong đó độ chính xác vị trí phụ thuộc\nvào kích thước của ô lưới. Ô càng nhỏ thì độ phân giải càng cao và yêu cầu bộ\nnhớ càng lớn. Ngược lại ô càng to thì độ phân giải càng thấp và yêu cầu bộ nhớ\ngiảm. Mỗi ô lưới chỉ có một giá trị duy nhất ứng với một thuộc tính nào đấy. Mỗi giá\ntrị thuộc tính có thể đặc trưng cho phép đo một điểm (như độ cao) hoặc phép đo\nvùng được chia nhỏ. Các thuộc tính cho nhiều đối tượng địa lý được khái niệm như\nnhiều lớp ảnh quét. Trong cấu trúc ảnh quét, các điểm được coi như những ô độc lập, các đường và các\ncùng được đặc trưng như các ô liên tục. Độ chính xác tọa độ của số liệu ảnh quét\nlà do kích thước ô quyết định. Sự cân nhắc rõ ràng các đặc tính không gian\n(điểm, đường, vùng) và các đặc điểm địa hình, hình học không gian sẵn có trong\nsố liệu rastơ , nhưng có thể trích ra từ cấu trúc nếu như cần thiết. Hiện nay, những phương pháp mới thu nhận thuộc tính trực liếp bằng các bộ cảm\nnhận điện từ và máy quét ảnh đã tăng khả năng sử dụng của cấu trúc rastơ - công\ntác xử lý ảnh và viễn thám đã tạo ra một khối lượng thông tin rất phong phú, mà\nphương pháp vectơ không thể so sánh nổi. I.2.4 - Số hóa bản đồ ¶ Việc đưa bản đồ theo nghĩa truyền thống về dạng số là tạo ra cơ sở dữ liệu mềm\nđược tin học hóa và được đảm bảo 3 mục tiêu sau: - Giữ được nội dung đồ họa bao gồm các chức năng và đối tượng thể hiện, cũng\nnhư các quan hệ giữa chúng và các thông tin tổng hợp dưới dạng các ký hiệu. - Khả năng phân tích không gian - Khả năng tích hợp các thông tin cơ sở để xác định các thông tin tổng hợp Khả năng sử dụng cấu trúc vec tơ và ma trận: Trong vài năm gần đây, những người sử dụng dữ liệu địa lý được chia làm hai\ntrường phái: trường phái vectơ và trường phái rastơ. Trường phái vectơ cho rằng\nkiểu ma trận yêu cầu bộ nhớ lớn và không đảm bảo độ chính xác cần thiết, trường\nphái ma trận thì cho rằng hệ vectơ nặng nề về kỹ thuật và rất đắt, hơn nữa khó\nthực hiện các xử lý trong nhiều lớp. Ngày nay, các nhược điểm trên đều có thể khắc phục nhờ những tiến bộ nhanh chóng\ncủa công nghệ tin học. Do đó các vấn đề tồn tại chỉ còn là: - Việc tối ưu hóa khối lượng bộ nhớ và thời gian tính toán - Đúng mức với hiện tượng phân tích Chuyển đổi giữa các kiểu cấu trúc Để tạo điều kiện thuận lợi khi xử lý các dữ liệu, việc chuyển đổi các kiểu cấu\ntrúc có tính khả thi dễ dàng. Chuyển đổi rastơ - vectơ sử dụng nhằm mục đích để vẽ, kiểu vectơ tạo ra những\nđường viền chính xác và làm cho tọa độ mềm mại hơn. Chuyển đổi vectơ - rastơ thường dùng để mô tả phân bố trong không gian một đối\ntượng, mà vị trí của nó không đòi hỏi độ chính xác cao. I.3 - Xây dựng cơ sở dữ liệu ¶ I.3.1 - Khái niệm ¶ CSDL là một tập hợp các lớp thông tin (các tệp dữ liệu) ở dạng vectơ, rastơ,\nbảng số liệu, văn bản với những cấu trúc chuẩn bảo đảm cho các bài toán chuyên\nđề đó có mức độ phức tạp khác nhau. Mục tiêu cuối cùng của việc xây dựng cơ sở dữ liệu là thành lập các bản đồ\nchuyên ngành, phục vụ cho việc quản lý tài nguyên thiên nhiên, quản lý lãnh thổ,\ndự báo thiên tai, theo dõi tình trạng phát triển của các hiện tượng thiên nhiên\nvà xã hội. Cũng có thể hiểu đơn giản một CSDL là tập hợp hệ thống hóa các tài liệu bản đồ,\nsố liệu thống kê, các văn bản và chuyển chúng sang ngôn ngữ của máy tính. Xây dựng CSDL bao gồm các nội dung sau: - Thu thập, sửa chữa và hiệu chỉnh dữ liệu - Nhập dữ liệu - Quản lý dữ liệu I.3.2 - Thu thập dữ liệu ¶ Bản đồ chứa những thông tin về các đối tượng trên bề mặt vỏ Quả đất. Các thông\ntin này được chia làm ba nhóm: - Vị trí địa lý (hệ toạ độ theo một hệ chiếu nhất định); - Những đặc trưng của đối tượng, hay các thuộc tính của chúng (tên, giá trị số,\nnội dung chuyên đề); - Mối quan hệ hình học giữa các đối tượng. Chọn dữ liệu bao gồm chọn các dữ liệu đã có, chọn các dữ liệu mới, tuyển lựa dữ\nliệu. Dữ liệu thông tin địa lý xuất phát từ nhiều nguồn khác nhau, nhiều dạng khác\nnhau và được lưu dưới nhiều phương pháp khác nhau. Công nghệ tin học cung cấp\ncác công cụ và phương pháp kết hợp những số liệu này vào một dạng cho phép so\nsánh chúng với nhau. Sơ đồ: Nguồn dữ liệu phục vụ xây dựng cơ sở dữ liệu I.3.3 - Nhập & quản lý dữ liệu ¶ Việc nhập dữ liệu để xây dựng cơ sở dữ liệu được thực hiện theo các bước sau: - Mã hóa - Nhập dữ liệu từ nguồn - Nhập dữ liệu có cấu trúc vectơ - Nhập dữ liệu phi không gian Độ chính xác dữ liệu Đối với các dữ liệu có sẵn, các sai số gồm: - Ngày tháng của dữ liệu - Độ chính xác của dữ liệu cùng họ với nhau - Tỷ lệ của dữ liệu - Sai số liên quan đến dữ liệu: vị trí, tỷ lệ, độ phân giải,… - Sai số do phân tích không chính xác các đặc tính topo - Sai số do chuyển đổi dữ liệu từ dạng vectơ sang rastơ và ngược lại Kiểm tra dữ liệu Kiểm tra dữ liệu thực hiện bằng cách cho hiển thị ra màn hình hoặc in ra giấy để\nkiểm tra. Đối với dữ liệu không gian có thể chồng một cách cơ học các kết quả\nlên nhau. Đối với dữ liệu phi không gian có thể kiểm tra trên máy tính hoặc theo\ndõi số liệu trên giấy so với các dữ liệu lúc chưa xử lý. Quản lý dữ liệu: - Phối hợp dữ liệu không gian và dữ liệu mô tả - Chỉnh lý, bổ sung dữ liệu - Chuyển đổi và lưu trữ dữ liệu I.4 - một số phần mềm xây dựng cơ sở dữ liệu ¶ I.4.1 - Phần mềm Mapinfo I.4.2 - Hệ thống phần mềm MicroStation & Mapping Office I.4.3 - Phần mềm Arcview I.4.4 - Hệ thống phần mềm Famis - Caddb I.4.5 - Phần mềm SB1990 CHƯƠNG II XÂY DỰNG CƠ SỞ DỮ LIỆU KHÔNG GIAN ¶ II.1 - Tổ chức thông tin không gian ¶ Hiện nay trên thế giới đang phổ biến các phần mềm phục vụ cho việc xây dựng CSDL\nthông tin địa lý như: ARC/INFO, ARCVIEW, INTERGRAPH, MAPINFO, IDRISI, ENTEC, … Việc tổ chức các lớp thông tin không gian trên một CSDL thông thường được tổ\nchức theo hai dạng: - Tổ chức trên một file dữ liệu chứa nhiều lớp thông tin: điển hình là cách tổ\nchức dữ liệu trên phần mềm MicroStation, Autocad - Mỗi file dữ liệu là một lớp thông tin: tổ chức dữ liệu của phần mềm Mapinfo,\nArcView Phần mềm Mapinfo là một công cụ khá hữu hiệu để tạo ra và quản lý cơ sở dữ liệu\nđịa lý vừa và nhỏ trên máy tính cá nhân. Sử dụng công cụ Mapinfo có thể thực\nhiện xây dựng một hệ thống thông tin địa lý, phục vụ cho mục đích nghiên cứu\nkhoa học và sản xuất của các tổ chức kinh tế và xã hội của các ngành và địa\nphương. Ngoài ra, Mapinfo là một phần mềm tương đối gọn nhẹ và dễ sử dụng. Đặc\nbiệt dùng cho mục đích giảng dạy về GIS, tin học chuyên ngành rất có hiệu quả.\nVì vậy, chúng tôi chỉ giới thiệu kỹ về phần mềm Mapinfo trong việc xây dựng cơ\nsở dữ liệu địa lý. Mapinfo là một phần mềm hệ thông tin địa lý (GIS - Geographical information\nsystems) cho giải pháp máy tính để bàn. Các thông tin trong Mapinfo được tổ chức\ntheo từng bảng (Table), mỗi một Table là một tập hợp các file về thông tin đồ\nhọa hoặc phi đồ họa chứa các bảng ghi dữ liệu mà hệ thống tạo ra. Chỉ có thể\ntruy cập vào các chức năng của phần mềm Mapinfo khi mà ta đã mở ít nhất một\nTable. Cơ cấu tổ chức thông tin của các đối tượng địa lý được tổ chức theo các\nfile sau đây: Bảng: Hệ thống các file dữ liệu File và phần mở rộng ý nghĩa của file *.tab Chứa các thông tin mô tả cấu trúc dữ liệu *.dat Chứa các thông tin nguyên thủy, phần mở rộng của tập tin này có thể là *.wks, dbf, xls nếu chúng ta làm việc với các thông tin nguyên thủy là các số liệu từ Lotus 1-2-3, dbase/Foxbase và Excel. *.map Chứa các thông tin mô tả các đối tượng đồ họa *.id Chứa các thông tin về sự liên kết giữa các đối tượng với nhau. *.ind Chứa các thông tin về chỉ số đối tượng. Tập tin này chỉ có khi trong cấu trúc của Table đã có ít nhất một trường dữ liệu đã được chọn làm chỉ số hóa (Index). Thông qua các thông tin của file này chúng ta có thể thực hiện tìm kiếm thông tin thông qua một chỉ tiêu cho trước bằng chức năng Find của Mapifo. *.wor Tập tin quản lý chung (lưu trữ tổng hợp các Table hoặc các cửa sổ thông tin khác nhau của Mapinfo) II.2 - Thao tác trên lớp dữ liệu và cửa sổ bản đồ ¶ II.2.1 - Các thủ tục chung để tạo ra bản đồ máy tính: ¶ - Mở ít nhất một lớp thông tin :  Menu File - chọn Open (Ctrl - O hoặc chọn\ncông cụ trên thanh Standar) Hình: Hộp thoại mở một Table Trong  Files of type , chọn MapInfo (*.tab) Trong  Preferred View , mặc định là  Automatic , có nghĩa: - Nếu lớp dữ liệu có chứa dữ liệu không gian (spatial data), tức các đối tượng\nđịa lý với tọa độ tương ứng được lưu trữ dưới dạng số (digital format), trên màn\nhình sẽ xuất hiện cửa sổ bản đồ (map window) với các đối tượng của lớp dữ liệu. - Nếu lớp dữ liệu không có dữ liệu không gian, chỉ có các dữ liệu phi không\ngian (còn gọi là dữ liệu thuộc tính): trên màn hình sẽ xuất hiện một cửa sổ dữ\nliệu theo dạng hàng - cột (browser window) của lớp dữ liệu đó. Ta có thể chọn Browser để xem dữ liệu, Current Mapper: mở trong cùng cửa số đang\nmở, New Mapper: cửa số bản đồ mới, hay No View: không hiện gì trên màn hình\n(nhưng sẽ được đưa vào trong bộ nhớ của máy tính) Chúng ta có thể mở nhiều lớp dữ liệu cùng một lúc, và thông thường để xây dựng\nmột bản đồ chúng ta sử dụng đến nhiều lớp dữ liệu. - Tạo cửa sổ bản đồ mới:  Menu Window - chọn New Map Window Hình: Hộp thoại tạo cửa sổ bản đồ mới Chọn các tên của Table mà chúng ta muốn hiển thị tạo thành bản đồ tổng hợp trong\ncửa sổ màn hình, sau đó chọn OK - Thêm các lớp thông tin đã mở vào bản đồ hiện thời:  Menu Map - chọn Layer\nControl - chọn Add (chi tiết trình bày trong phần thao tác trên lớp dữ liệu) II.2.2 - Thao tác trên lớp dữ liệu ¶ Chúng ta có thể truy cập hộp hội thoại điều khiển lớp thông tin bằng hai cách: Menu Map - chọn Layer Control (hoặc Ctrl + L) - Biểu tượng công cụ điều khiển lớp ( ) trong hộp công cụ Trong hộp hội thoại này sẽ hiện ra toàn bộ các lớp thông tin trong bản đồ hiện\nthời và các tham số điều khiển lớp như sau: Hình: Hộp thoại điều khiển lớp thông tin Để chọn một lớp thông tin chúng ta chỉ việc bấm chuột vào tên của lớp đó trên\nhộp hội thoại và khi đó dòng tên lớp đó sẽ bật sáng (highlight). Sau khi chọn\nxong tên lớp có thể chọn tham số điều khiển lớp như sau: - Tham số điều khiển ẩn/hiện (Visible): tham số này giúp chúng ta điều khiển\nẩn/hiện các thông tin của một lớp. - Tham số điều khiển biên tập (Editable): chúng ta chỉ có thể thay đổi và biên\ntập các thông tin trong một lớp khi lớp đó đang ở chế độ biên tập. Tại một thời\nđiểm  chỉ có một lớp  ở chế độ biên tập được. - Tham số điều khiển chọn (Selectable): trong Mapinfo chỉ có thể thực hiện các\nthao tác xử lý, phân tích dữ liệu và biên tập đối tượng được trong cửa sổ bản đồ\nkhi mà chúng ta đã chọn đối tượng đó. Nếu một lớp thông tin đã được đặt ở chế độ\nbiên tập được thì nó cũng tự động được đặt ở chế độ chọn được. Để đặt một lớp\nthông tin ở chế độ chọn được chúng ta chỉ cần chọn tên của lớp đó và đánh dấu\nlựa chọn ô biểu tượng Selectable. Tham số điều khiển mức độ phóng đại của lớp thông tin (Zoom Layer): màn hình\n    máy tính chỉ có một kích thước vật lý nhất định do vậy các thông tin bản đồ\n    tạo ra trên máy tính thường bị thu nhỏ lại trong khuôn khổ của màn hình.\n    Muốn xem thông tin chi tiết hơn thì chúng ta phải phóng đại một phần của màn\n    hình tương tự. Khi chọn tham số này nghĩa là chúng ta đã xác định cho hệ\n    thống chỉ thể hiện chi tiết theo một mức độ phóng đại nhất định. Hình: Hộp thoại điều khiển thể hiện thông tin và mức độ phóng đại của lớp thông tin Để đặt tham số phóng đại của lớp thông tin chúng ta đánh dấu lựa chọn ở ô Zoom\nLayer và nhập giá trị phóng đại cực tiểu và cực đại ở hộp Min Zoom và Max Zoom. - Tham số điều khiển thuộc tính thể hiện thông tin lớp (Display): chúng ta có\nthể dùng tham số này để lựa chọn các thuộc tính thể hiện khác nhau cho từng lớp\nthông tin khác nhau trên trang bản đồ. - Tham số xác định nhãn (Label): khi muốn hệ thống sẽ hiển thị nhãn các đối\ntượng trong một lớp thông tin theo giá trị của một trường trong CSDL thuộc tính\nta chọn tên lớp và chọn các thông số trong Label Options - Thêm vào hoặc loại bỏ một lớp thông tin trong trang bản đồ: để thêm một layer\nvào cửa sổ bản đồ hiện thời chọn nút  Add  trong khung cửa sổ Layer Control;\nsau đó chọn tên của lớp cần thêm vào trang bản đồ hiện thời. Nếu muốn loại bỏ\nmột lớp thông tin ra khỏi trang bản đồ hiện thời thì chúng ta chọn tên lớp cần\nloại bỏ rồi sau đó bấm chọn nút  Remove . - Sắp xếp lại thứ tự các lớp: các lớp thông tin trong trang bản đồ sẽ hiển thị\ntrên màn hình theo đúng thứ tự của danh sách các lớp trong hộp hội thoại điều\nkhiển lớp. Để sắp xếp lại thứ tự các lớp chúng ta chọn tên lớp và sau đó bấm vào\nnút  Up  để di chuyển lớp đó lên trên và bấm nút  Down  nếu muốn di chuyển\nlớp đó xuống dưới. Riêng đối với lớp Cosmetic chúng ta không thể thay đổi vị trí\ncủa nó trong danh sách được. Sửa đổi các thuộc tính của đối tượng - Để xem thông tin của một đối tượng, chúng ta chọn đối tượng đó rồi vào  Menu\nEdit - chọn Get Info  (hoặc F7), hoặc double click lên đối tượng. Chúng ta có\nthể thay đổi thuộc tính của đối tượng bằng cách click vào  Style  trong cửa sổ\ncủa đối tượng. - Để thay đổi vị trí địa lý chúng ta có thể dịch chuyển đối tượng bằng cách\ndùng chuột chọn đối tượng bấm và giữ chuột cho đến khi cursor có hình mũi tên\nbốn chiều sau đó dịch chuyển đối tượng đến một vị trí khác. Thay đổi hình dạng của đối tượng kiểu vùng hay kiểu đường chúng ta sử dụng\n    chức năng  Edit - Reshape . Các điểm trung gian (node) của đối tượng sẽ\n    hiện lên, chúng ta có thể di chuyển vị trí hay xóa các điểm này; cũng có thể\n    thêm các điểm này với biểu tương  Add Node  ( ) trong hộp công cụ\n     Drawing. Chúng ta cũng có thể sử dụng các chức năng Cut, Copy và Paste trong Menu Edit để\ncắt, sao và dán các đối tượng trong một lớp dữ liệu hay giữa các lớp dữ liệu. Lớp dữ liệu tạm thời trong cửa sổ bản đồ (Cosmetic Layer) Khi mở một hay nhiều lớp dữ liệu, mỗi lớp dữ liệu này trong cửa sổ bản đồ là một\n\"layer\". Trong cửa số bản đồ, ngoài các lớp bản đồ được mở, luôn có một lớp bản\nđồ tạm thời gọi là  Cosmetic Layer . Về bản chất có thể coi lớp thông tin này\nnhư là một trang giấy trắng chưa có thông tin nào trên đó và luôn nằm ở vị trí\ntrên cùng trong trang bản đồ. Chúng ta có thể sử dụng lớp thông tin này để làm\ntrang giấy nháp trong quá trình làm việc. Để lưu lại những thông tin trên lớp\nthông tin này chúng ta chọn  Menu Map - Save Cosmetic Objects  và nhập tên một\nlớp dữ liệu mới; để xóa các thông tin trên Cosmetic Layer với  Menu Map - Clear\nCosmetic Layer . Lớp Cosmetic có các đặc điểm sau: - Lớp Cosmetic luôn là lớp thông tin ở vị trí trên cùng trong danh sách lớp của\ncửa sổ bản đồ - Các thông tin của lớp Cosmetic không tự động ghi lại vào đĩa khi đóng cửa sổ\nbản đồ. - Chỉ được phép đặt lớp Cosmetic vào chế độ biên tập được và chọn được. - Lớp Cosmetic tự động lưu các nhãn đối tượng khi dùng chức năng vẽ nhãn hoặc\nhiển thị nhãn tự động. - Nội dung thông tin của lớp Cosmetic luôn kết nối tương ứng tỷ lệ với mức độ\nphóng đại của trang bản đồ hiện thời II.2.3 - Thao tác trên cửa sổ bản đồ ¶ 1. Thay đổi độ phóng đại và vùng nhìn thấy Tham số Zoom trên thanh trạng thái cho biết giá trị ngoài thực tế của chiều rộng\ncửa sổ bản đồ đang hoạt động (Tắt mở thanh trạng thái bằng  Show/Hide Status Bar  trong mục  Options  của\nmenu chính) Để thay đổi độ lớn của bản đồ trong cửa sổ bản đồ, chúng ta có thể sử dụng: + Biểu tượng phóng lớn hay thu nhỏ Menu Map - chọn Change View hay click vào biểu tượng của chức năng này , cửa\n    sổ Change View xuất hiện với các tham số: Zoom (Window widths): giá trị chiều rộng của cửa sổ bản đồ Map Scale: tỷ lệ bản đồ ~   toạ độ trung tâm của cửa sổ bản đồ Chúng ta có thể thay đổi một trong hai tham số Zoom và Map Scale; khi thay đổi\nmột trong hai tham số thì tham số kia cũng thay đổi tương ứng Hình: Hộp thoại thay đổi tỷ lệ bản đồ Sau khi mở một lớp dữ liệu, có thể trong cửa sổ bản đồ không chứa hết tất cả các\nđối tượng. Để thấy được tất cả các đối tượng trong cửa sổ bản đồ chúng ta thực\nhiện :  Menu Map - chọn View Entire Layer  rồi chọn lớp dữ liệu (hoặc tất cả\ncác lớp - All Layers) muốn xem trong cửa sổ  View Entire Layer. Chúng ta có thể trở lại tình trạng cửa sổ bản đồ trước khi vừa được thay đổi với\nMap - Previous View. Để có thêm một cửa sổ bản đồ giống như cửa sổ đang làm việc, chúng ta vào hoặc\n Map - Clone View  hoặc  Edit - Copy Map Windows  (Ctrl + C) rồi Edit -\n Paste Map Window  (Ctrl + V). Ngoài ra chúng ta còn có thể di chuyển các đối tượng trong cửa sổ bản đồ bằng\nbiểu tượng trong hộp công cụ (click vào ô có biểu tượng này, sau đó di chuyển\nmouse vào trong cửa sổ bản đồ rồi bấm và kéo mouse theo hướng chúng ta muốn) 2. Xác định các tham số cho cửa sổ bản đồ hiện thời Các tham số này chỉ có tác dụng trong cửa sổ bản đồ hiện thời. Chọn Menu Map -\nOption xuất hiện cửa sổ: Hình: Xác định tham số cho cửa sổ bản đồ Tại hộp thoại này có thể chọn các tham số: - Coordinate Units: đơn vị tọa độ - Distance Units: đơn vị khoảng cách - Area Units: đơn vị diện tích - Projection: chọn hệ quy chiếu - Hiện/tắt thanh cuốn (Scroll Bars) - Hiển thị thông tin trên thanh trạng thái: giá trị độ rộng của cửa sổ bản đồ;\ntỷ lệ bản đồ hiện thời; tọa độ vị trí hiện thời của con trỏ. - Điều khiển sự hiển thị của bản đồ trong cửa sổ hiện thời luôn luôn tính theo\nkích thước cửa sổ nếu chọn ô Fit Map to New Windows và giữ nguyên tỷ lệ hiển thị\nnếu chúng ta chọn ô Preserve Current Scale 3. Cách chọn đối tượng địa lý trong cửa sổ bản đồ Chúng ta có thể chọn trực tiếp đối tượng hiện diện trong khung nhìn của cửa sổ\nbản đồ bằng cách click vào biểu tượng trong hộp công cụ Main, sau đó di chuyển\ncursor vào trong cửa sổ bản đồ, đến vị trí của đối tượng muốn chọn và click, đối\ntượng được chọn sẽ hiện rõ lên. Để chọn nhiều đối tượng, chúng ta bấm giữ\n Shift  trong khi click các đối tượng kế tiếp. Sử dụng công cụ Marquee Select hay Radius Select để chọn tất cả các đối tượng\ntrong hình chữ nhật hay hình tròn do chúng ta sẽ vẽ ra. Sử dụng công cụ Boundary Select để chọn tất cả các đối tượng bên trong ranh giới\ncủa một đối tượng kiểu vùng đã xác định trước. Chúng ta còn có thể chọn một (hay nhiều) đối tượng từ danh sách các đối tượng\ntrong cửa sổ dữ liệu. Một lớp dữ liệu có đối tượng địa lý luôn đi kèm danh sách\ncác đối tượng này và để xem chúng chúng ta vào  Window - New Browser Window \n(hoặc F2), chọn tên lớp dữ liệu trong cửa sổ  Browse Table  và click OK II.3 - Nhập dữ liệu không gian ¶ Dữ liệu được nhập vào hệ thống thông qua 03 phương pháp như sau: - Tạo đối tượng điểm - Vectơ hóa (thực hiện phát sinh đối tượng bằng công cụ vẽ) - Trao đổi dữ liệu với hệ thống khác II.3.1 - Tạo đối tượng điểm trên trang bản đồ ¶ Từ số liệu về toạ độ địa lý (kinh độ và vĩ độ) của các điểm, Mapinfo có thể thể\nhiện các điểm này trên bản đồ với vị trí tương ứng. Sau khi mở lớp dữ liệu có số\nliệu về tọa độ địa lý, vào  Menu Table - chọn Create Points , cửa sổ  Create\nPoints  xuất hiện và được khai báo như sau: Hình: Hộp thoại tạo điểm từ số liệu về tọa độ địa lý - Vào  Projection  để khai báo phép chiếu phù hợp với số liệu về tọa độ trong\ncửa sổ Choose Projection. - Click vào khung using Symbol để chọn kiểu của đối tượng điểm sẽ được tạo ra. - Khai báo trường dữ liệu chứa dữ liệu về kinh độ và vĩ độ của các điểm cần\ntạo. - Chọn OK để thực hiện II.3.2 - Vectơ hóa các đối tượng bản đồ ¶ 1. Tạo dữ liệu mới trong Mapinfo Để xây dựng một dữ liệu mới theo dạng của Mapinfo (Table), chọn Menu File - chọn\nNew Table (Ctrl + N), cửa sổ New Table xuất hiện: Hình: Hộp thoại tạo một lớp dữ liệu mới - Chọn  Open New Mapper  hay  Add to Current Mapper  khi muốn tạo các đối tượng\nđồ họa. Nếu chỉ muốn tạo một cơ sở dữ liệu thì chọn Open New Browser. - Cấu trúc của cơ sở dữ liệu: trong Table Structure, chọn  Create New  nếu muốn\ntạo cấu trúc dữ liệu mới, chọn  Using Table  nếu muốn tạo cấu trúc dữ liệu theo\nmột Table có sẵn. - Click  Create , cửa sổ  New Table Structure  xuất hiện, cách khai báo trong\ncửa sổ này như sau: Mục Projection: Nếu tạo dữ liệu ở một vùng địa lý mới, trước hết phải khai báo mục Projection.\nProjection (hệ quy chiếu) là phương pháp làm giảm sự biến dạng xảy ra khi chuyển\ncác đối tượng địa lý trên mặt đất lên mặt phẳng của bản đồ giấy. Click Projection, cửa sổ Choose Projection xuất hiện: Hình: Hộp thoại chọn hệ quy chiếu - Nếu muốn khai báo theo tọa độ địa lý (kinh độ, vĩ độ) thì chọn\n Longtitude/Latitude  trong mục  Category . Trong phần  Category Members ,\ntùy theo hệ quy chiếu của bản đồ mà chúng ta có thể khai báo: + Longtitude/Latitude, chung cho các hệ quy chiếu + hay Longtitude/Latitude (WGS 84), nếu hệ quy chiếu là UTM + hay Longtitude/Latitude (Pulkovo 1942), nếu hệ quy chiếu là Gauss - Nếu muốn khai báo theo hệ quy chiếu, trường hợp hệ quy chiếu là UTM\n(Universal Transverse Mercator) ta chọn Universal Transverse Mercator (WGS 84)\ntrong mục Category. Tiếp theo tuỳ theo vị trí của vùng khảo sát mà chọn UTM Zone\n48, Northern Hemisphere (WGS 84) hay UTM Zone 49, Northern Hemisphere (WGS 84)\ntrong phần  Category Members . Trường hợp hệ quy chiếu là Gauss, trong mục\nCategory chúng ta chọn Gauss-Kruger (Pulkovo 1942) và trong phần  Category\nMembers  chọn GK Zone 18 (Pulkovo 1942). Trong Mapinfo chưa có hệ quy chiếu\nGauss đúng như ngành địa chính sử dụng, khai báo vừa rồi chỉ là tương đối. Trường hợp vùng dữ liệu đã có các bản đồ dạng số, chúng ta nên mở một trong các\nbản đồ đã có và tạo mới một dữ liệu trong cửa sổ bản đồ đang hoạt động, các tham\nsố liên quan đến Projection là giống như của cửa sổ bản đồ hoạt động. Dữ liệu được tạo trong Mapinfo có dạng là một bảng gồm các hàng và cột hay vùng\n(column, field). Mỗi vùng (cột) là một thuộc tính tương ứng của các hàng là các\nđối tượng. Click  Add field  để thêm vùng mới. Nhập tên của vùng này vào cửa sổ  Name  ví\ndụ: STT, sau đó chọn kiểu dữ liệu của vùng trong cửa sổ  Type . Click  Create  để đặt tên cho lớp dữ liệu mới cùng với thư mục thích hợp 2. Số hóa bản đồ a. Định nghĩa Số hóa bản đồ là quá trình vẽ lại một bản đồ giấy trên máy tính nhằm tạo một bản\nvẽ dạng số của bản đồ đó. Số hóa là một cách nhập dữ liệu không gian, nó ghi nhận tọa độ địa lý của các\nđối tượng trên mặt đất, lưu trữ dưới dạng số để có thể xử lý trên máy tính. Có hai phương pháp số hóa bản đồ: số hóa với bàn số hóa (digitizer) và vectơ hóa\ntừ ảnh quét (scanner) của bản đồ giấy. b. Tiến trình vectơ hóa dữ liệu rastơ Khai báo đăng nhập tọa độ ảnh quét Bản đồ được quét qua máy quét (scanner) tạo nên các tập tin ảnh với phần mở rộng\nlà *.tif, *.jpg, *.bmp, ... Tùy theo kích thước bản đồ mà quét thành những\ntập tin ảnh khổ A4 (với máy quét thông dụng), hay khổ A3, … Để sử dụng các tập tin ảnh này như là bản đồ giấy, ta phải khai báo đăng nhập\ntọa độ của nó và có thể sử dụng bản đồ nền trong quá trình vectơ hóa. Vào File - Open, chọn Raster Image trong mục Files of Type, kế đến chọn thư mục\nvà tập tin dạng ảnh đã được quét. Click OK, chọn Register trong cửa sổ xuất hiện\nđể đăng nhập tọa độ địa lý tương ứng. Cửa sổ  Image Registrtion  xuất hiện với\ncác mục khai báo như sau: - Projection: khai báo thông số của hệ quy chiếu - Units: khai báo đơn vị của bản đồ là độ (degrees) hay mét (meters) tùy theo\nhệ quy chiếu tương ứng và điều kiện của bản đồ tham khảo. - Khai báo các điểm xác định vị trí địa lý của khu vực bản đồ được quét. Tối\nthiểu phải khai báo 4 điểm và click vào khung  New  để khai báo một điểm mới Sử dụng button có dấu + hay - (phóng to hay thu nhỏ bản đồ ảnh) và các thanh\ntrượt để đưa một khu vực của bản đồ vào vùng nhìn trên máy tính. Hình: Hộp thoại đăng ký tọa độ ảnh Mỗi khi muốn khai báo một điểm mới, phải xác định rõ tọa độ của điểm đó trên bản\nđồ và dịch chuyển ảnh quét sao cho vị trí của điểm đó nằm ở trong khung nhìn.\nClick vào khung New, biểu thị vị trí của mouse thay đổi từ hình tượng mũi tên\nthành dấu chữ thập. Di chuyển mouse đến đúng vị trí tương ứng của điểm muốn định\nvị và click, sẽ xuất hiện cửa sổ  Edit Control Point. Hình: Hộp thoại sửa tọa độ điểm cần định vị Nhập kinh độ của điểm đã chọn vào khung  Map X  và vĩ độ vào khung  Map Y \ntheo tọa độ hệ mét hay hệ độ đã khai báo trong mục  Units  (Có thể đặt tên lại\ncho điểm này trong khung label) - chọn OK Khi đã khai báo tối thiểu 4 điểm, nên chú ý đến thông tin trong cột  Error .\nTrị số trong cột này sẽ được tính toán tự động theo tọa độ của các điểm đã được\nkhai báo. Dĩ nhiên các trị số này càng nhỏ thì bản đồ đăng nhập càng tương hợp\nvới vị trí địa lý thực. Di chuyển thanh sáng đến hàng ghi thông tin của một điểm, ta có thể thay đổi\nkhai báo của điểm đó bằng cách click  Edit , khai báo lại tọa độ trong mục Map X\nvà Map Y trong cửa sổ  Edit Control Point , hay xóa nó với  Remove  hay để\nđiểm này xuất hiện trong khung nhìn với  Goto . Khi các thông tin trong cột Error là chấp nhận được, click OK để kết thúc việc\nđăng nhập tọa độ của vùng ảnh quét. Mapinfo sẽ tạo một tập tin có tên giống như\ntên của tập tin ảnh và phần mở rộng là .TAB và hiện trên màn hình trong cửa sổ\nbản đồ của bản đồ ảnh vừa đăng nhập. Ta có thể điều chỉnh độ sáng và độ tương phản của ảnh trong  Menu Table -\nRaster - Adjust Image Style Sau khi đăng nhập muốn thay đổi khai báo tọa độ vị trí các điểm, vào  Menu\nTable - Raster - Modify Image Registration , sẽ xuất hiện cửa sổ  Image\nRegistration  để chúng ta thêm, xóa, sửa vị trí các điểm. Vectơ hóa Dữ liệu số hóa được ghi vào một lớp dữ liệu mới. Vào  Menu File - New Table ,\nmở chồng lên cửa sổ của tập tin ảnh đã được đăng nhập một table mới. Kiểm tra\nlớp này để biết rõ là đã ở chế độ được chọn (selectable) và sửa đổi được\n(editable) trong  Menu Map - Layer Control  hay click biểu tượng của chức này Tùy theo đối tượng muốn số hóa là điểm, đường hay vùng mà ta chọn các biểu tượng\nđồ họa tương ứng trong hộp công cụ  Drawing . Tính chất của các đối tượng này\n(kích cở, màu sắc, kiểu dạng,…) được xác định với các biểu tượng trong cửa sổ\nnày hay trong  Option - Line Style / Region Style / Symbol Style . Ta cũng có\nthể nhập văn bản với kiểu chữ, kích cở, màu sắc và độ nghiêng tùy chọn tại một\nvị trí bất kỳ. Để số hóa các đối tượng vùng tiếp giáp nhau cùng chung một ranh giới, nên sử\ndụng khả năng bắt điểm (snap to node) - bấm phím  S  để tắt mở chức năng này -\ncác chức năng xóa, cắt chia, xóa phần ngoài,… cũng thường được sử dụng trong quá\ntrình số hóa. Đế lưu dữ liệu, bấm Ctrl + S hay vào  File - Save Table II.3.3 - Trao đổi dữ liệu với hệ thống khác ¶ 1. Trao đổi với dữ liệu của Autocad Để trao đổi dữ liệu với một file dữ liệu của Autocad chúng ta vào:  Menu Table\n- Import , cửa sổ  Import File  : Hình: Hộp thoại nhập dữ liệu dạng *.dxf - Trong Files of Type: chọn AutoCad DXF (*.dxf) - Chọn file Autocad cần nhập - Chọn Open, cửa sổ DXF Import Information xuất hiện như sau: - Chọn Projection để chọn hệ quy chiếu - Trong khung DXF Layers to Import: chọn lớp thông tin cần nhập - Chọn OK để nhập dữ liệu - Đặt tên file dữ liệu 2. Trao đổi dữ liệu bằng chức năng Universal Tranlator Chọn  Menu Tool  -  Universal Tranlator  để tiến hành trao đổi dữ liệu: - Trong khung  Source  chọn định dạng file dữ liệu và file dữ liệu nguồn cần\ntrao đổi dữ liệu - Trong khung  Destination  chọn định dạng file dữ liệu và thư mục lưu dữ\nliệu. II.4 - biên tập các đối tượng bản đồ ¶ Việc biên tập các đối tượng bản đồ là một công việc rất công phu và đòi hỏi tính\nkiên nhẫn. Các đối tượng bản đồ muốn biên tập được phải nằm trên lớp thông tin\nđược thiết lập ở chế độ biên tập được. Các thao tác biên tập chính có thể thực\nhiện như sau: II.4.1 - Vẽ đối tượng mới ¶ Chọn các công cụ vẽ đối tượng mà chúng ta cần tạo ra trên thanh công cụ Drawing\nnhư: line, polyline, polygon, symbol, arc sau đó di chuyển chuột đến vị trí cần\ntạo ra đối tượng mới và xác định vị trí cho các đỉnh của đối tượng tạo ra. II.4.2 - Xóa đối tượng đã có ¶ Chọn đối tượng cần xóa rồi sau đó ấn phím Delete. Nếu muốn xóa nhiều đối tượng\ncùng lúc thì bấm chọn các đối tượng đồng thời với ấn phím Shift rồi sau đó ấn\nphím Delete. II.4.3 - Sao chép và dán các đối tượng ¶ Chọn các đối tượng cần sao chép sau đó bấm vào biểu tượng copy hoặc vào Edit -\nCopy hay sử dụng tổ hợp phím Ctrl + C sau đó chọn Edit - Paste hay sử dụng tổ\nhợp phím Ctrl + V Các đối tượng trong Mapinfo được sao chép theo địa lý cho nên các đối tượng sao\nchép sẽ có vị trí địa lý trùng với các đối tượng gốc. II.4.4 - Dịch chuyển vị trí đối tượng ¶ Chọn đối tượng cần dịch chuyển vị trí bằng chuột sau đó giữ chuột và di chuyển\nđến vị trí mới; bấm chuột để khẳng định vị trí mới II.4.5 - Biên tập các đỉnh của đối tượng ¶ Chọn đối tượng cần biên tập đỉnh sau đó bấm đối tượng bật điểm đỉnh trong hộp\nMain hoặc Edit - Reshape, khi đó các đỉnh của đối tượng sẽ bật lên và chúng ta\ncó thể thực hiện: - Xóa điểm đỉnh: chọn điểm đỉnh cần xóa sau đó bấm phím Delete Dịch chuyển vị trí điểm đỉnh: chọn điểm đỉnh cần di chuyển sau đó giữ chuột\n    và di chuyển đến vị trí mới; bấm chuột để khẳng định vị trí mới - Thêm điểm đỉnh: chọn công cụ thêm đỉnh xác định vị trí của điểm đỉnh cần thêm\nmới và bấm chuột. Sao chép một đoạn đối tượng: chọn đối tượng sau dó bật điểm đỉnh và dùng\n    chuột để chọn đỉnh đầu của đoạn - giữ phím Shift hoặc Ctrl - bấm chọn đỉnh\n    cuối cùng của đoạn, khi đó các đỉnh của đoạn sẽ được đánh dấu chọn; sau đó\n    bấm Ctrl+C và Ctrl + V. Chức năm này rất hữu ích khi chúng ta muốn nhân bản\n    các đường chung nhau của các đối tượng khi đóng các polygon trong hệ thống\n    tạo đối tượng quản lý II.5 - Phân tích không gian ¶ Phần mềm Mapinfo cung cấp một số chức năng phân tích không gian như kết hợp,\nchia cắt, xóa một phần đối tượng bản đồ, tạo vùng đệm của một đối tượng hay tạo\nđiểm chung của hai đối tượng. Các chức năng này được thực hiện cho các đối tượng\ntrong cùng một lớp dữ liệu hay trên hai lớp dữ liệu khác nhau. Các đối tượng này\nthường phải  xử lý được. Để cho phép đối tượng  xử lý được , trước hết lớp dữ liệu của đối tượng này phải\ncó thuộc tính sửa đổi được (editable), sau đó chúng ta chọn đối tượng (select),\nvào Objects - Set Target (hay Ctrl + T), đối tượng sẽ được đánh dấu là  xử lý\nđược. Xử lý dữ liệu không gian trong phần mềm Mapinfo được thực hiện theo 3 bước: - Thiết lập đối tượng muốn xử lý trở thành  xử lý được - Chọn  đối tượng chuẩn - Chọn chức năng phân tích không gian như: kết hợp (combine), chia cắt (split),\nxóa một phần (erase) Sau đây là bảng mô tả  đối tượng cắt / đối tượng xử lý  thích hợp cho từng chức\nnăng phân tích không gian: Kiểu đối tượng Tạo điểm giao Cắt, xóa, xóa ngoài Kết hợp Đối tượng cắt Đối tượng xử lý Đối tượng cắt Đối tượng xử lý Đối tượng xử lý Đ.tượng không xlý khác Vùng x x x x x x Đường x x x x x Chữ Điểm x x x Nhiều điểm x x x Nhiều kiểu x x x II.5.1 - Tổng hợp và phân tách dữ liệu (Aggregating and disaggregating data) ¶ Các dữ liệu thuộc tính đã được liên kết với các thông tin không gian do đó khi\nthực hiện các chức năng phân tích không gian hệ thống có thể tự động tổng hợp\nhoặc phân tách các dữ liệu đã có và gán chúng cho các đối tượng mới tạo ra Khi thực hiện tổng hợp các đối tượng, chúng ta có thể chọn một số phương pháp\ntổng hợp dữ liệu, bao gồm: - Sum: cộng các giá trị số của các trường dữ liệu và gán tổng số đó cho trường\ndữ liệu của đối tượng mới. - Average: tính giá trị trung bình của các dữ liệu trong các đối tượng gốc và\ngán nó cho trường dữ liệu của đối tượng mới - Weighted Average: tính giá trị trung bình có trọng số của các dữ liệu trong\ncác đối tượng gốc và gán nó cho trường dữ liệu của đối tượng mới - Value: lưu và gán giá trị nhất định của các đối tượng gốc cho đối tượng mới - No change: bảo lưu các giá trị của đối tượng mục tiêu cho đối tượng mới tạo\nra Khi thực hiện phân tách đối tượng, chúng ta có thể chọn một số phương pháp xử\nlý dữ liệu, bao gồm: - Blank: loại bỏ giá trị gốc của đối tượng xử lý - Value: lưu và gán giá trị nhất định của các đối tượng gốc cho đối tượng mới - Area proportion: tạo ra các giá trị mới trên cơ sở các số liệu gốc theo giá\ntrị hoặc kích thước của các đối tượng mới tạo ra so với đối tượng gốc II.5.2 - Kết hợp các đối tượng địa lý với đối tượng xử lý ¶ Các bước thực hiện chức năng tổng hợp các đối tượng địa lý có thể tóm tắt như\nsau: - Chọn đối tượng bản đồ xử lý (đối tượng cần xử lý phải thuộc lớp đang ở chế độ\nbiên tập được) - Menu Object - Set Target - Chọn một hoặc nhiều đối tượng địa lý cần tổng hợp với đối tượng cần xử lý - Chọn Menu Objects - Combine, cửa sổ Data Aggregation: Hình: Hộp thoại tổng hợp dữ liệu - Xác định các tham số tổng hợp dữ liệu cho từng trường dữ liệu, sau đó chọn\nOK. Nếu chọn No data sẽ thu được các bảng ghi có giá trị trắng. II.5.3 - Phân tách đối tượng ¶ Chức năng phân tách đối tượng cho phép chúng ta chia đối tượng xử lý thành những\nđối tượng nhỏ hơn, trong đó có sử dụng các đối tượng khác để làm đối tượng cắt. Để phân tách đối tượng, thực hiện theo các bước sau: - Chọn đối tượng bản đồ cần phân tách - Menu Object - Set Target - Chọn hoặc phát sinh một hoặc nhiều đối tượng khác làm đối tượng cắt (đối\ntượng này phải là đối tượng vùng) - Chọn Menu Objects - , cửa sổ Data Disaggregation: Hình: Hộp thoại phân tách dữ liệu - Xác định các tham số để phân tách dữ liệu từng trường dữ liệu, sau đó chọn\nOK. II.5.4 - Phân tách đối tượng sử dụng một đường (Polyline) ¶ Chức năng sử dụng đường (polyline) để phân tách đối tượng chỉ áp dụng để tách\ncác đối tượng kiểu đường, vùng, kiểu tập hợp nhiều điểm (multipoints), kiểu tập\nhợp nhiều kiểu đối tượng (collections), thực hiện theo các bước sau: - Chọn đối tượng bản đồ cần phân tách - Menu Objects - Set Target - Chọn đối tượng đường là đối tượng phát sinh hoặc ở bất kỳ lớp thông tin nào\nlàm đối tượng cắt (đối tượng cắt trong trường hợp này là đường liên tục không\nphải là đường phân nhánh) - Chọn Menu Objects - Polyline Split - Trường hợp đối tượng phân tách là kiểu vùng, một bảng thông báo hiện lên,\n\"Split narked target using selected region cutter ?\", chọn  Next  để tiếp tục,\nCancel để hủy bỏ. - Trường hợp chọn Next cửa sổ Data Disaggregation xuất hiện: Hình: Hộp thoại phân tách dữ liệu - Xác định các tham số để phân tách dữ liệu cho từng trường dữ liệu, sau đó\nchọn OK. II.5.5 - Xóa một phần của đối tượng ¶ - Chọn đối tượng bản đồ cần xử lý - Menu Object - Set Target - Chọn Menu Objects - Erase (hoặc Erase Outside), hộp thoại phân tách dữ liệu\nhiện ra tương tự như chức năng phân tách đối tượng. - Xác định các tham số để phân tách dữ liệu cho từng trường dữ liệu, sau đó\nchọn OK. ./img/image37.png Xóa một phần đối tượng bên trong đối tượng chuẩn (Erase) Xóa một phần đối tượng bên ngoài đối tượng chuẩn (Erase Outside) II.5.6 - Tạo các điểm nút của các đối tượng giao nhau ¶ Chức năng này tạo thêm những node mới cho đối tượng xử lý tại điểm giao nhau\ngiữa đối tượng xử lý và đối tượng cắt, được thực hiện theo các bước sau: - Chọn đối tượng cần xử lý (đối tượng này có thể là kiểu đường hoặc vùng) - Chọn Menu Object - Set Target - Chọn hoặc phát sinh một hoặc nhiều đối tượng khác làm đối tượng cắt (đối\ntượng này phải là kiểu đường hoặc kiểu vùng) - Chọn Menu Objects - Overlay Nodes. II.5.7 - Tạo vùng đệm bao quanh một (hay nhiều) đối tượng ¶ Vùng đệm là một vùng bao quanh một đối tượng đường, vùng, điểm trong cửa sổ bản\nđồ. Để tạo vùng đệm thực hiện theo các bước sau: - Đặt chế độ biên tập cho lớp thông tin chứa vùng đệm sẽ được tạo ra - Chọn đối tượng cần tạo ra vùng đệm - Chọn Objects - Buffer, cửa sổ Buffer Objects xuất hiện như sau: Hình: Hộp thoại tạo vùng đệm cho đối tượng - Value: giá trị bán kính của vùng đệm, phụ thuộc vào Units (đơn vị) ở dưới - From Column: có thể khai báo giá trị bán kính của vùng đệm là giá trị của một\ntrường dữ liệu, hay giá trị được tính thông qua các biểu thức (Expression) - Units: đợn vị đo, có thể là km, m, cm, mm,… - Smoothness: số đoạn tạo nên một vòng tròn, giá trị mặc định là 12, nhưng có\nthể khai báo giá trị bất kỳ. - Phương pháp tạo vùng đệm (Buffer method) + One buffer for all objects: tạo ra một vùng đệm chung + One buffer for each objects: tạo ra vùng đệm cho mỗi đối tượng - Chọn OK để thực hiện việc tạo vùng đệm Hình: Tạo vùng đệm cho đối tượng II.5.8 - Tạo vùng Voronoi (sơ đồ Voronoi) ¶ Bản đồ phân bổ các cột điện thoại của một thành phố như hình. Mỗi máy điện thoại\nsẽ được nối với cột gần nhất, do vậy ta phải chia thành phố thành các vùng, mỗi\nvùng có duy nhất một cột và khoảng cách từ mỗi vị trí trong vùng đến cột trong\nvùng đó là nhỏ nhất. Kết quả của phân hoạch này là sơ đồ Voronoi. Hình: Thí dụ sơ đồ Voronoi Để tạo sơ đồ Voronoi thực hiện theo các bước sau: - Chọn ít nhất 03 điểm - Chọn Objects - Voronoi, cửa sổ Voronoi Field Values xuất hiện cửa sổ Data\nAggregation - Chọn các tham số trên cửa sổ Voronoi Field Values - Chọn Ok II.5.9 - Tổng hợp đối tượng thông qua các trường dữ liệu thuộc tính ¶ Với chức năng này hệ thống không đòi hỏi phải chọn các đối tượng bản đồ trên màn\nhình, nhưng đòi hỏi phải có một cơ sở dữ liệu thuộc tính. Thực hiện chức năng\nnày hệ thống sẽ tự động tạo ra các đối tượng mới dựa trên cơ sở các dữ liệu\ntrong CSDL thuộc tính có cùng giá trị. Để thực hiện chức năng này, thực hiện theo các bước sau: - Chọn Table - Combine Objects Using Column, của sổ Combine Objects Using\nColumn như sau: Hình: Tổng hợp đối tượng sử dụng giá trị của một cột - Combine objects from table: chọn tên lớp chứa các đối tượng cần tổng hợp - Group Objects by column: chọn tên trường mà theo đó các đối tượng sẽ được\nnhóm lại nếu cùng một giá trị thuộc tính. - Store result in table: chọn Table lưu trữ kết quả (có thể tạo Table mới hoặc\nchọn Table có sẵn) CHƯƠNG III XÂY DỰNG CƠ SỞ DỮ LIỆU THUỘC TÍNH ¶ Sau khi xây dựng xong các lớp dữ liệu (Table) từ số hóa bản đồ, thể hiện các đối\ntượng trong vùng khảo sát, Mapinfo có khả năng bổ sung dữ liệu phi không gian\n(dữ liệu thuộc tính - yếu tố tự nhiên, kinh tế - xã hội) bằng nhiều phương pháp\nkhác nhau. Trong chương này chúng tôi sẽ trình bày các phương pháp cập nhật bổ sung dữ liệu\ncủa phần mềm Mapinfo III.1 - Tổ chức, thay đổi cấu trúc dữ liệu ¶ Để tổ chức hay thay đổi cấu trúc dữ liệu của Table chúng ta thực hiện như sau:\nMenu Table - Maintenance - Table Structure; cửa sổ Modify Table Structure xuất\nhiện như sau: Hình: Tổ chức, thay đổi cấu trúc của Table -  Add field : thêm một trường dữ liệu (field hay column) với tên được khai\nbáo trong hộp Name và kiểu dữ liệu trong hộp Type Tùy theo tính chất của dữ liệu mà kiểu dữ liệu có thể là: + Kiểu số nguyên (Integer): lưu các số nguyên từ - 2.100.000.000 đến +\n2.100.000.000 + Kiểu số nguyên ngắn (Small Integer): lưu các số nguyên từ - 32.768 đến +\n32.767 + Kiểu ký tự (Character): lưu trữ tối đa 254 ký tự + Kiểu số thập phân (Decimal) lưu trữ các số thập phân dạng dấu chấm cố định;\ntối đa dài 19 chữ số + Kiểu số thập phân động (Float): lưu các số thập phân dạng dấu chấm tự do + Kiểu ngày, tháng (Date): lư trữ dữ liệu dạng ngày tháng + Kiểu luận lý (Logical): chỉ có hai giá trị là True và False -  Remove field:  loại bỏ một trường dữ liệu -  Up - Down : di chuyển thứ tự của các trường dữ liệu trong bảng III.2 - nhập dữ liệu bằng công cụ info tool Theo phương pháp này, chúng ta sử dụng công cụ Info Tool để nhập dữ liệu thuộc\ntính cho từng đối tượng. Hình: Nhập dữ liệu thuộc tính bằng công cụ Info Tool III.3 - Liên kết dữ liệu có sẵn ¶ Phần mềm Mapinfo cho phép sử dụng cơ sở dữ liệu có sẵn được xây dựng trên các\nphần mềm Excel (*.xls), Foxpro, dbase (*.dbf), Lotus (*.wks), Access\n(*.mdb). Mapinfo sử dụng được các cơ sở dữ liệu này như là những lớp dữ liệu\nsau khi hoàn tất thủ tục khai báo. Vào  Menu File - Open , chọn tập tin cơ sở dữ liệu theo các dạng trên (chọn\nkiểu tương thích trong Files of Type) trong cửa sổ Open III.3.1 - Cơ sở dữ liệu .dbf (Dbase III, Dbase IV, Foxbase, Foxpro) ¶ Hình: Hộp thoại mở dữ liệu dạng dbf Chọn dBASE DBF (*.dbf) trong Files of type. Chọn tập tin dạng dbf, trong\nPrefferred View có thể chọn Automatic, Browser,…Chọn OK, xuất hiện cửa sổ dBASE\nDBF Infimation: giữ mặc định trong hộp File Character Set: Window US &.W Europe\n(“ANSI”) Chọn OK, sẽ xuất hiện cửa sổ dữ liệu (browser) của tập tin dbf tương ứng,\nMapinfo tự động tạo tập tin dạng .tab tương ứng trong cùng thư mục của tập tin\ndbf được chọn III.3.2 - Cơ sở dữ liệu Microsoft Excel ¶ Hình: Hộp thoại mở dữ liệu dạng xls Thông thường dữ liệu trong Excel lấy hàng trên cùng làm tên cột, do đó trong cửa\nsổ  Excel Information  phải khai báo như sau: Trong  Name Range  chọn Other sẽ xuất hiện khung ghi giới hạn của bảng dữ\nliệu. Mapinfo sẽ cho thấy toàn vùng dữ liệu, dữ liệu từ hàng cột nào đến hàng\ncột nào. Chúng ta sẽ thay đổi là tăng thêm một hàng đối với vị trí đầu tiên của\nvùng dữ liệu, Ví dụ thay vì A1:E80 thì đổi lại A2:E80 Đánh dấu vào mục Use Row Above Selected Range for Column Titles: xác định hàng\nđầu tiên là tên cột. Mapinfo tự động tạo tập tin dạng .tab tương ứng trong cùng\nthư mục của tập tin .xls được chọn Lưu ý:  Trong các dạng dữ liệu trên, tốt nhất là chuyển thành dạng .dbf trước\nkhi nhập vào Mapinfo vì Mapinfo chỉ thay đổi cấu trúc của dạng cơ sở dữ liệu\nnày. III.4 - Cập nhật và bổ sung dữ liệu ¶ III.4.1 - Bổ sung số liệu thống kê ¶ Mỗi tính chất của các đối tượng được bổ sung vào một vùng mới và tùy theo kiểu\nsố liệu để khai báo thích hợp trong kiểu vùng. Có thể nhập dữ liệu trong Mapinfo. Vào Window - New Browser Window, chọn lớp dữ\nliệu muốn bổ sung thêm, sẽ xuất hiện một cửa sổ dữ liệu (Browser) liệt kê dữ\nliệu theo hàng và cột, nhập dữ liệu vào các ô thích hợp. Hình: Bảng dữ liệu thuộc tính Để có thể sử dụng các phần mềm khác như Excel hay Foxpro để nhập dữ liệu, ta\nphải lưu lớp dữ liệu dưới dạng dBASE DBF với một tên khác: Vào Menu File - Save\nCopy As, chọn lớp dữ liệu cần lưu; trong cửa sổ Save Copy of Table As chọn Save\nas type là dbase DBF, ta sẽ có một tập tin của lớp dữ liệu tương ứng có phần mở\nrộng là .dbf. Vào Excel hay Foxpro và mở tập tin này để cập nhật, lưu ý không\nđược thay đổi vị trí của các hàng, vì Mapinfo đã lưu thông tin về các đối tượng\nđịa lý đồ họa theo thứ tự của các hàng. III.4.2 - Bổ sung số liệu do Mapinfo tính toán được ¶ Trong Mapinfo có sử dụng các hàm và các phép toán nên chúng ta có thể xác lập\ncác biểu thức tính toán tự động trên từng đối tượng (các hàng trong bảng). Biểu thức trong Mapinfo bao gồm  tên vùng  của lớp dữ liệu được mở - trong đó\n obj  là tên vùng đặc biệt chỉ đến đối tượng địa lý của các hàng trong bảng dữ\nliệu - các  toán tử  và các  hàm. 1. Toán tử trong Mapinfo Các toán tử trong Mapinfo bao gồm theo mức độ ưu tiên sau: (); \\^: dấu âm (-); * và /; + và -; Contains, contain entire, within, entirely within, intersects (các toán tử dùng\ncho đối tượng địa lý); =, \\<>, \\<, >, \\<=, >= (các toán tử so sánh); not, and, or (các toán tử luận lý). 2. Hàm trong Mapinfo Các hàm trong Mapinfo có dạng tên  hàm(tham số) , bao gồm: Các hàm toán học Abs(số): Trả về trị tuyệt đối của số Cos(số - đơn vị: radian): trả về cosin của số Sin(số - đơn vị: radian): trả về sin của số Tan(số - đơn vị: radian): trả về Tang của số Int(số): trả về phần nguyên của số Maximum (số 1, số 2): trả về số có giá trị lớn hơn Minimum (số 1, số 2): trả về số có giá trị bé hơn Các hàm chuỗi ký tự Str\\$(biểu thức): trả về chuỗi ký tự tương ứng của biểu thức Chr\\$(số): trả về ký tự tương ứng theo bảng mã ASCII InStr(số, chuỗi 1, chuỗi 2): tìm trong  chuỗi 1  bắt đầu tại vị trí  số , nếu có\n chuỗi 2  thì trả về vị trí của  chuỗi 2 , nếu không có thì trả về số 0 Ltrim\\$(chuỗi): trả về  chuỗi  sau khi cắt bỏ khoảng trắng phía trước Lease\\$(chuỗi): trả về chuỗi với chữ thường Left(chuỗi, số): trả về chuỗi với số ký tự bên trái Mid\\$(chuỗi, số 1, số 2): trả về  chuỗi  bắt đầu từ vị trí  số 1  và dài  số 2 \nký tự Proper\\$(chuỗi): Trả về chuỗi với ký tự đầu là chữ hoa Right\\$(chuỗi số): trả về chuỗi gồm số thứ tự từ bên phải Rtrim\\$(chuỗi): trả về  chuỗi  sau khi cắt bỏ khoảng trắng bên phải Ucase(chuỗi): trả về chuỗi hoa Len(chuỗi): trả về số ký tự của chuỗi Val(chuỗi): trả về giá trị bằng số của chuỗi Các hàm ngày tháng Curdate(): trả về ngày tháng năm của ngày hôm nay Day(ngày/tháng/năm): trả về ngày của  ngày/tháng/năm Month(ngày/tháng/năm): trả về tháng của  ngày/tháng/năm Year(ngày/tháng/năm): trả về năm của  ngày/tháng/năm Weekday(ngày/tháng/năm): trả về thứ tự của ngày trong tuần của  ngày/tháng/năm. \nChủ nhật có số thứ tự là 1 Các hàm liên quan đến đối tượng địa lý Area(obj, “đơn vị”): trả về diện tích của đối tượng theo  đơn vị CentroidX(obj): trả về trị số kinh độ điểm trọng tâm của đối tượng CentroidY(obj): trả về trị số vĩ độ điểm trọng tâm của đối tượng 3. Cập nhật bổ sung dữ liệu theo từng cột Để cập nhật dữ liệu theo từng cột vào Menu Table - Update Column xuất hiện cửa\nsổ với các mục sau: - Table to Update: chọn lớp dữ liệu muốn cập nhật trong số các lớp dữ liệu đang\nmở - Get value from table: lấy giá trị từ lớp dữ liệu nào, có 02 trường hợp: Từ lớp dữ liệu muốn cập nhật - Column to update: chọn vùng muốn cập nhật - Value: nhập một biểu thức hợp lệ. Thường sử dụng trong khung Assist để xây\ndựng biểu thức Ví dụ: Tính diện tích các đối tượng địa lý (xã, phường) của thị xã Cao Lãnh Vào Update khai báo như sau: Column to Update:D_tich Value:Area(obj, “hectare”) Hình: Tính diện tích của các đối tượng địa lý Từ một lớp dữ liệu khác Chọn khung  Join… để xác định vùng tham chiếu liên kết giữa hai lớp dữ\n  liệu -  Column to Update : chọn vùng có sẵn -  Calculate : cách tính toán (có thể là: value hay các biểu thức tổng hợp như:\nAverage, Count, Minimum, Maximum, sum, Weighted Average (trung bình gia trọng),\nProportion Sum (tổng số theo tỷ lệ), Proportion Average (trung bình theo tỷ lệ),\nProportion Weighted Average (trung bình gia trọng theo tỷ lệ) Ghi chú : các biểu thức Average, Count, Minimum, Sum, Weighted Average có\ntham số là các giá trị của dữ liệu; các biểu thức tỷ lệ (Proportion) thì xử lý\ncác đối tượng địa lý - of: thường là một cột hay biểu thức hợp lệ - Chọn OK để tiến hành cập nhật dữ liệu Ví dụ: 1 - Calculate: Value: Cập nhật dữ liệu cho cột DS_2000 của Table CL_RGHC.TAB dựa vào Table\nDANSO.TAB; thực hiện theo các bước sau: - Menu Table - Update Column, xuất hiện cửa sổ Update Column như sau: Hình: Cập nhật dữ liệu giữa hai Table Trong trường hợp này hai lớp dữ liệu có số hàng như nhau, chúng ta chọn trường\ndữ liệu khoá của hai Table bằng cách vào  Join… Hình: Chọn trường dữ liệu liên kết 2. Calculate: Sum Tính tổng dân số năm 2000 của từng vùng thuộc thị xã Cao Lãnh; thực hiện theo\ncác bước sau: - Menu Table - Update Column, xuất hiện cửa sổ Update Column, khai báo như sau: Table to Update: CL_VUNG Column to Update: DS_2000 Get Value from Table: DANSO Calculate: Sum Of: DS_2000 Hình: Cập nhật dữ liệu giữa hai Table Trong trường hợp này chúng ta chọn tên cột liên kết giữa hai lớp dữ liệu trong\ncửa sổ  Join…  là  Vung Hình: Chọn trường dữ liệu liên kết III.5 - chọn và kết hợp dữ liệu theo điều kiện ¶ III.5.1 - Chọn các đối tượng theo điều kiện trong một lớp dữ liệu ¶ Trong quá trình làm việc với cơ sở dữ liệu, hệ thống Mapinfo phát sinh lớp thông\ntin Selection. Các Selection có các đặc tính sau đây: chúng là những lớp thông\ntin trung gian (Temporary table) với tên mở rộng là Query, Query 1, Query 2,… Để chọn các đối tượng theo điều kiện thực hiện ta vào: Menu Query - Select, khai\nbáo các mục trong cửa sổ Select như sau: Hình: Chọn dữ liệu theo điều kiện - Mục Select Records from Table: Chọn lớp dữ liệu cần chọn theo điều kiện - Mục that Satisfy: nhập một biểu thức luận lý hợp lệ - Store Results in Table: lưu kết quả vào Table nào đó, có thể giữ mặc định là\nSelection. Muốn lưu dữ liệu này phải vào File - Save As - Sort Results by Column: chọn tên cột muốn sắp xếp thứ tự Với  Select  chúng ta xây dựng được một lớp dữ liệu mới thỏa điều kiện đặt ra.\nLớp dữ liệu này thường có ít đối tượng (số hàng) nhưng giữ nguyên cấu trúc của\ndữ liệu (số cột). Dĩ nhiên chúng ta có thể thay đổi cấu trúc theo ý đồ của chúng\nta, Mapinfo có một chức năng để thực hiện trực tiếp điều này đó là  SQL\nSelect . Ngoài ra,  SQL Select  còn thực hiện một số chức năng khác. Để thực hiện chức năng chọn bằng  SQL Select  vào Menu Query - SQL Select, cửa\nsổ SQL Select như sau: - from Table: nhập tên lớp dữ liệu, chúng ta có thể nhập trực tiếp từ bàn phím\ntên của lớp dữ liệu, nhưng nên click khung  Table  để chọn - Select Column: nếu chọn tất cả các cột thì giữ dấu * (mặc định), nếu chỉ\nchọn một số cột thì xóa dấu * rồi click khung Column để chọn. Ngoài ra ta còn\ncó thể xây dựng các biểu thức tính toán và hình thành thêm các cột mới. Điều này\ncó nghĩa lớp dữ liệu mới sẽ có một số cột cũ hay ít hơn cũng có thể nhiều hơn. Để đặt tên cho cột mới của một biểu thức tính toán, chúng ta ghi tên cột trong\ndấu ngoặc kép ngay sau biểu thức. Ví dụ: DS_2000/area(obj, “sq km”) “Mat do” - Where condition: có thể để trống hay nhập biểu thức luận lý hợp lệ. Trường\nhợp là một biểu thức luận lý thì chỉ những hàng phù hợp với điều kiện biểu thức\nnày mới xuất hiện trong lớp dữ liệu tạm thời (selection) - Group by column: chọn cột muốn tính gộp cho các giá trị giống nhau (subtotal)\ntrong cột đó. - Order by column: chọn cột muốn sắp xếp thứ tự, mặc định là sắp xếp theo thứ\ntự tăng dần; muốn sắp xếp theo thứ tự giảm dần, chúng ta thêm desc đằng sau tên\ncột đã chọn. Để xây dựng các biểu thức toán trong SQL Select, ngoài các thành phần thông\nthường là tên cột, hàm và toán tử; Mapinfo còn có thêm các hàm thống kê như: Avg\n(trung bình cộng), Sum (tổng), Min (giá trị tối thiểu), Max (giá trị tối đa),\nWtAvg (trung bình gia trọng) và Count (đếm số lượng), các hàm thống kê này được\nliệt kê trong Aggregates Các lớp dữ liệu mới phát sinh từ các hàm thống kê là lớp dữ liệu tổng hợp của\nmột cột nào đó, chúng được lưu tạm trong các Query hay selection Ví dụ 1: Sử dụng lớp dữ liệu DANSO.TAB Select Column: Count(*) “So xa”, sum(DS_2000) “Danso 2000” Kết quả là: So xa Danso 2000 13 142.037 Trường hợp có xét Group by Column số hàng sẽ là số giá trị khác nhau của cột\nnày. Ví dụ 2: Select Column: Vung, Count(*) “So xa”, sum(DS_2000) “Danso 2000” Group by Column: Vung Order by Column: Vung Kết quả là: III.5.2 - Kết hợp với một lớp dữ liệu khác ¶ Để kết hợp hai lớp dữ liệu với các thông tin khác nhau, chúng ta mở hai lớp dữ\nliệu này, sau đó vào Query - SQL Select. Trong cửa số SQL Select khai báo như\nsau: - from table: chọn các lớp dữ liệu để liên kết Sau khi chọn các lớp dữ liệu, trong vùng Where Condition tên cột dữ liệu để liên\nkết hai lớp dữ liệu với nhau - Select Column: dấu * là chọn tất cả các cột của hai lớp dữ liệu. Chúng ta có\nthể chọn các cột tuỳ ý và có thể tạo các biểu thức tính toán để phát sinh các\ncột mới - Group by column: chọn cột muốn tính gộp cho các giá trị giống nhau (subtotal)\ntrong cột đó. - Order by column: chọn cột muốn sắp xếp thứ tự, mặc định là sắp xếp theo thứ\ntự tăng dần; muốn sắp xếp theo thứ tự giảm dần, chúng ta thêm desc đằng sau tên\ncột đã chọn. CHƯƠNG IV XÂY DỰNG & KẾT XUẤT BẢN ĐỒ ¶ IV.1 - Bản đồ chuyên đề ¶ Thành lập bản đồ chuyên đề là một công cụ hiệu quả nhất để thể hiện sự phân tích\nvà hiển thị các dữ liệu. Thành lập bản đồ chuyên đề là một quá trình thể hiện\nthông qua tô vẽ các đối tượng bản đồ theo cột chuyên đề cụ thể. Ví dụ bản đồ thể\nhiện bằng phương pháp tô màu mật độ dân số theo đơn vị hành chính cấp huyện là\nmột bản đồ chuyên đề Trong phần mềm Mapinfo có 6 loại thể hiện bản đồ chuyên đề khác nhau: - Ranges: thể hiện theo khoảng dữ liệu - Bar Charts: thể hiện theo biểu đồ hình chữ nhật - Pie Charts: thể hiện theo biểu đồ hình tròn - Graduated: thể hiện theo ký hiệu có trọng số - Dot Density: thể hiện theo mật độ điểm - Individua: thể hiện theo giá trị độc lập Mỗi một loại bản đồ chuyên đề trên có thể sử dụng cho một mục đích cụ thể và\nchúng có các tính chất khác nhau. Các dữ liệu sử dụng để thành lập bản đồ chuyên\nđề có thể là giá trị số hoặc không nhất thiết phải là giá trị số. Thông qua bản đồ chuyên đề chúng ta có thể tạo ra một Xêri bản đồ dựa trên cùng\nmột nền các đối tượng bản đồ chung, hiển thị các vấn đề khác nhau liên quan đến\ncác chủ đề quan tâm. IV.2 - Khái quát về thành lập bản đồ chuyên đề ¶ - Tham số thành lập bản đồ chuyên đề: Trước khi chúng ta thành lập các bản đồ chuyên đề phải hiểu rõ về các thành phần\ntạo nên bản đồ chuyên đề và sự quan hệ giữa chúng khi thành lập bản đồ. Các dữ\nliệu hiển thị trên bản đồ chuyên đề được gọi là các tham số chuyên đề. Tùy theo\nkết quả phân tích chuyên đề mà chúng ta thực hiện và có thể xác định một hay\nnhiều tham số chuyên đề. Tham số chuyên đề có thể là giá trị của một trường dữ\nliệu hay là một biểu thức toán học của các trường dữ liệu - Thông qua thành lập bản đồ chuyên, các thông tin phục vụ để thành lập bản đồ\nchuyên đề có thể được truy cập từ một lớp thông tin hay nhiều lớp thông tin - Lớp thông tin chuyên đề (thematic layer), khi chúng ta tạo ra bản đồ chuyên\nđề thì hệ thống tự động tạo ra một lớp thông tin chuyên đề độc lập để quản lý và\nlưu trữ các thông tin đó. Trong hộp điều khiển của hệ thống nó cũng được tự động\nthêm vào. Nếu không muốn hiển thị lớp thông tin chuyên đề này trên bản đồ thì\nchúng ta tắt chế độ hiển thị của lớp thông tin chuyên đề đó. Để thực hiện tạo ra bản đồ chuyên đề vào Map - Create Thematic Map, xuất hiện\ncửa số sau: Hình: Xây dựng bản đồ chuyên đề IV.3 - Các phương pháp tạo bản đồ chuyên đề ¶ IV.3.1 - Tạo bản đồ chuyên đề theo phương pháp Range ¶ Chọn biểu trong cửa sổ  Create Thematic Map  và bấm nút  Next  khi đó màn hình\nhiện ra cửa sổ sau: Tại cửa sổ này chúng ta cần xác định: - Table: tên Table cần xây dựng bản đồ chuyên đề - Field: chọn trường dữ liệu tạo chuyên đề - Ignore Zeroes or Blank: bỏ qua các bảng ghi trắng Chọn Next để sang bước tiếp theo: Tại cửa sổ này chúng ta được cung cấp một tầm nhìn tổng quan về các nhóm chuyên\nđề sẽ tạo ra trong khung Preview. Các đối tượng trong lớp chuyên đề sẽ được nhóm\ntheo giá trị ngầm định của hệ thống trên cơ sở các thông tin thuộc tính đã gắn\nvới chúng Chúng ta có thể chọn phong cách thể hiện của các dữ liệu trong bảng ghi chú bằng\ncách chọn ô Ascending cho sự hiển thị giá trị từ nhỏ đến lớn hoặc Descending cho\nsự hiển thị từ lớn đến nhỏ Trước khi sang bước kế tiếp chúng ta có thể thay đổi các tham số của bản đồ\nchuyên đề tạo ra như định nghĩa lại các khoảng giá trị, thay đổi thuộc tính thể\nhiện và bảng ghi chú chuyên đề theo các nút trên cửa sổ như sau: Nếu chọn màn hình hiện ra cửa sổ như sau: Tại cửa sổ này chúng ta xác định: - Phương pháp tạo bản đồ chuyên đề tại hộp Method, trong danh sách các phương\npháp có thể chọn một trong các phương pháp như sau: + Equal count: tạo ra nhóm các đối tượng có số bảng ghi như nhau + Equal range: tạo ra nhóm các đối tượng có khoảng dữ liệu như nhau + Nature Break: sự phân tách các khoảng dữ liệu chuyên đề dựa trên cơ sở tối\nthiểu hóa (Minimum) các hiệu số của các dữ liệu với giá trị trung bình của chúng + Standar Deviation: khoảng giữa được phân tách tại giá trị trung bình của các\ndữ liệu, khoảng trên và khoảng dưới nó được xác định bằng khoảng giữa cộng trừ\nđi một giá trị sai lệch chuẩn. + Quantile: xác định sự phân bố của một biến dữ liệu dọc theo khoảng dữ liệu.\nKhi chọn phương pháp này màn hình hiện ra hộp Quantile và chúng ta chọn biểu\nthức để thực hiện thao tác xác định phân bố. + Custom: chọn phương pháp này chúng ta phải tự xác định các khoảng dữ liệu\nchuyên đề. Khi chọn Custom, màn hình hiện ra cửa sổ Custom range và chúng ta\nnhập khoảng dữ lệu vào đó thông qua giá trị Min và Max Chọn nút Style để tiến hành chọn màu thể hiện bản đồ chuyên đề Chọn nút Legend để biên tập chú dẫn cho bản đồ chuyên đề IV.3.2 - Tạo bản đồ chuyên đề bằng phương pháp Bar Chart ¶ IV.3.3 - Tạo bản đồ chuyên đề bằng phương pháp Pie Chart ¶ IV.3.4 - Tạo bản đồ chuyên đề bằng phương pháp Graduated ¶ IV.3.5 - Tạo bản đồ chuyên đề bằng phương pháp Dot Density ¶ IV.3.6 - Tạo bản đồ chuyên đề bằng phương pháp Individual ¶ Sau khi đã hình thành bản đồ chuyên đề chúng ta có thể sửa đổi các tham số đã\nchọn bằng cách vào Map - Modify Thematic Map IV.4 - Tạo trang Layout để xuất bản đồ ¶ IV.4.1 - Tạo lưới bản đồ ¶ Để tạo lưới cho bản đồ chúng thực hiện theo các bước sau: - Chạy chương trình Mapbasic tạo lưới: Menu Tool - Run Mapbasic Program - chọn\n\\Mapinfo\\Professional\\Tools\\Gridmark - Chọn công cụ tạo lưới, kéo tạo khung lưới trên cửa sổ bản đồ, xuất hiện cửa\nsổ sau: Hình: Tạo lưới bản đồ Trong cửa sổ trên chọn các tham số như sau: - Object Types: nên chọn kiểu Straight Polylines - Spacing between lines: khoảng cách giữa các đường - Projection: chọn hệ quy chiếu - New table: đặt tên cho lưới sắp tạo IV.4.2 - Tạo thước tỷ lệ ¶ - Chạy chương trình Mapbasic thước tỷ lệ: Menu Tool - Run Mapbasic Program -\nchọn \\Mapinfo\\Professional\\Tools\\Scalebar Xác định vị trí đặt thước tỷ lệ, xuất hiện cửa sổ: Hình: Tạo thước tỷ lệ - Width of scalebar: khai báo chiều dài của thước tỷ lệ - Chọn màu, font chữ cho thước tỷ lệ IV.4.3 - Thiết kế trang in (layout) ¶ 1. Tạo ra trang trình bày bản đồ mới Trang trình bày là một công cụ của Mapinfo cho phép tổng hợp các cửa sổ thông\ntin như cửa sổ bản đồ, cửa sổ xét duyệt, cửa sổ biểu đồ, cửa sổ ghi chú,… trên\nmột trang bản đồ và có thể in ấn ra thiết bị đầu ra. Chúng ta có thể thêm bớt\nbất kỳ một cửa sổ thông tin hiện thời đang mở nào vào trang trình bày hoặc thêm\ncác tiêu đề, ghi chú vào trang trình bày. Chúng ta vào Menu Window - New Layout Window, cửa sổ New Layout Window như sau: Trong cửa sổ này có thể chọn một trong ba lựa chọn: - One Frame for Window: tự động tạo ra một khung tại trung tâm của trang trình\nbày và chúng ta có thể chọn tên của cửa sổ thông tin hiện thời đang mở. Chúng ta\ncó thể biên tập, thay đổi kích thước, vị trí cũng như các thuộc tính thể hiện\ncủa khung giống như là một đối tượng vùng - Frames for All Currently Open Windows: tự động tạo một khung và gán toàn bộ\ncác cửa sổ thông tin đang mở vào trang trình bày. - No Frame: chức năng này tương tự như khi tạo ra cửa sổ Layout mới nếu chúng\nta chưa có một cửa sổ thông tin nào được mở Frame trong Layout có các tính chất sau: - Frame là một đối tượng đồ thị và chúng ta có thể chọn, biên tập - Chúng ta có thể sao chép, xóa và dán đối tượng vào cửa sổ Layout khác 2. Tạo ra các Frame trên Layout Khi mở ít nhất một trang Layout biểu tượng công cụ Frame (trên thanh Drawing) sẽ\nbật sáng, chúng ta có thể dụng công cụ này để xác định các Frame trên trang\nLayout. Khi tạo xong Frame cửa sổ  Frame Object  như sau: Hình: Tạo các Frame cho Layout - Chọn cửa sổ bản đồ cần trình bày trên trang Layout trong hộp  Window - Đặt tỷ lệ cho bản đồ cần in ra trong hộp  Map Scale Option Tiếp theo   Trước Xây dựng bởi  Dammediachat.com  © 2017 \n\n  Xây dựng bởi  MkDocs  sử dụng  theme  của  Read the Docs .\n",
          "url": "http://docs.dammediachat.com/mapinfo/",
          "relevance": "0"
        },
        {
          "title": "Hướng dẫn sử dụng công cụ vẽ hình Geogebra 5.0",
          "content": "SOẠN BÀI Toán Lý Hóa Sinh Tin Văn Sử Địa Anh SÁCH HAY TÀI LIỆU Khối lớp 6 Khối lớp 7 Khối lớp 8 Khối lớp 9 Khối lớp 10 Khối lớp 11 Khối lớp 12 Luyện thi HSG THCS Luyện thi HSG THPT Luyện thi vào 10 Luyện thi Đại học Toán cho Sinh viên Toán Casio Violympic KHTN-KHXH Vật lý và Tuổi trẻ Toán nước ngoài Sách luyện thi TOÁN TUỔI THƠ 1 TOÁN TUỔI THƠ 2 TOÁN HỌC TUỔI TRẺ BLOG TRUNG TÂM Hướng dẫn sử dụng công cụ vẽ hình Geogebra 5.0 GeoGebra  là một chương trình miễn phí về toán học hỗ trợ việc học các môn hình học, đại số và giải tích. Ứng dụng đa năng này cung cấp những hình biểu diễn các đối tượng liên kết động. Nó giúp liên kết tương tác các hình biểu diễn khác nhau nên người sử dụng có thể nghiên cứu và làm việc với nhiều cách giải khác nhau. Chương trình có thể thực hiện với điểm, đường thẳng, vectơ, và đường cô-nic. Bạn cũng có thể nhập và thao tác với phương trình và tọa độ, cũng như tạo các điểm, đường thẳng, vectơ và đường cô-nic. GeoGebra cũng cho phép người dùng đưa vào một số câu lệnh như Root hoặc Sequence. Việc đó giúp giải các phương trình phức tạp dễ dàng và đơn giản hơn. Vì đây là chương trình phức tạp nên nó không được thiết kế cho những người mới làm quen với ứng dụng toán cao cấp. GeoGebra vẫn có hướng dẫn chi tiết khi mới bắt đầu sử dụng nhưng đây vẫn là chương trình khá phức tạp đối với những người mới học toán cao cấp. Do đó, công cụ này rất thích hợp cho những người dùng thường xuyên làm việc với các môn đại số, hình học, hay các phép tính. Với tính linh hoạt và hữu dụng của mình, GeoGebra xứng đáng là \"bạn đồng hành\" của các nhà toán học. Bài 1. Giao diện phần mềm 1. Giới thiệu giao diện chung: Tôi sẽ tranh thủ thời gian viết các hướng dẫn sử dụng nhanh phần mềm Geogebra phiên bản 5.0 dành cho GV đang giảng dạy môn Toán trong các nhà trường từ phổ thông đến đại học. Trong hình 1 thể hiện 3 khu vực chính: (1) Vùng làm việc, thể hiện các hình phẳng chính; (2) danh sách các đối tượng hình học và (3) Thanh công cụ vẽ hình chính của phần mềm. Khi cài đặt, mặc định giao diện là tiếng Anh, chúng ta có thể chuyển giao diện sang Tiếng Việt hoàn toàn như trong hình. Hình 1: các khu vực chính của màn hình Geogebra. Để làm ẩn / hiện các khu vực làm việc chính của phần mềm chúng ta quan sát thực đơn Hiển thị (View) trong Hình 2. Tổ hợp phím nóng thường dùng: - Hiển thị/ẩn vùng làm việc 2D: Ctrl+Shift+1 - Hiển thị /ẩn DS các đối tượng: Ctrl+Shift+A. Các tổ hợp phím Ctrl+Shift+3 và Ctrl+Shift+K dùng để hiển thị 2 khung cửa sổ quan trọng nữa là Khung hình 3D và Khung đại số (CAS) nhưng ta sẽ làm quen sau. Thanh Công cụ (Tool Bar) là công cụ quan trọng nhất mà mỗi người sử dụng cần thao tác để làm việc khi vẽ hình. Chúng ta sẽ được học các công cụ này trong các bài tiếp theo. Hình 2. Thực đơn Hiển thị (View) của phần mềm.   Bài 2. Đối tượng hình học, quan hệ giữa các đối tượng Một trong những điểm quan trọng nhất của phần mềm Geogebra là khái niệm Đối tượng Toán học và QUAN HỆ giữa chúng. Đối tượng hình học ví dụ như điểm, đoạn, tia, đường thẳng, hình tròn, cung tròn, ellip, ...  Quan hệ giữa các đối tượng là các quan hệ TOÁN HỌC giữa chúng như nằm trên, đi qua, giao điểm, song song, vuông góc, .... Hiểu rõ bản chất các đối tượng và quan hệ toán hoc giữa chúng là điểm mấu chốt nhất để hiểu phần mềm Geogebra (và các phần mềm toán học động tương tự). Khi một đối tượng A phụ thuộc vào đối tượng B, ta có thể nói \"A là con của B\" hay \"B là cha của A\". Các đối tượng không phụ thuộc vào bất kỳ đối tượng nào khác gọi là đối tượng Tự do, ngược lại gọi là đối tượng Phụ thuộc. Trong hình 1, 2 điểm A, B là đối tượng tự do, đường thằng đi qua A, B sẽ phụ thuộc vào A, B, do đó là đối tượng phụ thuộc.   Hình 1. A, B là 2 điểm tự do, đường thẳng a đi qua A, B sẽ phụ thuộc vào A, B.   Hình 2. Hai điểm A, B nằm trên đường thẳng d và phụ thuộc vào d. Như vậy nhìn hình bên ngoài không thể biết được đối tượng nào là tự do, đối tượng nào là phụ thuộc và chúng phụ thuộc nhau như thế nào. Cần tìm hiểu sâu hơn để nắm vững sự phụ thuộc này. Trong hình 3 chỉ ra, nếu 2 đường thẳng d, d1 giao nhau tại A thì A là đối tượng \"con\" của 2 đối tượng d và d1. Hai đường tròn giao nhau tại 2 điểm C, D như vậy 2 đối tượng mẹ (2 vòng tròn) sẽ tạo ra 2 đối tượng con (2 điểm).   Hình 3. Quan sát hình chưa thể biết đối tượng nào tự do, đối tượng nào phụ thuộc. Trong phần mềm Geogebra, khung DS các đối tượng (bên trái) sẽ thể hiện DS các đối tượng, trong đó phân loại rõ 2 loại đối tượng tự do và phụ thuộc. Bài 3: Nguyên tắc cơ bản của hình học động Như vậy chúng ta đã biết là một hình hình học động bao gồm các đối tượng có quan hệ phụ thuộc lẫn nhau. Các quan hệ này là quan hệ TOÁN HỌC. Nhìn vào 1 hình từ bên ngoài chúng ta không thể biết và nhận ra các quan hệ đó. Hình 1 phía dưới là hình vẽ bài toán đường thẳng Sim Son. Nhìn vào hình này chúng ta không thể biết quan hệ giữa 3 điểm A, B, C và vòng tròn: vòng tròn đi qua 3 điểm hay 3 điểm nằm trên vòng tròn? Chúng ta cần hiểu sâu hơn nữa về các quan hệ này.   Hình 1. Đường thẳng Sim Sơn.   Nguyên tắc cơ bản: Quan hệ phụ thuộc giữa các đối tượng hình học một khi đã thiết lập thì không bao giờ thay đổi. Ba hệ quả sau rất quan trong mà mỗi người sử dụng cần biết về các phần mềm Toán học động, chúng đều suy ra từ Nguyên tắc trên:       1. Mọi đối tượng đều có thể chuyển động tối đa tự do trong phạm vi cho phép của quan hệ phụ thuộc.        2. Khi một đối tượng chuyển động, tất cả các đối tượng phụ thuộc sẽ chuyển động theo.       3. Khi một đối tượng bị xóa thì tất cả các đối tượng phụ thuộc sẽ bị xóa theo. Ba hệ quả trên là kim chỉ nam để các GV thực hiện công việc của mình khi tiến hành vẽ hình bằng phần mềm Geogebra. Do phải thiết lập các quan hệ toán học chằng chịt giữa các đối tượng chúng ta thường phải vẽ thêm rất nhiều đối tượng phụ, sau đó ẩn đi các đối tượng không cần thiết thể hiện trên hình. Hình 2 vẽ 1 tam giác và vẽ các đường tròng nội tiếp, bàng tiếp và vòng tròn Euler (màu đỏ). Để vẽ được hình này chúng ta cần vẽ thêm các hình phụ.Hình 3 thể hiện tất cả các hình phụ này. Sau khi ẩn đi các đối tượng không cần thiết sẽ còn lại hình như mong muốn.   Hình 2. Hình ảnh 1 tam giác với các đường tròn nội tiếp và bàng tiếp.   Hình 3. Đây chính là hình 2 nhưng hiện tất cả các đối tượng.   Bài 4: Làm quen với thanh công cụ vẽ hình Để làm quen và vẽ được các hình học động như ý muốn, các GV bắt buộc phải làm quen với các công cụ vẽ của phần mềm. Toàn bộ các công cụ vẽ được thể hiện trên Thanh công cụ chính. Hình 1. Thanh công cụ chính Thanh công cụ chỉ hiện trên 1 hàng, nhưng tại mỗi vị trí lại chứa nhiều công cụ khác phía dưới. Muốn chọn 1 công cụ phía dưới cần nháy chuột lên 1 nút nhỏ tại góc phải dưới của biểu tượng này Hình 2. Các chức năng trong mỗi nút công cụ Tại 1 thời điểm chỉ có 1 công cụ duy nhất được chọn. Công cụ này sẽ hiện ngay trên thanh công cụ, có viền đậm. GV cần chú ý đến điều này. Khi công cụ được chọn, GV được phép vẽ và kiến tạo nhiều đối tượng liên tục theo cùng 1 kiểu của công cụ này. Hình 3. Công cụ vẽ đang làm việc hiện thời Trong các công cụ đó có 1 công cụ đặc biệt gọi là Di chuyển (Move). Công cụ này không dùng để vẽ, mà để di chuyển, dịch chuyển hình. Chính việc dịch chuyển này mà ta gọi là Hình học ĐỘNG. Tại bất cứ thời điểm nào bấm ESC để quay về chế độ Move (Dịch chuyển này).  Hình 4. Công cụ di chuyển   Thao tác đơn giản để vẽ 1 hình tam giác. Ta sẽ vẽ bằng 2 cách: - Cách 1, xem phía trên. Sử dụng 2 công cụ Điểm mới và Đoạn thẳng. - Cách 2, xem phía dưới. Sử dụng 1 công cụ Đa giác để tạo 1 tam giác. Sau khi tạo các hình này rồi, chúng ta có thể dịch chuyển chúng trên màn hình phẳng sau khi đã chuyển về chế độ dịch chuyển. Hình 5. Thao tác đơn giản để vẽ hình tam giác   Bài 5: Các bước chuẩn bị để sẵn sàng vẽ hình Khi mới cài đặt phần mềm, thực đơn và giao diện sẽ là tiếng Anh, các GV có thể chuyển đổi về giao diện tiếng Việt hoàn toàn. Hình 1. Cài đặt tiếng Việt cho phần mềm Geogebra. Có thể phóng to cỡ chữ làm việc màn hình để quan sát cho rõ. Hình 2. Thiết lập cỡ chữ mặc định cho hệ thống thực đơn, thanh công cụ, hộp hội thoại.   Đặt lại các lựa chọn thể hiện màn hình. Với chế độ vẽ hình (2D) thì không cần hiện lưới và trục tọa độ. Hình 3. Nháy chuột phải trên vùng làm việc xuất hiện hộp hội thoại thiết lập các thông số vùng làm việc.   Có thể làm ẩn hoặc hiện DS các đối tượng bên trái màn hình. Hình 4. Ba khu vực làm việc chính. Bây giờ chúng ta đã có thể sẵn sàng cho các bài luyện tập vẽ hình động trên Geogebra. Bài 6: Bài thực hành đầu tiên: vẽ tam giác động Đây là bài thực hành đầu tiên với Geogebra. Chúng ta sẽ cùng nhau tập vẽ một hình động đơn giản nhất, đó là hình tam giác. Chúng ta sẽ thực hành vẽ hình tam giác theo 2 cách: Cách 1: - Sử dụng công cụ Điểm mới để tạo ra 3 điểm bất kỳ trên mặt phẳng. - Sử dụng công cụ Đoạn thẳng để nối các đỉnh trên tạo ra 3 cạnh của tam giác. Cách 2: - Sử dụng công cụ Đa giác để tạo ra 1 tam giác bằng cách nháy chuột lần lượt tại 3 điểm bất kỳ trên mặt phẳng, sau đó nháy chuột vào điểm đầu tiên để kết thúc việc tạo ra tam giác. Chú ý: Khi nháy chuột lên 1 điểm đã có, chú ý khi di chuyển con trỏ chuột tới gần điểm đó, chuột sẽ bị hút vào điểm đó (như nam châm), lúc đó mới nháy chuột). Hình sau mô tả kết quả của bài thực hành đầu tiên này. Video thực hành:   Bài 7: Thực hành vẽ tam giác cân, tam giác vuông Đây là bài thực hành đơn giản tiếp theo với Geogebra. Chúng ta sẽ cùng nhau tập vẽ một tam giác cân và một tam giác vuông. Đây là bài thực hành đầu tiên băt đầu có các yêu cầu quan hệ toán học giữa các đối tượng của hình. Chúng ta sẽ thực hành vẽ lần lượt 2 tam giác trên theo yêu cầu: 1. Vẽ tam giác cân. - Trước tiên cần vẽ cạnh đáy của tam giác. - Sử dụng công cụ Đoạn thẳng để vẽ cạnh đáy của tam giác. - Sử dụng công cụ Đường trung trực để vẽ đường trung trực của đoạn thẳng vừa vẽ trong bước trên. - Vẽ 1 điểm chuyển động tự do trên đường thằng trung trục này bằng cách sử dụng công cụ Điểm, sau đó nháy chuột trên đường trung trực trên. - Sử dụng công cụ Đoạn thẳng để nối cạnh bên của tam giác. - Ẩn đi đường trung trực. 2. Vẽ tam giác vuông. - Sử dụng công cụ Đoạn thẳng để vẽ 1 cạnh góc vuông của tam giác. - Sử dụng công cụ đường vuông góc để vẽ 1 đường thẳng vuông góc với cạnh vừa vẽ và đi qua 1 đỉnh. - Vẽ 1 điểm chuyển động tự do trên đường thằng vuông góc vừa vẽ bằng cách sử dụng công cụ Điểm , sau đó nháy chuột trên đường vuông góc trên. - Ẩn đi đường vuông góc. - Sử dụng công cụ Đoạn thẳng để nối 2 cạnh còn lại của tam giác. Chú ý : Khi nháy chuột lên 1 điểm đã có, chú ý khi di chuyển con trỏ chuột tới gần điểm đó, chuột sẽ bị hút vào điểm đó (như nam châm), lúc đó mới nháy chuột). Hình sau mô tả kết quả của bài thực hành đầu tiên này.   Video bài thực hành này:   Bài 8: Thực hành vẽ hình bình hành Chúng ta sẽ cùng nhau tập vẽ một hình bình hành. - Sử dụng công cụ Đoạn thẳng Geogebrađể vẽ 2 cạnh liền nhau bất kỳ của hình bình hành. Như vậy sau bước này chúng ta đã có 3 đỉnh tự do và 2 cạnh của hình. Bước tiếp theo là cần xác định đỉnh còn lại của hình. - Sử dụng công cụ Song song Geogebrađể tạo ra 2 đường thẳng đi qua 2 đỉnh đối diện đã có và song song với cạnh đối diện. - Sử dụng công cụ Geogebrađể xác định giao điểm của hai đường song song vừa tạo. Thao tác như sau: di chuyển chuột đến giao điểm, khi thấy cả 2 đường được chọn thì nháy chuột. - Ẩn đi 2 đường song song này. - Sử dụng công cụ Đoạn thẳng Geogebrađể nối 2 cạnh còn lại của hình bình hành. Hình sau mô tả kết quả của bài thực hành đầu tiên này. Video bài thực hành:   Bài 9: Thực hành vẽ hình vuông Trong bài học này chúng ta sẽ thực hành tập vẽ một hình vuông. Với bài thực hành này có nhiều quan hệ toán học phức tạp hơn. Chúng ta sẽ bắt đầu vẽ từ một cạnh của hình vuông. - Sử dụng công cụ Đoạn thẳng Geogebrađể vẽ 1 cạnh đầu tiên của hình vuông. - Sử dụng công cụ Vuông góc Geogebrađể tạo ra hai đường thẳng đi qua hai điểm đầu mút của cạnh và vuông góc với cạnh này. Kết quả thể hiện ở hình sau: Hình 1. Đoạn thẳng và hai đường vuông góc. Tiếp theo cần xác định 2 đỉnh còn lại của hình vuông nằm trên hai đường thẳng vuông góc này. Thao tác như sau: - Sử dụng công cụ Tạo vòng tròn biết tâm và 1 điểm Geogebrađể lần lượt tạo 2 vòng tròn đi qua tâm là 1 trong 2 điểm đầu mút của đoạn thẳng và đi qua điểm còn lại. Ta sẽ thu được hình như sau: Hình 2. Bổ sung thêm 2 vòng tròn. - Sử dụng công cụ Geogebrađể xác định giao điểm của hai đường tròn vừa vẽ với hai đường thẳng vuông góc. Thao tác như sau: di chuyển chuột đến giao điểm, khi thấy cả 2 đối tượng (đường tròn và đường thẳng) được chọn thì nháy chuột. - Ẩn đi 2 đường thằng vuông góc và 2 vòng tròn vừa tạo. - Sử dụng công cụ Đoạn thẳng để nối các cạnh còn lại của hình vuông. Hình sau mô tả kết quả của bài thực hành này. Hình 3. Hình vuông đã hoàn thành. Video bài thực hành này:   Bài 10: Làm thế nào để vẽ hình đúng và chính xác Trong bài thực hành này chúng ta sẽ lần lượt vẽ các hình đơn giản: vẽ một tam giác với các đường trung tuyến, phân giác và đường cao. Qua bài học này chúng ta sẽ hiểu và phân biệt được thế nào là vẽ đúng và chính xác. Trong bài học này chúng ta sẽ thực hành các thao tác vẽ sau: 1. Vẽ tam giác với ba đường trung tuyến và trọng tâm - Sử dụng công cụ Đa giácgeogebrađể vẽ tam giác ABC. - Sử dụng công cụ Trung điểm geogebrađể tạo các điểm là trung điểm của các cạnh tam giác. - Nối các đỉnh và các trung điểm đối diện để tạo ra 3 đường trung tuyến. Kết quả như hình sau:   2. Vẽ tam giác với ba đường phân giác, tâm và vòng tròn nội tiếp - Sử dụng công cụ Đa giácgeogebrađể vẽ tam giác ABC. - Sử dụng công cụ Đường phân giác để vẽ 3 đường phân giác các góc của tam giác. - Xác định giao của 3 đường phân giác này bằng công cụ Điểm . Đổi tên điểm này là I. - Từ điểm I dùng công cụ Đường vuông gócgeogebrakẻ đường vuông góc với BC. Lấy giao điểm của đường vuông góc này với BC. - Sử dụng công cụ Đường tròn để vẽ vòng tròn tâm I đi qua điểm giao trên. - Làm ẩn đi 3 đường phân giác. Kết quả như hình dưới đây:   3. Vẽ tam giác với ba đường cao Nếu chúng ta sử dụng công cụ geogebrađể tạo ngay tam giác ABC sau đó kẻ các đường cao thì hình tuy đúng nhưng không chính xác và hình sẽ không dùng để minh họa được tam giác với 3 đường cao khi chúng ta cho các điểm A, B, C chuyển động tự do trên mặt phẳng. Cách vẽ chính xác phải như sau: - Sử dụng công cụ Đường thẳng geogebrađể vẽ tam giác ABC với các cạnh là 3 đường thẳng. - Sử dụng công cụ Đường vuông góc geogebrahạ từ đỉnh xuống các cạnh đối diện 3 đường vuông góc. - Lấy giao của chân các đường vuông góc và xác định trực tâm H. - Thay đổi kiểu của các đường thẳng có trên màn hình thành đường dạng -----. - Sử dụng công cụ Đa giácgeogebrađể vẽ lại tam giác ABC. - Sử dụng công cụ Đoạn thẳng geogebrađể vẽ lại các đường cao. Kết quả như hình dưới đây: Xem video thực hành bài luyện này:   Bài 11: Sử dụng thêm công cụ thể hiện điểm, góc và đoạn thẳng Bài học này sẽ hướng dẫn các GV thực hiện các thao tác sau: - Cách thiết lập và hiển thị các điểm. - Cách hiển thị góc. - Cách đánh dấu các đoạn thẳng. 1. Cách thiết lập và hiển thị các điểm.   2. Cách hiển thị góc.   3. Cách đánh dấu các đoạn thẳng.   Xem video phần thực hành của bài học:   Bài 12: Sử dụng các công cụ đại số để chia ba đoạn thẳng và góc Trong bài thực hành này chúng ta sẽ sử dụng thêm các công cụ đại số của phần mềm Geogebra để thực hiện việc chia 3 một đoạn thẳng và một góc cho trước. Các công cụ đại số này rất hữu ích trong rất nhiều trường hợp. Mục đích của bài thực hành sẽ làm 2 việc sau: 1. Cho trước một đoạn thẳng trên mặt phẳng. Hãy vẽ và xác định 2 điểm trên đoạn thằng này sao cho chúng chia 3 đoạn thẳng đã cho. 2. Cho trước một góc trên mặt phẳng. Hãy vẽ thêm 2 tia sao cho chia 3 góc đã cho. Xem video phần thực hành của bài học:   Bài 13: Vẽ 1 hình hoàn chỉnh: đường thẳng Simson Trong bài học này chúng ta sẽ thực hành vẽ một hình hoàn chỉnh: đường thẳng Simson. Bài toán đường thẳng Simson rất nổi tiếng như sau:   Cho trước tam giác ABC. Điểm D chuyển động tự do trên vòng tròn ngoại tiếp tam giác này. Khi đó chân của 3 đường vuông góc hạ từ D xuống 3 cạnh của tam giác ABC sẽ nằm trên một đường thẳng. Đó chính là đường thẳng Simson. Sau khi vẽ xong, chúng ta sẽ trình bày sao cho hình được thể hiện chính xác và nổi bật. Điểm D sẽ được tự động chuyển động trên đường tròn và chúng ta quan sát được sự chuyển động của đường thẳng Simson. Xem video phần thực hành của bài học:   Bài 14: Làm quen với các công cụ vẽ đường tròn Bài học này sẽ làm quen và thực hành với các công cụ vẽ liên quan đến đường tròn. Trong phần mềm Geogebra có 4 công cụ vẽ đường tròn, 1 công cụ vẽ nửa vòng tròn và 2 công cụ vẽ 1 cung tròn. Tất cả các công cụ này đều rất hữu ích. Xem video phần thực hành của bài học:   Bài 15: Làm quen với vẽ hình không gian trong Geogebra Trong bài học này chúng ta sẽ làm quen với các khái niệm ban đầu của hình học 3 chiều trong phần mềm Geogebra. Một số điểm cần chú ý: - Cách di chuyển các điểm trong không gian 3 chiều: theo chiều mặt ngang và chiều thẳng đứng. - Mặc định sẽ hiện 1 mặt phẳng chuẩn ngang. Mặt phẳng này không phải là 1 đối tượng của hình, tuy nhiên chúng ta có thể thực hiện các thao tác với nó tương tự như một đối tương. Xem video phần thực hành của bài học:   Bài 16: Phân biệt các đối tượng hình học trong các cửa sổ 2 chiều  và 3 chiều trong Geogebra Trong bài thực hành này chúng ta sẽ làm quen đồng thời với các đối tượng hình học 2 chiều và 3 chiều trong Geogebra. Chú ý rằng các đối tượng 2D và 3D là khác nhau trong phần mềm. Các đối tượng 3D nếu nằm trên mặt phẳng chuẩn thì có thể xuất hiện trong cửa sổ làm việc 2 chiều. ngược lại mọi đối tượng trong mặt phẳng 2D đều xuất hiện trên mặt phẳng chuẩn trong không gian 3 chiều. Xem video phần thực hành của bài học:   Bài 17: Làm việc với các đối tượng mặt phẳng trong không gian Trong bài thực hành này chúng ta sẽ làm quen với đối tượng mặt phẳng trong phần mềm Geogebra, quan hệ song song và vuông góc giữa mặt phẳng và mặt phẳng. Xem video phần thực hành của bài học:   Bài 18: Làm việc với các đối tượng đường tròn,  hình chóp và hình lăng trụ trong không gian Trong bài thực hành này chúng ta sẽ làm quen với các đối tượng tiếp theo: đường tròn, hình chóp và hình lăng trụ trong không gian. Trong Geogebra 3D có 3 công cụ tạo đường tròn. Và đây là các công cụ tạo hình chóng, hình lăng trụ, hình tứ diện đều và hình lập phương. Xem video phần thực hành của bài học:   Bài 19: Làm việc với hình nón và hình trụ trong Geogebra 5.0 Trong bài thực hành này chúng ta sẽ làm quen với các công cụ làm với với hình nón và hình trụ. Trong phần mềm Geogebra có 2 công cụ làm việc với hình nón và 2 công cụ làm việc với hình trụ. Xem video phần thực hành bài học:   Bài 20: Làm việc với công cụ hình cầu Trong bài thực hành này chúng ta sẽ làm quen với các công cụ làm với hình cầu. Trong phần mềm Geogebra có 2 công cụ làm việc với hình cầu. Hai công cụ này khá đơn giản. Với bài học này chúng ta đã kết thúc phần I: làm quen với các công cụ vẽ hình cơ bản của phần mềm Geogebra 5.0. Các chức năng nâng cao và các kỹ thuật vẽ hình khác sẽ được trình bày trong các bài tiếp theo. Xem video hướng dẫn thực hành:   Bài 21: Các thao tác nâng cao. Thực hành vẽ hình hộp chữ nhật Từ bài học này chúng ta sẽ bắt đầu thực hành các bài luyện nâng cao, đòi hỏi suy luận toán học nhiều hơn trong khi vẽ hình.  Chúng ta sẽ cùng nhau thực hành vẽ hình hộp chữ nhật trong không gian 3 chiều Xem video phần thực hành của bài học: ----------------------- HẾT ----------------------- Tài liệu liên quan Hướng dẫn soạn bài 31: Vai trò, đặc điểm của công nghiệp. Các nhân tố ảnh hưởng tới phát triển và phân bố công nghiệp Hướng dẫn soạn bài 32: Địa lí các ngành công nghiệp Hướng dẫn soạn bài 32: Địa lí các ngành công nghiệp (tiếp theo) Hướng dẫn soạn bài 33: Một số hình thức chủ yếu của tổ chức lãnh thổ công nghiệp Hướng dẫn soạn bài 34: Thực hành: Vẽ biểu đồ tình hình sản xuất một số sản phẩm công nghiệp trên thế giới Hướng dẫn soạn bài 35: Vai trò, các nhân tố ảnh hưởng và đặc điểm phân bố các ngành dịch vụ Hướng dẫn soạn bài 36: Vai trò, đặc điểm và các nhân tố ảnh hưởng đến phát triển, phân bố ngành giao thông vận tải Hướng dẫn soạn bài 37: Địa lí các ngành giao thông vận tải Hướng dẫn soạn bài 38: Thực hành: Viết báo cáo ngắn về kênh đào Xuy-ê và kênh đào Pa-na-ma Hướng dẫn soạn bài 39: Địa lí ngành thông tin liên lạc Hướng dẫn soạn bài 40: Địa lí ngành thương mại Hướng dẫn soạn bài 41: Môi trường và tài nguyên thiên nhiên Hướng dẫn soạn bài 42: Môi trường và sự phát triển bền vững Toán chuyên vào 10 hay Hướng dẫn soạn bài 1: Sự tương phản về trình độ phát triển kinh tế - xã hội của các nhóm nước. Cuộc cách mạng khoa học và công nghệ hiện đại Hướng dẫn soạn bài 2: Xu hướng toàn cầu hóa, khu vực hóa kinh tế Hướng dẫn soạn bài 3: Một số vấn đề mang tính chất toàn cầu Hướng dẫn soạn bài 4: Thực hành: Tìm hiểu những cơ hội và thách thức của toàn cầu hóa đối với các nước đang phát triển Hướng dẫn soạn bài 5: Một số vấn đề của châu lục và khu vực Hướng dẫn soạn bài 6: Hợp chủng quốc Hoa Kì Hướng dẫn soạn bài 7: Liên minh châu Âu (EU) Hướng dẫn soạn bài 8: Liên bang Nga Hướng dẫn soạn bài 9: Nhật Bản Hướng dẫn soạn bài 10: Cộng hòa nhân dân Trung Hoa (Trung Quốc) Hướng dẫn soạn bài 11: Khu vực Đông Nam Á Hướng dẫn soạn bài 12: Ô-xtrây-li-a Hướng dẫn soạn bài 1: Việt Nam trên đường đổi mới và hội nhập Hướng dẫn soạn bài 2: Vị trí địa lí, phạm vi lãnh thổ Hướng dẫn soạn bài 3: Thực hành: Vẽ lược đồ Việt Nam Hướng dẫn soạn bài 4: Lịch sử hình thành và phát triển lãnh thổ Hướng dẫn soạn bài 5: Lịch sử hình thành và phát triển lãnh thổ (tiếp theo) Hướng dẫn soạn bài 6: Đất nước nhiều đồi núi Hướng dẫn soạn bài 7: Đất nước nhiều đồi núi (tiếp theo) Hướng dẫn soạn bài 8: Thiên nhiên chịu ảnh hưởng sâu sắc của biển Hướng dẫn soạn bài 9: Thiên nhiên nhiệt đới ẩm gió mùa Hướng dẫn soạn bài 10: Thiên nhiên nhiệt đới ẩm gió mùa (tiếp theo) Hướng dẫn soạn bài 11: Thiên nhiên phân hóa đa dạng Hướng dẫn soạn bài 12: Thiên nhiên phân hóa đa dạng (tiếp theo) Hướng dẫn soạn bài 13: Thực hành: Đọc bản đồ địa hình, điền vào lược đồ trống một số dãy núi và đỉnh núi Hướng dẫn soạn bài 14: Sử dụng và bảo vệ tài nguyên thiên nhiên Tài liệu đọc nhiều Hướng dẫn sử dụng công cụ giải toán Wolframalpha 20 điều khiến cuộc sống của bạn thêm hoàn hảo Những lý do khiến bạn mãi kém cỏi và tầm thường Bài học quản lý từ con kiến Khoá học Bồi dưỡng môn Toán khối lớp 10 Khoá học Toán dành cho học sinh lớp 10 lên lớp 11 Khoá học Bồi dưỡng môn Toán lớp 9 và Luyện thi vào 10 Khoá học Bồi dưỡng môn Toán khối lớp 8 Khoá học Bồi dưỡng môn Toán khối lớp 7 Khoá học Bồi dưỡng môn Toán khối lớp 6 Khoá học luyện thi HSG dành cho học sinh giỏi THPT Khoá học luyện thi HSG dành cho học sinh giỏi THCS Khóa học Toán trắc nghiệm luyện thi THPT Quốc gia Nghèo đói là trường đại học lớn nhất Chiêm ngưỡng nhan sắc của 10 nữ thủ khoa 2014 Khóa học môn Toán luyện thi vào 10 chuyên Toán Tuyển thành viên cho CLB VYT Khóa học luyện thi cấp tốc vào Đại học FPT Thông tin liên hệ của LTTK  Phương pháp luyện thi Đại học tại trung tâm Copyright © 2017 by Chuyen Toan LTTK. Liên hệ Quảng cáo banner:  Nhấn vào đây",
          "url": "http://www.luyenthithukhoa.vn/index.php/blog-trung-tam/246-huong-dan-su-dung-cong-cu-ve-hinh-geogebra-5-0",
          "relevance": "0"
        },
        {
          "title": "Dịch thuật bằng các thuật toán",
          "content": "Cồ Việt - Tri thức không giới hạn  >  VIỆT HỌC  >  Tiếng Việt  >  Dịch thuật  >  Dịch thuật bằng các thuật toán PDA View Full Version :  Dịch thuật bằng các thuật toán ANBU 16-10-2013, 09:59 AM Dịch... được giải quyết bằng các thuật toán \n Theo nhóm các nhà nghiên cứu của Google, quá trình dịch thuật sẽ là  quá trình thực hiện thuật toán chuyển đổi giữa các không gian véctơ.  \n                          http://sgtt.vn/ImageHandler.ashx?ImageID=207822 (http://sgtt.vn/ImageHandler.ashx?ImageID=207822) \n Khoa học máy tính đang thay đổi bản chất của dịch  thuật, nhưng những người từng dùng Babel Fish hay Google Translate để  dịch đều biết rằng dù hữu ích, những công cụ này chưa hoàn hảo, nếu  không nói là còn nhiều khiếm khuyết.  \n Nhóm nghiên cứu của Google tìm một hướng đi mới giải  quyết vấn đề đó. “Rất đơn giản, để dịch từ ngôn ngữ này sang ngôn ngữ  khác, chúng tôi dùng sự biến đổi tuyến tính để có thể kết nối từ này  sang từ khác”, nhóm các nhà nghiên cứu của Google nói.  \n TS Lê Viết  Quốc, thành viên của nhóm giải thích thêm: “Trong dịch thuật có hai vấn  đề cần giải quyết. Thứ nhất là tạo ra bảng chuyển ngữ (translation  table) như “xinh đẹp” chuyển thành “beautiful”, “khoẻ mạnh” thành  “healthy”. Nhưng dịch thuật không đơn giản như vậy. Ví dụ “Tôi đi học ở  một ngôi trường rất xa” chuyển ngữ trực tiếp sẽ thành “I go to a school  very far” – tập hợp từ này không có nghĩa trong tiếng Anh. Vì vậy người  ta phải tìm cách chắp nối những từ này lại cho thành câu. Bước thứ hai  này được gọi là sử dụng mô hình ngôn ngữ (language modelling). Các  phương pháp dịch thuật nói chung cần giải quyết hai bước này thật tốt.  Thực tế đã có rất nhiều hướng nghiên cứu khác nhau. Đa phần đều muốn tìm  cách làm bước thứ hai cho thật tốt, và họ đều dùng phiên dịch chuyên  nghiệp – tức là người biết rành cả hai ngôn ngữ để tạo ra bảng chuyển  ngữ. Nghiên cứu của nhóm đã – lần đầu tiên – tạo ra translation table mà  không cần người dịch thuật”.  \n Theo đó, phương pháp tiếp cận mới tương đối đơn giản.  Nó dựa trên quan điểm cho rằng mọi ngôn ngữ phải được mô tả là một tập  hợp tương tự về ý nghĩa. Vì thế, từ ngữ mang cùng ý nghĩa trong các ngôn  ngữ khác nhau cũng phải có nét tương đồng. Thí dụ, hầu hết các ngôn ngữ  đều có từ chỉ những động vật phổ biến như “chó”, “mèo”, “bò”... và  những từ này có thể được dùng theo cách giống nhau trong các câu, thí dụ  “một con mèo là một động vật nhỏ hơn một con chó”.  \n                          http://sgtt.vn/ImageHandler.ashx?ImageID=207823 (http://sgtt.vn/ImageHandler.ashx?ImageID=207823) \n                          Với  một số từ cơ bản được chọn làm mốc, mỗi từ trong một ngôn ngữ nào đó có  mối tương quan được thể hiện ở vị trí nhất định trong không gian đa  chiều. Ở trên là hình chiếu biểu diễn một số con số từ không gian đa  chiều xuống không gian hai chiều. Hình ảnh này cho thấy véctơ đại diện  cho các con số từ 1 – 5 trong tiếng Anh và tiếng Tây Ban Nha giống nhau  như thế nào. \n   TS Quốc nói: “Nếu suy nghĩ kỹ về ngôn ngữ, ta sẽ nhận  thấy định nghĩa của nhiều từ là có tính tương đối, như “xinh đẹp” có thể  nghĩ là “không xấu”, “dễ nhìn”… Câu hỏi đặt ra là có cách tổng quát nào  để định nghĩa một từ bằng cách xác định mối tương quan của nó với các  từ khác? Khi ta tra từ điển, có một từ rất mới, ta sẽ định nghĩa nó một  cách tương đối so với những từ còn lại trong từ điển. Từ đó hình thành  giải thiết: nếu ta dùng một không gian mà ở đó có một số từ rất thông  dụng làm cột mốc thì những từ còn lại có thể định nghĩa theo cột mốc đó.  Một không gian như vậy trong toán học là không gian véctơ. Thực chất  đây chỉ là một phương pháp định nghĩa theo mặt toán học”.  \n Phương pháp của nhóm là biểu diễn toàn bộ một ngôn ngữ  dựa trên mối quan hệ giữa các từ của nó. Một tập hợp tất cả các mối quan  hệ tạo thành “không gian ngôn ngữ” có thể được biểu diễn bởi một tập  hợp các véctơ. Và trong nhiều năm gần đây, nhóm của TS Quốc đã phát hiện  rằng có thể vận dụng phép tính toán học cho những véctơ này. Thí dụ,  phép tính “vua” + “nữ giới” sẽ cho kết quả trên véctơ, có nghĩa tương tự  là “nữ hoàng”.  \n Nếu đã tìm ra được một “không gian ngôn ngữ” biểu diễn  bằng các véctơ thì quá trình chuyển đổi từ một ngôn ngữ này sang một  ngôn ngữ khác tương đương với việc chuyển đổi từ một không gian véctơ  này sang không gian véctơ khác. Như vậy, quá trình dịch thuật đã trở  thành quá trình được giải quyết bởi các thuật toán. Ở đây, nhóm nghiên  cứu của Google phải tìm một cách kết nối chính xác một không gian véctơ  vào một không gian véctơ khác. Họ đã dùng một từ điển song ngữ do con  người biên soạn – so sánh với bộ sưu tập các văn bản viết hoặc nói của  các từ trong hai ngôn ngữ khác nhau, mang lại một biến đổi tuyến tính có  sẵn để triển khai phương pháp này.  \n Nhóm nghiên cứu cho biết nó hoạt động khá tốt: “Mặc dù  đơn giản, phương pháp của chúng tôi có hiệu quả đáng ngạc nhiên, chúng  tôi có thể đạt được gần 90% độ chính xác đối với dịch các từ giữa tiếng  Anh và tiếng Tây Ban Nha”. \n Phương pháp này có thể được dùng để mở rộng và cải tiến  từ điển, thậm chí có thể phát hiện sai sót trong từ điển. Thực tế, nhóm  Google đã làm điều đó với một từ điển tiếng Anh – tiếng Czech, và tìm  ra rất nhiều lỗi trong đó.  \n Nhóm nghiên cứu cũng chỉ ra rằng, có thể áp dụng kỹ  thuật này với các ngôn ngữ hoàn toàn không có sự liên quan. Thí dụ, dù  được xây dựng trên hai ngôn ngữ có nhiều sự tương đồng là tiếng Anh và  tiếng Tây Ban Nha nhưng nghiên cứu cho thấy kỹ thuật mới cũng hoạt động  tốt đối với các cặp ngôn ngữ không có nhiều liên quan, như tiếng Anh và  tiếng Việt. \n(http://sgtt.vn/Khoa-giao/184127/Dich-bang%E2%80%A6-toan.html) obaasan 16-10-2013, 10:10 AM ANBU tìm được bài viết này hay quá.  Cảm ơn nhiều.",
          "url": "http://www.coviet.vn/diendan/archive/index.php?t-43063.html&s=d18bed60ff6a9ab98f0ebc4d1df72c1e",
          "relevance": "0"
        },
        {
          "title": "Cấu trúc cơ sở dữ liệu trong GIS",
          "content": "Project name Trang chủ Tra cứu Tài liệu Đóng góp Giới thiệu Đăng ký Đăng nhập Đăng nhập \n                      Ghi nhớ\n                   Quên mật khẩu? Đăng nhập Bạn chưa có tài khoản? Hãy đăng ký. Tên đăng nhập hoặc mật khẩu chưa đúng Tài liệu Cấu trúc cơ sở dữ liệu trong GIS 0 Một cơ sở dữ liệu của hệ thống thông tin địa lý có thể chia ra làm 2 loại số liệu cơ bản: số liệu không gian và phi không gian. Mỗi loại có những đặc điểm riêng và chúng khác nhau về yêu cầu lưu giữ số liệu, hiệu quả, xử lý và hiển thị.  Số liệu không gian là những mô tả số của hình ảnh bản đồ, chúng bao gồm toạ độ, quy luật và các ký hiệu dùng để xác định một hình ảnh bản đồ cụ thể trên từng bản đồ. Hệ thống thông tin địa lý dùng các số liệu không gian để tạo ra một bản đồ hay hình ảnh bản đồ trên màn hình hoặc trên giấy thông qua thiết bị ngoại vi, …  Số liệu phi không gian là những diễn tả đặc tính, số lượng, mối quan hệ của các hình ảnh bản đồ với vị trí địa lý của chúng. Các số liệu phi không gian được gọi là dữ liệu thuộc tính, chúng liên quan đến vị trí địa lý hoặc các đối tượng không gian và liên kết chặt chẽ với chúng trong hệ thống thông tin địa lý thông qua một cơ chế thống nhất chung. 4.1. MÔ HÌNH THÔNG TIN KHÔNG GIAN  Dữ liệu là trung tâm của hệ thống GIS, hệ thống GIS chứa càng nhiều thì chúng càng có ý nghĩa. Dữ liệu của hệ GIS được lưu trữ trong CSDL và chúng được thu thập thông qua các mô hình thế giới thực. Dữ liệu trong hệ GIS còn được gọi là thông tin không gian. Đặc trưng thông tin không gian là có khả năng mô tả “vật thể ở đâu” nhờ vị trí tham chiếu, đơn vị đo và quan hệ không gian. Chúng còn khả năng mô tả “hình dạng hiện tượng” thông qua mô tả chất lượng, số lượng của hình dạng và cấu trúc. Cuối cùng, đặc trưng thông tin không gian mô tả “quan hệ và tương tác” giữa các hiện tượng tự nhiên. Mô hình không gian đặc biệt quan trọng vì cách thức thông tin sẽ ảnh hưởng đến khả năng thực hiện phân tích dữ liệu và khả năng hiển thị đồ hoạ của hệ thống. 4.1.1.Hệ thống Vector  4.1.1.1 Kiểu đối tượng điểm (Points) Điểm được xác định bởi cặp giá trị đ. Các đối tượng đơn, thông tin về địa lý chỉ gồm cơ sở vị trí sẽ được phản ánh là đối tượng điểm. Các đối tượng kiểu điểm có đặc điểm: Là toạ độ đơn (x,y) Không cần thể hiện chiều dài và diện tích  Hình 4.1: Số liệu vector được biểu thị dưới dạng điểm (Point). Tỷ lệ trên bản đồ tỷ lệ lớn, đối tượng thể hiện dưới dạng vùng. Tuy nhiên trên bản đồ tỷ lệ nhỏ, đối tượng này có thể thể hiện dưới dạng một điểm. Vì vậy, các đối tượng điểm và vùng có thể được dùng phản ánh lẫn nhau. 4.1.1.2. Kiểu đối tượng đường (Arcs) Đường được xác định như một tập hợp dãy của các điểm. Mô tả các đối tượng địa lý dạng tuyến, có các đặc điểm sau: Là một dãy các cặp toạ độ  Một arc bắt đầu và kết thúc bởi node Các arc nối với nhau và cắt nhau tại node Hình dạng của arc được định nghĩa bởi các điểm vertices Độ dài chính xác bằng các cặp toạ độ  Hình 4.2: Số liệu vector được biểu thị dưới dạng Arc 4.1.1.3. Kiểu đối tượng vùng (Polygons) Vùng được xác định bởi ranh giới các đường thẳng. Các đối tượng địa lý có diện tích và đóng kín bởi một đường được gọi là đối tượng vùng polygons, có các đặc điểm sau: Polygons được mô tả bằng tập các đường (arcs) và điểm nhãn (label points)  Một hoặc nhiều arc định nghĩa đường bao của vùng  Một điểm nhãn label points nằm trong vùng để mô tả, xác định cho mỗi một vùng. Hình 4.3: Số liệu vector được biểu thị dưới dạng vùng (Polygon) Hình 4.4: Một số khái niệm trong cấu trúc cơ sở dữ liệu bản đồ.  4.2.2. Hệ thống Raster Mô hình dữ liệu dạng raster phản ánh toàn bộ vùng nghiên cứu dưới dạng một lưới các ô vuông hay điểm ảnh (pixcel). Mô hình raster có các đặc điểm: Các điểm được xếp liên tiếp từ trái qua phải và từ trên xuống dưới. Mỗi một điểm ảnh (pixcel) chứa một giá trị. Một tập các ma trận điểm và các giá trị tương ứng tạo thành một lớp (layer). Trong cơ sở dữ liệu có thể có nhiều lớp. Mô hình dữ liệu raster là mô hình dữ liệu GIS được dùng tương đối phổ biến trong các bài toán về môi trường, quản lý tài nguyên thiên nhiên. Mô hình dữ liệu raster chủ yếu dùng để phản ánh các đối tượng dạng vùng là ứng dụng cho các bài toán tiến hành trên các loại đối tượng dạng vùng: phân loại; chồng xếp. Các nguồn dữ liệu xây dựng nên dữ liệu raster có thể bao gồm: Quét ảnh  Ảnh máy bay, ảnh viễn thám  Chuyển từ dữ liệu vector sang  Lưu trữ dữ liệu dạng raster. Nén theo hàng (Run lengh coding). Nén theo chia nhỏ thành từng phần (Quadtree). Nén theo ngữ cảnh (Fractal).  Trong một hệ thống dữ liệu cơ bản raster được lưu trữ trong các ô (thường hình vuông) được sắp xếp trong một mảng hoặc các dãy hàng và cột. Nếu có thể, các hàng và cột nên được căn cứ vào hệ thống lưới bản đổ thích hợp.  Việc sử dụng cấu trúc dữ liệu raster tất nhiên đưa đến một số chi tiết bị mất. Với lý do này, hệ thống raster-based không được sử dụng trong các trường hợp nơi có các chi tiết có chất lượng cao được đòi hỏi.  Hình 4.5: Sự biểu thị kết quả bản đồ dưới dạng Raster 4.2.3. Chuyển đổi cơ sở dữ liệu dạng vector và raster  Việc chọn của cấu trúc dử liệu dưới dạng vector hoặc raster tuỳ thuộc vào yêu cầu của người sử dụng, đối với hệ thống vector, thì dữ liệu được lưu trữ sẽ chiếm diện tích nhỏ hơn rất nhiều so với hệ thống raster, đồng thời các đường contour sẽ chính xác hơn hệ thống raster. Ngoài ra cũng tuỳ vào phần mềm máy tính đang sử dụng mà nó cho phép nên lưu trữ dữ liệu dưới dạng vector hay raster. Tuy nhiên đối với việc sử dụng ảnh vệ tinh trong GIS thì nhất thiết phải sử dụng dưới dạng raster.   Một số công cụ phân tích của GIS phụ thuộc chặt chẽ vào mô hình dữ liệu raster, do vậy nó đòi hỏi quá trình biến đổi mô hình dữ liệu vector sang dữ liệu raster, hay còn gọi là raster hoá. Biến đổi từ raster sang mô hình vector, hay còn gọi là vector hoá, đặc biệt cần thiết khi tự động quét ảnh. Raster hoá là tiến trình chia đường hay vùng thành các ô vuông (pixcel). Ngược lại, vector hoá là tập hợp các pixcel để tạo thành đường hay vùng. Nết dữ liệu raster không có cấu trúc tốt, thí dụ ảnh vệ tinh thì việc nhận dạng đối tượng sẽ rất phức tạp.  Nhiệm vụ biến đổi vector sang raster là tìm tập hợp các pixel trong không gian raster trùng khớp với vị trí của điểm, đường, đường cong hay đa giác trong biểu diễn vector. Tổng quát, tiến trình biến đổi là tiến trình xấp xỉ vì với vùng không gian cho trước thì mô hình raster sẽ chỉ có khả năng địa chỉ hoá các vị trí toạ độ nguyên. Trong mô hình vector, độ chính xác của điểm cuối vector được giới hạn bởi mật độ hệ thống toạ độ bản đồ còn vị trí khác của đoạn thẳng được xác định bởi hàm toán học. Hình 4.6: Sự chuyển đổi dữ liệu giữa raster và vector (Nguồn : Tor Bernhardsen, 1992) 4.2.4. Thuận lợi và bất lợi của hệ thống dữ liệu raster và vector 4.2.4.1. Thuận lợi của hệ thống cơ sở dữ liệu raster  Vị trí địa lý của mỗi ô được xác định bởi vị trí của nó trong ô biểu tượng, hình ảnh có thể được lưu trữ trong một mảng tương xứng trong máy vi tính cung cấp đủ dữ liệu bất kỳ lúc nào. Vì vậy mỗi ô có thể nhanh chóng và dễ dàng được định địa chỉ trong máy theo vị trí địa lý của nó. Những vị trí kế cận được hiện diện bởi các ô kế cận, vì vậy mối liên hệ giữa các ô có thể được phân tích một cách thuận tiện Quá trình tính toán đơn giản hơn và dễ dàng hơn cơ sở hệ thống dữ liệu vector. Đơn vị bản đồ ranh giới thửa được trình bày một cách tự nhiên bởi giá trị ô khác nhau, khi giá trị thay đổi, việc chỉ định ranh giới thay đổi. 4.2.4.2. Bất lợi của hệ thống dữ liệu raster Khả năng lưu trữ đòi hỏi lớn hơn nhiều so với hệ thống cơ sở dữ liệu vector. Kích thước ô định rõ sự quyết định ở phương pháp đại diện ở phương pháp đại diện. Điều này đặc biệt khó dễ cân xứng với sự hiện diện đặc tính thuộc về đường thẳng.  Thường hầu như hình ảnh gần thì nối tiếp nhau, điều này có nghĩa là nó phải tiến hành một bản đồ hoàn chỉnh chính xác để thay đổi 1 ô đơn. Quá trình tiến hành của dữ liệu về kết hợp thì choáng nhiều chỗ hơn với 1 hệ thống cơ sở vector.  Dữ liệu được đưa vào hầu như được số hoá trong hình thức vector, vì thế nó phải chính xác 1 vector đến sự thay đổi hoạt động raster để đổi dữ liệu hệ số hoá vào trong hình thức lưu trữ thích hợp.  Điều này thì khó hơn việc xây dựng vào trong bản đồ từ dữ liệu raster. 4.2.4.3. Thuận lợi của hệ thống cơ sở vector Việc lưu trữ được đòi hỏi ít hơn hệ thống cơ sở dữ liệu raster Bản đồ gốc có thể được hiện diện ở sự phân giải gốc của nó. Đặc tính phương pháp như là các kiểu từng, đường sá, sông suối, đất đai có thể được khôi phục lại và tiến triển 1 cách đặc biệt. Điều này dễ hơn để kết hợp trạng thái khác nhau của phương pháp mô tả dữ liệu với 1 đặc tính phương pháp đơn. Hệ số hoá các bản đổ không cần được khôi phục lại từ hình thức raster. Dữ liệu lưu trữ có thể được tiến triển trong bản đồ kiểu dạng đường thẳng mà không 1 raster để sự khôi phục vector. 4.2.4.4. Bất lợi của hệ thống cơ sở dữ liệu vector Vị trí của điểm đỉnh cần được lưu trữ 1 cách rõ ràng  Mối quan hệ của những điểm này phải được định dạng trong 1 cấu trúc thuộc về địa hình học, mà nó có lẽ khó để hiểu và điều khiển. Thuật toán cho việc hoàn thành chức năng thì hoàn toàn tương đương trong hệ thống cơ sở dữ liệu raster là quá phức tạp và việc hoàn thành có lẽ là không xác thực. Sự thay đổi 1 cách liên tiếp dữ liệu thuộc về không gian không thể được hiện diện như raster. 1 sự khôi phục để raster được yêu cầu tiến hành dữ liệu kiểu này. 4.3. MÔ HÌNH THÔNG TIN THUỘC TÍNH   Số liệu phi không gian hay còn gọi là thuộc tính là những mô tả về đặc tính, đặc điểm và các hiện tượng xảy ra tại các vị trí địa lý xác định. Một trong các chức năng đặc biệt của công nghệ GIS là khả năng của nó trong việc liên kết và xử lý đồng thời giữa dữ liệu bản đồ và dữ liệu thuộc tính. Thông thường hệ thống thông tin địa lý có 4 loại số liệu thuộc tính:  - Đặc tính của đối tượng: liên kết chặt chẽ với các thông tin không gian có thể thực hiện SQL (Structure Query Language) và phân tích  - Số liệu hiện tượng, tham khảo địa lý: miêu tả những thông tin, các hoạt động thuộc vị trí xác định.  - Chỉ số địa lý: tên, địa chỉ, khối, phương hướng định vị, …liên quan đến các đối tượng địa lý.  - Quan hệ giữa các đối tượng trong không gian, có thể đơn giản hoặc phức tạp (sự liên kết, khoảng tương thích, mối quan hệ đồ hình giữa các đối tượng). Để mô tả một cách đầy đủ các đối tượng địa lý, trong bản đồ số chỉ dùng thêm các loại đối tượng khác: điểm điều khiển, toạ độ giới hạn và các thông tin mang tính chất mô tả (annotation). Annotation:  Các thông tin mô tả có các đặc điểm: Có thể nằm tại một vị trí xác định trên bản đồ  Có thể chạy dọc theo arc Có thể có các kích thước, màu sắc, các kiểu chữ khác nhau  Nhiều mức của thông tin mô tả có thể được tạo ra với ứng dụng khác nhau. Có thể tạo thông tin cơ sở dữ liệu lưu trữ thuộc tính  Có thể tạo độc lập với các đối tượng địa lý ïcó trong bản đồ  Không có liên kết với các đối tượng điểm, đường, vùng và dữ liệu thuộc tính của chúng  Bản chất một số thông tin dữ liệu thuộc tính như sau: - Số liệu tham khảo địa lý: mô tả các sự kiện hoặc hiện tượng xảy ra tại một vị trí xác định. Không giống các thông tin thuộc tính khác, chúng không mô tả về bản thân các hình ảnh bản đồ. Thay vào đó chúng mô tả các danh mục hoặc các hoạt động như cho phép xây dựng, báo cáo tai nạn, nghiên cứu y tế, … liên quan đến các vị trí địa lý xác định. Các thông tin tham khảo địa lý đặc trưng được lưu trữ và quản lý trong các file độc lập và hệ thống không thể trực tiếp tổng hợp chúng với các hình ảnh bản đồ trong cơ sở dữ liệu của hệ thống. Tuy nhiên các bản ghi này chứa các yếu tố xác định vị trí của sự kiện hay hiện tượng. - Chỉ số địa lý: được lưu trong hệ thống thông tin địa lý để chọn, liên kết và tra cứu số liệu trên cơ sở vị trí địa lý mà chúng đã được mô tả bằng các chỉ số địa lý xác định. Một chỉ số có thể bao gồm nhiều bộ xác định cho các thực thể địa lý sử dụng từ các cơ quan khác nhau như là lập danh sách các mã địa lý mà chúng xác định mối quan hệ không gian giữa các vị trí hoặc giữa các hình ảnh hay thực thể địa lý. Ví dụ: chỉ số địa lý về đường phố và địa chỉ địa lý liên quan đến phố đó. - Mối quan hệ không gian: của các thực thể tại vị trí địa lý cụ thể rất quan trọng cho các chức năng xử lý của hệ thống thông tin địa lý. Các mối quan hệ không gian có thể là mối quan hệ đơn giản hay lôgic, ví dụ tiếp theo số nhà 101 phải là số nhà 103 nếu là số nhà bên lẻ hoặc nếu là bên chẵn thì cả hai đều phải là các số chẵn kề nhau. Quan hệ Topology cũng là một quan hệ không gian. Các quan hệ không gian có thể được mã hoá như các thông tin thuộc tính hoặc ứng dụng thông qua giá trị toạ độ của các thực thể. - Mối quan hệ giữa dữ liệu không gian và phi không gian: thể hiện phương pháp chung để liên kết hai loại dữ liệu đó thông qua bộ xác định, lưu trữ đồng thời trong các thành phần không gian và phi không gian. Các bộ xác định có thể đơn giản là một số duy nhất liên tục, ngẫu nhiên hoặc các chỉ báo địa lý hay số liệu xác định vị trí lưu trữ chung. Bộ xác định cho một thực thể có thể chứa toạ độ phân bố của nó, số hiệu mảnh bản đồ, mô tả khu vực hoặc con trỏ đến vị trí lưu trữ của số liệu liên quan. Bộ xác định được lưu trữ cùng với các bản ghi toạ độ hoặc mô tả số khác của các hình ảnh không gian và cùng với các bản ghi số liệu thuộc tính liên quan.  Sự liên kết giữa hai loại thông tin cơ bản trong cơ sở dữ liệu GIS thể hiện theo sơ đồ sau: ID (mã) Tính chất 1 Tính chất 2 Tính chất 3 1 x x x 2 x x x 3 x x x … … … … 1243\n     Hình 4.7: Mối quan hệ giữa thông tin bản đồ và thông tin thuộc tính (Nguồn : Nguyễn Thế Thận, Trần Công Yên, 2000) 0  Tải về   Tái sử dụng  Tài liệu PDF Tài liệu EPUB unknown 0 Giáo trình\n                  | \n                 1060 Tài liệu\n                 Đánh giá: 0  dựa trên\n         0  đánh giá\n\n        \n     Nội dung cùng tác giả   Nội dung tương tự   × VOER message Thư viện Học liệu Mở Việt Nam (VOER) được tài trợ bởi  Vietnam Foundation  và vận hành trên nền tảng  Hanoi Spring . Các tài liệu đều tuân thủ giấy phép Creative Commons Attribution 3.0 trừ khi ghi chú rõ ngoại lệ.",
          "url": "https://voer.edu.vn/m/cau-truc-co-so-du-lieu-trong-gis/09b7db8b",
          "relevance": "0"
        },
        {
          "title": "Dịch bằng… toán",
          "content": "Trang chủ Tạp chí STINFO Thư viện KH&CN Cổng giao dịch công nghệ  Dịch vụ Hỏi - đáp Thống kê KH&CN Liên hệ Tạp chí MIT Technology Review – Hoa Kỳ vừa có bài giới thiệu nghiên cứu của nhóm các nhà khoa học thuộc Google gồm: TS Tomas Mikolov (Cộng hoà Czech), TS Lê Viết Quốc (Việt Nam) và Ilya Sutskever (Canada). Ý tưởng chính của nghiên cứu này là thiết lập và biểu diễn bản đồ ngôn ngữ bằng một không gian véctơ. Khi đó, việc dịch chỉ là thực hiện thuật toán chuyển đổi giữa các không gian véctơ. \r\nHai bước chuyển ngữ \r\nKhoa học máy tính đang thay đổi bản chất của dịch thuật, nhưng những người từng dùng Babel Fish hay Google Translate để dịch đều biết rằng dù hữu ích, những công cụ này chưa hoàn hảo, nếu không nói là còn nhiều khiếm khuyết. \r\nNhóm nghiên cứu của Google tìm một hướng đi mới giải quyết vấn đề đó. “Rất đơn giản, để dịch từ ngôn ngữ này sang ngôn ngữ khác, chúng tôi dùng sự biến đổi tuyến tính để có thể kết nối từ này sang từ khác”, nhóm các nhà nghiên cứu của Google nói. \r\nTrao đổi với phóng viên Sài Gòn Tiếp Thị, TS Lê Viết Quốc, thành viên của nhóm giải thích thêm: “Trong dịch thuật có hai vấn đề cần giải quyết. Thứ nhất là tạo ra bảng chuyển ngữ (translation table) như “xinh đẹp” chuyển thành “beautiful”, “khoẻ mạnh” thành “healthy”. Nhưng dịch thuật không đơn giản như vậy. Ví dụ “Tôi đi học ở một ngôi trường rất xa” chuyển ngữ trực tiếp sẽ thành “I go to a school very far” – tập hợp từ này không có nghĩa trong tiếng Anh. Vì vậy người ta phải tìm cách chắp nối những từ này lại cho thành câu. Bước thứ hai này được gọi là sử dụng mô hình ngôn ngữ (language modelling). Các phương pháp dịch thuật nói chung cần giải quyết hai bước này thật tốt. Thực tế đã có rất nhiều hướng nghiên cứu khác nhau. Đa phần đều muốn tìm cách làm bước thứ hai cho thật tốt, và họ đều dùng phiên dịch chuyên nghiệp – tức là người biết rành cả hai ngôn ngữ để tạo ra bảng chuyển ngữ. Nghiên cứu của nhóm đã – lần đầu tiên – tạo ra translation table mà không cần người dịch thuật”. \r\nTheo đó, phương pháp tiếp cận mới tương đối đơn giản. Nó dựa trên quan điểm cho rằng mọi ngôn ngữ phải được mô tả là một tập hợp tương tự về ý nghĩa. Vì thế, từ ngữ mang cùng ý nghĩa trong các ngôn ngữ khác nhau cũng phải có nét tương đồng. Thí dụ, hầu hết các ngôn ngữ đều có từ chỉ những động vật phổ biến như “chó”, “mèo”, “bò”... và những từ này có thể được dùng theo cách giống nhau trong các câu, thí dụ “một con mèo là một động vật nhỏ hơn một con chó”. gy Review – Hoa Kỳ vừa có bài giới thiệu nghiên cứu của nhóm các nhà khoa học thuộc Google gồm: TS Tomas Mikolov (Cộng hoà Czech), TS Lê Viết Quốc (Việt Nam) và Ilya Sutskever (Canada). Ý tưởng chính của nghiên cứu này là thiết lập và biểu diễn bản đồ ngôn ngữ bằng một không gian véctơ. Khi đó, việc dịch chỉ là thực hiện thuật toán chuyển đổi giữa các không gian véctơ. Với một số từ cơ bản được chọn làm mốc, mỗi từ trong một ngôn ngữ nào đó có mối tương quan được thể hiện ở vị trí nhất định \r\ntrong không gian đa chiều. Ở trên là hình chiếu biểu diễn một số con số từ không gian đa chiều xuống không gian hai chiều.  \r\nHình ảnh này cho thấy véctơ đại diện cho các con số từ 1 – 5 trong tiếng Anh và tiếng Tây Ban Nha giống nhau như thế nào.   Một không gian tập hợp \r\nTS Quốc nói: “Nếu suy nghĩ kỹ về ngôn ngữ, ta sẽ nhận thấy định nghĩa của nhiều từ là có tính tương đối, như “xinh đẹp” có thể nghĩ là “không xấu”, “dễ nhìn”… Câu hỏi đặt ra là có cách tổng quát nào để định nghĩa một từ bằng cách xác định mối tương quan của nó với các từ khác? Khi ta tra từ điển, có một từ rất mới, ta sẽ định nghĩa nó một cách tương đối so với những từ còn lại trong từ điển. Từ đó hình thành giải thiết: nếu ta dùng một không gian mà ở đó có một số từ rất thông dụng làm cột mốc thì những từ còn lại có thể định nghĩa theo cột mốc đó. Một không gian như vậy trong toán học là không gian véctơ. Thực chất đây chỉ là một phương pháp định nghĩa theo mặt toán học”. \r\nPhương pháp của nhóm là biểu diễn toàn bộ một ngôn ngữ dựa trên mối quan hệ giữa các từ của nó. Một tập hợp tất cả các mối quan hệ tạo thành “không gian ngôn ngữ” có thể được biểu diễn bởi một tập hợp các véctơ. Và trong nhiều năm gần đây, nhóm của TS Quốc đã phát hiện rằng có thể vận dụng phép tính toán học cho những véctơ này. Thí dụ, phép tính “vua” + “nữ giới” sẽ cho kết quả trên véctơ, có nghĩa tương tự là “nữ hoàng”. \r\nNếu đã tìm ra được một “không gian ngôn ngữ” biểu diễn bằng các véctơ thì quá trình chuyển đổi từ một ngôn ngữ này sang một ngôn ngữ khác tương đương với việc chuyển đổi từ một không gian véctơ này sang không gian véctơ khác. Như vậy, quá trình dịch thuật đã trở thành quá trình được giải quyết bởi các thuật toán. Ở đây, nhóm nghiên cứu của Google phải tìm một cách kết nối chính xác một không gian véctơ vào một không gian véctơ khác. Họ đã dùng một từ điển song ngữ do con người biên soạn – so sánh với bộ sưu tập các văn bản viết hoặc nói của các từ trong hai ngôn ngữ khác nhau, mang lại một biến đổi tuyến tính có sẵn để triển khai phương pháp này. \r\nNhóm nghiên cứu cho biết nó hoạt động khá tốt: “Mặc dù đơn giản, phương pháp của chúng tôi có hiệu quả đáng ngạc nhiên, chúng tôi có thể đạt được gần 90% độ chính xác đối với dịch các từ giữa tiếng Anh và tiếng Tây Ban Nha”. \r\nPhương pháp này có thể được dùng để mở rộng và cải tiến từ điển, thậm chí có thể phát hiện sai sót trong từ điển. Thực tế, nhóm Google đã làm điều đó với một từ điển tiếng Anh – tiếng Czech, và tìm ra rất nhiều lỗi trong đó. \r\nNhóm nghiên cứu cũng chỉ ra rằng, có thể áp dụng kỹ thuật này với các ngôn ngữ hoàn toàn không có sự liên quan. Thí dụ, dù được xây dựng trên hai ngôn ngữ có nhiều sự tương đồng là tiếng Anh và tiếng Tây Ban Nha nhưng nghiên cứu cho thấy kỹ thuật mới cũng hoạt động tốt đối với các cặp ngôn ngữ không có nhiều liên quan, như tiếng Anh và tiếng Việt. \r\nTS Quốc cho biết thêm: “Việc nghiên cứu đã được tiến hành gần một năm nay và có kết quả từ cách đây hai tháng. Với nghiên cứu này chưa thể gọi là hoàn thành, nhưng về kết quả thực nghiệm thì như thế cũng là khá đầy đủ”. Tạp chí MIT Technology Review cũng nhận định “đây là một bước tiến hữu ích cho tương lai của truyền thông đa ngôn ngữ”. \r\nTuy nhiên, “đây mới chỉ là bước khởi đầu, rõ ràng, vẫn có nhiều thứ cần được khám phá”, TS Tomas Mikolov, đại diện nhóm nghiên cứu nói. Nguồn: Theo SGTT   Các tin khác: Nhà khoa học châu Phi đoạt giải Hội Hoàng gia Anh cho nghiên cứu sốt rét  -  (08/11/2013 16:07) Công bố giải Dược phẩm Nano đầu tiên tại BIO-Europe 2013  -  (05/11/2013 14:45) Đường trong máu cao làm giảm trí nhớ  -  (05/11/2013 09:30) Phát hiện hành tinh ngoài Thái Dương hệ giống trái đất nhất  -  (05/11/2013 09:18) Laser có thể chữa bệnh não như Alzheimer và Parkinson  -  (04/11/2013 14:34) Kính thông minh giúp người khiếm thị thấy được  -  (01/11/2013 09:45) Ảnh hưởng của chất resveratrol trên gen clock của chuột  -  (30/10/2013 10:38) “Trứng” trong tổ trình diễn vi thế giới  -  (25/10/2013 11:24) Sốc khí hậu gây nên tình trạng chết dần của nai sừng tấm?  -  (23/10/2013 17:14) Lịch sử tiến hóa của loài người 'có thể được viết lại'  -  (22/10/2013 08:48) Trang kế >> Tin tức Sự kiện KH&CN KH&CN trong nước KH&CN quốc tế Thư viện Tài liệu mới CSDL tiếng Việt CSDL tiếng Anh Hình thức phục vụ Hoạt động thư viện Thông tin nhiệm vụ KH&CN Nhiệm vụ KH&CN đang tiến hành Kết quả thực hiện nhiệm vụ KH&CN Ứng dụng kết quả thực hiện nhiệm vụ KH&CN STINFO số 6 / 2017 \n\n       ﻿ Trang chủ  |\n Thông tin về CESTI  |\n Liên hệ  |\n © Mạng Thông tin Khoa học và Công nghệ Thành phố Hồ Chí Minh; Giấy phép số 168/GP-BVHTT, ngày 28/05/1999 do Bộ VHTT cấp. \nCơ quan quản lý mạng: Trung tâm Thông tin và Thống kê Khoa học và Công nghệ TP.HCM - CESTI \nĐịa chỉ: 79 Trương Định, Quận 1, TP.HCM; Tel: 84-28-38297040 (84-28-3256320) - Fax: 84-28-38291957; E-mail: webadmin@cesti.gov.vn; Website:  www.cesti.gov.vn \nGhi rõ nguồn www.cesti.gov.vn khi bạn sử dụng lại thông tin từ website này",
          "url": "http://www.cesti.vn/kh-cn-quoc-te/dich-bang-toan.html",
          "relevance": "0"
        },
        {
          "title": "Truy vấn văn bản – Document Retrieval",
          "content": "Ông Xuân Hồng Chia sẻ kiến thức và thông tin về Machine learning Menu Skip to content Data Science Big Data Deep learning Exploratory Data Analysis Getting and cleaning data Machine learning Lập trình Python Spark R Weka Kiến thức Toán Statistical Inference Xử lý ngôn ngữ tự nhiên – Natural Language Processing (NLP) Chia sẻ Dự án About Truy vấn văn bản – Document Retrieval Tháng Một 16, 2017 Tháng Tám 3, 2017 Ông Xuân Hồng 18 phản hồi cosine-similarity logarithm-graph Facebook Twitter Google LinkedIn Like this: Số lượt thích Đang tải... Liên quan Điều hướng bài viết ←  Hệ thống recommend bài nhạc Hướng dẫn deploy Spark  → 11NLTK nói: \n\t\t\t\t\t\t\t\tTháng Một 26, 2017 lúc 7:55 sáng\t\t\t\t\t\t\t Chao ban. Dau tien rat cam on vi nhung kien thuc chia se tren blog nay. Minh hoc duoc rat nhieu dieu. Minh co 1 cau hoi lien quan cu the den bai viet nay. Trong phan mao dau cua muc Topic Modeling, ban co de cap den Categorizer va Clusterizer. Nhung o phan sau ban chi phat trien cac vi du cho classification. Neu minh muon tim hieu ve Topic Clustering thi ban co nguon tham khao nao huu ich khong. Cam on nhieu. Than ai. Số lượt thích Số lượt thích Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Một 26, 2017 lúc 8:17 sáng\t\t\t\t\t\t\t Chào bạn, Cám ơn bạn đã quan tâm tới blog. \nVề phân loại văn bản nói riêng cũng như machine learning nói chung, về cơ bản ta có 2 hướng tiếp cận thường dùng đó là supervised (học từ dữ liệu được gán nhãn trước) và un-supervised (học từ dữ liệu không được gán nhãn trước) learning. Trong trường hợp này là categorizer và clusterizer. Categorizer được xếp vào bài toán muticlassification. Mình đề cập đến clusterizer liên quan đến phương pháp topic modeling. Ta sẽ cho máy tự gom nhóm các chủ đề theo thống kê từ vựng. Từ đó, ta quan sát và tự đặt tên lại cho các chủ đề vừa khai phá được. \nBạn có thể tìm thấy các nguồn tham khảo ở bài viết này https://ongxuanhong.wordpress.com/2016/09/24/topic-modeling-la-gi/ Số lượt thích Liked by  2 people Phản hồi 11NLTK nói: \n\t\t\t\t\t\t\t\tTháng Một 26, 2017 lúc 10:07 sáng\t\t\t\t\t\t\t Cam on ban. Minh da doc qua bai viet nay truoc day nhung theo minh hieu o day la phan loai tu vung theo nhom chu khong hoan toan la phan loai chu de cua van ban, nhat la khi chieu dai cua cac van ban ngan (short sentences). Bai toan cua minh dai loai the nay: minh co cac ghi chu hien truong (Site observation) thuong rat ngan gon (1 den 2 cau rat ngan); muc tieu la phai phan loai cac ghi chu ay theo tung chu de voi so luong va noi dung cac chu de chua biet. Theo ban nen tiep can bai toan nay theo huong nao ? Than ai. Số lượt thích Số lượt thích Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Một 26, 2017 lúc 10:20 sáng\t\t\t\t\t\t\t Mình nghĩ bạn nên thực nghiệm để quyết định được số lượng chủ đề cần phân loại. Dù là 1,2 câu ngắn nhưng nguyên lý vẫn không thay đổi vì từ vựng phản ánh chủ đề đang diễn đạt. Ta đề cập đến chính trị thì sẽ có các từ vựng tương ứng, tương tự nếu ta đề cập đến thời trang thì trong câu chắc chắn xuất hiện các từ vựng liên quan. \nSau khi clustering ta vẫn chưa biết tên chủ đề là gì, nhờ vào từ vựng đã gom nhóm được mà ta đặt tên cho chủ đề. Từ đó, khi bạn nhập vào một câu mới có chứa các từ vựng liên quan, hệ thống sẽ tự động phân loại vào chủ đề bạn đã đặt tên. \nDo đó, thuật toán topic modeling luôn có tham số k để quyết định trước số lượng chủ đề bạn muốn gom nhóm. \nHơn nữa, bạn có thể lựa chọn tập dữ liệu có nhiều câu để training không nhất thiết phải sử dụng tập dữ liệu chỉ có 1,2 câu.  Số lượt thích Số lượt thích Phản hồi 11NLTK nói: \n\t\t\t\t\t\t\t\tTháng Một 26, 2017 lúc 10:38 sáng\t\t\t\t\t\t\t Cam on nhieu. Chuc ban va nguoi than an Tet vui ve ! Số lượt thích Liked by  1 person Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Một 26, 2017 lúc 10:40 sáng\t\t\t\t\t\t\t Chúc bạn và gia đình năm mới thành công và an lạc 🙂 Số lượt thích Số lượt thích Phản hồi Luan nói: \n\t\t\t\t\t\t\t\tTháng Hai 2, 2017 lúc 1:02 chiều\t\t\t\t\t\t\t Cảm ơn anh vì những bài viết bổ ích . Năm mới chúc anh luôn tràn đầy năng lượng để làm việc hiệu quả. Số lượt thích Số lượt thích Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Hai 2, 2017 lúc 1:39 chiều\t\t\t\t\t\t\t Cám ơn em, chúc em sức khoẻ và nhiều thành công nhé. Số lượt thích Số lượt thích Phản hồi Nguyen Trung nói: \n\t\t\t\t\t\t\t\tTháng Hai 23, 2017 lúc 2:04 sáng\t\t\t\t\t\t\t Em chào anh ạ, \nEm muốn hỏi anh một chút về phần cuối cùng của bài viết : “Trong bài viết này, ta sẽ biểu diễn văn bản dưới dạng vector là TF hoặc TF-IDF. Sau đó, sử dụng feature vector này để gom nhóm văn bản bằng hai phương pháp là NMF (Non-Negative Matrix Factorization) và LDA (latent Dirichlet allocation).” \nCách mà LDA áp dụng vector TF-IDF cụ thể ở chỗ nào và như nào ạ? \nVì theo em tìm hiểu thì khi chạy bình thường LDA ( vs Gibblda ++ chẳng hạn)  thì output cũng là các topic như trên. \nThứ 2, là comment ở trên anh có nói là khi nhập câu mới thì hệ thống sẽ tự động phân loại??? A có thể giải thích giúp e một chút chỗ này ko ạ? Em cứ mơ hồ là mình phải tính xác xuất xuất hiện các topics trong câu đó, rồi mới kết luận được.  Mong nhận sự phản hồi từ anh. \nThanks a! Số lượt thích Số lượt thích Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Hai 23, 2017 lúc 2:40 sáng\t\t\t\t\t\t\t Hi em, việc chúng ta xây dựng các vector Tf-Idf cho từng văn bản nhằm mục đích tạo ra ma trận X là tập huấn luyện (dòng = vector Tf-Idf, cột = văn bản hoặc ngược lại ). Từ đây, em sẽ phân tách ra được 2 ma trận latent một là topic distributions over words (dòng = vector các từ, cột = topic), hai là document distributions over topics (dòng = topic, cột = văn bản). \nEm có thể tham khảo slide này  https://www.slideshare.net/clauwa/topic-models-5274169 . \nNhập câu mới ở đây em cũng sẽ phân tích ra được vector Tf-Idf và sử dụng Cosine similarity để tìm kiếm tập các văn bản nào gần nó nhất hay topic nào nó thuộc về. Số lượt thích Số lượt thích Phản hồi Hoàng Minh nói: \n\t\t\t\t\t\t\t\tTháng Sáu 29, 2017 lúc 9:55 sáng\t\t\t\t\t\t\t Để tăng độ chính xác thì tôi nghĩ nên bổ sung thêm vấn đề remove stop words. Số lượt thích Liked by  1 person Phản hồi Hoàng Anh nói: \n\t\t\t\t\t\t\t\tTháng Tám 9, 2017 lúc 2:36 chiều\t\t\t\t\t\t\t Em chào Anh, Rất cảm ơn anh đã chia sẻ các kiến thức. \nEm mới bắt đầu tìm hiểu python, em đã chạy đoạn code như phần chia sẻ bên trên của anh, nhưng nó báo lỗi ở đoạn #Load wiki data, cụ thể là dòng ” print people.head() ” ạ. Lỗi như sau: \n/usr/bin/python2.7 /home/hoanganh/PycharmProjects/hocpython/tfidf.py \n— people_wiki.csv found locally \nTraceback (most recent call last): \n  File “/home/hoanganh/PycharmProjects/hocpython/tfidf.py”, line 35, in \n    print (people.head()) Anh xem giup em lỗi này là bị sao với ạ, em không biết phải sửa thế nào nữa. \nEm cảm ơn anh nhiều ạ. Số lượt thích Số lượt thích Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Tám 9, 2017 lúc 2:39 chiều\t\t\t\t\t\t\t Lỗi không tìm thấy file people_wiki.csv \nEm download ở đây  https://drive.google.com/file/d/0BwA7lod1B3NpUlI0SlZyQ0ZCVlk/view \nđể vào chung thư mục của python script. Số lượt thích Số lượt thích Phản hồi Hoàng Anh nói: \n\t\t\t\t\t\t\t\tTháng Tám 9, 2017 lúc 2:51 chiều\t\t\t\t\t\t\t E đã download file đó về rồi, và đang để cùng trong thư mục của file code này anh ạ.  Nó có dòng thông báo này \n/usr/bin/python2.7 /home/hoanganh/PycharmProjects/hocpython/tfidf.py \n— people_wiki.csv found locally Tức là đã tìm thấy file people_wiki.csv đúng không ạ? Số lượt thích Số lượt thích Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Tám 9, 2017 lúc 2:55 chiều\t\t\t\t\t\t\t vậy là load thành công rồi em. Lỗi có thể là do từ Data frame, em thử debug xem. Số lượt thích Số lượt thích Phản hồi atrang nguyen nói: \n\t\t\t\t\t\t\t\tTháng Tám 30, 2017 lúc 1:52 sáng\t\t\t\t\t\t\t Bài viết của anh rất bổ ích. \nAnh cho em hỏi 1 chút, em bị lỗi này \nUnboundLocalError: local variable ‘df’ referenced before assignment \nSau đó em có khai báo global cho biến df nhưng lại bị lỗi \nNameError: name ‘df’ is not defined \nAnh giải đáp giúp em với. \nEm cảm ơn. Số lượt thích Số lượt thích Phản hồi Ông Xuân Hồng nói: \n\t\t\t\t\t\t\t\tTháng Tám 30, 2017 lúc 5:22 sáng\t\t\t\t\t\t\t Bạn load dữ liệu vào df \n    people = load_wiki_data(“people_wiki.csv”) \n    print people.head() \n    print len(people) \nLink source:  https://github.com/ongxuanhong/data-science-works/blob/master/python/clustering/document_retrieval.py Số lượt thích Số lượt thích Phản hồi trangminhcute nói: \n\t\t\t\t\t\t\t\tTháng Chín 1, 2017 lúc 4:47 sáng\t\t\t\t\t\t\t Em cảm ơn Số lượt thích Số lượt thích Phản hồi Nhập bình luận của bạn tại đây... Mời bạn điền thông tin vào ô dưới đây hoặc kích vào một biểu tượng để đăng nhập: Thư điện tử  (bắt buộc) (Địa chỉ của bạn được giấu kín) Tên  (bắt buộc) Trang web  Bạn đang bình luận bằng tài khoản WordPress.com  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Twitter  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Facebook  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Google+  (  Đăng xuất  /  Thay đổi  ) Hủy bỏ Connecting to %s Notify me of new comments via email. Thông báo cho tôi bằng email khi có bài đăng mới. Bạn Không Thể Sống Một Mình Nếu ngày mai tớ trở thành người nổi tiếng. Gặp tớ giữa đường bạn đừng ngại tớ nghe. Cứ nhìn tớ như ngày xưa bạn thấy. Đừng nên nghĩ tớ quá xa vời.  Nếu một ngày tớ là kẻ ăn xin. Thì bố thí cho tớ vài kỷ niệm. Dù rất nhỏ nhưng đừng nên khinh tớ. Vì tớ vẫn là tớ của ngày xưa.  Nếu ngày mai tớ chẳng biết đường về. Hãy chỉ giùm tớ con đường đi đúng đắn. Đừng để tớ sa chân nơi bùn nhớp. Lúc rút chân lên thì đã bẩn mất rồi.  Nếu ngày mai tớ gặp bạn giữa đường. Thì ta sẽ gặp nhau và cười nhé. Đừng nên trách và đừng nên đánh mất. Đôi khi có một số người lướt qua cuộc đời bạn và ngay tức khắc bạn nhận ra rằng sự có mặt của họ ý nghĩa như thế nào. Họ đã dạy bạn những bài học, đã giúp bạn nhận ra giá trị của chính mình hoặc trở thành con người mà bạn từng mơ ước. Có lẽ bạn sẽ không biết được những con người này từ đâu đến ( bạn cùng phòng, người hàng xóm, vị giáo sư, người bạn mất liên lạc từ lâu hay thậm chí là một người hoàn toàn xa lạ ). Nhưng khi bạn thờ ơ với họ, hãy nhớ rằng trong từng khoảnh khắc họ sẽ ảnh hưởng rất sâu sắc đến cuộc đời bạn.  …Cuộc sống không hề làm khó bạn mà chính bạn đã tự làm khó mình bằng cách đặt ra những câu hỏi đại loại như: “Tại sao lại thế này?… Tại sao lại thế kia?…”. Không ai có thể trả lời giúp bạn mà chính bạn phải tự đi tìm lời giải đáp cho mình. Hãy luôn nghĩ về người khác trước khi nghĩ đến bản thân, chắc chắn lúc đó bạn sẽ hài lòng với câu trả lời của chính mình. Và đừng quên luôn đặt câu hỏi: “Mình đã làm được gì?” trước khi tự hỏi: “Mình đã nhận được gì?” nhé! Tôi tin là bạn sẽ thành công!   Thật ra trên đời, con người luôn cần có nhau để giải quyết rất nhiều vấn đề. Nhờ những lúc qua lại giúp đỡ đó mà hoá giải bao muộn phiền, nảy sinh các tình cảm, cùng trải nghiệm biết bao điều thú vị, hiểu biết thêm nhiều tri thức mới. Đây chính là cuộc sống! Vì thế hãy trân trọng những người đang “làm phiền” bên cạnh bạn nhé! Đừng để đến khi mất đi rồi mới nghẹn ngào lại bảo “giá như” … ĐĐ. GS. Thích Phước Tiến \n__(())__ Namo Bụt Sakyamuni Nhận Email khi có bài viết mới Liên hệ Email (bắt buộc) Comment (bắt buộc) Top bài viết Bài viết mới FAQ: Big data cho người mới bắt đầu Tháng Mười 2, 2017 Làm việc với Spark DataFrames – Truy vấn nâng cao (Subqueries, Cube, Rollup) Tháng Chín 15, 2017 Thống kê ứng dụng 3: Các vấn đề trong thống kê Tháng Chín 14, 2017 Thống kê ứng dụng 2: Suy luận thống kê Tháng Chín 12, 2017 Thống kê ứng dụng 1: Quan sát dữ liệu Tháng Chín 7, 2017 AI, Machine Learning, Deep Learning phân biệt như thế nào cho đúng Tháng Chín 4, 2017 Information extraction – Bài toán rút trích thông tin trong văn bản Tháng Tám 28, 2017 Bắt đầu nghiên cứu big data từ đâu và như thế nào Tháng Tám 3, 2017 Data Science – Mỏ vàng của Kỉ nguyên số Tháng Tám 3, 2017 SMA 2017 – Lý thuyết ra quyết định Tháng Sáu 17, 2017 Big Data Chia sẻ Data Science Deep learning Dự án Exploratory Data Analysis Getting and cleaning data Kiến thức Lập trình Machine learning Python R Spark Statistical Inference Toán Weka Xử lý ngôn ngữ tự nhiên - Natural Language Processing (NLP) This slideshow requires JavaScript. Thư viện Tháng Mười 2017  (1) Tháng Chín 2017  (5) Tháng Tám 2017  (3) Tháng Sáu 2017  (3) Tháng Năm 2017  (3) Tháng Ba 2017  (1) Tháng Một 2017  (3) Tháng Mười Hai 2016  (1) Tháng Mười Một 2016  (2) Tháng Mười 2016  (2) Tháng Chín 2016  (1) Tháng Tám 2016  (2) Tháng Bảy 2016  (2) Tháng Năm 2016  (1) Tháng Tư 2016  (1) Tháng Ba 2016  (2) Tháng Hai 2016  (2) Tháng Một 2016  (1) Tháng Mười Hai 2015  (6) Tháng Mười Một 2015  (5) Tháng Mười 2015  (6) Tháng Chín 2015  (11) Tháng Tám 2015  (16) Tháng Bảy 2015  (25) Tháng Sáu 2015  (4) Thống kê 360,709 hits Tháng Một 2017 H B T N S B C « Th12   Th3 »   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31   Data Science Lập trình Kiến thức Chia sẻ Dự án About Tạo một website miễn phí hoặc 1 blog với WordPress.com. Post to Hủy bỏ %d  bloggers like this:",
          "url": "https://ongxuanhong.wordpress.com/2017/01/16/truy-van-van-ban-document-retrieval/",
          "relevance": "0"
        },
        {
          "title": "Máy vectơ hỗ trợ",
          "content": "Bách khoa toàn thư mở Wikipedia \n\t\t\t\t\tBước tới:\t\t\t\t\t menu , \t\t\t\t\t tìm kiếm Máy vectơ hỗ trợ  ( SVM  - viết tắt tên tiếng Anh  support vector machine ) là một khái niệm trong  thống kê  và  khoa học máy tính  cho một tập hợp các phương pháp  học có giám sát  liên quan đến nhau để  phân loại  và  phân tích hồi quy . SVM dạng chuẩn nhận dữ liệu vào và phân loại chúng vào hai lớp khác nhau. Do đó SVM là một  thuật toán phân loại nhị phân . Với một bộ các ví dụ luyện tập thuộc hai thể loại cho trước, thuật toán luyện tập SVM xây dựng một mô hình SVM để phân loại các ví dụ khác vào hai thể loại đó. Một mô hình SVM là một cách biểu diễn các điểm trong không gian và lựa chọn ranh giới giữa hai thể loại sao cho khoảng cách từ các ví dụ luyện tập tới ranh giới là xa nhất có thể. Các ví dụ mới cũng được biểu diễn trong cùng một không gian và được thuật toán dự đoán thuộc một trong hai thể loại tùy vào ví dụ đó nằm ở phía nào của ranh giới. Mục lục 1 Tổng quan về máy vectơ hỗ trợ 2 Lịch sử 3 Đặt vấn đề 4 SVM tuyến tính 4.1 Dạng ban đầu 4.2 Dạng đối ngẫu 5 Lề mềm 5.1 Dạng đối ngẫu 6 Xem thêm 7 Ghi chú 8 Liên kết ngoài 9 Tham khảo Tổng quan về máy vectơ hỗ trợ [ sửa  |  sửa mã nguồn ] Một máy vectơ hỗ trợ xây dựng một  siêu phẳng  hoặc một tập hợp các siêu phẳng trong một không gian nhiều chiều hoặc vô hạn chiều, có thể được sử dụng cho phân loại, hồi quy, hoặc các nhiệm vụ khác. Một cách trực giác, để phân loại tốt nhất thì các siêu phẳng nằm ở càng xa các điểm dữ liệu của tất cả các lớp (gọi là hàm lề) càng tốt, vì nói chung lề càng lớn thì  sai số tổng quát hóa  của thuật toán phân loại càng bé. Trong nhiều trường hợp, không thể phân chia các lớp dữ liệu một cách tuyến tính trong một không gian ban đầu được dùng để mô tả một vấn đề. Vì vậy, nhiều khi cần phải  ánh xạ  các điểm dữ liệu trong không gian ban đầu vào một không gian mới nhiều chiều hơn, để việc phân tách chúng trở nên dễ dàng hơn trong không gian mới. Để việc tính toán được hiệu quả, ánh xạ sử dụng trong thuật toán SVM chỉ đòi hỏi  tích vô hướng  của các vectơ dữ liệu trong không gian mới có thể được tính dễ dàng từ các tọa độ trong không gian cũ. Tích vô hướng này được xác định bằng một hàm hạt nhân  K ( x , y ) phù hợp. [1]  Một siêu phẳng trong không gian mới được định nghĩa là tập hợp các điểm có tích vô hướng với một vectơ cố định trong không gian đó là một hằng số. Vectơ xác định một siêu phẳng sử dụng trong SVM là một tổ hợp tuyến tính của các vectơ dữ liệu luyện tập trong không gian mới với các hệ số  α i . Với siêu phẳng lựa chọn như trên, các điểm  x  trong không gian đặc trưng được ánh xạ vào một siêu mặt phẳng là các điểm thỏa mãn: Σ i α i K ( x i , x ) = hằng số. Ghi chú rằng nếu  K ( x , y ) nhận giá trị ngày càng nhỏ khi  y  xa dần khỏi  x  thì mỗi số hạng của tổng trên được dùng để đo độ tương tự giữa  x  với điểm  x i  tương ứng trong dữ liệu luyện tập. Như vậy, tác dụng của tổng trên chính là so sánh khoảng cách giữa điểm cần dự đoán với các điểm dữ liệu đã biết. Lưu ý là tập hợp các điểm  x  được ánh xạ vào một siêu phẳng có thể có độ phức tạp tùy ý trong không gian ban đầu, nên có thể phân tách các tập hợp thậm chí không lồi trong không gian ban đầu. Lịch sử [ sửa  |  sửa mã nguồn ] Thuật toán SVM ban đầu được tìm ra bởi  Vladimir N. Vapnik  và dạng chuẩn hiện nay sử dụng lề mềm được tìm ra bởi Vapnik và  Corinna Cortes  năm 1995. [2] Đặt vấn đề [ sửa  |  sửa mã nguồn ] \nH3 (màu xanh lá cây) không chia tách hai lớp dữ liệu. H1 (màu xanh lơ) phân tách hai lớp với lề nhỏ và H2 (màu đỏ) phân tách với lề cực đại. Phân loại thống kê  là một nhiệm vụ phổ biến trong  học máy . Trong mô hình học có giám sát, thuật toán được cho trước một số điểm dữ liệu cùng với nhãn của chúng thuộc một trong hai lớp cho trước. Mục tiêu của thuật toán là xác định xem một điểm dữ liệu  mới  sẽ được thuộc về lớp nào. Mỗi điểm dữ liệu được biểu diễn dưới dạng một vector p-chiều, và ta muốn biết liệu có thể chia tách hai lớp dữ liệu bằng một  siêu phẳng p  − 1 chiều. Đây gọi là  phân loại tuyến tính . Có nhiều siêu phẳng có thể phân loại được dữ liệu. Một lựa chọn hợp lý trong chúng là siêu phẳng có lề lớn nhất giữa hai lớp. SVM tuyến tính [ sửa  |  sửa mã nguồn ] Ta có một tập huấn luyện  D {\\displaystyle {\\mathcal {D}}}  gồm  n  điểm có dạng D = { ( x i , y i ) ∣ x i ∈ R p , y i ∈ { − 1 , 1 } } i = 1 n {\\displaystyle {\\mathcal {D}}=\\left\\{(\\mathbf {x} _{i},y_{i})\\mid \\mathbf {x} _{i}\\in \\mathbb {R} ^{p},\\,y_{i}\\in \\{-1,1\\}\\right\\}_{i=1}^{n}} với  y i  mang giá trị 1 hoặc −1, xác định lớp của điểm  x i {\\displaystyle \\mathbf {x} _{i}} . Mỗi  x i {\\displaystyle \\mathbf {x} _{i}}  là một vectơ thực  p -chiều. Ta cần tìm siêu phẳng có lề lớn nhất chia tách các điểm có  y i = 1 {\\displaystyle y_{i}=1}  và các điểm có  y i = − 1 {\\displaystyle y_{i}=-1} . Mỗi siêu phẳng đều có thể được viết dưới dạng một tập hợp các điểm  x {\\displaystyle \\mathbf {x} }  thỏa mãn \nSiêu phẳng với lề cực đại cho một SVM phân tách dữ liệu thuộc hai lớp. Các ví dụ nằm trên lề được gọi là các vectơ hỗ trợ. w ⋅ x − b = 0 , {\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} -b=0,\\,} với  ⋅ {\\displaystyle \\cdot }  ký hiệu cho  tích vô hướng  và  w {\\displaystyle {\\mathbf {w} }}  là một  vectơ pháp tuyến  của siêu phẳng. Tham số  b ∥ w ∥ {\\displaystyle {\\tfrac {b}{\\|\\mathbf {w} \\|}}}  xác định khoảng cách giữa gốc tọa độ và siêu phẳng theo hướng vectơ pháp tuyến  w {\\displaystyle {\\mathbf {w} }} . Chúng ta cần chọn  w {\\displaystyle {\\mathbf {w} }}  và  b {\\displaystyle b}  để cực đại hóa lề, hay khoảng cách giữa hai siêu mặt song song ở xa nhau nhất có thể trong khi vẫn phân chia được dữ liệu. Các siêu mặt ấy được xác định bằng w ⋅ x − b = 1 {\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} -b=1\\,} và w ⋅ x − b = − 1. {\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} -b=-1.\\,} Để ý rằng nếu dữ liệu huấn luyện có thể được chia tách một cách tuyến tính, thì ta có thể chọn hai siêu phẳng của lề sao cho không có điểm nào ở giữa chúng và sau đó tăng khoảng cách giữa chúng đến tối đa có thể. Bằng hình học, ta tìm được khoảng cách giữa hai siêu phẳng là  2 ∥ w ∥ {\\displaystyle {\\tfrac {2}{\\|\\mathbf {w} \\|}}} . Vì vậy ta muốn cực tiểu hóa giá trị  ∥ w ∥ {\\displaystyle \\|\\mathbf {w} \\|} . Để đảm bảo không có điểm dữ liệu nào trong lề, ta thêm vào các điều kiện sau: Với mỗi  i {\\displaystyle i}  ta có w ⋅ x i − b ≥ 1  cho  x i {\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} _{i}-b\\geq 1\\qquad {\\text{ cho }}\\mathbf {x} _{i}}  thuộc lớp thứ nhất hoặc w ⋅ x i − b ≤ − 1  cho  x i {\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} _{i}-b\\leq -1\\qquad {\\text{ cho }}\\mathbf {x} _{i}}  thuộc lớp thứ hai Có thể viết gọn lại như sau với mọi  1 ≤ i ≤ n {\\displaystyle 1\\leq i\\leq n} : y i ( w ⋅ x i − b ) ≥ 1 , ( 1 ) {\\displaystyle y_{i}(\\mathbf {w} \\cdot \\mathbf {x} _{i}-b)\\geq 1,\\qquad \\qquad (1)} Tóm lại, ta có bài toán tối ưu hóa sau: Cực tiểu hóa (theo  w , b {\\displaystyle {\\mathbf {w} ,b}} ) ∥ w ∥ {\\displaystyle \\|\\mathbf {w} \\|} với điều kiện (với mọi  i = 1 , … , n {\\displaystyle i=1,\\dots ,n} ) y i ( w ⋅ x i − b ) ≥ 1. {\\displaystyle y_{i}(\\mathbf {w} \\cdot \\mathbf {x_{i}} -b)\\geq 1.\\,} Dạng ban đầu [ sửa  |  sửa mã nguồn ] Bài toán tối ưu ở mục trên tương đối khó giải vì hàm mục tiêu phụ thuộc vào || w ||, là một hàm có khai căn. Tuy nhiên có thể thay || w || bằng hàm mục tiêu  1 2 ∥ w ∥ 2 {\\displaystyle {\\tfrac {1}{2}}\\|\\mathbf {w} \\|^{2}}  (hệ số 1/2 để tiện cho các biến đổi toán học sau này) mà không làm thay đổi lời giải (lời giải của bài toán mới và bài toán ban đầu có cùng  w  và b). Đây là một bài toán  quy hoạch toàn phương . Cụ thể hơn: Cực tiểu hóa (theo  w , b {\\displaystyle {\\mathbf {w} ,b}} ) 1 2 ∥ w ∥ 2 {\\displaystyle {\\frac {1}{2}}\\|\\mathbf {w} \\|^{2}} với điều kiện (với mọi  i = 1 , … , n {\\displaystyle i=1,\\dots ,n} ) y i ( w ⋅ x i − b ) ≥ 1. {\\displaystyle y_{i}(\\mathbf {w} \\cdot \\mathbf {x_{i}} -b)\\geq 1.} Bằng cách thêm các nhân tử Lagrange  α {\\displaystyle {\\boldsymbol {\\alpha }}} , bài toán trên trở thành min w , b max α ≥ 0 { 1 2 ∥ w ∥ 2 − ∑ i = 1 n α i [ y i ( w ⋅ x i − b ) − 1 ] } {\\displaystyle \\min _{\\mathbf {w} ,b}\\max _{{\\boldsymbol {\\alpha }}\\geq 0}\\left\\{{\\frac {1}{2}}\\|\\mathbf {w} \\|^{2}-\\sum _{i=1}^{n}{\\alpha _{i}[y_{i}(\\mathbf {w} \\cdot \\mathbf {x_{i}} -b)-1]}\\right\\}} nghĩa là ta cần tìm một  điểm yên ngựa . Khi đó, tất cả các điểm không nằm trên lề, nghĩa là  y i ( w ⋅ x i − b ) − 1 > 0 {\\displaystyle y_{i}(\\mathbf {w} \\cdot \\mathbf {x_{i}} -b)-1>0}  đều không ảnh hưởng đến giá trị hàm mục tiêu vì ta có thể chọn  α i {\\displaystyle \\alpha _{i}}  bằng không. Có thể giải bài toán này bằng các kĩ thuật thông thường cho  quy hoạch toàn phương . Theo điều kiện Karush–Kuhn–Tucker, lời giải có thể được viết dưới dạng tổ hợp tuyến tính của các vectơ luyện tập w = ∑ i = 1 n α i y i x i . {\\displaystyle \\mathbf {w} =\\sum _{i=1}^{n}{\\alpha _{i}y_{i}\\mathbf {x_{i}} }.} Chỉ có một vài  α i {\\displaystyle \\alpha _{i}}  nhận giá trị lớn hơn 0. Các điểm  x i {\\displaystyle \\mathbf {x_{i}} }  tương ứng là các  vectơ hỗ trợ  nằm trên lề và thỏa mãn  y i ( w ⋅ x i − b ) = 1 {\\displaystyle y_{i}(\\mathbf {w} \\cdot \\mathbf {x_{i}} -b)=1} . Từ điều kiện này, ta nhận thấy w ⋅ x i − b = 1 / y i = y i ⟺ b = w ⋅ x i − y i {\\displaystyle \\mathbf {w} \\cdot \\mathbf {x_{i}} -b=1/y_{i}=y_{i}\\iff b=\\mathbf {w} \\cdot \\mathbf {x_{i}} -y_{i}} từ đó ta suy ra được giá trị  b {\\displaystyle b} . Trên thực tế, một cách thức tốt hơn để tính  b {\\displaystyle b}  là tính giá trị trung bình từ tất cả  N S V {\\displaystyle N_{SV}}  vectơ hỗ trợ: b = 1 N S V ∑ i = 1 N S V ( w ⋅ x i − y i ) {\\displaystyle b={\\frac {1}{N_{SV}}}\\sum _{i=1}^{N_{SV}}{(\\mathbf {w} \\cdot \\mathbf {x_{i}} -y_{i})}} Dạng đối ngẫu [ sửa  |  sửa mã nguồn ] Nếu viết điều kiện phân loại dưới  dạng đối ngẫu  không điều kiện thì sẽ dễ dàng nhận thấy siêu phẳng với lề lớn nhất, và do đó nhiệm vụ phân loại, chỉ phụ thuộc vào các điểm luyện tập nằm trên lề, còn gọi là các  vectơ hỗ trợ . Vì  ∥ w ∥ 2 = w ⋅ w {\\displaystyle \\|\\mathbf {w} \\|^{2}=w\\cdot w}  và  w = ∑ i = 1 n α i y i x i {\\displaystyle \\mathbf {w} =\\sum _{i=1}^{n}{\\alpha _{i}y_{i}\\mathbf {x_{i}} }} , ta nhận thấy bài toán đối ngẫu của SVM là chính là bài toán tối ưu hóa sau: Cực đại hóa (theo  α i {\\displaystyle \\alpha _{i}} ) L ~ ( α ) = ∑ i = 1 n α i − 1 2 ∑ i , j α i α j y i y j x i T x j = ∑ i = 1 n α i − 1 2 ∑ i , j α i α j y i y j k ( x i , x j ) {\\displaystyle {\\tilde {L}}(\\mathbf {\\alpha } )=\\sum _{i=1}^{n}\\alpha _{i}-{\\frac {1}{2}}\\sum _{i,j}\\alpha _{i}\\alpha _{j}y_{i}y_{j}\\mathbf {x} _{i}^{T}\\mathbf {x} _{j}=\\sum _{i=1}^{n}\\alpha _{i}-{\\frac {1}{2}}\\sum _{i,j}\\alpha _{i}\\alpha _{j}y_{i}y_{j}k(\\mathbf {x} _{i},\\mathbf {x} _{j})} với điều kiện (với mọi  i = 1 , … , n {\\displaystyle i=1,\\dots ,n} ) α i ≥ 0 , {\\displaystyle \\alpha _{i}\\geq 0,\\,} và điều kiện sau ứng với việc cực tiểu hóa theo  b {\\displaystyle b} ∑ i = 1 n α i y i = 0. {\\displaystyle \\sum _{i=1}^{n}\\alpha _{i}y_{i}=0.} Ở đây hàm hạt nhân được định nghĩa là  k ( x i , x j ) = x i ⋅ x j {\\displaystyle k(\\mathbf {x} _{i},\\mathbf {x} _{j})=\\mathbf {x} _{i}\\cdot \\mathbf {x} _{j}} . Sau khi giải xong, có thể tính  w {\\displaystyle \\mathbf {w} }  từ các giá trị  α {\\displaystyle \\alpha }  tìm được như sau: w = ∑ i α i y i x i . {\\displaystyle \\mathbf {w} =\\sum _{i}\\alpha _{i}y_{i}\\mathbf {x} _{i}.} Lề mềm [ sửa  |  sửa mã nguồn ] Năm 1995,  Corinna Cortes  và  Vladimir N. Vapnik  đề xuất một ý tưởng mới cho phép thuật toán gán nhãn sai cho một số ví dụ luyện tập. [2]  Nếu không tồn tại siêu phẳng nào phân tách được hai lớp dữ liệu, thì thuật toán  lề mềm  sẽ chọn một siêu phẳng phân tách các ví dụ luyện tập tốt nhất có thể, và đồng thời cực đại hóa khoảng cách giữa siêu phẳng với các ví dụ được gán đúng nhãn. Phương pháp này sử dụng các biến bù  ξ i {\\displaystyle \\xi _{i}} , dùng để đo độ sai lệch của ví dụ  x i {\\displaystyle x_{i}} y i ( w ⋅ x i − b ) ≥ 1 − ξ i 1 ≤ i ≤ n . ( 2 ) {\\displaystyle y_{i}(\\mathbf {w} \\cdot \\mathbf {x_{i}} -b)\\geq 1-\\xi _{i}\\quad 1\\leq i\\leq n.\\quad \\quad (2)} Hàm mục tiêu có thêm một số hạng mới để phạt thuật toán khi  ξ i {\\displaystyle \\xi _{i}}  khác không, và bài toán tối ưu hóa trở thành việc trao đổi giữa lề lớn và mức phạt nhỏ. Nếu hàm phạt là tuyến tính thì bài toán trở thành: min w , ξ , b { 1 2 ∥ w ∥ 2 + C ∑ i = 1 n ξ i } {\\displaystyle \\min _{\\mathbf {w} ,\\mathbf {\\xi } ,b}\\left\\{{\\frac {1}{2}}\\|\\mathbf {w} \\|^{2}+C\\sum _{i=1}^{n}\\xi _{i}\\right\\}} với điều kiện (với mọi  i = 1 , … n {\\displaystyle i=1,\\dots n} ) y i ( w ⋅ x i − b ) ≥ 1 − ξ i ,         ξ i ≥ 0 {\\displaystyle y_{i}(\\mathbf {w} \\cdot \\mathbf {x_{i}} -b)\\geq 1-\\xi _{i},~~~~\\xi _{i}\\geq 0} Có thể giải bài toán trên bằng  nhân tử Lagrange  tương tự như trường hợp cơ bản ở trên. Bài toán cần giải trở thành: min w , ξ , b max α , β { 1 2 ∥ w ∥ 2 + C ∑ i = 1 n ξ i − ∑ i = 1 n α i [ y i ( w ⋅ x i − b ) − 1 + ξ i ] − ∑ i = 1 n β i ξ i } {\\displaystyle \\min _{\\mathbf {w} ,\\mathbf {\\xi } ,b}\\max _{{\\boldsymbol {\\alpha }},{\\boldsymbol {\\beta }}}\\left\\{{\\frac {1}{2}}\\|\\mathbf {w} \\|^{2}+C\\sum _{i=1}^{n}\\xi _{i}-\\sum _{i=1}^{n}{\\alpha _{i}[y_{i}(\\mathbf {w} \\cdot \\mathbf {x_{i}} -b)-1+\\xi _{i}]}-\\sum _{i=1}^{n}\\beta _{i}\\xi _{i}\\right\\}} với  α i , β i ≥ 0 {\\displaystyle \\alpha _{i},\\beta _{i}\\geq 0} . Dạng đối ngẫu [ sửa  |  sửa mã nguồn ] Cực đại hóa (theo  α i {\\displaystyle \\alpha _{i}} ) L ~ ( α ) = ∑ i = 1 n α i − 1 2 ∑ i , j α i α j y i y j k ( x i , x j ) {\\displaystyle {\\tilde {L}}(\\mathbf {\\alpha } )=\\sum _{i=1}^{n}\\alpha _{i}-{\\frac {1}{2}}\\sum _{i,j}\\alpha _{i}\\alpha _{j}y_{i}y_{j}k(\\mathbf {x} _{i},\\mathbf {x} _{j})} với điều kiện (với mọi  i = 1 , … , n {\\displaystyle i=1,\\dots ,n} ) 0 ≤ α i ≤ C , {\\displaystyle 0\\leq \\alpha _{i}\\leq C,\\,} và ∑ i = 1 n α i y i = 0. {\\displaystyle \\sum _{i=1}^{n}\\alpha _{i}y_{i}=0.} Ưu điểm của việc dùng hàm phạt tuyến tính là các biến bù biến mất khỏi bài toán đối ngẫu, và hằng số  C  chỉ xuất hiện dưới dạng một chặn trên cho các nhân tử Lagrange. Cách đặt vấn đề trên đã mang lại nhiều thành quả trong thực tiễn, và Cortes và Vapnik đã nhận được  giải Paris Kanellakis  của  ACM  năm 2008 cho đóng góp này. [3]  Các hàm phạt  phi tuyến  cũng được sử dụng, đặc biệt là để giảm ảnh hưởng của các trường hợp ngoại lệ, tuy nhiên nếu không lựa chọn hàm phạt cẩn thận thì bài toán trở thành không lồi, và việc tìm lời giải tối ưu toàn cục thường là rất khó. Xem thêm [ sửa  |  sửa mã nguồn ] In situ adaptive tabulation Máy hạt nhân Predictive analytics Relevance vector machine , Một mô hình máy hạt nhân thưa xác suất có dạnghàm số giống như SVM. Tối ưu hóa nhỏ nhất tuần tự Ghi chú [ sửa  |  sửa mã nguồn ] ^ Press, William H.; Teukolsky, Saul A.; Vetterling, William T.; Flannery, B. P. (2007).  “Section 16.5. Support Vector Machines” .  Numerical Recipes: The Art of Scientific Computing  (ấn bản 3). New York: Cambridge University Press.  ISBN   978-0-521-88068-8 .   ^  a ă Cortes, Corinna; and Vapnik, Vladimir N.; \"Support-Vector Networks\", Machine Learning, 20, 1995.  http://www.springerlink.com/content/k238jx04hm87j80g/ ^ ACM Website, Press release of March 17th 2009.  http://www.acm.org/press-room/news-releases/awards-08-groupa Liên kết ngoài [ sửa  |  sửa mã nguồn ] Burges, Christopher J.C. (1998),  “A Tutorial on Support Vector Machines for Pattern Recognition” (PDF) ,  Data Mining and Knowledge Discovery 2 : 121–167    *  www.kernel-machines.org (thông tin tổng quan và danh sách các bài báo nghiên cứu) www.support-vector-machines.org (Bài báo nghiên cứu, đánh giá,, phần mềm, liên kết có liên quan đến máy vectơ hỗ trợ) videolectures.net (Video bài giảng về SVM) Phim ngắn : Minh họa SVM sử dụng hàm hạt nhân đa thức. Một hướng dẫn sử dụng SVM cho người mới học bởi Tristan Fletcher  [1] . www.shogun-toolbox.org ( Shogun (hộp công cụ)  gồm khoảng 20 thư viện lập trình SVM) libsvm  libsvm là một thư viện lập trình SVM liblinear  liblinear là một thư viện lập trình gồm nhiều thuật toán phân loại tuyến tính, trong đó có SVM flssvm  flssvm là một thư viện lập trình svm bình phương nhỏ nhất viết bằng fortran Shark  Shark là một thư viện học máy viết bằng C++ có chứa nhiều loại SVM dlib  dlib là một thư viện C++ cho máy hạt nhân và SVM SVM light  là một bộ phần mềm cho học máy và phân loại bằng SVM. Tham khảo [ sửa  |  sửa mã nguồn ] Sergios Theodoridis and Konstantinos Koutroumbas \"Pattern Recognition\", 4th Edition, Academic Press, 2009,  ISBN 978-1-59749-272-0 Nello Cristianini and John Shawe-Taylor.  An Introduction to Support Vector Machines and other kernel-based learning methods . Cambridge University Press, 2000.  ISBN 0-521-78019-5 ( [2]  SVM Book) Huang T.-M., Kecman V., Kopriva I. (2006), Kernel Based Algorithms for Mining Huge Data Sets, Supervised, Semi-supervised, and Unsupervised Learning, Springer-Verlag, Berlin, Heidelberg, 260 pp. 96 illus., Hardcover,  ISBN 3-540-31681-7 [3] Vojislav Kecman: \"Learning and Soft Computing — Support Vector Machines, Neural Networks, Fuzzy Logic Systems\", The MIT Press, Cambridge, MA, 2001. [4] Bernhard Schölkopf and A. J. Smola:  Learning with Kernels . MIT Press, Cambridge, MA, 2002.  (Partly available on line:  [5] .) ISBN 0-262-19475-9 Bernhard Schölkopf, Christopher J.C. Burges, and Alexander J. Smola (editors). \"Advances in Kernel Methods: Support Vector Learning\". MIT Press, Cambridge, MA, 1999.  ISBN 0-262-19416-3 .  [6] John Shawe-Taylor and Nello Cristianini.  Kernel Methods for Pattern Analysis . Cambridge University Press, 2004.  ISBN 0-521-81397-2 ( [7]  Kernel Methods Book) Ingo Steinwart and Andreas Christmann.  Support Vector Machines . Springer-Verlag, New York, 2008.  ISBN 978-0-387-77241-7 ( [8]  SVM Book) P.J. Tan and  D.L. Dowe  (2004),  MML Inference of Oblique Decision Trees , Lecture Notes in Artificial Intelligence (LNAI) 3339, Springer-Verlag,  pp1082-1088 . (This paper uses  minimum message length  ( MML ) and actually incorporates probabilistic support vector machines in the leaves of  decision trees .) Vladimir Vapnik.  The Nature of Statistical Learning Theory . Springer-Verlag, 1995.  ISBN 0-387-98780-0 Vladimir Vapnik, S.Kotz \"Estimation of Dependences Based on Empirical Data\" Springer, 2006.  ISBN 0-387-30865-2 , 510 pages [this is a reprint of Vapnik's early book describing philosophy behind SVM approach. The 2006 Appendix describes recent development]. Dmitriy Fradkin and Ilya Muchnik \"Support Vector Machines for Classification\" in J. Abello and G. Carmode (Eds) \"Discrete Methods in Epidemiology\", DIMACS Series in Discrete Mathematics and Theoretical Computer Science, volume 70, pp. 13–20, 2006.  [9] . Succinctly describes theoretical ideas behind SVM. Kristin P. Bennett and Colin Campbell, \"Support Vector Machines: Hype or Hallelujah?\", SIGKDD Explorations, 2,2, 2000, 1–13.  [10] . Excellent introduction to SVMs with helpful figures. Ovidiu Ivanciuc, \"Applications of Support Vector Machines in Chemistry\", In:  Reviews in Computational Chemistry , Volume 23, 2007, pp. 291–400. Reprint available:  [11] Catanzaro, Sundaram, Keutzer, \"Fast Support Vector Machine Training and Classification on Graphics Processors\", In:  International Conference on Machine Learning , 2008  [12] \n\t\t\t\t\t\tLấy từ “ https://vi.wikipedia.org/w/index.php?title=Máy_vectơ_hỗ_trợ&oldid=26464254 ”\t\t\t\t\t Thể loại :  Giải thuật phân loại Học máy Phân loại thống kê Phân loại bằng thống kê Thể loại ẩn:  Trang sử dụng liên kết tự động ISBN Trình đơn chuyển hướng Công cụ cá nhân Chưa đăng nhập Thảo luận cho địa chỉ IP này Đóng góp Mở tài khoản Đăng nhập Không gian tên Bài viết Thảo luận Biến thể Các hiển thị Đọc Sửa đổi Sửa mã nguồn Xem lịch sử Khác Tìm kiếm Xem nhanh Trang Chính Bài viết chọn lọc Tin tức Bài viết ngẫu nhiên Thay đổi gần đây Phản hồi lỗi Quyên góp Tương tác Hướng dẫn Giới thiệu Wikipedia Cộng đồng Thảo luận chung Giúp sử dụng Liên lạc Công cụ Các liên kết đến đây Thay đổi liên quan Các trang đặc biệt Liên kết thường trực Thông tin trang Khoản mục Wikidata Trích dẫn trang này In/xuất ra Tạo một quyển sách Tải về dưới dạng PDF Bản để in ra Ngôn ngữ khác العربية Català Čeština Deutsch English Español Euskara فارسی Français 한국어 Italiano עברית Nederlands 日本語 Polski Português Русский Slovenščina Suomi Svenska தமிழ் Türkçe Українська 中文 Sửa liên kết  Trang này được sửa đổi lần cuối lúc 12:01 ngày 8 tháng 4 năm 2017. Văn bản được phát hành theo  Giấy phép Creative Commons Ghi công–Chia sẻ tương tự ; có thể áp dụng điều khoản bổ sung. Với việc sử dụng trang web này, bạn chấp nhận  Điều khoản Sử dụng  và  Quy định quyền riêng tư . \nWikipedia® là thương hiệu đã đăng ký của  Wikimedia Foundation, Inc. , một tổ chức phi lợi nhuận. Quy định quyền riêng tư Giới thiệu Wikipedia Lời phủ nhận Nhà phát triển Tuyên bố về cookie Phiên bản di động",
          "url": "https://vi.wikipedia.org/wiki/M%C3%A1y_vect%C6%A1_h%E1%BB%97_tr%E1%BB%A3",
          "relevance": "0"
        },
        {
          "title": "Mô hình hồi quy ứng dụng trong bài toán dự đoán giá bất động sản - Machine Learning (phần 3)",
          "content": "Posts Questions Discussions Announcements  No announcement yet. All Announcements  Sign In/Sign up\n     +2 Pham Van Toan  Follow Published Nov 29th, 1:11 am Mô hình hồi quy ứng dụng trong bài toán dự đoán giá bất động sản - Machine Learning (phần 3) Regression Model Prediction Learning Machine Learning  Nov 29th, 1:11 am\n          227  2  0  Report\n     Xin chào tất cả các bạn, chúng ta cùng khởi động một tuần mới với bài viết tiếp theo về mô hình hồi quy áp dụng trong bài toán dự đoán giá của bất động sản nhé. Trong bài viết này mình sẽ trình bày một số kĩ thuật để cải tiến mô hình hồi quy của các bạn cho hiệu năng tốt hơn các phương pháp đã áp dụng trong  bài trước . OK, kiếm một tách cà phê và bắt đầu ngay thôi. Vấn đề Sắp có lương rồi    quẩy lên anh em ơi Xàm quá, quay lại chủ đề chính nào. Như bài trước chúng ta thấy rằng thông qua mô hình hồi quy chúng ta biểu diễn được giá của một ngôi nhà phụ thuộc như thế nào vào một số feature của nó như số phòng tắm, số phòng ngủ, năm xây dựng... Sự phụ thuộc này là phụ thuộc đơn lẻ giữa từng thuộc tính vào giá. Có nghĩa là chúng ta đang xem xét xem một thuộc tính thay đổi sẽ ảnh hưởng đến giá cả như thế nào. Tuy nhiên, mô hình này chưa tính đến trường hợp, nếu hai thuộc tính cùng thay đổi (ví dụ như diện tích và năm bán) thì ngoài việc phụ thuộc đơn lẻ giữa từng thuộc tính vào giá, thì sự kết hợp giữa hai yếu tố thay đổi này cũng ảnh hưởng đến giá không kém. Vậy làm thế nào để tìm được sự ảnh hưởng này. Chà chà...đau đầu phết đấy, nhưng không sao cả, điều gì cũng có cách giải quyết của nó...  Chúng ta bắt đầu ngay thôi Giải pháp Xét một mô hình đơn giản ở không gian hai chiều. Ta giả sử giá của bất động sản chỉ phụ thuộc vào hai yếu tố là diện tích ( X! ) và vị trí ( X2 ) thì hàm giá của chúng ta được biểu diễn như sau: Đây là một hàm số bậc nhất với đồ thị dạng đường thẳng: và như chúng ta đã phân tích thì sự phụ thuộc tuyến tính bậc nhất chưa biểu diễn được sự ảnh hưởng của việc kết hợp giữa các yếu tố khi các thuộc tính cùng thay đổi. Vậy nên một ý tưởng của chúng ta đó là nâng bậc cho mô hình tuyến tính. Giả sử như mô hình trên được nâng bậc thành bậc 2, chúng ta có hàm giá của bất động sản có dạng như sau: Lúc này đường hồi quy chúng ta có dạng một đường cong đi qua các điểm dữ liệu: Dễ thấy mô hình này vẫn tuân theo quy luật tuyến tính với các biến và theo dõi sự ảnh hưởng kết hợp của các thuộc tính băng các hệ số bậc 2...ví dụ trong hàm giá trên là các hệ số  W3, W4  và  W5  đó. Bây giờ chúng ta cùng thử chạy thử nghiệm xem. Trong bài viết này mình sẽ sử dụng  PolynomialFeatures  trong Scikit Learn để nâng bậc của mô hình. Và sau đó vẫn áp dụng các phương pháp đã dùng ở  bài trước  để so sánh hiệu quả của các phương pháp. OK các bạn đã sẵn sàng rồi phải không nào. Cùng mình bắt tay vào coding nhé. Thử nghiệm Chúng ta sẽ thử nghiệm phương pháp vừa nói trên bằng việc viết một hàm  polynomialRegression  và so sánh với mô hình ban đầu với cùng một phương pháp training cho tập dữ liệu huấn luyện là  LinearRegression def polynomialRegression (X_train, Y_train, X_test, Y_test, degree) : \n    poly_model = Pipeline([( 'poly' , PolynomialFeatures(degree)),\n                           ( 'linear' , linear_model.LinearRegression(fit_intercept= False ))])\n    poly_model = poly_model.fit(X_train, Y_train)\n    score_poly_trained = poly_model.score(X_test, Y_test)\n\n     return  score_poly_trained\n\n Để so sánh trong hàm  main  chúng ta viết như sau: if  __name__ ==  \"__main__\" :\n    data = getData()\n     if  data  is not None :\n         # Selection few attributes \n        attributes = list(\n            [\n                 'num_bed' ,\n                 'year_built' ,\n                 'num_room' ,\n                 'num_bath' ,\n                 'living_area' ,\n            ]\n        )\n         # Vector price of house \n        Y = data[ 'askprice' ]\n         # print np.array(Y) # Vector attributes of house \n        X = data[attributes]\n         # Split data to training test and testing test \n        X_train, X_test, Y_train, Y_test = train_test_split(np.array(X), np.array(Y), test_size= 0.2 )\n         # Linear Regression Model \n        linearScore = linearRegressionModel(X_train, Y_train, X_test, Y_test)\n         print 'Linear Score = '  , linearScore\n         # Poly Regression Model \n        polyScore = polynomialRegression(X_train, Y_train, X_test, Y_test,  2 )\n         print 'Poly Score = ' , polyScore\n Ở đây chúng ta sẽ so sánh mô hình bậc hai và mô hình bậc nhất. Sau khi chạy hàm main kết quả chúng ta thu được như sau: Connected to pydev debugger (build  162.1967 .10 )\n-- home_data.csv found locally\nLinear Score =   0.523266077498 \nPoly Score =   0.601673555506 \n\nProcess finished  with  exit code  0 Chúng ta có thể thấy được hiệu quả của mô hình hồi quy có nâng bậc tốt hơn một chút so với mô hình tuyến tính đơn thuần. Tuy nhiên không có một phương pháp nào là đúng cho mọi tập dữ liệu. Phương pháp nâng bậc cho chúng ta thấy được sự ảnh hưởng của các thuộc tính kết hợp tuy nhiên nó cũng có những nhược điểm của nó, chúng ta sẽ bàn luận trong phần tiếp theo đây Nhược điểm Bản chất của việc nâng bậc mô hình là chuyển vector thuộc tính X từ một vector có số chiều nhỏ thành một vector có số chiều lớn hơn rất nhiều theo quy luật số mũ. Nếu số lượng dữ liệu của chúng ta nhỏ mà số chiều của vector thuộc tính lại quá lớn dẫn đến việc giải quyết bài toán hồi quy với số chiều cao và hiển nhiên là độ chính xác sẽ rất thấp. Điều đó cho chúng ta thấy cần phải sàng lọc và lựa chọn thuộc tính thật tốt trước khi đưa vào mô hình. Đây là cả một nganh khoa học mà chúng ta có thể tìm hiểu thêm  tại đây . Nếu có cơ hội mình sẽ viết một bài chia sẻ riêng về Feature Engineering. Nó thật sự rất thú vị đó Kết luận Qua việc nâng bậc của mô hình hồi quy có thể tìm ra được sự phụ thuộc kết hợp giữa các Feature đến giá của bất động sản. Tuy nhiên nếu tập dữ liệu quá nhỏ so với số lượng Feature thì khi nâng bậc sẽ dẫn đến bài toán tối ưu với số chiều cao và làm giảm độ chính xác của mô hình. Điều đó cho thấy việc lựa chọn khéo léo các Feature là vô cùng cần thiết trước khi áp dụng bất kì mô hình nào. Code dùng trong bài Xem tại đây Tham khảo Polynomial Regression Models Feature engineering Pham Van Toan @pham.van.toan Follow  1193\n          74\n          15\n         \n                            Clip this post\n                         Have problems with  Regression Model, Prediction Learning or Machine Learning ?  Ask on Viblo » Comments  No comments yet.\n         +2 • • • Facebook Viblo Hot authors Resources Posts Questions Videos Tags Authors Help Terms  RSS Feed Browser extension Atom plugin Feedback FAQ \n                    © 2017  Viblo . All rights reserved.\n                 \n        Viblo - Free service for technical knowledge sharing\n     Feedback \n                        Your feedback will be shared with our product delivery teams, and taken into consideration\n                        for future development.\n                     Name Email Send \n                Sign In\n             \n                Sign Up\n             Join us to find useful information required to improve your skills \n            Forgot your password?\n         Sign In Free  service for technical knowledge  sharing Join us to find useful information required to improve your skills \n                    I agree to  Viblo Terms of Service Sign Up or",
          "url": "https://viblo.asia/p/mo-hinh-hoi-quy-ung-dung-trong-bai-toan-du-doan-gia-bat-dong-san-machine-learning-phan-3-PwRkgmXJGEd",
          "relevance": "0"
        },
        {
          "title": "Không gian vectơ",
          "content": "Bách khoa toàn thư mở Wikipedia Bạn có  tin nhắn mới  ( thay đổi gần đây ). \n\t\t\t\t\tBước tới:\t\t\t\t\t menu , \t\t\t\t\t tìm kiếm \nKhông gian vectơ là một tập các đối tượng có định hướng (được gọi là các vectơ) có thể co giãn và cộng. Trong  toán học ,  không gian vectơ  là một tập hợp mà trên đó hai  phép toán ,  phép cộng vectơ  và  phép nhân vectơ  với một số, được  định nghĩa  và thỏa mãn các  tiên đề  được liệt kê dưới đây. Các không gian  vectơ  quen thuộc là  không gian Euclid  hai chiều và ba chiều. Các vectơ trong các không gian này là các cặp  số thực  hay các bộ 3 số thực, có trật tự, và thường được biểu diễn như là một  vectơ hình học  với  độ lớn  và  phương hướng . Mục lục 1 Định nghĩa 2 Ví dụ 3 Xem thêm 4 Tham khảo 5 Liên kết ngoài Định nghĩa [ sửa  |  sửa mã nguồn ] Giả sử F là một  trường  (có thể là trường  số thực  hay trường  số phức ). Các phần tử của F được gọi là  số vô hướng . Một không gian vectơ V định nghĩa trên trường F là một tập hợp V không rỗng mà trên đó hai phép cộng vectơ và phép nhân với số vô hướng được định nghĩa sao cho các  tính chất cơ bản  sau đây được thỏa mãn: Phép cộng vectơ có tính  kết hợp :\n Với mọi  u ,  v ,  w ∈ {\\displaystyle \\in } V , ta có  u  + ( v  +  w ) = ( u  +  v ) +  w . Phép cộng vectơ có tính  giao hoán :\n Với mọi  v ,  w ∈ {\\displaystyle \\in } V , ta có  v  +  w  =  w  +  v . Phép cộng vectơ có  phần tử trung hòa :\n Có một phần tử  0 ∈ {\\displaystyle \\in } V , gọi là  vectơ không , sao cho  v  +  0  =  v  với mọi  v ∈ {\\displaystyle \\in } V . Phép cộng vectơ có  phần tử đối :\n Với mọi  v  ∈ V, có một phần tử  w ∈ {\\displaystyle \\in } V , gọi là  phần ngược  của  v , sao cho  v  +  w  =  0 . Phép nhân vô hướng phân phối với phép cộng vectơ:\n Với mọi  a ∈ {\\displaystyle \\in } F  và  v ,  w ∈ {\\displaystyle \\in } V , ta có  a  ( v  +  w ) =  a v  +  a w . Phép nhân vô hướng phân phối với phép cộng vô hướng:\n Với mọi  a ,  b ∈ {\\displaystyle \\in } F  và  v ∈ {\\displaystyle \\in } V , ta có ( a  +  b )  v  =  a v  +  b v . Phép nhân vô hướng tương thích với phép nhân trong trường các số vô hướng:\n Với mọi  a ,  b ∈ {\\displaystyle \\in } F  và  v ∈ {\\displaystyle \\in } V , ta có  a  ( b v ) = ( ab )  v . Phần tử đơn vị của trường  F  có tính chất của phần tử đơn vị với phép nhân vô hướng: Với mọi  v ∈ {\\displaystyle \\in } V , ta có 1  v  =  v , 1 ký hiệu  đơn vị của phép nhân  trong  F . Với mọi x; y  ∈ {\\displaystyle \\in }  V, ta có x + y  ∈ {\\displaystyle \\in }  V Với mọi x  ∈ {\\displaystyle \\in }  V và a  ∈ {\\displaystyle \\in }  V, ta có a.x  ∈ {\\displaystyle \\in }  V Một cách chính xác, những tiên đề trên là cho một  module , do vậy không gian vectơ có thể được mô tả ngắn gọn là một \"module trên một trường\". Một không gian vectơ chỉ là một trường hợp đặc biệt của một module. Để ý rằng trong định đề thứ 7, nói rằng  a  ( b v ) = ( ab )  v , là không phải khẳng định về  tính kết hợp  của một toán tử, bởi vì có hai toán tử đang nói đến, nhân vô hướng:  b v ; và nhân trên trường số:  ab . Có người cho thêm hai tính chất  đóng  trong định nghĩa của không gian vectơ: V  đóng dưới phép cộng vectơ:\n Nếu  u ,  v ∈ {\\displaystyle \\in } V , thì  u  +  v ∈ {\\displaystyle \\in } V . V  đóng dưới phép nhân vô hướng:\n Nếu  a ∈ {\\displaystyle \\in } F ,  v ∈ {\\displaystyle \\in } V , thì  a v ∈ {\\displaystyle \\in } V . Tuy nhiên, nếu hiểu phép toán là  ánh xạ  trên  miền V  thì không cần thêm các tiên đề tính chất đóng trong định nghĩa không gian vectơ. Ví dụ [ sửa  |  sửa mã nguồn ] Không gian  R n {\\displaystyle \\mathbb {R} ^{n}} Không gian  M m n ( R ) {\\displaystyle M_{mn}(\\mathbb {R} )}  của các  ma trận  số thực kích thước (m,n) Không gian gồm tất cả các hàm  f : [ a , b ] → R {\\displaystyle f:[a,b]\\to \\mathbb {R} } Những thí dụ này cho thấy một \"không gian vectơ\" không nhất thiết gồm các \"vectơ\" như vẫn hiểu theo nghĩa phổ thông. Xem thêm [ sửa  |  sửa mã nguồn ] Không gian con Đại số tuyến tính Không gian metric Không gian định chuẩn Tham khảo [ sửa  |  sửa mã nguồn ] Liên kết ngoài [ sửa  |  sửa mã nguồn ] Wikimedia Commons có thư viện hình ảnh và phương tiện truyền tải về  Không gian vectơ Không gian vectơ  trong  hình học Afin Đại số vertơ  trong  hình học giải tích Không gian vectơ  trên  Từ điển bách khoa Việt Nam Các chủ đề chính trong  toán học Nền tảng toán học  |  Đại số  |  Giải tích  |  Hình học  |  Lý thuyết số  |  Toán học rời rạc  |  Toán học ứng dụng  | Toán học giải trí  |  Toán học tô pô  |  Xác suất thống kê Bài viết về chủ đề  toán học  này vẫn còn  sơ khai . Bạn có thể giúp Wikipedia bằng cách  mở rộng nội dung  để bài được hoàn chỉnh hơn. x t s \n\t\t\t\t\t\tLấy từ “ https://vi.wikipedia.org/w/index.php?title=Không_gian_vectơ&oldid=30780107 ”\t\t\t\t\t Thể loại :  Sơ khai toán học Đại số tuyến tính Khái niệm vật lý Lý thuyết nhóm Trình đơn chuyển hướng Công cụ cá nhân Chưa đăng nhập Thảo luận cho địa chỉ IP này Đóng góp Mở tài khoản Đăng nhập Không gian tên Bài viết Thảo luận Biến thể Các hiển thị Đọc Sửa đổi Sửa mã nguồn Xem lịch sử Khác Tìm kiếm Xem nhanh Trang Chính Bài viết chọn lọc Tin tức Bài viết ngẫu nhiên Thay đổi gần đây Phản hồi lỗi Quyên góp Tương tác Hướng dẫn Giới thiệu Wikipedia Cộng đồng Thảo luận chung Giúp sử dụng Liên lạc Công cụ Các liên kết đến đây Thay đổi liên quan Các trang đặc biệt Liên kết thường trực Thông tin trang Khoản mục Wikidata Trích dẫn trang này In/xuất ra Tạo một quyển sách Tải về dưới dạng PDF Bản để in ra Tại dự án khác Wikimedia Commons Ngôn ngữ khác Afrikaans العربية Bahasa Indonesia বাংলা Bân-lâm-gú Беларуская Bosanski Български Català Čeština Cymraeg Dansk Deutsch Eesti Ελληνικά English Español Esperanto Euskara فارسی Français Galego 한국어 Հայերեն हिन्दी Hrvatski Íslenska Italiano עברית Кыргызча ລາວ Latina Latviešu Lietuvių Lumbaart Magyar Македонски മലയാളം Nederlands 日本語 Norsk Norsk nynorsk Occitan ਪੰਜਾਬੀ Piemontèis Polski Português Română Русский Scots Sicilianu Simple English Slovenčina Slovenščina کوردی Српски / srpski Srpskohrvatski / српскохрватски Suomi Svenska Tagalog தமிழ் ไทย Türkçe Українська اردو Vèneto 文言 粵語 中文 Sửa liên kết  Trang này được sửa đổi lần cuối lúc 09:54 ngày 13 tháng 9 năm 2017. Văn bản được phát hành theo  Giấy phép Creative Commons Ghi công–Chia sẻ tương tự ; có thể áp dụng điều khoản bổ sung. Với việc sử dụng trang web này, bạn chấp nhận  Điều khoản Sử dụng  và  Quy định quyền riêng tư . \nWikipedia® là thương hiệu đã đăng ký của  Wikimedia Foundation, Inc. , một tổ chức phi lợi nhuận. Quy định quyền riêng tư Giới thiệu Wikipedia Lời phủ nhận Nhà phát triển Tuyên bố về cookie Phiên bản di động",
          "url": "https://vi.wikipedia.org/wiki/Kh%C3%B4ng_gian_vect%C6%A1",
          "relevance": "0"
        },
        {
          "title": "Bài 2:  Giới thiệu đồ họa máy tính",
          "content": "Please wait... search close TỰ HỌC LẬP TRÌNH search dieuninh1997 ttdn1997@gmail.com extension Kinh nghiệm - Chia sẻ C/C++ C++ cơ bản Đồ họa máy tính Java Java core Java Swing TUT Java Game đi cảnh C# phone_android Mobile Android IOS public Web cơ bản TUT PHP MVC Code thuần HTML CSS PHP public PHP Framework CodeIgniter Laravel public Web mã nguồn mở Wordpress NukeViet Database \n                © 2016  AdminBSB - Material Design .\n             Bài 2:  Giới thiệu đồ họa máy tính date_range 2017-02-25 -Đồ họa máy tính  là 1 ngành khoa học Tin học chuyên nghiên cứu về các phương pháp và kỹ thuật để có thể  mô tả và thao tác trên các đối tượng của thế giới thực bằng máy tính. Về bản chất: đó là 1 quá trình xây dựng và phát triển các công cụ trên cả 2 lĩnh vực phần cứng và phần mềm hỗ trợ cho các lập trình viên thiết kế các chương trình có khả năng đồ họa cao. Với việc mô tả dữ liệu thông qua các hình ảnh và màu sắc đa dạng của nó, các chương trình đồ họa thường thu hút người sử dụng bởi tính thân thiện, dễ dùng,… kích thích khả năng sang tạo và nâng cao năng suất làm việc. Do vậy, đồ họa máy tính được ứng dụng trong nhiều lĩnh vực: giáo dục, thương mại.. -Các kỹ thuật đồ họa: * Kỹ thuật đồ họa điểm :  ( ở bộ môn ĐHMT thì ta sẽ học kỹ thuật đồ họa này) Các hình ảnh được hiển thị thông qua từng Pixel ( từng mẫu rời rạc). Với kỹ thuật này, chúng ta có thể tạo ra, xóa hoặc thay đổi thuộc tính của từng Pixel của các đối tượng. Các hình ảnh được hiển thị như 1 lưới điểm rời rạc (grid), từng điểm đều có vị trí xác định được hiển thi với 1 giá trị nguyên biểu thị màu sắc hoặc độ sáng của điểm đó. Tập hợp tất cả các Pixel của grid tạo nên hình ảnh của đối tượng mà ta muốn biểu diễn. *Kỹ thuật đồ họa vector: Xây dựng mô hình hình học cho hình ảnh đối tượng, xác định các thuộc tính của mô hình hình học, sau đó dựa trên mô hình này để thực hiện quá trình tô trát để hiển thị từng điểm của mô hình,hình ảnh của đối tượng. Kỹ thuật này chỉ lưu trữ mô hình toán học của các thành phần trong mô hình hình học cùng với các thuộc tính tương ứng mà không cần lưu lại tòa bộ tất cả các Pixel của hình ảnh đối tượng. Vd: -Màn hình đồ họa: Mỗi máy tính đều có 1 CARD dùng để quản lý màn hình, gọi là Video Adapter hay Graphics Adapter. Có nhiều loại adapter như: CGA, MCGA, EGA, VGA. Các adapter có thể làm việc ở 2 chế độ: văn bản ( Text Mode) và đồ họa (Graphics Mode). -Biểu diễn tọa độ: -Vẽ điểm: Trong các hệ thống đồ họa, 1 điểm (Pixel) được biểu thị bởi các tọa độ bằng số (nguyên) Vd: trong mặt phẳng, một điểm là một cặp (x,y). Trong không gian 3 chiều, một điểm là bộ ba (x,y,z). Trên màn hình của máy tính, mỗi điểm là 1 vị trí trong vùng nhớ màn hình , dùng để lưu trữ các thông tin về độ sang của điểm tương ứng trên màn hình. Số điểm vẽ trên màn hình được gọi là độ phân giải của màn hình (320×200, 480×640, 1024×1024,..) Cảm ơn các bạn đã đọc bài viết . Mong bài viết giúp ích cho việc học tập của các bạn ! Bài 1: Cài đặt Devc++ và thêm thư viện đồ họa graphics/ winbgim Bài 3: Thuật toán vẽ đường thẳng DDA  Đồ họa máy tính  Bài 1: Cài đặt Devc++ và thêm thư viện đồ họa graphics/ winbgim Bài 2:  Giới thiệu đồ họa máy tính Bài 3: Thuật toán vẽ đường thẳng DDA Các hàm trong chế độ đồ họa (phần 1) Bài 4: Thuật toán vẽ đường thẳng Bresenham Bài 5: Thuật toán vẽ đường thẳng Midpoint Bài 6: Thuật toán vẽ đường Ellipse Bresenham Bài 7: Thuật toán vẽ đường Ellipse Midpoint Bài 8: Thuật toán tô màu Scanline Bài 9: Thuật toán tô màu loang Đăng ký nhận bài viết mới nhất qua email \n\t\t\t\t\t\t\tĐịa chỉ email\t\t\t\t\t\t Facebook Page",
          "url": "https://tuhoclaptrinh.vn/2017/02/25/bai-2-gioi-thieu-do-hoa-may-tinh/",
          "relevance": "0"
        },
        {
          "title": "Vector Và Ứng Dụng Của Chúng",
          "content": "    Đăng ký thành viên để sử dụng đầy đủ dịch vụ, thảo luận, học hỏi tại STDIO.  Học lập trình từ BÀI BẢN? Các khóa lập trình của STDIO Training luôn từ nền tảng & chuyên nghiệp. BUGS RULEK  Bạn chưa đăng nhập   Đăng nhập   Đăng ký   STDIO?   Bài viết   Đào tạo  Học lập trình từ BÀI BẢN? Các khóa lập trình của STDIO Training luôn từ nền tảng & chuyên nghiệp.   Đào tạo   Bài viết   STDIO?   Liên hệ home  :: Bài viết Nội dung bài viết     Xem trên di động\t\t\t\t\t\t\t   BOOKMARK\t\t\t\t\t\t\t #45 access_time \r\n\t\t\t\t\t\t\t\t17/09/2014 09:36\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\t\t visibility  13306\t\t\t\t\t\t\t Vector Và Ứng Dụng Của Chúng Toán học Vũ Quang Huy Vector là một khái niệm rất quen trong toán học; và chúng được ứng dụng trong rất nhiều lĩnh vực khác nhau: đồ họa máy tính, mô phỏng vật lý … Bài viết này muốn giới thiệu đến bạn đọc khái niệm về vector và một số phép toán phổ biến với vector như: cộng, trừ, nhân … cùng với vai trò của chúng. Giới thiệu Vector là một khái niệm rất quen trong toán học; và chúng được ứng dụng trong rất nhiều lĩnh vực khác nhau: đồ họa máy tính, mô phỏng vật lý … Bài viết này muốn giới thiệu đến bạn đọc khái niệm về vector và một số phép toán phổ biến với vector như: cộng, trừ, nhân … cùng với vai trò của chúng.  Tiền đề bài viết Khi làm việc cũng như giảng dạy về lập trình 3D, tôi nhận thấy vector là một trong những kiến thức cực kì quan trọng đối với một lập trình viên chuyên về độ họa – đặc biệt là vector dùng trong không gian 3 chiều. Vì thế bài viết này ra đời với mong muốn làm rõ một số vấn đề về vector và thao tác với chúng trong không gian 3 chiều. Bài viết này dựa trên kinh nghiệm làm việc thực tế của tôi với sự góp ý của anh  La Kiến Vinh . Đối tượng hướng đến Bài viết này hướng đến bạn đọc quan tâm đến các vấn đề về toán học, đặc biệt là vector. Bài viết không yêu cầu bạn đọc phải có kiến thức về vector mà chỉ cần một số kiến thức toán học cơ bản. Vector là gì? Trong toán học, người ta định nghĩa vector như sau: Vector là một đại lượng biểu diễn cho cả độ lớn và hướng. Ví dụ như để biểu diễn một lực nào đó tác dụng lên vật, ta có một vector gồm có 2 thành phần – độ lớn lực tác động lên vật đó và hướng tác động. Hay ta có thể dùng vector để biểu diễn vận tốc – tốc độ và hướng. Ngoài ra, ta còn có dạng vector thuần chỉ hướng. Đơn cử như việc diễn tả hướng nhìn của một camera trong không gian, hay ta muốn ám chỉ đến hướng mà ánh sáng di chuyển trong không gian. Ta có thể dễ dàng hình dung một vector trong không gian như thế nào. Tuy nhiên trong không gian hình học, ta không thể “hình dung” như thế. Vì vậy yêu cầu đặt ra là cần phải có một cách biểu diễn vector trong không gian hình học. Biểu diễn vector trong không gian Trong không gian, một vector được xác định bằng một đoạn thẳng với các tính chất như sau: Độ dài đại diện cho độ lớn của vector Hướng của vector (cả phương và chiều) Cần lưu ý là trong hầu hết các trường hợp ta không đặt nặng vấn đề vị trí của vector vì khi thay đổi vị trí của vector thì độ lớn và phương của chúng vẫn được giữ nguyên (2 tính chất để hình thành nên một vector). Chính vì tính chất này, do đó 2 vector bằng nhau khi và chỉ khi chúng có cùng độ lớn và cùng chỉ tới một phương xác định – như trong hình bên dưới ta có thể thấy được vector  s  và  t  là bằng nhau cho dù ta có thay đổi vị trí đặt của 2 vector thế nào đi nữa. \n  \nVector và hệ trục tọa độ Với những nội dung đã trình bày ở trên, ta đã có thể ứng dụng một số phương pháp hình học để tính toán với vector, điển hình như: Tuy nhiên, trong toán học đại số hay gần gũi hơn là trong hệ thống máy tính, ta phải tìm một cách nào giúp “hiện thực” các khái niệm vector và từ đó chúng ta có thể dùng các phương pháp số học mà tính toán các vector này.  Và nhờ các hệ trục tọa độ khác nhau – mà cụ thể trong bài viết này tôi muốn đề cập đến  hệ trục không gian 3 chiều – 3D coordinate system , ta có thể áp dụng các phương pháp số học để thao tác với vector. Trong hệ trục tọa độ không gian 3 chiều, một vector được biểu diễn bởi 3 giá trị x, y và z. Giả sử ta có điểm \nvà điểm trong không gian, để tính được vector AB ta áp dụng công thức sau: Một số phép toán cơ bản với vector Cộng, trừ hai vector và nhân vector với một số Giả sử ta có 2 vector \nvà vậy tổng của 2 vector này được tính theo công thức sau: \nTương tự như vậy, hiệu của 2 vector được tính với công thức: Lưu ý là tính chất giao hoán được áp dụng cho việc tính tổng 2 vector. Để dễ hình dung hơn, ta có thể lấy minh họa 1 trái bóng được tác dụng bởi các lực khác nhau. Nếu 2 lực này cùng hướng ta sẽ tìm được tổng lực bằng phép cộng hai vector, ngược lại nếu 2 lực tác động này ngược hướng nhau, ta sẽ tính được lực cuối cùng áp dụng lên trái bóng bằng cách trừ 2 vector lực với nhau \n  \nNgoài ra, ta còn có thể nhân vector với một số thực. Khi đó ta sẽ được một vector tỷ lệ. Độ lớn của vector và vector đơn vị Ta có độ lớn của vector \nký hiệu là được tính bởi công thức sau: Tuy nhiên, trong  một số trường hợp ta chỉ quan tâm đến hướng của vector đó mà bỏ qua độ lớn của nó . Với trường hợp như vậy, ta mong muốn độ lớn của vector đó bằng 1 – những vector có tính chất như trên ta gọi chúng là  vector đơn vị . Song song đó, việc chuyển một vector về thành vector đơn vị ta gọi là  chuẩn hóa  vector  (normalize vector) . Ta chuẩn hóa một vector thành vector đơn vị kí hiệu là  như sau: Tích vô hướng hai vector Giả sử ta có 2 vector \nvà t ích vô hướng của hai vector này (hay còn gọi là  dot product  vì được biểu diến bằng một dấu chấm)  được tính bởi công thức sau: Bằng việc áp dụng các phương pháp hình học, ta chứng minh được mối quan hệ giữa tích vô hướng và góc giữa các vector này như sau: Dựa theo công thức trên, ta có thể suy ra một số đặc điểm của 2 vector khi có tích vô hướng của chúng: Nếu   : 2 vector vuông góc với nhau. Nếu   : góc giữa 2 vector nhỏ hơn 90 độ. Nếu  : góc giữa 2 vector lớn hơn 90 độ. Tích có hướng hai vector Nếu như tích vô hướng cho ta biết được một số tính chất của 2 vector thì  tích có hướng  giúp ta tìm ra được  vector pháp tuyến của mặt phẳng được tạo bởi 2 vector cho trước . Giả sử ta có 2 vector và tích có hướng giữa chúng được tính bằng công thức sau: Cần lưu ý thêm là  hướng của vector này phụ thuộc vào hệ trục đang được sử dụng  (bàn tay trái – left handed hay bàn tay phải – right handed) Lời kết Vector là một trong những kiến thức cốt lõi của lập trình đồ họa cả 2D và 3D, nhờ có vector ta có thể dễ dàng hiện thực nên một không gian như ý mình muốn. Có thể nói nếu không có sự hiện diện của vector thì gần như ngành đồ họa máy tính, cũng như các ứng dụng thực tiễn liên quan đến đồ họa ngày nay đã không ra đời. Qua bài viết này, hy vọng có thể giúp cho bạn đọc hiểu sâu hơn về sự ra đời của vector cũng như một số các phép toán thông dụng trên vector. Tham khảo [1] Introduction to 3D Game Programming with DirectX 11 – Frank Luna (Author) – ISBN: 9781936420223 – Pub Date: January 2012 – Pages: 600 attachment Tags vector math   Về tác giả Vũ Quang Huy \r\n                        Programmer Trainer                     Tìm hiểu tôi Là một người yêu công nghệ, luôn tìm hiểu và đào sâu nghiên cứu về các kiến thức và công nghệ nền tảng - nhất là các công nghệ liên quan đến lĩnh vực đồ hoạ phần cứng lẫn phần mềm. Nắm vững các lý thuyết đồ hoạ trong không gian 3 chiều và sử dụng ở mức chuyên sâu các thư viện đồ họa OpenGL, Direct3D ... Ngoài ra có thể lập trình với GPU qua các công nghệ như CUDA, OpenCL ... Ngoài công việc về lập trình cho máy tính và thiết bị di động, lập trình nhúng và chế tạo robot là một sở thích khác của tôi. \r\n        Học lập trình C++ từ nền tảng đến chuyên sâu tại STDIO Training \r\n        Chọn khác biệt? Chọn STDIO Training.\r\n     NỀN TẢNG QUYẾT ĐỊNH TẤT CẢ  forward   THẢO LUẬN   Bài cùng loại \r\n                Cơ Bản Về Ma Trận  Và Các Phép Biến Đổi Sơ Cấp Trên Dòng Của Một Ma Trận  keyboard_arrow_right \r\n                Các Phép Đếm Cơ Bản  keyboard_arrow_right \r\n                Tính Căn Bậc 2 Theo Phương Pháp Newton-Raphson  keyboard_arrow_right \r\n                Chuyển Đổi Số Thực Sang Nhị Phân Theo Chuẩn IEEE 754  keyboard_arrow_right  \r\n\t\t\tLập trình như tự đá vào mặt mình, sớm hay muộn mũi bạn cũng sẽ chảy máu.\t\t\t Jim McCarthy  Tham gia STDIO \r\n\t\t\tĐăng ký thành viên để sử dụng đầy đủ dịch vụ, thảo luận, học hỏi tại STDIO.\t\t \r\n        Học lập trình C/C++ tại STDIO Training\r\n     \r\n        Bạn muốn học lập trình C/C++ từ đầu, bài bản đến chuyên sâu?\r\n     \r\n        Tìm hiểu & Đăng ký\r\n     Quét mã để đọc trên điện thoại. ĐÓNG Trang chính Bài Viết Nhật Ký STDIO? Liên Hệ Sản phẩm & Dịch vụ Đào Tạo Bugs Rulek STDIO Starter Kit v3 Cộng đồng STDIO Fanpage STDIO Tube Chính Sách Bảo Mật Chính Sách Hoạt Động ©STDIO, 2013-2017",
          "url": "https://www.stdio.vn/articles/read/45/vector-va-ung-dung-cua-chung",
          "relevance": "0"
        },
        {
          "title": "Chương 1: Bài tập khởi động",
          "content": "Skip to content \n                  Features\n \n                  Business\n \n                  Explore\n \n                      Marketplace\n \n                  Pricing\n This repository Sign in or Sign up \n    Watch\n   \n    21\n   \n    Star\n   \n      62\n     \n        Fork\n       \n      47\n     minhpqn / nlp_100_drill_exercises Code Issues 0 Pull requests 0 \n      Projects\n       0 \n    Insights\n \n              Dismiss\n             Join GitHub today GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together. Sign up \n            100 bài luyện tập xử lý ngôn ngữ tự nhiên\n           \n                63\n               \n              commits\n           \n              1\n             \n            branch\n           \n              0\n             \n            releases\n           \n      1\n     \n    contributor\n Python 99.1% Shell 0.9% Python Shell \n    Clone or download\n     \n          Clone with HTTPS\n           \n          Use Git or checkout with SVN using the web URL.\n         \n  Download ZIP\n \n      Find file\n     Branch: master Switch branches/tags Branches Tags \n                master\n               Nothing to show Nothing to show \n          New pull request\n         \n      Latest commit\n       \n        219240a\n       Nov 16, 2016 minhpqn Add data Permalink Failed to load latest commit information. data Add data Nov 16, 2016 python Revised Sep 22, 2016 Learning_note.md  version Feb 17, 2016 README.md Revised Sep 22, 2016 environment.yml Revised Sep 22, 2016 markdown_toclify.py  version Feb 17, 2016 tohoku_100_exercises_draft.md Revised Sep 22, 2016 \n      README.md\n     Table of Contents Chương 1: Bài tập khởi động Chương 2: Các lệnh cơ bản trên môi trường Unix Chương 3: Biểu thức chính quy (regular expressions) Chương 4: Morphological Analysis trong tiếng Nhật (形態素解析) Chương 5: Dependency parsing (係り受け解析) Chương 6: Xử lý văn bản tiếng Anh Chương 7: Database Chương 8: Machine Learning Chương 9: Không gian vector (I) Chương 10: Không gian vector (II) Phụ lục: Corpus, data sử dụng trong 100 bài luyện tập NLP 100 bài luyện tập xử lý ngôn ngữ tự nhiên Dịch từ tài liệu  言語処理100本ノック  của lab Inui-Okazaki, đại học Tohoku, Nhật Bản. Người dịch: Phạm Quang Nhật Minh\n(minhpqn). Tham khảo thêm phiên bản cũ của tài liệu tại  NLP 100 Drill\nExercises Chú ý: Khi sử dụng tài liệu 100 bài luyện tập xử lý ngôn ngữ tự nhiên, cần trích dẫn các nguồn sau: Tài liệu \"言語処理100本ノック\" của lab Inui-Okazaki, đại\nhọc Tohoku, Nhật Bản. URL:  http://www.cl.ecei.tohoku.ac.jp/nlp100 Đường link tới bản dịch hiện tại:  https://github.com/minhpqn/nlp_100_drill_exercises . Người dịch: Phạm Quang Nhật Minh. Chương 1: Bài tập khởi động 00. Đảo ngược xâu ký tự Hãy đảo ngược xâu ký tự \"stressed\" (theo thứ tự từ cuối xâu đến đầu xâu ký tự). 01. Trích xuất ký tự từ xâu ký tự Từ xâu ký tự \"MPyaktQrBoilk RCSahr\", hãy trích xuất các ký tự ở vị trí\n2,4,6,8,10,12,14,16,18,20 và kết hợp theo thứ tự đó để tạo thành 1 xâu ký tự mới\n(ký tự space cũng được tính, các ký tự được đánh số từ 1). 02. Kết hợp hai xâu ký tự Hãy kết hợp hai xâu ký tự \"Partrol\" và \"Car\" để tạo thành xâu mới \"PatrolCar\". 03. Tokenize và thống kê số lượng ký tự của mỗi từ Tokenize câu sau: \"Now I need a drink, alcoholic of course, after the heavy\nlectures involving quantum mechanics.\" Đưa ra danh sách gồm số ký tự alphabet trong mỗi từ theo thứ tự xuất hiện\ncủa từ đó trong câu. 04. Ký tự thành phần Tokenize câu sau: \"Hi He Lied Because Boron Could Not Oxidize Fluorine. New\nNations Might Also Sign Peace Security Clause. Arthur King Can.\" Lấy ra ký tự đầu tiên của các từ ở vị trí 1, 5, 6, 7, 8, 9, 15, 16, 19; với\ncác từ còn lại lấy ra 2 ký tự đầu tiên. Tạo ra một map từ các xâu ký tự được\ntrích ra tới vị trí của từ trong câu. 05. n-gram Viết hàm sinh ra tất cả các n-gram từ một dãy cho trước (xâu ký tự hoặc danh\nsách). Sử dụng hàm đã viết, sinh ra word bi-gram và character bi-gram từ câu sau:\n\"I am an NLPer\" 06. Tập hợp Sinh ra tập X và Y tương ứng là tập các character bi-gram từ hai xâu ký tự\n\"paraparaparadise\" và \"paragraph\". Sinh ra các tập hợp union, intersection và difference của X và Y Kiểm tra xem bi-gram 'se' có thuộc tập X (Y) hay không? 07. Sinh ra câu từ template Viết hàm số nhận vào 3 biến x, y, z và trả về xâu ký tự \"y vào lúc x giờ là z\"\nSinh ra kết quả với các giá trị x, y, z sau đây x=\"12\" y=\"Nhiệt độ\" z=22.4 08. Xâu mật mã Từ các ký tự của một xâu cho trước, cài đặt hàm có tên cipher để mã hoá xâu như\nsau: Nếu là ký tự tiếng Anh ở dạng thường (lower-case characters) thì chuyển\nthành ký tự có mã là (219 - mã ký tự). Các ký tự khác giữ nguyên. Sử dụng hàm đã viết để mã hoá và giải mã các xâu ký tự tiếng Anh. 09.  Typoglycemia Cho đầu vào là một câu tiếng Anh bao gồm các word ngăn cách nhau bằng ký tự\nspace. Viết chương trình thực hiện việc sau: Với mỗi word, giữ nguyên ký tự\nđầu và ký tự cuối, đảo thứ tự một cách ngẫu nhiên các ký tự còn lại của (tất\nnhiên các word có ít hơn 4 ký tự thì không cần làm gì) Cho trước một câu tiếng\nAnh hợp lệ, ví dụ \"I couldn't believe that I could actually understand what I\nwas reading : the phenomenal power of the human mind .\", chạy chương trình đã\nviết để đưa ra kết quả. Chương 2: Các lệnh cơ bản trên môi trường Unix Các bài tập trong chương này sử dụng dữ liệu trong file\n hightemp.txt . File\nnày bao gồm các bản ghi nhiệt độ cao nhất ở Nhật Bản. Mỗi bản ghi bao gồm 3 cột\n\"Tỉnh\", \"Địa điểm\", \"Nhiệt độ\" (độ C), và ngày tháng đo. Các cột dữ liệu được\nphân cách bằng ký tự tab. Viết các chương trình thực hiện các nhiệm vụ trong các\nbài tập dưới đây. Sau đó, chỉ sử dụng các lệnh trong unix để thực hiện các nhiệm\nvụ đó và xác nhận xem kết quả của chương trình bạn viết có giống với kết quả các\nlệnh unix đưa ra hay không. 10. Đếm số dòng trong file Đếm số dòng trong file. Xác nhận kết quả bằng lệnh wc trong unix. 11. Biến đổi các ký tự tab thành space Chuyễn mỗi ký tự tab thành ký tự space. Xác nhận kết quả lần lượt bằng các lệnh\nsed, tr, và expand. 12. Lưu cột 1 vào file col1.txt, cột 2 vào file col2.txt Trích xuất nội dung trong cột 1, cột 2 và lưu vào các file tương ứng: col1.txt\nvà col2.txt. Thử thực hiện công việc chỉ dùng lệnh cut trong unix. 13. Trộn hai file col1.txt và col2.txt Kết hợp nội dung trong 2 file col1.txt và col2.txt để tạo thành một file mới có\nnội dung giống với cột 1 và cột 2 trong file ban đầu (các cột cách nhau bởi ký\ntự tab). Sử dụng lệnh paste để thực hiện bài tập và xác nhận kết quả của chương\ntrình bạn viết. 14. Trích xuất ra N hàng đầu tiên của file Viết chương trình trích xuất ra N hàng đầu tiên của file. Biến số dạng dòng lệnh\nlà số tự nhiên N. Sử dụng lệnh head trong unix để thực hiện công việc. 15. Trích xuất ra N hàng cuối cùng của file Viết chương trình trích xuất ra N hàng cuối cùng của file. Chương trình nhận đầu\nvào từ dòng lệnh là số tự nhiên N. Sử dụng lệnh tail trong unix để thực hiện\ncông việc. 16. Chia file thành N phần Chia file thành các files nhỏ với N lines mỗi file (đơn vị là các hàng trong\nfile). Chương trình nhận đầu vào từ dòng lệnh là số tự nhiên N. Sử dụng lệnh\nsplit để thực hiện công việc (split -l N). Sau đó, cải tiến chương trình để chia file thành thành N phần bằng nhau (thay vì\nN lines mỗi file). 17. Đưa ra các các xâu ký tự duy nhất (unique) trong cột 1 Đưa ra các xâu ký tự duy nhất trong cột 1 của file. Sử dụng lệnh cut, sort, uniq\nđể thực hiện nhiệm vụ. 18. Sắp xếp các hàng theo thứ tự giảm dần của giá trị (numeric value) của cột thứ 3 Viết chương trình thực hiện nhiệm vụ trên. Dùng lệnh sort để xác nhận (trong bài\ntập này, kết quả của chương trình của bạn với lệnh sort có thể khác nhau do có\nthể có các giá trị giống nhau trong cột thứ 3). 19. Sắp xếp theo tần suất xuất hiện Đưa ra tần suất xuất hiện của các giá trị trong cột 1; sắp xếp các giá trị trong\ncột 1 theo thứ tự từ cao đến thấp của tần suất xuất hiện. Chỉ dùng lệnh cut,\nuniq, sort để thực hiện nhiệm vụ. Chương 3: Biểu thức chính quy (regular expressions) Bài tập trong chương 3 sử dụng file\n jawiki-country.json.gz .\nFile này lưu trữ các tài liệu Wikipedia và có định dạng sau đây. - Dòng thứ nhất\nlưu trữ thông tin về tài liệu dưới định dạng JSON. - Ở các dòng tiếp theo, dòng\ntiêu đề của văn bản được lưu trữ tại khoá \"title\"; nội dung của tài liệu được\nlưu trữ tại khoá \"text\". Các dòng này được lưu trữ ở định dạng JSON. Lập trình để xử lý các việc sau đây. 20. Đọc vào dữ liệu JSON Đọc dữ liệu từ file JSON chứa các tài liệu Wikipedia, trích xuất & hiển thị nội\ndung của tài liệu (trường text của JSON object) có liên quan đến \"イギリス\" (có\nnghĩa là nước Anh). Sử dụng các nội dung của tài liệu được trích xuất này để\nthực thi các nhiệm vụ trong các bài tập từ 21-29. 21. Trích xuất các dòng có chứa tên đề mục Trong các tài liệu, trích xuất các dòng có chứa tên đề mục (category name hay\nカテゴリ名). 22. Trích xuất các tên đề mục (Category name) Trích xuất tên đề mục của trong các tài liệu. Trong bài tập này, cần trích xuất\nchính xác các tên đề mục chứ không phải dòng chứa tên đề mục. 23. Cấu trúc của các Section Hiển thị tên của các section và level của các section trong các tài liệu\nWikipedia (Ví dụ với section == Section Name ==\" thì level bằng 1) 24. Trích xuất các liên kết file Trích xuất toàn bộ các liên kết đến các media files trong tài liệu. 25. Trích xuất templates Trích xuất vị trí và tên các folder có template \"基礎情報\" trong tài liệu. Lưu\nkết quả trong các đối tượng dictionary. Tham khảo về templates tại\n đây . 26. Loại bỏ các emphasis markups Trong khi làm các xử lý ở bài tập 25, xoá các emphasis markup (italic, bold,\nboth) từ vị trí của các templates và biến đổi thành plain text. (Tham khảo về\ncác loại markup tại  Wiki\nmarkup , bảng tham khảo bằng\ntiếng Nhật tại\n マークアップ早見表 ). 27. Xoá các link đến các trang Wikipedia khác Nhiệm vụ giống như bài 26 và thêm vào xử lý sau. Xoá các markups của liên kết\nđến các trang Wikipedia khác từ các templates được trích xuất và biến đổi thành\ndạng text. (Tham khảo về các loại markup tại  Wiki\nmarkup , bảng tham khảo bằng\ntiếng Nhật tại\n マークアップ早見表 ). 28. Xoá các markup trong văn bản Thêm vào xử lý ở bài 27. Xoá các markup trong các templates càng nhiều càng tốt\nvà in ra các thông tin cơ bản về quốc gia. 29. Lấy ra các URL của quốc kỳ Sử dụng nội dung của các template và lấy ra URl đến quốc kỳ (国旗画像のURL).\nHint: Gọi API  imageinfo  của\n MediaWWiki , biến đổi các file\nreferences thành URL. Chương 4: Morphological Analysis trong tiếng Nhật (形態素解析) Download file  neko.txt \nlà nội dung bằng plain text của cuốn tiểu thuyết \"吾輩は猫である\" của nhà văn\n夏目漱石 (Soseki Natsume). Sử dụng package  MeCab  để thực hiện\n\"morphological analysis\" (Tham khảo trang tiếng Nhật tại\n đây ).\nLưu kết quả vào file neko.txt.mecab. Sử dụng file kết quả để thực hiện các công\nviệc ở các bài tập dưới đây. Chú ý: Trong các bài tập 37, 38, 39 có thể sử dụng các phần mềm\n matplitlib  hoặc\n Gnuplot . @All: Với các bạn không quen xử lý text tiếng Nhật, có thể dùng một file text\ntiếng Anh và thực hiện POS tagging, sau đó làm các bài tập dưới đây. 30. Đọc vào kết quả morphological analysis Viết chương trình đọc vào kết quả morphological analysis (file neko.txt.mecab). Yêu cầu: Với mỗi morpheme, lưu các thông tin: 表層形 (surface form), 基本形\n(base form), 品詞 (pos), 品詞細分類1 (pos1) bằng cấu trúc dữ liệu hash map với\ncác key tương ứng là: surface, base, pos, pos1. Lưu trữ mỗi câu bằng danh sách\ncủa các morpheme. Trong các bài tập còn lại trong chương 4, hãy sử dụng cách tổ\nchức dữ liệu trong bài 30 này. 31. Động từ Trích xuất tất cả các surface forms của động từ (pos=動詞). 32. Dạng nguyên thể của động từ (動詞の原形) Trích xuất tất cả dạng nguyên thể của động từ (base form). 33. Danh từ dạng サ (サ変名詞) Trích xuất toàn bộ các danh từ dạng サ (サ変名詞). Tham khảo trang Wikipedia\ntiếng Nhật về\n サ行変格活用 . 34. 「AのB」 Trích xuất tất cả các danh từ ghép (compound nouns) gồm 2 danh từ kết nối bằng\nの. 35. Trích xuất các kết nối danh từ (noun connections hay 名詞の連接) Trích xuất tất cả các noun connections (các danh từ đứng cạnh nhau liên tiếp).\nKhi trích xuất, chú ý trích xuất chuỗi danh từ matching dài nhất có thể. Ví dụ\nABC trong đó A, B, C là danh từ thì phải trích xuất ABC thay vì AB. 36. Tần suất xuất hiện của từ Lập trình tính tần suất xuất hiện của từ trong văn bản. Đưa ra các từ theo thứ\ntự giảm dần của tần suất xuất hiện. 37. Top 10 từ xuất hiện nhiều nhất Vẽ đồ thị (ví dụ bar graph) của tần suất xuất hiện của 10 từ xuất hiện nhiều\nnhất trong văn bản. 38. Histogram Vẽ đồ thị histogram tần suất xuất hiện của các từ. Trục ngang là tần suất xuất\nhiện. Trục dọc là các từ. 39. Luật Zipf Vẽ đồ thị với trục ngang là rank của các từ theo tần suất xuất hiện (cao đến\nthấp), trục dọc là tần suất xuất hiện của các từ. Chương 5: Dependency parsing (係り受け解析) Thực hiện phân tích cấu trúc ngữ pháp (dependency parsing) bằng công cụ\n CaboCha  cho file\n neko.txt  và lưu kết\nquả vào file neko.txt.cabocha. Sử dụng file kết quả này làm đầu vào cho các bài\ntập dưới đây. 40. Đọc vào kết quả dependency parsing (theo morphemes) Cài đặt lớp Morph cho các morphemes. Lớp này có các biến thành phần (member\nvariables) là surface (cho surface forms của morphems), base (cho base form),\npos (cho POS tag), pos1 (cho detailed POS tag 1 品詞細分類1). Sau đó đọc vào kết\nquả phân tích dependency parsing trong file neko.txt.cabocha. Mỗi câu sẽ bao gồm\nmột danh sách các Morph objects. Hiển thị danh sách các morphemes cho câu thứ 3\ntrong văn bản. 41. Đọc vào kết quả dependency parsing (theo chunks và depedency relations) Tiếp theo bài 40, cài đặt lớp Chunk để lưu trữ các chunk (hay bunsetsu (文節)).\nLớp này có các biến thành phần là: morphs (để lưu trữ danh sách các Morph\nobjects) dst để lưu trữ index của chunk mà chunk hiện tại trỏ đến (chunk đích - destination) srcs để lưu trữ danh sách các indexes của các chunk trỏ đến chunk hiện tại. Sau đó, đọc vào kết quả dependency parsing. Mỗi câu sẽ bao gồm danh sách của các\nChunk objects. Hiển thị xâu ký tự và giá trị của biến dst của các chunk trong câu\nthứ 8 của file đầu vào. Các bài tập còn lại trong chương 5 sẽ sử dụng các chương trình được tạo ra\ntrong bài tập 40 và 41. 42. Hiển thị chunk nguồn (head) và chunk đích (modifier) trong các depedency relations Hiển thị nội dung (dạng text) các chunk nguồn (head) và chunk đích (modifier)\ntrên mỗi dòng và cách nhau bởi ký tự tab. Chú ý không hiển thị các dấu\n(punctuation marks) trong các chunk. 43. Trích xuất các dependency relations giữa các chunk chứa danh từ và các chunk chứa động từ Trích xuất các dependency relations giữa các chunk chứa danh từ và các chunk\nchứa động từ. In ra theo định dạng tab. Tương tự như bài 42, không hiển thị các\ndấu (punctuation marks) trong các chunk. 44. Visualize cây dependency Visualize dependency trees của một câu cho trước. Khi visualize, biến các\ndependency trees theo  ngôn ngữ\nDOT , hay\nsử dụng  Graphviz . Nếu sử dụng Python, có thể hiển\nthị đồ thị có hướng bằng thư viện/package\n pydot . 45. Trích xuất các verb case patterns Yêu cầu của bài tập này là tìm hiểu (investigate) về case frame trong tiếng Nhật\nsử dụng dữ liệu trong file đầu vào neko.txt. Coi các động từ là vị ngữ\n(predicate), các trợ từ (như が,を,...) của chunk liên với với động từ là các case, hãy\nin ra các động từ và các \"case\" theo định dạng cách nhau bởi ký tự tab. Output\ncủa chương trình cần thoả mãn các điều kiện sau: Các vị ngữ là dạng nguyên thể\n(base form) của động từ, tính từ bên trái nhất (lef-most) của các chunk\n(bunsetsu). Coi các trợ từ liên kết với các vị ngữ là các \"case\" trong case\nframe. Nếu một vị ngữ được liên kết bởi nhiều trợ từ, in tất cả các trợ từ\ntheo thứ tự từ điển. Các trợ từ cách nhau bởi dấu cách Xem xét ví dụ sau: 「吾輩はここで始めて人間というものを見た」(câu thứ 8 trong\nfile neko.txt.cabocha). Câu này gồm hai động từ 始める và 見る. Nếu trong kết\nquả phân tích cú pháp, động từ 始める liên kết với chunk ここで, động từ 見る\nliên kết với với chunk 吾輩は và ものを, chương trình sẽ in ra: 始める で\n見る は を\n Lưu output của chương trình ra file, xác nhận các mục sau chỉ với các lệnh của Unix. Kết hợp các động từ xuất hiện trong corpus và các case patterns Các case patterns của các động từ する, 見る, 与える (theo thứ tự từ cao đến thấp của tần suất xuất hiện trong corpus). Tham khảo về case frame structures trong tiếng Nhật tại\n đây  (Kawahara et al., 2010). 46. Trích xuất thông tin của case frames của các động từ Chỉnh sửa bài tập 45, trích xuất thêm các chunks mà các vị ngữ (predicate) liên\nkết tới. In ra theo định dạng tab. Ngoài các điều kiện đưa ra ở bài tập 45,\noutput phải thoả mãn các điều kiện sau. In ra các chunk (bunsetsu) ở dạng dãy các từ (không cần phải xoá đuôi và các\ntrợ từ). Trong trường hợp một predicates liên kết với nhiều chunk (bunsetsu), in ra\ncác chunk này theo cùng thứ tự với các trợ từ. Dùng ký tự space giữa các\nchunk. Xem xét ví dụ sau: 「吾輩はここで始めて人間というものを見た」(câu thứ 8 trong\nfile neko.txt.cabocha). Câu này gồm hai động từ 始める và 見る. Nếu trong kết\nquả phân tích cú pháp, động từ 始める liên kết với chunk ここで, động từ 見る\nliên kết với với chunk 吾輩は và ものを, chương trình sẽ in ra: 始める で ここで\n見る は を 吾輩 ものを\n 47. Mining các cấu trúc câu có động từ chức năng (cấu trúc này có tên tiếng Nhật là 機能動詞構文) Bài tập này tập trung vào các case frame を của các động từ, trong đó động từ có dạng liên kết サ変接続名詞. Cải tiến bài tập 46 để thoả mãn các yêu cầu sau đây. Bài tập này tập trung vào các trường hợp một bunsetsu có dạng sau đây liên kết với động từ. 「サ変接続名詞+を（助詞）」 Biến đổi các vị ngữ (predicates) về dạng 「サ変接続名詞+を+動詞の基本形」.\nNếu trong 1 chunk có nhiều động từ, sử dụng động từ bên trái nhất. Trong trường hợp một vĩ ngữ có liên kết với nhiều trợ từ (chunk), in tất cả\ncác trợ từ này theo thứ tự từ điển. Các trợ từ cách nhau bởi dấu cách. Trong trường hợp có nhiều chunks liên kết với một vị ngữ (predicate), in tất\ncả các chunk này đồng nhất với thứ tự in của các trợ từ mà nó bao gồm. Các\nchunk được cách nhau bởi ký tự space. Ví dụ, cho câu sau. 「別段くるにも及ばんさと、主人は手紙に返事をする。」. Chương\ntrình sẽ in ra kết quả sau. 返事をする と に は 及ばんさと 手紙に 主人は\n Lưu kết quả của bài tập 47 ra file, chỉ sử dụng lệnh unix để xác nhận: Các vị ngữ (predicates) xuất hiện trong file. Các vị ngữ và các case patterns 48. Trích xuất ra dependency path từ các danh từ đến gốc Chương trình yêu cầu trích xuất ra depedency path từ các chunk có chứa\ndanh từ đến root của cây depedency. Các dependency path phải\nthoả mãn yêu cầu sau đây. Biểu diễn các chunk (bunsetsu) dưới dạng chuỗi của các morpheme (surface\nform) Biểu diễn liên kết giữa các bunsetsu bằng ký tự mũi tên  -> . Ví dụ, đầu ra cho câu ví dụ 「吾輩はここで始めて人間というものを見た」(câu thứ 8\ntrong file neko.txt.cabocha) như sau: 吾輩は -> 見た\nここで -> 始めて -> 人間という -> ものを -> 見た\n人間という -> ものを -> 見た\nものを -> 見た\n 49. Trích xuất ra chuỗi liên kết giữa các danh từ Trích xuất dependency path ngắn nhất liên kết giữa các noun chunk. Đối với cặp\nnoun chunk với index tương ứng là  i  và  j  ( i  <  j ), các dependency paths\nthoả mãn các yêu cầu sau. Giống như bài 48, biểu diễn liên kết giữa các\nbunsetsu bằng ký tự mũi tên (->). Thay các noun chunk  i , và  j  tương ứng\nthành X và Y. Thêm nữa, các dependency path trong bài tập này có thể được diễn dịch như sau. Trên đường đi của noun chunk  i  tới gốc của cây, nếu tồn tại noun chunk  j :\ntrích xuất dependency path giữa noun chunk  i  và noun chunk  j . Ngoài trường hợp nói trên, nếu đường đi của noun chunk  i  và noun chunk  j  tới gốc của cây cắt nhau ở bunsetsu  k : In ra đường đi từ  i  tới bunsetsu ngay trước  k  và\nđường đi từ bunsetsu  j  tới bunsetsu ngay trước  k . Biểu diễn liên kết với\nbunsetsu  k  bằng ký tự |. Ví dụ, kết quả đưa ra cho câu ví dụ\n「吾輩はここで始めて人間というものを見た」(câu thứ 8 trong file\nneko.txt.cabocha) như sau: Xは | Yで -> 始めて -> 人間という -> ものを | 見た\nXは | Yという -> ものを | 見た\nXは | Yを | 見た\nXで -> 始めて -> Y\nXで -> 始めて -> 人間という -> Yを\nXという -> Y\n Chương 6: Xử lý văn bản tiếng Anh Cài đặt các chương trình xử lý văn bản tiếng Anh\n( nlp.txt ). 50. Tách câu (Sentence segmentation) Sử dụng patterns (. or ; or : or ? or !)  ->  ký tự space  ->  chữ cái tiếng Anh\nviết hoa (captial letter) để tách các câu trong văn bản. Đầu vào là văn bản\n nlp.txt , in ra mỗi câu\ntrong văn bản trên 1 dòng. 51. Tách từ Coi ký tự trắng (space) là ký tự phân tách các từ. Lấy đầu ra của bài 50 làm đầu\nvào, trích xuất các từ trong các câu và in ra theo định dạng: mỗi dòng 1 từ. In\nra dòng trắng để đánh dấu kết thúc câu. 52. Stemming Đầu vào là đầu ra của bài tập 51, áp dụng thuật toán Porter stemming để lấy ra\ngốc của các từ (stem). In ra các từ và stem cách nhau bởi ký tự tab. Nếu bạn sử\ndụng Python, bạn có thể sử dụng module\n stemming . 53. Tokenization Sử dụng tool  Stanford Core\nNLP  để phân tích văn bản đầu\nvào và lấy ra output theo định dạng XML. Sau đó đọc vào đầu ra XML và trích xuất\nra các token (word) theo định dạng mỗi word trên 1 dòng. 54. POS Tag Đọc vào kết quả phân tích dạng XML của Stanford Core NLP. Trích xuất ra word,\nlemma, POS tag và in ra các thuộc tính của mỗi word trên một dòng; các thuộc\ntính cách nhau bởi dấu tab. 55. Trích xuất named entities Trích xuất tất cả các named entities trong văn bản đầu vào. 56. Phân tích coreference Dựa trên kết quả phân tích coreference của Stanford Core NLP, thay thế các\nmention bằng representative mention. Chú ý khi thay thế các mention bằng\nrepresentative mention, lưu lại các mention gốc trong dấu ngoặc theo định dạng\nrepresentative mention (mention). 57. Phân tích cấu trúc dependency Từ kết quả phân tích dependency của Stanford Core NLP (collapsed-dependencies),\nvisualize câu đầu vào bằng đồ thị có hướng. Khi visualize các dependency trees,\ncó thể sử dụng  ngôn ngữ\nDOT , hay\nsử dụng  Graphviz . Nếu sử dụng Python, có thể hiển\nthị đồ thị có hướng bằng thư viện/package\n pydot . 58. Trích xuất tuples Từ kết quả phân tích dependency của Stanford Core NLP (collapsed-dependencies),\ntrích xuất các bộ 3 [Subject, Predicate, Purpose] và in ra các bộ 3 này (các\nthành phần cách nhau bởi ký tự tab). Subject, Predicate, Purpose được xác định\ndựa vào các tiêu chuẩn sau: Predicate: Là word ở các có các node con (dependant) trong các dependency relations: nsubj, dobj Subject: Các node con (dependant) trong các quan hệ nsubj từ predicate Purpose: Các node con (dependant) trong các quan hệ dobj từ predicate 59. Phrase structure analysis Từ kết quả phân tích cây cú pháp phrase structure (theo định dạng\n S-expression ), in ra tất cả các\nnoun phrases trong văn bản. Chú ý, cần in ra cả các noun phrases nằm trong các\nnoun phrases khác (nested NP). Chương 7: Database File\n artist.json.gz \nlà file nén lưu trữ thông tin về các artist trong cơ sở dữ liệu mở\n MusicBrainz . Các thông tin về các artist được lưu\ntrữ ở định dạng JSON, mỗi dòng lưu thông tin về một artist. Các trường thông tin (field) của file JSON như sau. Field Nội dung Format Ví dụ id id của artist Integer 20660 gid global id String \"ecf9f3a3-35e9-4c58-acaa-e707fba45060\" name Tên artist String \"Oasis\" sort_name Teen artist (dùng để sắp xếp) String \"Oasis\" area Khu vực hoạt động String \"United Kingdom\" aliases Các Tên khác Dictionary aliases[].name Tên khác String \"Oasis\" aliases[].sort_name Tên khác (dùng để sắp xếp) String \"Oasis\" begin Ngày bắt đầu hoạt động Dictionary begin.year Năm bắt đầu hoạt động Integer 1991 begin.month Tháng bắt đầu hoạt động Integer begin.date Ngày bắt đầu hoạt động Integer end Ngày dừng hoạt động Dictionary begin.year Năm dừng hoạt động Integer 2009 begin.month Tháng dừng đầu hoạt động Integer begin.date Ngày dừng hoạt động Integer tags Tag List các Dictionary objects tags[].count Số tag Integer 1 tags[].value Nội dung tag String \"rock\" rating Rating Dictionary rating.count Số phiếu bình chọn Integer 13 rating.value Giá trị bình chọn (trung bình) Integer 86 Hãy suy nghĩ phương pháp lưu trữ, tìm kiếm dữ liệu trong file artist.json.gz\nbằng các database  Key-Value-Store\n(KVS)  hay\n document-oriented database . Với\nKVS database, có thể sử dụng  LevelDB ,\n Redis ,  KyotoCabinet .\nVới document-oriented database, có thể sử dụng\n MongoDB ,  CouchDB \nhoặc  RethinkDB . 60. Tạo KVS database Để giúp cho việc tìm kiếm các trường (fields) từ name đến area của dữ liệu, hãy\nsử dụng Key-Value-Store (KVS) để lưu trữ dữ liệu. 61. Tìm kiếm với KVS Sử dụng cơ sở dữ liệu đã tạo ra trong bài tập 60, tìm kiếm khu vực hoạt động\n(area) của một artist cho trước. 62. Xử lý kiểu vòng lặp trong KVS Sử dụng cơ sở dữ liệu đã tạo ra trong bài tập 60, hãy đưa ra số artist có khu\nvực hoạt động (area) là Japan. 63. Lưu trữ các objects (đối tượng) trong KVS Sử dụng KVS, hãy tạo ra database cho việc tìm kiếm các trường từ name đến tag và\nsố lượng tag. Thử tìm kiếm các trường từ name đến tag và số lượng tag với\ndatabase đã tạo ra. 64. Tạo MongoDB Hãy lưu thông tin của artist (artist.json.gz) vào cơ sở dữ liệu MongoDB. Thêm\nnữa, hãy tạo indexes với các trường sau: name, aliases.name, tags.value,\nrating.value. 65. Tìm kiếm trong cơ sở dữ liệu MongoDB Sử dụng interactive shell của MongoDB, đưa ra các thông tin liên quan đến artist\ncó tên \"Queen\". Tiếp theo, cài đặt chương trình thực hiện chức năng đó. 66. Lấy số kết quả tìm kiếm Sử dụng interactive shell của MongoDB, tính số lượng các artist có khu vực hoạt\nđộng (area) là \"Japan.\" 67. Đưa ra multiple documents Tìm kiếm các artist có aliases cho trước. 68. Sắp xếp Trong số các artist có tag \"dance\", lấy ra top 10 artist có số phiếu bình chọn\ncao nhất. 69. Tạo Web application Tạo ứng dụng Web cho phép người dùng nhập vào các điều kiện tìm kiếm và hiển thị\ncác artist phù hợp với điều kiện tìm kiếm. Các điều kiện tìm kiếm bao gồm: tên\nartist (name), aliases, tag, etc. Hiển thị thông tin các artist (theo dòng) theo\nthứ tự từ cao tới thấp của lượng rating. Chương 8: Machine Learning Chương này yêu cầu bạn thực hiện bài toán sentiment analysis trên corpus\n sentence polarity dataset\nv1.0 \ntrong  Moview Review\nData  của tác giả Bo\nPang và Lillian Lee. Yêu cầu của bài toán sentiment analysis là phân loại các\ncâu thành positive và negative sentiments. 70. Download và tiền xử lý dữ liệu Sử dụng dữ liệu liên quan đến sentiment polarity của các câu (download tại\n đây ),\ntạo dữ liệu chuẩn hoá (sentiment.txt) theo hướng dẫn dưới đây. Thêm vào '+1' ở bắt đầu các dòng trong file rt-polarity.pos (giữa +1 và nội\ndung của câu cách nhau bởi ký tự trắng). Thêm vào '-1' ở bắt đầu các dòng trong file rt-polarity.neg (giữa -1 và nội\ndung của câu cách nhau bởi ký tự trắng). Kết hợp nội dung thu được trong phần 1 và 2 để tạo thành file sentiment.txt Sau khi đã thu được file sentiment.txt, xác nhận số lượng các câu với positive\npolarity và các câu với negative polarity. 71. Stopwords Tạo ra danh sách các stopwords trong tiếng Anh. Sau đó viết 1 hàm để kiểm tra\nmột từ có thuộc danh sách stopwords hay không. Hàm sẽ trả về giá trị TRUE nếu từ\ncho trước thuộc danh sách stopwords. Ngược lại hàm sẽ trả về giá trị FALSE. Sau\nđó viết mô tả về các test cho hàm đã viết. 72. Trích xuất đặc trưng Tự thiết kế các đặc trưng cho bài toán sentiment analysis. Sau đó trích xuất đặc\ntrưng từ dữ liệu training. Hint: phương pháp trích xuất đặc trưng đơn giản nhất là sử dụng từ gốc (stem)\ncác từ không trong danh sách các stopwords. Phương pháp này có thể sử dụng để\nlàm hệ thống baseline. 73. Training Training model bằng phương pháp logistics regressions sử dụng các đặc trưng tạo\nra trong bài 72. 74. Prediction Sử dụng mô hình logistics regressions đã huấn luyện trong bài 73, hãy viết\nchương trình dự đoán polarity cho một câu đầu vào và tính xác suất cho các nhãn\n(+1, -1). 75. Trọng số của các features (Feature weights) Trong mô hình logistics regression đã huấn luyện trong bài 73, đưa ra top 10 các\nfeatures có trọng số cao nhất và top 10 các features có trọng số thấp nhất. 76. Dự đoán trên dữ liệu training Sử dụng mô hình đã học trong bài 73 để đưa ra dự đoán trên dữ liệu training. Đưa\nra nhãn gốc, nhãn dự đoán, và xác suất của nhãn dự đoán cho mỗi câu trong dữ\nliệu (cách nhau bởi ký tự tab). 77. Tính độ chính xác của mô hình Sử dụng đầu ra của bài 76, tính accuracy cho toàn bộ dữ liệu; precision, recall,\nF1 cho nhãn +1. 78. 5-fold cross validation Vì các thực nghiệm trong bài 76, 77 đánh giá model trên dữ liệu huấn luyện nên\nkhó có thể nói đó là các đánh giá hợp lý. Các đánh giá này chỉ đánh giá khả năng\nmô hình \"fit\" với dữ liệu training chứ không đánh giá khả năng khái quát\n(generalization) của mô hình. Vì thế bài tập 78 yêu cầu bạn đánh giá mô hình sử\ndụng 5-fold cross validation. Đưa ra accuracy, precision, recall, F1 score cho\n5-fold cross validation (tính trung bình của 5 folds). 79. Vẽ đồ thị precision-recall Vẽ đồ thị precision-recall theo sự thay đổi của giá trị threshold trong mô hình\nlogistic regression. Chương 9: Không gian vector (I) File\n enwiki-20150112-400-r10-105752.txt.bz2 \nlà file nén dạng bzip2 của 105752 file text được lấy mẫu ngẫu nhiên (tỷ lệ 1/10)\ntừ các bài báo trên Wikipedia có trên 400 từ. Các bài báo trên Wikipedia được\nlấy vào ngày 12 tháng 1 năm 2015. Sử dụng dữ liệu file này làm corpus để học các\nvector thể hiện ý nghĩa của các từ. Trong nửa đầu của chương 9, bạn được yêu cầu\ntrích xuất các context của các từ, trích xuất đặc trưng, và dùng phương pháp PCA\nđể giảm bớt số chiều của dữ liệu. Nửa sau của chương 9 yêu cầu bạn tính độ tương\ntự của các từ sử dụng các word vectors đã học từ corpus. Chú ý, bài 83 yêu cầu 7GB memory. Trong trường hợp lượng memory của bạn không\nđủ, bạn cần có các phương pháp xử lý thích hợp hoặc sử dụng sample 1/100 của dữ\nliệu trong file\n enwiki-20150112-400-r10-105752.txt.bz2 . 80. Tiền xử lý dữ liệu Sử dụng khoảng trắng là ký tự ngăn cách để tokenize các từ trong các câu. Phương\npháp này có nhược điểm là các từ thu được sẽ còn các ký tự đặc biệt như dấu câu,\nhoặc dấu ngoặc. Vì thế sau khi tokenize các từ trong corpus, tiến hành các xử lý\nsau đây. Xoá các ký tự đặc biệt xuất hiện ở đầu và cuối các từ: .,!?;:()[]'\" Xoá các từ chỉ gồm ký tự trắng Sau khi tiền xử lý dữ liệu, lưu file dữ liệu gồm danh sách các từ cách nhau bởi\nkhoảng trắng. 81. Xử lý tên các nước tạo thành từ các compound words Trong tiếng Anh, nhiều từ cạnh nhau có thể tạo thành một từ có ý nghĩa. Ví dụ,\nhợp chủng quốc Hoa Kỳ là \"United States\", vương quốc Anh là \"United Kingdom\".\nNếu chỉ dùng các \"United\", \"State\", hay \"Kingdom\" như các từ riêng lẻ, ý nghĩa\ncủa các từ này sẽ nhập nhằng. Vì thế trong khi tiền xử lý dữ liệu, ta cần xác\nđịnh các từ ghép này. Đoán nhận các từ ghép là một bài toán khó, nên ở đây ta\nchỉ đoán nhận các từ ghép là tên của các nước. Trước hết, download danh sách tên của các nước trên Internet. Dùng danh sách tên\ncác nước này để đoán nhận các từ ghép là tên nước trong dữ liệu sử dụng ở bài\n80, sau đó biến đổi các ký tự spaces thành ký tự underscore (_) để nối các từ\nthành phần. Ví dụ \"United States\" sẽ trở thành \"United_States\", \"Isle of Man\"\nsẽ trở thành \"Isle_of_Man.\" 82. Trích xuất context Sử dụng corpus được tạo ra trong bài tập 81, trích xuất context của tất cả các\ntừ xuất hiện trong corpus. Context  c  của mỗi từ  t  trong dữ liệu sẽ cặp với từ  t  và xuất ra theo định dạng: các thông tin cách nhau bởi ký tự tab. Context của mỗi từ  t  được định nghĩa như sau: Trích xuất các từ ở trước và sau của\n t  với kích thước cửa sổ là  d  (chú ý context words của  t  sẽ không bao gồm\nbản thân của từ  t ) Với mỗi từ  t , kích thước của context (window size)  d \nsẽ được chọn ngẫu nhiên trong tập {1, 2, 3, 4, 5}. 83. Tính tần xuất xuất hiện của từ/context Sử dụng đầu ra của bài 82, tính phân bố xuất hiện và các hằng số sau: f ( t , c ): là số lần đồng xuất hiện của từ  t  và context word  c . f ( t ,*): số lần xuất hiện của từ  t . f (*, c ): số lần xuất hiện của context word  c . N : Tổng số lần xuất hiện của từ và các context word (hằng số). 84. Tạo Matrix của các từ và context words Sử dụng đầu ra của bài 83, tạo ma trận word/context  X . Các thành phần X_tc\ntrong ma trận  X  được định nghĩa như sau. Nếu  f ( t , c ) >= 10, X_tc =\nPPMI(t,c) = max{log N*f(t,c)/f(t,*) x f(*,c),0} Nếu f(t,c) < 10, X_tc = 0. Ở đây PPMI(t,c) là ký hiệu của Pointwise Mutual Information. Chú ý vì kích thước\nma trận X là rất lớn, nên lưu tất cả các giá trị của ma trận vào bộ nhớ là không\nthể. Bạn có thể sử dụng kỹ thuật lưu trữ ma trận thưa với chú ý rằng, phần lớn\ngiá trị của các phần tử trong X bằng 0. 85. Sử dụng PCA để giảm số chiều dữ liệu Sử dụng thuật toán PCA cho ma trận thu được trong bài tập 84 để giảm số chiều dữ\nliệu sao cho các word vector thu được có số chiều là 300. 86. Hiển thị word vectors Đọc vào các word vectors trong bài tập 85, hiển thị vector cho từ \"United\nStates\". Chú ý là từ \"United States\" trong corpus đã được biến đổi thành\n\"United_States.\" 87. Tính word similarity Sử dụng word vectors thu được trong bài tập 85, tính cosine similarity cho hai\ntừ \"United States\" và \"U.S.\" Chú ý là từ \"U.S.\" trong corpus được lưu trữ là\n\"U.S\" 88. Hiển thị top 10 có giá trị similarity cao nhất Sử dụng word vectors trong bài 85, hiển thị top 10 từ với cosine similarity cao\nnhất với từ \"England\" và các giá trị cosine similarity tương ứng. 89. Các thao tác cộng/trừ word vectors Đọc vào các word vectors thu được trong bài 85, tính vec(\"Spain\") -\nvec(\"Madrid\") + vec(\"Athens\") sau đó hiển thị top 10 từ có cosine similarity gần\nnhất với vector thu được cùng với các giá trị cosine similarity tương ứng. Chương 10: Không gian vector (II) Trong chương 10, chúng ta sẽ tiếp tục nội dung của chương 9 về không gian\nvector. 90. Sử dụng word2vec để học word vectors Áp dụng  word2vec  trên corpus đã tạo ra\nở bài 81 để học word vectors. Sau đó, sử dụng các word vectors đã học với\nword2vec để áp dụng cho các bài tập 86-89. 91. Chuẩn bị dữ liệu analogy Download dữ liệu  analogy\nevaluation .\nTrong dữ liệu, các dòng bắt đầu bằng \":\" thể hiện tên của section. Ví dụ dòng \":\ncapital-common-countries\" bắt đầu cho section \"capital-common-countries.\" Hãy\ntrích xuất các dòng của section \"family\" trong file đã download và lưu ra file. 92. Vận dụng dữ liệu analogy data Với các dòng trong dữ liệu analogy tạo ra trong bài 91, tính vector sau:\nvec(word ở cột 2) - vec(word ở cột 1) + vec(word ở cột 3) sau đó tìm word với\nword vector với độ tương tự cao nhất với word vector đã tính đồng thời tính độ\ntương tự (cosine similarity). Thêm vào cuối của các dòng từ tìm được và độ tương\ntự. Trong bài tập này, hãy thử sử dụng word vector đã học được sau bài 85 và bài\n90. 93. Tính độ chính xác của mô hình trên dữ liệu analogy Sử dụng dữ liệu của bài 92, tính độ chính xác của các mô hình với mô hình\nanalogy. 94. Tính word similarity trên dữ liệu WordSimilarity-353 Sử dụng đầu vào là dữ liệu  The WordSimilarity-353 Test\nCollection ,\ntính độ tương tự của các từ ở cột 1 và cột 2 và thêm vào cuối các dòng giá trị\nđộ tương tự này. Hãy áp dụng các mô hình word vectors đã học ở bài 85 và bài 90. 95. Đánh giá trên dữ liệu WordSimilarity-353 Sử dụng dữ liệu trong bài 94, sử dụng ranking với các giá trị độ tương tự đã\ntính với các mô hình và ranking do con người đưa ra để tính  Spearman\ncorrelation . 96. Trích xuất vectors liên quan đến tên nước Sử dụng mô hình đã học với word2vec, trích xuất các vectors của các từ liên quan\nđến tên các nước. 97. k-means clustering Lấy đầu vào là các word vectors từ bài tập 96, thực hiện clustering bằng thuật\ntoán k-means với số lượng clusters  k  = 5. 98. Clustering với phương pháp Ward Lấy đầu vào là các word vectors từ bài tập 96 (các word vectors của tên các\nnước), thực hiện hierarchical clustering bằng  phương pháp\nWard . Sau đó, visualize kết quả\nclustering bằng  dendrogram . 99. Visualize word vectors bằng phương pháp t-SNE Với các word vectors thu được từ bài tập 96, visualize không gian vectors bằng\n phương pháp t-SNE . Phụ lục: Corpus, data sử dụng trong 100 bài luyện tập NLP hightemp.txt :\nDữ liệu nhiệt độ cao nhất ở các địa phương qua các thời kỳ do nha khí tượng\nNhật Bản cung cấp. jawiki-country.json.gz :\nDữ liệu Wikipedia tiếng Nhật gồm các bài báo (trong ngày 18/10/2014) về các\nquốc gia, được trích xuất từ  Wikipedia Dump data (tiếng\nNhật) .\nDữ liệu được lưu trữ bằng định dạng JSON. neko.txt : là nội\ndung bằng plain text của cuốn tiểu thuyết \"吾輩は猫である\" của nhà văn\n夏目漱石 (Soseki Natsume) được cung cấp miễn phí tại trang Web:\n 青空文庫 . nlp.txt : nội dung\ncủa trang Wikipedia nói về  Natural Language\nProcessing  với\nđịnh dạng 1 dòng 1 câu. artist.json.gz :\nlà file nén lưu trữ thông tin về các artist trong cơ sở dữ liệu mở\n MusicBrainz . Các thông tin về các artist được\nlưu trữ ở định dạng JSON, mỗi dòng lưu thông tin về một artist. enwiki-20150112-400-r10-105752.txt.bz2 \nlà file nén dạng bzip2 của 105752 file text được lấy mẫu ngẫu nhiên (tỷ lệ\n1/10) từ các bài báo trên Wikipedia có trên 400 từ. Các bài báo trên\nWikipedia được lấy vào ngày 12 tháng 1 năm 2015. © 2017  GitHub , Inc. Terms Privacy Security Status Help Contact GitHub API Training Shop Blog About",
          "url": "https://github.com/minhpqn/nlp_100_drill_exercises",
          "relevance": "0"
        },
        {
          "title": "MÔ TẢ VẮN TẮT NỘI DUNG VÀ KHỐI LƯỢNG CÁC HỌC PHẦN",
          "content": "TRƯỜNG ĐẠI HỌC VIỆT BẮC VIET BAC UNIVERSITY Trang chủ Giới thiệu Giới thiệu Cơ cấu tổ chức Đội ngũ cán bộ Giáo viên cơ hữu Giáo viên thỉnh giảng Tin tức Tin tức Sự kiện Tuyển sinh Đăng ký tuyển sinh Thông báo tuyển sinh 2017 Thông Tin Tuyển Sinh Văn bản, mẫu biểu Trung tâm đào tạo hợp tác quốc tế Trung tâm đào tạo theo yêu cầu Trung tâm ngoại ngữ tin học Đào tạo Thông báo Kế hoạch đào tạo Chương trình đào tạo đại học Ngành Kỹ thuật Cơ khí Ngành Kỹ thuật Điện, Điện tử Ngành Công nghệ thông tin Ngành Truyền thông và mạng máy tính Ngành Kế toán Ngành Quản trị Kinh doanh Chương trình đào tạo cao đẳng Ngành Kế toán Ngành Quản trị kinh doanh Ngành Công nghệ thông tin Quy định - Quy chế Khoa học Công nghệ Nghiên cứu khoa học Thông tin khoa học công nghệ Sinh viên Thông báo Lịch học, Lịch thi Văn bản, Mẫu biểu Quy chế, Quy định Ký túc xá và dịch vụ internet 3 Công khai Chất lượng giáo dục Điều kiện đảm bảo CLGD Thu chi tài chính Liên hệ  QUYẾT ĐỊNH  MỤC TIÊU VÀ CHUẨN ĐẦU RA  NỘI DUNG CHƯƠNG TRÌNH  MÔ TẢ VẮN TẮT CÁC HỌC PHẦN MÔ  TẢ  \t\tVẮN TẮT NỘI DUNG VÀ KHỐI LƯỢNG CÁC HỌC PHẦN  I. Khối kiến thức giáo dục đại cương  55 TC  1. Những nguyên lý cơ bản của Chủ nghĩa Mác – Lênin  5 TC  Nội dung thực hiện theo công văn số 2488/BGDĐT-ĐH&SDH, ngày 25/3/2008  \t\tcủa Bộ trưởng Bộ Giáo dục và Đào tạo. \t\t  Học phần Những nguyên lý cơ bản của chủ nghĩa Mác - Lênin bao gồm những  \t\tnội dung kiến thức sau đây: những nội dung cơ bản về thế giới quan,  \t\tPhương pháp luận khoa học của chủ nghĩa Mác-Lênin;  Những nội dung  \t\ttrọng tâm của học thuyết kinh tế Mác-Lênin; Những nội dung cơ bản thuộc  \t\tlý luận của chủ nghĩa Mác-Lênin về Chủ nghĩa xã hội.  2. Đường lối cách mạng của Đảng Cộng sản Việt Nam   3 TC  Nội dung thực hiện theo công văn số 2488/BGDĐT-ĐH&SDH, ngày 25/3/2008  \t\tcủa Bộ trưởng Bộ Giáo dục và Đào tạo. Học phần Đường lối cách mạng của  \t\tĐảng cộng sản Việt Nam bao gồm những nội dung kiến thức sau đây: Đường  \t\tlối công nghiệp hoá, hiện đại hoá đất nước; Xây dựng nền kinh tế thị  \t\ttrường định hướng xã hội chủ nghĩa; Xây dựng và phát triển văn hoá xã  \t\thội; Xây dựng nhà nước pháp quyền xã hội chủ nghĩa Việt Nam; Bảo vệ tổ  \t\tquốc xã hội chủ nghĩa; Đường lối đối ngoại và hội nhập kinh tế quốc tế.  3. Tư tưởng Hồ Chí Minh   2 TC  Nội dung thực hiện theo công văn số 2488/BGDĐT-ĐH&SDH, ngày 25/3/2008  \t\tcủa Bộ trưởng Bộ Giáo dục và Đào tạo. \t\t  Học phần Tư tưởng Hồ Chí Minh bao gồm những nội dung kiến thức sau đây:  \t\tTư tưởng Hồ Chí Minh về giáo dục và xây dựng con người mới, Phương pháp  \t\tluận Hồ Chí Minh.  4, 5. Giáo dục thể chất   5 TC  Nội dung ban hành kèm theo Quyết định số 3244/GD-ĐT ngày 12/9/1995 và  \t\tQuyết định số 1262/GD-ĐT ngày 12/4/1997 của Bộ trưởng Bộ Giáo dục và Đào  \t\ttạo. Học phần Giáo dục thể chất bao gồm những nội dung kiến thức sau  \t\tđây: Huấn luyện cho người học những kiến thức cơ bản về thể thao quần  \t\tchúng và thể thao quân sự bao gồm: Hiểu biết nguyên tắc, phương pháp  \t\thuấn luyện thể lực, luật và tổ chức thi đấu một số môn thể thao.  6, 7, 8. Giáo dục quốc phòng và an ninh   8 TC  Nội dung học phần Giáo dục quốc phòng và an ninh gồm các n ội  \t\tdung ban hành tại Thông tư số 31/2012/TT- BGDĐT, ngày 12/9/2012 của Bộ  \t\ttrưởng Bộ Giáo dục và Đào tạo. Học phần này chi tiết gồm những nội dung  \t\tkiến thức sau đây: Trang bị những kiến thức cơ bản về Đường lối quân sự  \t\tcủa Đảng; Công tác quốc phòng, an ninh; Quân sự chung và chiến thuật,   \t\tkỹ thuật bắn súng tiểu liên AK (CKC).  9. Đại số tuyến tính  3 TC  Học phần Đại số tuyến tính bao gồm các nội dung chính: Không gian véctơ,  \t\tkhông gian con, cơ sở và số chiều của không gian véctơ hữu hạn chiều. Ma  \t\ttrận, các phép toán trên ma trận. Định thức và các phương pháp tính định  \t\tthức. Lý thuyết hệ phương trình tuyến tính tổng quát, các phương pháp  \t\tgiải hệ phương trình tuyến tính. Hệ phương trình tuyến tính thuần nhất.  \t\tÁnh xạ tuyến tính, các phép toán trên các ánh xạ tuyến tính, giá trị  \t\triêng và vectơ riêng của một ánh xạ tuyến tính, chéo hóa ma trận, dạng  \t\tsong tuyến tính, dạng toàn phương, đưa dạng toàn phương về dạng chính  \t\ttắc, và không gian véc tơ Ơclít,...  10. Giải tích 1  4 TC  Nội dung học phần  Giải tích 1 bao gồm các kiến thức về: Hàm số, sự liên tục của hàm số,  \t\tphép tính vi phân, tích phân, lý thuyết chuỗi. Nội dung chương trình đề  \t\tcập đến những vấn đề cơ bản nhất của toán học, yêu cầu sinh viên phải  \t\tnắm được các tính chất của hàm một biến số (bao gồm giới hạn, tính liên  \t\ttục, phép tính vi phân) và mối quan hệ giữa các tính chất đó. Đồng thời  \t\tcần nắm được khái niệm tích phân suy rộng và sự hội tụ của chúng. Từ đó  \t\tcó khả năng vận dụng các kiến thức vào các chuyên ngành mà sinh viên  \t\tđược đào tạo.  11. Giải tích 2  4 TC  Nội dung học phần Giải tích 2 bao gồm: Chuỗi số, chuỗi hàm số. Đạo hàm  \t\tvà vi phân của hàm nhiều biến. Tích phân phụ thuộc tham số, tích phân  \t\tbội. Nội dung chương trình đề cập đến những vấn đề cơ bản nhất của toán  \t\thọc, yêu cầu sinh viên phải nắm được  các khái niệm và tính chất của  \t\tchuỗi số, chuỗi hàm số. Từ đó có khả năng vận dụng các kiến thức vào các  \t\tchuyên ngành mà sinh viên được đào tạo.  12. Tiếng Anh 1   3 TC  Môn học củng cố và ôn tập lại kiến thức ngữ pháp về thì hiện tại thường,  \t\thiện tại tiếp diễn, quá khứ thường, so sánh hơn và hơn nhất; Sinh viên  \t\tđược bổ sung thêm một số kiến thức ngữ pháp mới về câu điều kiện loại 1,  \t\tcách dùng thì quá khứ tiếp diễn …  Nắm vững được hệ thống từ vựng  \t\tliên quan đến chủ đề của bài học, củng cố lại những cấu trúc câu thông  \t\tdụng đã học và trang bị thêm cấu trúc nâng cao để hiểu rõ văn phong cách  \t\tdiễn đạt trong tiếng Anh.  13. Tiếng Anh 2   3 TC  Người học được củng cố và ôn tập lại kiến thức ngữ pháp về thì hiện tại  \t\thoàn thành, danh từ đếm được và không đếm được. Bổ sung thêm một số kiến  \t\tthức ngữ pháp mới về cách dùng would, câu điều kiện loại 2, thì tương  \t\tlai… Nắm  vững được  hệ  thống từ vựng liên quan đến chủ đề của bài học,  \t\tcủng cố lại những cấu trúc câu thông dụng đã học cũng như trang bị thêm  \t\tcấu trúc nâng cao để hiểu rõ văn phong cách diễn đạt trong tiếng Anh.  14. Tiếng Anh 3   3 TC  Người học được cung cấp kiến thức ngữ pháp về thì tương lai, quá khứ  \t\thoàn thành, thể bị động…Nắm vững được hệ thống từ vựng liên quan đến các  \t\tchủ đề của bài học,  củng cố lại các cấu trúc thông dụng đã học cũng như  \t\tđược trang bị thêm các cấu trúc nâng cao, hiểu rõ văn phong, cách diễn  \t\tđạt trong tiếng Anh. Người học phải hình thành được các kĩ năng đọc hiểu  \t\tnhư: kĩ năng đọc lướt, kĩ năng đọc tìm ý chính, kĩ năng đoán ý tác giả,  \t\tkĩ năng đoán nghĩa của từ thông qua văn cảnh… Người  \t\thọc biết vận dụng sáng tạo kiến thức đã học để có thể viết bài về những  \t\tchủ đề khác nhau;  Người học phát triển kĩ năng nghe, nói, và có  \t\tthể vận dụng hợp lí những kiến thức đã học vào giao tiếp thực tế.  15.   Tiếng Anh chuyên ngành  2 TC  Tài liệu nhằm cung cấp cho sinh viên vốn từ vựng chung về chuyên ngành  \t\tcông nghệ thông tin. Sinh viên được luyện kỹ năng giao tiếp sử dụng các  \t\tthuật ngữ chuyên môn, các cấu trúc câu thường gặp trong chuyên ngành   công nghệ thông tin . Sau học phần tiếng Anh chuyên ngành  công  \t\tnghệ thông tin , sinh viên có khả năng đọc, dịch, viết, phân tích các  \t\ttài liệu có liên quan đến chuyên ngành. Sinh viên có được phương pháp  \t\tnghiên cứu tài liệu chuyên môn bằng tiếng Anh nhằm phục vụ tốt cho các  \t\tmôn chuyên ngành và tự nghiên cứu trong công việc hay nâng cao trình độ  \t\tsau này.   16. Tin học cơ sở   3 TC  Học phần Tin học cơ sở trang bị cho sinh viên những kiến thức cơ bản về  \t\tcông nghệ thông tin; hệ điều hành, phần mềm ứng dụng, phần mềm tiện ích;  \t\tKhai thác hệ điều hành phổ thông MS Windows; Sử dụng các phần mềm văn  \t\tphòng để làm tài liệu, quản trị dữ liệu ở mức đơn giản; Sử dụng các dịch  \t\tvụ trên Internet như e-mail, tìm kiếm tin tức.  17. Xác suất thống kê  2 TC  Học phần Xác suất thống kê trang bị cho sinh viên các khái niệm cơ bản  \t\tcủa lý thuyết xác suất như các hiện tượng ngẫu nhiên, tất nhiên, các  \t\tloại phân bố gián đoạn, phân bố liên tục, phân bố xác suất các đại lượng  \t\tngẫu nhiên. Học phần trình bày phương pháp thống kê xử lý các số liệu  \t\tthực nghiệm và mối tương quan giữa các đại lượng tham số. Kiến thức lý  \t\tthuyết được trình bày để sinh viên có thể áp dụng dễ dàng vào các ngành  \t\thọc thích hợp trong kỹ thuật.  18.   Tiếng Việt thực hành  2 TC  Học phần Tiếng Việt thực hành bao gồm những nội dung kiến thức sau đây:  \t\tTrang bị cho người học những kiến thức cơ bản của tiếng Việt, góp phần  \t\trèn luyện tư duy khoa học cho sinh viên, Rèn luyện năng lực sử dụng  \t\ttiếng Việt một cách chính xác, mạch lạc, chặt chẽ và trong sáng. Môn học  \t\tsẽ giúp trang bị cho sinh viên những kỹ năng soạn thảo và trình bày các  \t\tloại hình văn bản và báo cáo thông dụng.  19.   Hóa học đại cương  3 TC  Học phần Hoá học đại cương bao gồm những nội dung kiến thức sau đây: Hệ  \t\tthống tuần hoàn - Nhiệt động học áp dụng cho hoá học; chiều hướng và  \t\tgiới hạn tự diễn biến của các quá trình; cân bằng hoá học; động hoá học;  \t\tdung dịch, dung dịch chất điện ly, dung dịch chất không điện ly; hiện  \t\ttượng bề mặt; điện hoá học; ăn mòn kim loại; phương pháp chống ăn mòn  \t\tkim loại.  20. Vật lý đại cương 1  3 TC  Học phần Vật lý 1 bao gồm những nội dung kiến thức sau đây: Cơ học chất  \t\tđiểm; Trường hấp dẫn Newton; Cơ học hệ chất điểm – Cơ học vật rắn; Động  \t\tlực học chất khí; Phương trình cơ bản thuyết động lực học; Giới thiệu về  \t\tnguyên lý thứ nhất nhiệt động lực học, nguyên lý thứ hai nhiệt động lực  \t\thọc; Hàm sóng, ý nghĩa thống kê hàm sóng; Chu trình Carnot. Trường và  \t\tsóng điện từ.  21. Vật lý đại cương 2  3 TC  Học phần Vật lý 2 bao gồm những nội dung kiến thức sau đây: Sóng ánh  \t\tsáng; Thuyết tương đối Einstein; Quang lượng tử; nguyên tử - Phân tử;  \t\tvật liệu điện và từ; vật liệu quang Laser; phương trình cơ bản cơ học  \t\tlượng tử; hệ thức bất định Heidelberg; sắt từ; điện môi; đặc tính V – A  \t\tcủa Transitor và Diode.  22. Thí nghiệm Vật lý  1 TC  Học phần Thí nghiệm Vật lý cung cấp cho sinh viên những kiến thức thực  \t\ttế về những vấn đề cơ bản của các học phần Vật lý 1, Vật lý 2.  23.   Pháp luật đại cương  2 TC  Học phần Pháp luật đại cương bao gồm những nội dung kiến thức sau đây:  \t\tGiới thiệu các khái niệm, các phạm trù chung cơ bản nhất về Nhà nước và  \t\tPháp luật dưới góc độ của khoa học quản lý. Trên cơ sở đó, đi vào phân  \t\ttích: Cấu trúc của bộ mày Nhà nước cũng như chức năng, thẩm quyền và địa  \t\tvị pháp lý và cơ cấu của hệ thống các văn bản quy phạm pháp luật; một số  \t\tnội dung cơ bản của Luật  Hành chính, Luật Dân sự, Luật Hình sự.  24.   Các học phần tự chọn cơ bản    24.1.   Logic học đại cương  2 TC  Học phần giới thiệu về logic học hình thức, các qui luật và hình thức cơ  \t\tbản của tư duy. Qua đó sinh viên được rèn luyện và nâng cao tư duy khoa  \t\thọc.  24.2.   Quản trị học đại cương  2 TC  Học phần Quản trị học đại cương bao gồm những nội dung kiến thức sau  \t\tđây: Cung cấp các kiến thức cơ bản về quản trị và sự vận dụng thực tiễn  \t\tdoanh nghiệp của nó như: khái niệm và bản chất của quản trị; Nhà quản  \t\ttrị; Môi trường quản trị; Các lý thuyết quản trị (cổ điển và hiện đại);  \t\tCác chức năng của quản trị; Hoạch định, tổ chức, lãnh đạo và kiểm tra.  \t\tHọc phần còn cập nhật một số vấn đề mới của quản trị học hiện đại như  \t\tquản trị thông tin và ra quyết định, quản trị sự đổi mới/thay đổi, quản  \t\ttrị xung đột, quản trị rủi ro và cơ hội của một doanh nghiệp.   24.3.   Môi trường và con người  2 TC  Học phần Môi trường và con người bao gồm những nội dung kiến thức sau  \t\tđây: Cung cấp các thông tin về tình hình môi trường; bảo vệ môi trường,  \t\tkhung pháp lý về bảo vệ môi trường ở Việt Nam; các phương pháp quản lý  \t\tcác nguồn tác động môi trường điển hình.  24.4.   Phương pháp luận nghiên cứu khoa học  2 TC  Phương pháp luận nghiên cứu khoa học thuộc về lĩnh vực khoa học tư duy.  \t\tMục đích của học phần là trang bị cho sinh viên những tri thức và kỹ  \t\tnăng cơ bản về nghiên cứu khoa học. Nội dung học phần bao gồm: Khái quát  \t\tvề Phương pháp luận NCKH; Đại cương về Nghiên cứu khoa học; Cơ sở phương  \t\tpháp luận nghiên cứu khoa học; Phương pháp nghiên cứu khoa học; Xử lý dữ  \t\tliệu trong NCKH; Logic tiến hành một công trình NCKH.  24.5.   Xã hội học đại cương  2 TC  Học phần Xã hội học đại cương cung cấp cho sinh viên các kiến thức cơ  \t\tbản về xã hội học: như lịch sử hình thành và phát triển của xã hội học,  \t\tđối tượng chức năng của xã hội học, các khái niệm, và phạm trù xã hội  \t\thọc, phương pháp nghiên cứu và các chuyên ngành xã hội học. Sinh viên  \t\tnắm vững các khái niệm cơ bản, những luận điểm cơ bản của các lối tiếp  \t\tcận xã hội học; Sinh viên hiểu biết được một số quy luật cơ bản của các  \t\tsự kiện, hiện tượng xã hội và vận dụng lý giải một số hiện tượng xã hội  \t\tở VN.  24.6.   Cơ sở văn hóa Việt Nam  2 TC  Môn học này trang bị cho sinh viên những hiểu biết cơ bản về tiến trình  \t\tphát triển của văn hoá Việt Nam và đặc trưng văn hoá Việt Nam, qua đó có  \t\tthể thấy được những nét tương đồng và khác biệt giữa văn hóa Việt Nam và  \t\tvăn hóa các nước khác, đặc biệt là với văn hóa các nước trong khu vực và  \t\tvăn hóa các nước từng có nhiều ảnh hưởng đến Việt Nam. Môn học này cung  \t\tcấp cho sinh viên một hành trang văn hoá Việt Nam để họ hiểu rõ hơn dân  \t\ttộc mình, nền văn hoá của mình. Sinh viên có thể phân biệt rõ khái niệm  \t\tvăn hoá, văn hiến, văn minh, văn vật, hiểu được các giai đoạn phát triển  \t\tcủa văn hoá Việt Nam và những nét riêng, đặc trưng của văn hoá Việt Nam.  II. Khối kiến thức giáo dục chuyên nghiệp  100 TC     25. Toán rời rạc  3 TC          Môn học trình bày những kiến thức cơ bản về lý thuyết tổ hợp thông qua  \t\tviệc giải quyết bốn bài toán cơ bản đó là: Bài toán đếm, Bài toán tồn  \t\ttại, Bài toán liệt kê và Bài toán tối ưu. Lý thuyết đồ thị: khái niệm,  \t\tđịnh nghĩa, các thuật toán trên đồ thị, đồ thị Euler, đồ thị Hamilton.  \t\tMột số bài toán có ứng dụng thực tiễn quan trọng khác của lý thuyết đồ  \t\tthị cũng được chú trọng giải quyết đó là Bài toán tô màu đồ thị, Bài  \t\ttoán tìm đường đi ngắn nhất và Bài toán luồng cực đại trong mạng.   26.   Đ ại  \t\t\t\tcương kỹ thuật   3 TC  Giới thiệu cho sinh viên kỹ thuật năm đầu các khái niệm căn bản: các  \t\tngành nghề kỹ thuật; chức năng và yêu cầu của cán bộ kỹ thuật; cách giải  \t\tquyết các vấn đề kỹ thuật; căn bản về máy vi tính và sử dụng máy vi tính  \t\ttrong kỹ thuật; giao tiếp trong kỹ thuật và làm việc nhóm; đạo đức nghề  \t\tnghiệp; bài học từ các sai sót…  27.   Cấu trúc dữ liệu & giải thuật  3 TC  Nội dung môn học bao gồm hai phần: Những vấn đề cơ bản và mối quan hệ  \t\tgiữa cấu trúc dữ liệu và giải thuật, phân tích thiết kế thuật toán, giải  \t\tthuật đệ qui; Giới thiệu một số cấu trúc dữ liệu (mảng, danh sách, cây,  \t\tđồ thị...), thuật toán sắp xếp, tìm kiếm.  28.   Kiến trúc máy tính  3 TC  Khái niệm chung liên quan đến lĩnh vực kiến trúc máy tính, sơ lược những  \t\tkiến thức cơ sơ liên quan đến môn học.  Các khái niệm, kiến trúc của bộ  \t\tnhớ chính, bộ nhớ cache và các thiết bị lưu trữ dữ liệu, sơ  lược về  \t\tliên kết trong máy tính, tổ chức và kiến trúc các hệ thống bus. Tập lệnh  \t\tvà các phương thức truy cập dữ liệu trong bộ nhớ. Tổ chức và các chức  \t\tnăng của bộ vi xử lý CPU.  29.   Kỹ thuật lập trình  2 TC  Nội dung của học phần Kỹ thuật lập trình gồm 3 phần: Phần một ôn tập lại  \t\tvề các kiến thức cơ bản trong lập trình, kỹ năng xác định bài toán và  \t\tthành lập giải thuật, kỹ năng đọc và phân tích mã lệnh của chương trình.  \t\tPhần hai cung cấp kiến thức từ cơ bản đến nâng cao về lập trình Windows  \t\tForm, lập trình với bộ thư viện COM của Microsoft Office, phối hợp các  \t\tkiến thức để viết một ứng dụng quản lý. Phần ba trang bị cho sinh viên  \t\tcác kiến thức cơ bản về cấu trúc dữ liệu và các thuật toán phổ biến,  \t\tcách thức phát triển phần mềm hướng đối tượng.  30.   Hệ điều hành  2 TC  Môn học gồm 5 phần chính, trong đó phần 1 giới thiệu về tổng quan, lịch  \t\tsử của các hệ điều hành. Phần 2 nghiên cứu các phương thức quản lý tiến  \t\ttrình, phần 3 giới thiệu về quản lý lưu trữ (bao gồm quản lý bộ nhớ  \t\ttrong và ngoài). Phần 4 dành để nghiên cứu vào ra của hệ điều hành và  \t\tphần 5 dành cho vấn đề bảo vệ, an ninh hệ thống.  31.   Mạng máy tính  3 TC  Nội dung môn học trình bày các chức năng, các giao thức chính trong mỗi  \t\ttầng theo mô hình tham chiếu OSI. Trong mỗi tầng có định hướng trọng tâm  \t\tvào các giao thức của mạng internet/intranet và các mạng đương thời.  32.   Cơ sở dữ liệu  3 TC  Môn học giới thiệu các bước xây dựng một cơ sở dữ liệu thực tế, các  \t\tphương tiện lưu giữ cơ sở dữ liệu và kỹ thuật tổ chức các file. Quá  \t\ttrình xử lý truy vấn và tối ưu truy vấn kiểm tra cạnh tranh.  33. Lập trình hướng đối tượng  3 TC  Môn học giới thiệu về các khái niệm cơ sở của lập trình hướng đối tượng  \t\tvà được minh họa bằng ngôn ngữ lập trình Java. Môn học cũng giới thiệu  \t\tvề những lợi ích mà phương pháp hướng đối tượng mang lại cho người lập  \t\ttrình so với các phương pháp lập trình trước đây như tái sử dụng, bảo  \t\tmật,... Các khái niệm cơ sở của lập trình hướng đối tượng được giới  \t\tthiệu bao gồm: Đối tượng, Lớp, Kế thừa, Đa hình, Đóng gói, Liên kết  \t\tđộng, Trừu tượng hóa dữ liệu, Truyền thông điệp. Cú pháp, ngoại lệ, các  \t\tluồng dữ liệu của ngôn ngữ Java dược trình bày giúp sinh viên hiểu sâu  \t\thơn về ngôn ngữ lập trình. Ngoài ra, môn học cũng giới thiệu sơ lược về  \t\tthiết kế hướng đối tượng dùng các ký pháp của UML và thiết kế mẫu.  34. Phát triển phần mềm mã nguồn  3 TC  Học phần Phát triển phần mềm mã nguồn mở cung cấp cho học viên: Các khái  \t\tniệm cơ bản về Phần mềm nguồn mở, cách phát triển phần mềm nguồn mở, tìm  \t\thiểu và triển khai các ứng dụng Web-Based sử dụng PHP & MySQL.  35. Phân tích và thiết kế hệ thống thông tin  3 TC  Môn học giới thiệu về các tiến trình phát triển phần mềm và những khái  \t\tniệm liên quan. Tiếp đó trình bày các mô hình và phương pháp hướng cấu  \t\ttrúc khác nhau được vận dụng để tiến hành các bước xác định yêu cầu,  \t\tphân tích và thiết kế một hệ thống thông tin. Sau khi được nghe giảng,  \t\tsinh viên được chia thành nhóm và nhận bài tập về nhà tiến hành thực  \t\thành phân tích, thiết kế và làm tài liệu. Kết quả làm bài tập được trình  \t\tbày ở xemina để thảo luận và sau đó hoàn thiện tài liệu nộp giáo viên  \t\tchấm điểm học phần 1. Cuối kỳ thi hết môn lấy điểm học phần 2.   36. Thực tập cơ sở  2 TC  Trong chương trình đào tạo các chuyên ngành CNTT, sinh viên phải trải  \t\tqua 5 tuần thực tập cơ sở. Củng cố kiến thức lý thuyết đã được trang bị  \t\ttrong nhà trường và vận dụng những kiến thức đó vào thực tế. Rèn luyện  \t\tkỹ năng thực hành thông qua việc thực hiện các công việc thực tiễn mà cơ  \t\tquan tiếp nhận sinh viên thực tập giao cho. Rèn luyện ý thức chấp hành  \t\tkỷ luật lao động, thái độ giao tiếp với mọi người, phát huy tinh thần  \t\thọc hỏi, chủ động sáng tạo trong việc giải quyết các vấn đề của thực  \t\ttiễn sản xuất, lao động và cuộc sống.  37. Đồ án I  2 TC  Trong chương trình đào tạo các chuyên ngành CNTT, sinh viên phải trải  \t\tqua 5 tuần thực hiện đề tài. Củng cố kiến thức lý thuyết đã được trang  \t\tbị trong nhà trường và vận dụng những kiến thức đó vào thực tế. Rèn  \t\tluyện kỹ năng thực hành thông qua việc thực hiện các công việc thực tiễn  \t\tmà giáo viên hướng dẫn giao cho.  38. Phân tích thiết kế hướng đối tượng  3 TC  Học phần Phân tích và thiết kế hướng đối tượng bao gồm những nội dung  \t\tkiến thức sau đây: Giới thiệu một số khái niệm cơ bản của hướng đối  \t\ttượng, phân tích và thiết kế hướng đối tượng, các ngôn ngữ hướng đối  \t\ttượng thường dùng; Quy trình phát triển phần mềm; Giới thiệu ngôn ngữ  \t\tUML dùng trong thiết kế hướng đối tượng; Các mẫu thiết kế hướng đối  \t\ttượng được dùng phổ biến trong các ứng dụng hiện hành và ứng dụng tương  \t\tlai. Có thể sử dụng C++ hoặc Java để thực hiện việc triển khai một dự án  \t\thoàn chỉnh để minh hoạ.  39. An toàn và bảo mật thông tin  3 TC  Môn học cung cấp nội dung của an toàn và bảo mật thông tin, các khái  \t\tniệm cơ bản về mật mã học. Trình bày các thuật toán mã hóa từ cổ điển  \t\tđến các thuật toán hiện đại được sử dụng nhiều nhất hiện nay, các sơ đồ  \t\tký số, các hệ mật mã đối xứng, mật mã công khai.  40. Trí tuệ nhân tạo  3 TC  Trí tuệ nhân tạo là lĩnh vực nghiên cứu thiết kế các tác nhân thông  \t\tminh. Môn hoc gồm 2 phần. Phần 1: Trình bày các phương pháp tìm kiếm cơ  \t\tbản, đạc biệt là các phương pháp tìm kiếm heuristic.  Phần 2: Trình bày  \t\tcác ngôn ngữ biểu diễn tri thức và các phương pháp lập luận tự động làm  \t\tcơ sở cho việc thiết kế các tác nhân dựa trên trí thức.  41. Multimedia  3 TC  Môn học bao hàm những kiến thức tổng quan về multimedia, các lĩnh vực  \t\tứng dụng; Các yêu cầu đối với hệ thống multimedia nhằm đảm bảo chất  \t\tlượng dịch vụ; Các nguyên tắc, quy trình, công cụ xây dựng và phát triển  \t\tứng dụng và tìm hiểu về các loại dữ liệu multimedia và kỹ thuật nén. Môn  \t\thọc có ý nghĩa, vai trò trong việc hỗ trợ sinh viên có tầm nhìn tổng  \t\tquan và kỹ năng về truyền thông đa phương tiện – Một lĩnh vực thiết yếu  \t\tkhông thể thiếu trong cuộc sống hiện đại.   42. Otomat và ngôn ngữ hình thức   3 TC  Automat hữu hạn và ngôn ngữ chính qui. Automat đẩy xuống và ngôn ngữ phi  \t\tngữ cảnh. Automat bị chặn tuyến tính và ngôn ngữ cảm ngữ cảnh. Máy  \t\tTuring. Các ứng dụng.   43. Phát triển ứng dụng trên môi trường mạng  3 TC  Môn học giúp sinh viên nắm được khái niệm cơ bản về một số công nghệ mới  \t\ttrong việc xây dựng các ứng dụng dựa trên nền Web, từ đó có thể áp dụng  \t\tvào để triển khai các công việc thực tế một cách hiệu quả.  44. Xử lý ảnh  2 TC  Môn học giới thiệu các khái niệm cơ bản trong xử lý ảnh như điểm ảnh,  \t\tlấy mẫu, màu sắc, ... Đồng thời, môn học cũng giới thiệu một số thao tác  \t\tcơ bản trong xử lý ảnh như: biến đổi Fourier, nhân chập, các thao tác  \t\ttoàn cục, thao tác cục bộ, …Ngoài ra, môn học còn giới thiệu về nén ảnh  \t\tvà trích chọn đặc trưng trong ảnh.  45.   Lý thuyết thông tin  2 TC  Môn học lý thuyết thông tin giới thiệu các khái niệm cơ bản về hệ thống  \t\ttruyền tin, các kiến thức về lượng tin, entropi nguồn rời rạc, các lý  \t\tthuyết về mã hóa, các phương pháp biểu diễn mã, các phương pháp mã hóa  \t\tthống kê tối ưu, mã chống nhiễu, mã vòng, mã tuyến tính. Môn học là tiền  \t\tđề cho các môn: An toàn bảo mật thôn tin, thống kê phân tích số liệu, kỹ  \t\tthuật truyền số liệu, mạng máy tính, nguyên lý hệ điều hành...  46.   Kỹ năng giao tiếp và làm việc nhóm  2 TC  Bài giảng cung cấp cho sinh viên những kiến thức cơ bản về giao tiếp và  \t\tlàm   việc nhóm như: các khái niệm về giao tiếp, kỹ năng lắng nghe, kỹ năng  \t\tnói, kỹ năng thuyết trình và một số kỹ năng làm việc nhóm. Bài giảng  \t\tđược viết tóm tắt trong bốn bài, giúp sinh viên bước đầu tiếp cận với  \t\tphương pháp học theo tín chỉ: tự học, tự nghiên cứu, tự làm bài tập và  \t\tthảo luận nhóm.  47. Hệ quản trị dữ liệu  3 TC  Khái niệm về CSDL và hệ quản trị CSDL; Các thành phần cơ bản trong MS  \t\tSQL Server; Ngôn ngữ T-SQL và các đối tượng CSDL; Các tác vụ quản trị hệ  \t\tthống.  48.1 Kinh tế học đại cương  2 TC  Môn học bao gồm hai phần chính: Kinh tế học vi mô: đề cập đến các vấn đề  \t\tvề luật cung cầu, cơ chế thị trường của người tiêu dùng, hành vi của nhà  \t\tsản xuất, và các cấu trúc thị trường cạnh tranh; Kinh tế học vĩ mô: đề  \t\tcập đến các vấn đề chu kỳ kinh tế, đo lường sản lượng kinh tế quốc dân,  \t\ttổng cung, tổng cầu, các chính sách ổn định kinh tế vĩ mô.  48.2 Marketing căn bản  2 TC  Môn học Marketing căn bản giới thiệu những triết lý kinh doanh hiện đại,  \t\tđã chi phối cách thức doanh nghiệp tham gia vào thị trường, với bí quyết  \t\tcủa Marketing hiện đại là hiểu biết cặn kẽ nhu cầu, đây là nội dung cốt  \t\tlõi, là xuất phát của mọi hoạt động Marketing. Do vậy Marketing sẽ trang  \t\tbị cho người học khả năng thực hành Marketing một vũ khí độc đáo trên cơ  \t\tsở hiểu biết thị trường và khách hàng. Để xây dựng và triển khai chiến  \t\tlược Marketing hỗn hợp qua 4 công cụ : Sản phẩm, giá cả, phân phối và  \t\txúc tiễn hỗn hợp.  48.3 Quản lý chất lượng  2 TC  Học phần Quản lý chất lượng bao gồm những nội dung kiến thức sau đây:  \t\tcung cấp những khái niệm cơ bản về chất lượng, các quan điểm về chất  \t\tlượng trong nền kinh tế thị trường, các tiêu chuẩn chất lượng và đo  \t\tlường chất lượng; các phương pháp quản lý chất lượng; các hình thức kiểm  \t\ttra chất lượng. Hệ thống chất lượng ISO.  48.4. Quản l ý  \t\t\t\tcông nghệ  2 TC  Học phần này cung cấp kiến thức cơ bản về các khái niệm về công nghệ và  \t\tquản trị công nghệ; Dự báo, hoạch định công nghệ; Lựa chọn và đổi mới  \t\tcông nghệ và chuyển giao công nghệ; Các chiến lược công nghệ và quản lý  \t\tcông nghệ tại doanh nghiệp.  48.5 Quản l ý  \t\t\t\tdự án  2 TC  Học phần này cung cấp kiến thức cơ bản về dự án đầu tư như tổng quan về  \t\tdự án đầu tư; quá trình lập một dự án đầu tư (phân tích kỹ thuật công  \t\tnghệ, phân tích tài chính, phân tích kinh tế – xã hội và môi trường của  \t\tdự án đầu tư). Thẩm định một dự án đầu tư (cơ sở pháp lý, phương pháp và  \t\tkỹ thuật thẩm định). Quản lý dự án đầu tư (nội dung, phương pháp, quản  \t\tlý thời gian, tiến độ, phân phối nguồn lực, quản lý chất lượng và rủi ro  \t\tcủa dự án..).  48.6 Quản l ý  \t\t\t\tsản xuất  2 TC  Học phần Quản lý sản xuất bao gồm những nội dung kiến thức sau đây: Cung  \t\tcấp các kiến thức cơ bản về quản trị sản xuất và tác nghiệp trong doanh  \t\tnghiệp của nó như dự báo, quản trị công suất, lập kế hoạch, tổ chức sản  \t\txuất trong doanh nghiệp, quản trị dự trữ sản xuất hệ thống cung cấp đúng  \t\tthời hạn trong hệ thống sản xuất sản phẩm; Xác định mối quan hệ chặt chẽ  \t\tvới các chức năng quản lý khác.  49. Quản l ý  \t\t\t\tdự án công nghệ thông tin  2 TC  Môn học cung cấp những kiến thức cơ bản về quản lý dự án, quản lý dự án  \t\tCông nghệ thông tin, những kiến thức và kỹ năng liên quan đến việc lập  \t\tkế hoạch dự án, ước lượng chi phí,  quản lý chất lượng sản phẩm, quản lý  \t\trủi ro, lựa chọn nhân sự tham gia dự án.  50. An ninh mạng  3 TC  Mục tiêu của môn học là đào tạo các kiến thức cơ bản và nâng cao trong  \t\tvấn đề an ninh hệ thống mạng máy tính. Thông qua kiến thức môn học, học  \t\tviên nắm bắt được các thông tin, cách thức triển khai và phát triển các  \t\tgiải pháp an ninh hạ tầng mạng máy tính trong thực tế.  51. Công nghệ DOT NET  3 TC  Môn học nhằm cung cấp cho người học kiến thức cơ bản về lập trình .NET  \t\tthông qua Visual Basic.NET (C#.NET). Sau khi hoàn tất môn học, người học  \t\tcó thể vận dụng để xây dựng các ứng dụng nói chung (cơ sở dữ liệu, hệ  \t\tthống…). Tuy môn học không giới thiệu tất cả các hỗ trợ của .NET nhưng  \t\tngười học có thể tự tìm hiểu chúng qua các tài liệu tham khảo. Qua đó,  \t\tngười học được trang bị một công cụ lập trình đang được thịnh hành hiện  \t\tnay.  52.  Khai phá dữ liệu Web  2 TC  Môn học Khai phá dữ liệu web giới thiệu về các khái niệm cơ bản, các  \t\tphương pháp biểu diễn văn bản text-web, một số nội dung cơ bản trong xử  \t\tlý ngôn ngữ tự nhiên, tìm kiếm trên Internet và máy tìm kiếm, bài toán  \t\tphân cụm web và một số phương pháp cơ bản; bài toán phân lớp web và một  \t\tsố phương pháp cơ bản, bài toán trích chọn thông tin trên web và một số  \t\tphương pháp máy trạng thái hữu hạn cơ bản, sơ bộ về web ngữ nghĩa. Tổng  \t\thợp các phương pháp học máy bán giám sát trong khai phá web.  53. Cơ sở dữ liệu phân tán (distributed database)  3 TC  Cung cấp những kiến thức cơ bản về nguyên lý các hệ cơ sở dữ liệu phân  \t\ttán, bao gồm các nội dung về các chiến lược thiết kế và kiểm soát dữ  \t\tliệu. Lý thuyết phân mảnh không tổn thất thông tin và bài toán cấp phát  \t\tdữ dữ liệu phân tán trên mạng máy tính. Vấn đề về quản lý giao dịch, đặc  \t\ttrưng và các tính chất giao dịch. Vấn đề tương tranh và hiệu năng xử lý  \t\tphân tán.  54.   Thiết kế phát triển Web  3 TC  Khái niệm tổng quát về web tĩnh, website tĩnh, web động, website động.  \t\tGiới thiệu ngôn ngữ đánh dấu siêu văn bản: HTML, DHTML. Hướng dẫn kỹ  \t\tnăng và phương pháp tổ chức ứng dụng trên website. Ôn lại kiến thức về  \t\thệ quản trị CSDL SQL Server hoặc Access. Giới thiệu ngôn ngữ lập trình  \t\tweb động ASP và phương pháp kết nối CSDL với web. Áp dụng xây dựng một  \t\tứng dụng cụ thể về các website trên mạng.  60.1. Thương mại điện tử  2 TC  Môn học này cung cấp kiến thức tổng quát về lĩnh vực kinh doanh và thực  \t\tthi thương mại thông qua các phương tiện điện tử dựa trên nền tảng công  \t\tnghệ hiện đại, thương mại điện tử (eCommerce). Giới thiệu các mô hình,  \t\tcách thức quản lý và triển khai các ứng dụng thương mại trên nền tảng  \t\tcông nghệ hiện tại.  60.2. Kỹ nghệ phần mềm  2 TC  Môn học cung cấp cho sinh viên nền tảng lý thuyết và thực hành của kỹ  \t\tnghệ phần mềm. Trong phần lý thuyết, sinh viên sẽ học về các khái niệm  \t\tcơ bản của kỹ nghệ phần mềm và các hoạt động chính của kỹ nghệ phần mềm  \t\tnhư xác định tiến trình sản xuất phần mềm, quản lý dự án phần mềm, phân  \t\ttích và đặc tả các yêu cầu phần mềm, thiết kế hệ thống, lập trình, kiểm  \t\tthử, chuyển giao và bảo trì phần mềm. Đối với mỗi hoạt động này, môn học  \t\tgiới thiệu các khái niệm liên quan, một số phương pháp, công nghệ, kỹ  \t\tthuật, công cụ hỗ trợ và các nguyên lý thực hiện. Trong phần thực hành,  \t\tsinh viên sẽlàm quen với quá trình phát triển các sản phẩm phần mềm một  \t\tcách công nghiệp. Ngoài ra, sinh viên sẽ được tiếp cận một số hướng phát  \t\ttriển mới của kỹ nghệ phần mềm.  60.3. Quản trị mạng  2 TC  Môn học trang bị cho sinh viên những kiến thức cơ bản về giao thức quản  \t\ttrị mạng SNMP. Các kiến thức về Mạng máy tính, và các công nghệ hỗ trợ  \t\ttrong quản trị mạng đang được triển khai hiện nay. Cung cấp những kiến  \t\tthức cơ bản khi quản trị mạng dựa trên Hệ điều hành và các phần mềm hỗ  \t\ttrợ.  60.4. Lập trình mạng  2 TC  Cung cấp cho sinh viên những kiến thức về lập trình mạng bao gồm các kỹ  \t\tthuật lập trình dựa trên Socket, RMI, Protacol và một số kỹ thuật lập  \t\ttrình phân tán để ứng dụng nó trong những ứng dụng mạng.  60.5. Lập trình ứng dụng cho Mobile   2 TC  Học phần Lập trình ứng dụng cho các thiết bị di động trang bị cho sinh  \t\tviên các kiến thức cơ bản về công nghệ và kỹ thuật phát triển phần mềm  \t\tứng dụng cho thiết bị di động, có kỹ năng lập trình phát triển phần mềm  \t\tứng dụng cho thiết bị di động trên một vài nền tảng cụ thể như J2ME.  \t\tSinh viên được giới thiệu về Lập trình giao diện người dùng, lưu trữ với  \t\tMIDP; Lập trình mạng với MIDP.  60.6. Công nghệ WEB  2 TC  Học phần sẽ cung cấp cho sinh viên kiến thức ứng dụng công nghệ web như:  \t\tNgôn ngữ HTML, XHTML, CSS, Javascript và kỹ thuật lập trình web với  \t\tASP.NET sử dụng ngôn ngữ lập trình C#, kết hợp với hệ quản trị cơ sở dữ  \t\tliệu SQL Server gồm một số nội dung sau: Hiểu biết được các khái niệm  \t\ttổng quan liên quan về công nghệ lập trình web động và quản lý ứng dụng  \t\tthông qua các đối tượng trong ASP.Net;  Hiểu và vận dụng được các ngôn  \t\tngữ để thiết kế web như: HTML, XHTML, CSS, Javascript.  60.7. Công nghệ và thiết bị mạng  2 TC  Sinh viên sau khi kết thúc môn học sẽ nắm được các kiến thức cơ bản về  \t\tcác thiết bị quan trọng trong một mạng nội bộ cũng như mạng diện rộng,  \t\ttạo điều kiện cho sinh viên làm quen với một số thiết bị mạng của các  \t\thãng nổi tiếng như: Cisco, Nortel, IBM, Intel...   Đăng ký học phần Thư viện trực tuyến Thông báo mới Thông báo THÔNG BÁO  VÀ KẾ HOẠCH TỔ CHỨC GIẢI BÓNG ĐÁ HỌC SINH, SINH VIÊN  TRƯỜNG ĐẠI HỌC VIỆT BẮC   New \r\n\t\t\t\t\tTHÔNG BÁO VỀ DỊP NGHỈ KỶ NIỆM LỄ QUỐC KHÁNH 2/9 \r\n\t\t\t\t\tTHÔNG BÁO Về việc nộp hồ sơ xét tốt nghiệp đợt 1 của sinh viên lớp K1 Kinh tế   \r\n\t\t\t\t\tTHỜI KHÓA BIỂU HỌC KỲ I, NĂM HỌC 2017 - 2018   Thông báo Thông báo về việc xét kết quả học tập học kì II năm học 2014 - 2015  New   Trung tâm học liệu Thống kê Hôm nay 99 Hôm qua 569 Tuần này 668 Tuấn trước 4933 Tháng này 34199 Tháng trước 12853 Toàn bộ 1377019 \n\t\t\t\t\t\t\t\t\t\t\tHiện có 118 khách Trực tuyến\t\t\t\t\t Trường Đại học Việt Bắc Về Việt Bắc Tuyển sinh Đào tạo Hợp tác quốc tế Sinh viên Việt Bắc Sinh viên tương lai Cựu sinh viên Khoa Khoa Kế toán và Quản trị kinh doanh Khoa Kỹ thuật cơ khí Khoa Điện - Điện tử Khoa Công nghệ thông tin Trung tâm  Trung tâm Đào tạo theo nhu cầu xã hội Service Learning Thư viện Truy cập nhanh Liên hệ Dự tuyển vào Việt Bắc Tuyển dụng Trang chủ Giới thiệu Liên hệ \n    Địa chỉ: Đường 1B - Đồng Bẩm - TP Thái Nguyên \n    Điện Thoại: 0280 3755 878        \tFax: 0280 3753 746 \n    Email:  daihocvb@vietbac.edu.vn Copyright © 2011 - 2017 Trường Đại học Việt Bắc",
          "url": "http://www.vietbac.edu.vn/index.php/dao-tao/chuong-trinh-dao-tao-dai-hoc/cong-nghe-thong-tin/185",
          "relevance": "0"
        },
        {
          "title": "Chương 12: Bây giờ véc-tơ mới thật là véc-tơ",
          "content": "Blog của Chiến Skip to content #37 (không đề) Gửi bạn đọc Café Mẫu hình IV GIS Commons Hóa học Nguyên lý hóa học Lập trình Haskell MatLab Java Think Java Python Think Python Thống kê Think Stats Ngẫu nhiên và mô phỏng Toán-Cơ-Tin Vật lý tính toán Xử lý ảnh Cơ sở ←  Chương 11: Tối ưu hóa và nội suy Chương 1: Cơ chế của chương trình máy tính  → Hãy viết một dãy lệnh MATLAB để tính  F12  là véc-tơ biểu diễn lực mà hành tinh tác dụng lên ngôi sao, và  F21 , là lực do ngôi sao tác dụng lên hành tinh. Hãy gói các câu lệnh sau vào một hàm có tên  gravity_force_func  nhận vào các biến  P1 ,  m1 ,  P2 , và  m2  rồi trả lại  F12 . Hãy viết một chương trình mô phỏng quỹ đạo của Sao Mộc quanh Mặt Trời. Khối lượng của Mặt Trời vào khoảng  2,0 × 10 30  kg. Bạn có thể lấy số liệu khối lượng của Sao Mộc cũng như khoảng cách đến Mặt Trời và vận tốc của Sao Mộc trên quỹ đạo từ trang  http://en.wikipedia.org/wiki/Jupiter . Chạy chương trình để đảm bảo rằng Sao Mộc quay trọn 1 vòng quỹ đạo quanh Mặt Trời hết khoảng 4332 ngày. Để chắc rằng bạn hiểu được cách hoạt động của  animate_func , hãy thử chú thích để loại ra một số dòng lệnh xem điều gì xảy ra. Khi gọi  ode45  bạn có thể chỉ định một véc-tơ chứa các thời điểm cần phải tính kết quả. Sau đây là một ví dụ:\n end_time = 1000;\r\nstep = end_time/200;\r\n[T, M] = ode45(@rate_func, [0:step:end_time], W);\r\n Đối số thứ hai là một véc-tơ khoảng số chạy từ 0 đến 1000 với bước chạy quy định bởi  step . Vì  step  bằng  end_time/200 , nên sẽ có khoảng 200 hàng trong  T  và  M  (chính xác là 201). Tùy chọn này sẽ không ảnh hưởng đến độ chính xác của kết quả;  ode45  vẫn dùng biến thời gian để thực hiện ước tính, nhưng rồi nó sẽ nội suy các giá trị trước khi trả lại kết quả. Bạn có thể dùng  pause  để chạy hình động theo đúng thời gian thật. Sau khi vẽ từng hình và gọi nó  drawnow , bạn có thể tính thời gian từ đó đến hình kế tiếp và dùng  pause  để đợi:\n dt = T(i+1) - T(i);\r\npause(dt);\r\n Một hạn chế của phương pháp này là nó bỏ qua thời gian cần để vẽ hình, vì vậy nó có xu hướng chạy chậm, đặc biệt nếu hình vẽ phức tạp hoặc bước thời gian quá ngắn. Hãy dùng  animate_func  và  draw_func  để hiển thị kết quả mô phỏng Sao Mộc. Sửa các hàm này để sao cho thời gian một ngày mô phỏng tương đương với 0,001 giây đồng hồ—mỗi vòng quay sẽ mất khoảng 4,3 giây. Hãy viết một hàm có tên  energy_func  để nhận vào các kết quả  T  và  M  từ chương trình mô phỏng Sao Mộc, rồi tính tổng năng lượng (động năng và thế năng) của hệ với mỗi vị trí và vận tốc ước tính được. Vẽ đồ thị của kết quả này như một hàm theo thời gian và đảm bảo rằng nó giảm dần trong quá trình mô phỏng. Hàm cần viết phải tính được sự thay đổi tương đối về năng lượng, độ chênh lệch giữa năng lượng ban đầu và lúc kết thúc, và biểu thị độ chênh này theo phần trăm so với năng lượng ban đầu. Hãy chạy  ode45  với một loạt các giá trị của  RelTol  và đảm bảo rằng khi dung sai càng nhỏ thì tốc độ thất thoát năng lượng cũng chậm lại. Hãy chạy chương trình mô phỏng bạn viết với một trong các hàm giải ODE khác có trong MATLAB và xem chúng có bảo toàn năng lượng không. Nếu bạn đặt hai bát nước giống hệt nhau vào ngăn đá tủ lạnh, nước ở trong một bát thì ở nhiệt độ bình thường còn ở bát kia thì đang sôi, liệu bát nước nào sẽ đông đá trước? Gợi ý: bạn có thể sẽ cần tìm hiểu thêm về hiệu ứng Mpemba. Bạn được yêu cầu phải thiết kế một dường dốc để trượt ván; khác với những dốc trượt thông thường, cái này có thể xoay (kiểu bập bênh) quanh một chốt cố định. Người trượt ván tiến đến đường dốc, trước đó còn đi trên đất bằng, rồi lập tức leo dốc; họ không được phép đặt chân xuống. Nếu họ trượt đủ nhanh thì đường dốc sẽ xoay và họ sẽ chuyển tư thế thành trượt xuống một cách nhẹ nhàng. Sẽ có ban giám khảo chấm điểm kĩ thuật và nghệ thuật cho động tác trượt. Việc của bạn là thiết kế một dốc trượt để cho phép người trượt hoàn thành bài biểu diễn này, và tạo ra một mô hình vật lý của hệ, một chương trình mô phỏng để tính ra chuyển động của người trượt trên dốc, và hình động biểu diễn kết quả tính được. Một hệ hai sao bao gồm hai ngôi sao quay xung quanh lẫn nhau và đôi khi có cả những hành tinh quay quanh từng ngôi sao  4 . Trong hệ hai sao, một số quỹ đạo là “bền vững” theo nghĩa một hành tinh có thể bay trong quỹ đạo đó mà không bị đâm vào một trong hai sao, và không bị bay tuốt vào không trung. Mô phỏng là công cụ có ích phục vụ cho việc nghiên cứu bản chất của các quỹ đạo này, như đề cập đến trong Holman, M.J. and P.A. Wiegert, 1999, “Long-Term Stability of Planets in Binary Systems,”  Astronomical Journal  117, có thể tải về từ  http://citeseer.ist.psu.edu/358720.html . Hãy đọc bài báo này rồi sửa lại chương trình mô phỏng hành tinh đã viết để lặp lại hoặc mở rộng kết quả. Bình chọn Share this: Facebook Thư điện tử Google Twitter PrintFriendly RSS Feed Like this: Số lượt thích Đang tải... Liên quan 8 phản hồi Filed under  Mô hình hóa ,  Sách Tagged as  học sinh ,  hướng dẫn ,  khoa học ,  lập trình ,  MatLab ,  mô hình ,  mô phỏng ,  tài liệu ,  toán ,  vật lý ←  Chương 11: Tối ưu hóa và nội suy Chương 1: Cơ chế của chương trình máy tính  → quangchien \n\t\t\t\t\t29-11-2011 lúc 13:04 CHÚ Ý: Đường link đến bài báo trong Bài tập thứ hai (Hệ hai sao) là: http://arxiv.org/pdf/astro-ph/9809315 Phản hồi quangchien \n\t\t\t\t\t29-11-2011 lúc 13:08 ===== HẾT ===== \nChương 12 này đã kết thúc cuốn sách: “Mô hình hóa hiện tượng vật lý bằng MATLAB” (Physical Modeling with MATLAB) của tác giả Allen B. Downey. Tôi rất vui khi dịch cuốn sách này; hi vọng nó sẽ giúp bạn ít nhiều, không chỉ về lập trình mà còn về cách hiểu được cách thiết lập một mô hình toán. \n–NQC Phản hồi quangchien \n\t\t\t\t\t20-02-2012 lúc 4:18 Bạn đọc có thể tải về một bản PDF của cuốn sách, với nội dung gần giống hệt các chương đã được post lên đây, chỉ khác ở chỗ dùng ngôn ngữ Octave (có cú pháp giống MATLAB, nhưng phần mềm là tự do / miễn phí). Hãy bấm vào Menu “MATLAB” rồi xem “Ebook mới …” Phản hồi Pingback:  Chương 1. Các biến và giá trị | Blog của Chiến Pingback:  Chương 11: Tối ưu hóa và nội suy | Blog của Chiến Pingback:  Chương 6: Tìm nghiệm | Blog của Chiến Pingback:  Mô hình hóa hiện tượng vật lý bằng MATLAB | Blog của Chiến Huệ \n\t\t\t\t\t20-03-2016 lúc 10:35 cám ơn thầy đã dịch quyển sách này, nó rất dễ hiểu và hữu ích cho một du học sinh vừa bắt đầu học matlab như em! Nếu có thể mong thầy sẽ post nhiều bài về mô hình hóa vật lý trong Matlab hơn nữa ạ. \nTrân trọng cám ơn thầy và chúc thầy nhiều sức khỏe Phản hồi Nhập bình luận của bạn tại đây... Mời bạn điền thông tin vào ô dưới đây hoặc kích vào một biểu tượng để đăng nhập: Thư điện tử  (Địa chỉ của bạn được giấu kín) Tên Trang web  Bạn đang bình luận bằng tài khoản WordPress.com  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Twitter  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Facebook  (  Đăng xuất  /  Thay đổi  )  Bạn đang bình luận bằng tài khoản Google+  (  Đăng xuất  /  Thay đổi  ) Hủy bỏ Connecting to %s Notify me of new comments via email. Thông báo cho tôi bằng email khi có bài đăng mới. Đáng chú ý \n\t\t\t\t\t\tChương 1: Nguyên tử, phân tử, và ion\t\t\t\t\t \n\t\t\t\t\t\tChương 5: Cân bằng dung dịch: Axit và Bazơ\t\t\t\t\t \n\t\t\t\t\t\tChương 1. Các biến và giá trị\t\t\t\t\t \n\t\t\t\t\t\tChương 6: Phương thức trả lại giá trị\t\t\t\t\t \n\t\t\t\t\t\tChương 3: Các định luật chất khí và thuyết động học\t\t\t\t\t Bài viết mới Chương 8: Thuyết lượng tử và cấu trúc nguyên tử Hỏi – đáp bên lề dành cho tác giả Web (phần 2) Hỏi-đáp dành cho tác giả web (phần 1) Ngôn ngữ lập trình Lua: những hỏi – đáp bên lề (phần 2) Chương 7: Bảng tuần hoàn Các bài đã viết Các bài đã viết Môn học  Tháng Một 2017  (1)  Tháng Mười 2016  (1)  Tháng Năm 2016  (1)  Tháng Chín 2015  (1)  Tháng Tám 2015  (2)  Tháng Tư 2015  (1)  Tháng Hai 2015  (1)  Tháng Mười Một 2014  (1)  Tháng Chín 2014  (1)  Tháng Sáu 2014  (1)  Tháng Ba 2014  (1)  Tháng Hai 2014  (1)  Tháng Mười Hai 2013  (1)  Tháng Mười Một 2013  (6)  Tháng Mười 2013  (4)  Tháng Chín 2013  (2)  Tháng Tám 2013  (3)  Tháng Bảy 2013  (5)  Tháng Sáu 2013  (10)  Tháng Năm 2013  (6)  Tháng Ba 2013  (2)  Tháng Mười Hai 2012  (3)  Tháng Mười Một 2012  (3)  Tháng Mười 2012  (3)  Tháng Chín 2012  (5)  Tháng Tám 2012  (4)  Tháng Bảy 2012  (1)  Tháng Sáu 2012  (2)  Tháng Năm 2012  (4)  Tháng Tư 2012  (13)  Tháng Ba 2012  (20)  Tháng Hai 2012  (7)  Tháng Một 2012  (7)  Tháng Mười Hai 2011  (24)  Tháng Mười Một 2011  (17)  Tháng Sáu 2009  (1)  Tháng Sáu 2007  (1)  Tháng Bảy 2006  (4) Trang #37 (không có tiêu đề) Gửi bạn đọc AbiWord biến đổi Fourier Blitz bản đồ C con lắc công nghệ cơ sở dữ liệu cấu trúc dữ liệu dữ liệu enthalpy fractal GIS gnuplot Haskell html hàm hình thái hóa học hướng dẫn hướng đối tượng học sinh hỏi đáp hỗn loạn Java khoa học LibreOffice Lua lập trình lập trình cấu trúc lập trình hàm lọc số MatLab Microsoft Microsoft Office Monte-Carlo mô hình môi trường mô phỏng nguyên tử ngẫu nhiên nhập môn OpenOffice phi tuyến phân tử phương trình vi phân phổ thông Python raster sinh viên soạn thảo sóng tham số thuật toán thông tin thống kê toán trò chơi tài liệu tương quan vector văn bản vật lý vật lý tính toán vẽ đồ thị web Word xác suât xác suất xử lý xử lý ảnh đại chúng đệ quy địa vật ước lượng Truy cập mới nhất Blog của Chiến  · Học. Thực hành. Sáng tạo\t\t\t\t Tạo một website miễn phí hoặc 1 blog với WordPress.com. %d  bloggers like this:",
          "url": "https://quangchien.wordpress.com/2011/11/29/ch12/",
          "relevance": "0"
        },
        {
          "title": "Giải tích hàm",
          "content": "Bách khoa toàn thư mở Wikipedia Bạn có  tin nhắn mới  ( thay đổi gần đây ). \n\t\t\t\t\tBước tới:\t\t\t\t\t menu , \t\t\t\t\t tìm kiếm Giải tích hàm  là một ngành của  giải tích toán học  nghiên cứu các không gian vector được trang bị thêm một cấu trúc tôpô phù hợp và các toán tử tuyến tính liên tục giữa chúng. Chính việc nghiên cứu phổ của các toán tử đã dẫn đến việc nghiên cứu các đại số topo, một đối tượng khác của giải tích hàm. Các kết quả và phương pháp của nó thâm nhập vào nhiều ngành khác nhau như lý thuyết  phương trình vi phân thường ,  phương trình đạo hàm riêng , lý thuyết các bài toán  cực trị  và  biến phân , phương pháp tính, lý thuyết biểu diễn,... Ra đời vào những năm đầu của thế kỷ 20, bắt nguồn từ các công trình về phương trình tích phân của Hilbert, Fredholm,..., đến nay giải tích hàm tích lũy được những thành tựu quan trọng và nó đã trở thành chuẩn mực trong việc nghiên cứu và trình bày các kiến thức  toán học . Các khái niệm cơ bản [ sửa  |  sửa mã nguồn ] Không gian vector tôpô lồi địa phương. Đây có lẽ là loại không gian tổng quát nhất trong giải tích hàm. Các không gian Frechet, định chuẩn, Banach, Hilbert, là các trường hợp riêng quan trọng của các không gian vector tôpô lồi địa phương (sắp xếp theo thứ tự tính tổng quát giảm dần -> sự \"tinh tế\" tăng lên). Các toán tử tuyến tính liên tục giữa các không gian (còn gọi là đồng cấu). 2 trường hợp đặc biệt quan trọng là các phiếm hàm tuyến tính liên tục (dạng tuyến tính liên tục) và các tự đồng cấu. Giống như với các không gian, ta có các đại số tương ứng. Các đại số này dựa trên mô hình của đại số các tự đồng cấu, vì thế nên lý thuyết tổng quát về các đại số còn được gọi là lý thuyết đại số toán tử. Chú ý là khác với các không gian, các đại số thường chỉ xét trên trường số phức. Điều này là tự nhiên vì các tự đồng cấu chỉ có thể nghiên cứu \"tốt\" khi trường cơ sở là đóng đại số. Ngoài ra, dựa trên các tự đồng cấu tự liên hợp, người ta định nghĩa một lớp đại số định chuẩn rất quan trọng là các C*-đại số, không có sự tương ứng với các không gian! Vào năm 1932, Banach xuất bản cuốn sách \"Lý thuyết toán tử\", nội dung bao gồm những kết quả được biết vào thời đó về lý thuyết các không gian định chuẩn, đặc biệt là các định lý của Banach đã công bố trong các bài báo từ năm 1922-1929... Cuốn sách này làm cho Giải tích hàm có một tác động như cuốn sách của Van der Waerden về đại số, được xuất bản hai năm trước đó. Các nhà giải tích trên thế giới bắt đầu nhận thức được sức mạnh của phương pháp mới và áp dụng chúng vào các lĩnh vực khác nhau; các ký hiệu và thuật ngữ của Banach được chấp nhận rộng rãi, không gian định chuẩn đầy đủ được gọi là không gian Banach rồi chẳng bao lâu, lý thuyết này trở thành một phần bắt buộc trong chương trình đại học...  (Theo J. Dieudonné (1981)) Xem thêm [ sửa  |  sửa mã nguồn ] Brezis, H.:  Analyse Fonctionnelle , Dunod  ISBN 978-2-10-004314-9  or  ISBN 978-2-10-049336-4 Conway, John B.:  A Course in Functional Analysis , 2nd edition, Springer-Verlag, 1994,  ISBN 0-387-97245-5 Dunford, N. and Schwartz, J.T.:  Linear Operators, General Theory , and other 3 volumes, includes visualization charts Eidelman, Yuli, Vitali Milman, and Antonis Tsolomitis:  Functional Analysis: An Introduction , American Mathematical Society, 2004. Hutson, V., Pym, J.S., Cloud M.J.:  Applications of Functional Analysis and Operator Theory , 2nd edition, Elsevier Science, 2005,  ISBN 0-444-51790-1 Kolmogorov, A.N and Fomin, S.V.:  Elements of the Theory of Functions and Functional Analysis , Dover Publications, 1999 Kreyszig, Erwin:  Introductory Functional Analysis with Applications , Wiley, 1989. Lax, P.:  Functional Analysis , Wiley-Interscience, 2002 Lebedev, L.P. and Vorovich, I.I.:  Functional Analysis in Mechanics , Springer-Verlag, 2002 Michel, Anthony N. and Charles J. Herget:  Applied Algebra and Functional Analysis , Dover, 1993. Riesz, F. and Sz.-Nagy, B.:  Functional Analysis , Dover Publications, 1990 Rudin, W.:  Functional Analysis , McGraw-Hill Science, 1991 Schechter, M.:  Principles of Functional Analysis , AMS, 2nd edition, 2001 Shilov, Georgi E.:  Elementary Functional Analysis , Dover, 1996. Sobolev, S.L. :  Applications of Functional Analysis in Mathematical Physics , AMS, 1963 Yosida, K.:  Functional Analysis , Springer-Verlag, 6th edition, 1980 Tham khảo [ sửa  |  sửa mã nguồn ] Bài viết này vẫn còn  sơ khai . Bạn có thể giúp Wikipedia bằng cách  mở rộng nội dung  để bài được hoàn chỉnh hơn. x t s \n\t\t\t\t\t\tLấy từ “ https://vi.wikipedia.org/w/index.php?title=Giải_tích_hàm&oldid=26672971 ”\t\t\t\t\t Thể loại :  Sơ khai Giải tích hàm Thể loại ẩn:  Trang sử dụng liên kết tự động ISBN Trình đơn chuyển hướng Công cụ cá nhân Chưa đăng nhập Thảo luận cho địa chỉ IP này Đóng góp Mở tài khoản Đăng nhập Không gian tên Bài viết Thảo luận Biến thể Các hiển thị Đọc Sửa đổi Sửa mã nguồn Xem lịch sử Khác Tìm kiếm Xem nhanh Trang Chính Bài viết chọn lọc Tin tức Bài viết ngẫu nhiên Thay đổi gần đây Phản hồi lỗi Quyên góp Tương tác Hướng dẫn Giới thiệu Wikipedia Cộng đồng Thảo luận chung Giúp sử dụng Liên lạc Công cụ Các liên kết đến đây Thay đổi liên quan Các trang đặc biệt Liên kết thường trực Thông tin trang Khoản mục Wikidata Trích dẫn trang này In/xuất ra Tạo một quyển sách Tải về dưới dạng PDF Bản để in ra Tại dự án khác Wikimedia Commons Ngôn ngữ khác العربية Беларуская Български Català Čeština Dansk Deutsch Eesti Ελληνικά English Español فارسی Français Galego 한국어 Italiano עברית ქართული Қазақша Magyar မြန်မာဘာသာ Nederlands 日本語 Norsk nynorsk Piemontèis Polski Português Русский Shqip Slovenčina کوردی Suomi Svenska Türkçe Українська 中文 Sửa liên kết  Trang này được sửa đổi lần cuối lúc 02:19 ngày 18 tháng 6 năm 2017. Văn bản được phát hành theo  Giấy phép Creative Commons Ghi công–Chia sẻ tương tự ; có thể áp dụng điều khoản bổ sung. Với việc sử dụng trang web này, bạn chấp nhận  Điều khoản Sử dụng  và  Quy định quyền riêng tư . \nWikipedia® là thương hiệu đã đăng ký của  Wikimedia Foundation, Inc. , một tổ chức phi lợi nhuận. Quy định quyền riêng tư Giới thiệu Wikipedia Lời phủ nhận Nhà phát triển Tuyên bố về cookie Phiên bản di động",
          "url": "https://vi.wikipedia.org/wiki/Gi%E1%BA%A3i_t%C3%ADch_h%C3%A0m",
          "relevance": "0"
        },
        {
          "title": "Vận tốc",
          "content": "Bách khoa toàn thư mở Wikipedia \n\t\t\t\t\tBước tới:\t\t\t\t\t menu , \t\t\t\t\t tìm kiếm Bài viết này  không được  chú giải  bất kỳ  nguồn tham khảo nào .  Mời bạn giúp  hoàn thiện bài viết này  bằng cách bổ sung chú thích cho từng nội dung cụ thể trong bài viết tới  các nguồn đáng tin cậy . Các nội dung không có nguồn có thể bị nghi ngờ và  xóa bỏ . (tháng 5 năm 2016) Vận tốc  là  đại lượng vật lý  mô tả cả mức độ nhanh chậm lẫn chiều của chuyển động và được xác định bằng quãng đường đi được trong một đơn vị thời gian. Vận tốc ở đây được hiểu là  vận tốc dài  hay  vận tốc tuyến tính , phân biệt với  vận tốc góc . Trong vật lý, vận tốc được biểu diễn bởi  vectơ  (có thể hiểu là \"đoạn thẳng có hướng\"). Độ dài của vectơ vận tốc cho biết tốc độ nhanh chậm của chuyển động, và chiều của vectơ biểu thị chiều của chuyển động. Do đó, vận tốc là một  đại lượng hữu hướng , khác với  tốc độ , một  đại lượng vô hướng  đơn thuần mô tả tính nhanh chậm của chuyển động. Tốc độ là độ lớn của vectơ vận tốc.  Mục lục 1 Vận tốc trong chuyển động thẳng đều 2 Vận tốc trung bình 3 Vận tốc tức thời 3.1 Đơn vị 4 Tính tương đối 4.1 Cộng vận tốc trong Cơ học cổ điển 4.2 Cộng vận tốc trong Cơ học tương đối tính 5 Vận tốc góc 6 Thuyết tương đối hẹp 7 Xem thêm 8 Tham khảo 9 Liên kết ngoài Vận tốc trong chuyển động thẳng đều [ sửa  |  sửa mã nguồn ] Đối với một vật chuyển động thẳng đều, tốc độ và chiều chuyển động không thay đổi theo thời gian. Do đó, vector vận tốc có giá trị xác định và không đổi. Nếu đã biết chiều chuyển động, điều chúng ta quan tâm là tốc độ chuyển động, hay  quãng đường đi được trong một đơn vị thời gian . Để tính tốc độ chuyển động, chúng ta đơn giản lấy quãng đường đi được chia cho thời gian đi hết quãng đường đó. Trong chuyển động thẳng của một chất điểm, nếu chất điểm chuyển động theo một chiều ta chọn chiều đó làm chiều dương thì độ lớn của độ dời bằng quãng đường đi được của chất điểm. v = s t {\\displaystyle \\mathbf {v} ={\\frac {\\mathbf {s} }{t}}} t {\\displaystyle t}  là thời gian s {\\displaystyle \\mathbf {s} }  là quãng đường v {\\displaystyle \\mathbf {v} }  là tốc độ của chuyển động thẳng đều Trong  SI , quãng đường đo bằng  mét  (m), thời gian đo bằng  giây  (s) thì tốc độ có đơn vị là mét trên giây (m/s). Tốc độ có thể có những đơn vị khác, chẳng hạn như  kilomet / giờ  (km/h hoặc kgh), phụ thuộc vào đơn vị mà ta chọn cho  quãng đường  và  thời gian . Như vậy, khi nói một vật chuyển động thẳng đều với vận tốc 5 m/s (giả sử ta đã biết chiều chuyển động nên vận tốc ở đây đơn giản là tốc độ), điều đó có nghĩa là cứ mỗi 1 giây, vật đi được quãng đường 5 mét.1 km/h≈0,28 m/s. Vận tốc âm thanh là 344 m/s. Nếu quan tâm đến chiều chuyển động, ta có thể quy ước 1 trong 2 chiều là chiều dương và gán cho vận tốc giá trị dương khi vật chuyển động cùng chiều với chiều dương đã chọn và giá trị âm khi vật chuyển động theo chiều ngược lại. Vận tốc trung bình [ sửa  |  sửa mã nguồn ] Khi vận tốc của vật thay đổi theo thời gian, người ta có thể sử dụng khái niệm  vận tốc trung bình .  Vận tốc trung bình trong một khoảng thời gian nhất định được định nghĩa là tỉ số giữa sự thay đổi vị trí trong khoảng thời gian đang xét và khoảng thời gian đó . Phương trình toán học như sau: v t b = r − r 0 t − t 0 = Δ r Δ t {\\displaystyle \\mathbf {v_{tb}} ={\\frac {\\mathbf {r} -\\mathbf {r} _{0}}{t-t_{0}}}={\\frac {\\Delta {\\mathbf {r} }}{\\Delta t}}} v t b {\\displaystyle \\mathbf {v_{tb}} } : vận tốc trung bình r {\\displaystyle \\mathbf {r} } : vị trí cuối r 0 {\\displaystyle \\mathbf {r} _{0}} : vị trí đầu t {\\displaystyle t} : thời điểm cuối t 0 {\\displaystyle t_{0}} : thời điểm đầu kết quả phép trừ vector  r − r 0 {\\displaystyle \\mathbf {r} -\\mathbf {r} _{0}}  còn gọi là  độ dịch chuyển Vận tốc trung bình trên những khoảng thời gian khác nhau có thể mang những giá trị khác nhau. Thêm nữa, cần phân biệt với  tốc độ trung bình  được định nghĩa là tổng quãng đường đi được chia cho khoảng thời gian được xét: v ¯ = s ¯ t = s ¯ 1 + s ¯ 2 + . . . + s ¯ n t 1 + t 2 + . . . + t n {\\displaystyle {\\bar {v}}={\\frac {\\bar {s}}{t}}={\\frac {{\\bar {s}}_{1}+{\\bar {s}}_{2}+...+{\\bar {s}}_{n}}{t_{1}+t_{2}+...+t_{n}}}} v ¯ {\\displaystyle {\\bar {v}}} : tốc độ trung bình s : tổng quãng đường đi được trong khoảng thời gian t : khoảng thời gian s 1 ,  s 2 ,...,  s n  là những quãng đường thành phần đi được trong các khoảng thời gian thành phần  t 1 ,  t 2 ,...,  t n Theo định nghĩa này, tốc độ trung bình  không phải  là độ lớn của vận tốc trung bình. Khi nghiên cứu chuyển động biến đổi một cách chi tiết và chính xác, một đại lượng quan trọng hơn vận tốc trung bình được sử dụng. Đó là  vận tốc tức thời . Vận tốc tức thời [ sửa  |  sửa mã nguồn ] Vận tốc tức thời  mô tả  sự nhanh chậm và chiều chuyển động tại một thời điểm nào đó trên đường đi của vật . Nếu vận tốc trung bình cho ta một cái nhìn tổng quát về vận tốc của vật trong một khoảng thời gian xác định thì vận tốc tức thời cho ta một cái nhìn cụ thể, tại một thời điểm. Để tính vận tốc tức thời tại một thời điểm ta xét vận tốc trung bình trong khoảng thời gian vô cùng nhỏ tính từ thời điểm đó. Khái niệm  giới hạn  trong toán  giải tích  là công cụ quý giá giúp ta làm điều này: v = lim t → t 0 r − r 0 t − t 0 = lim Δ t → 0 Δ r Δ t {\\displaystyle \\mathbf {v} =\\lim _{t\\to t_{0}}{{\\mathbf {r} -\\mathbf {r} _{0}} \\over {t-t_{0}}}=\\lim _{\\Delta t\\to 0}{\\Delta {\\mathbf {r} } \\over \\Delta t}} Phương trình toán học trên cho biết: khi khoảng thời gian được xét  tiến dần  đến 0 thì vận tốc trung bình tiến dần đến vận tốc tức thời (tại thời điểm t 0 ). Giới hạn này đồng nghĩa với  đạo hàm  của vị trí theo thời gian. Từ đó, vận tốc tức thời được định nghĩa như sau: v = d r d t {\\displaystyle \\mathbf {v} ={\\frac {d\\mathbf {r} }{dt}}} Trong đó: v {\\displaystyle \\mathbf {v} }  là vectơ vận tốc tức thời r {\\displaystyle \\mathbf {r} }  là vectơ vị trí như một hàm số của thời gian t {\\displaystyle t}  là thời gian Diễn đạt bằng lời:  Vận tốc tức thời là đạo hàm của vị trí theo thời gian . Đơn vị [ sửa  |  sửa mã nguồn ] Trong hệ đo lường quốc tế  SI , vận tốc có  đơn vị  mét trên giây ( m / s ). Các đơn vị khác có thể được dùng để đo vận tốc là  km / h , km/s... Tính tương đối [ sửa  |  sửa mã nguồn ] Vận tốc của cùng một chuyển động có thể có những giá trị khác nhau đối với những  quan sát viên  khác nhau. Do đó, vận tốc có  tính tương đối . Ví dụ, một vật chuyển động (có vận tốc khác không) so với vật khác nhưng lại đứng yên (có vận tốc bằng không) so với chính mình. Để đo giá trị của vận tốc, người ta gắn với mỗi quan sát viên nói trên một  hệ trục tọa độ  để xác định  vị trí  trong  không gian  và một đồng hồ để xác định  thời gian . Hệ trục tọa độ và đồng hồ được gọi là  hệ quy chiếu . Các quan sát viên khác nhau có thể có hệ quy chiếu khác nhau và quan sát thấy các vận tốc khác nhau của cùng một vật thể đang chuyển động. Như vậy, vận tốc của chuyển động phụ thuộc vào hệ quy chiếu tại đó vị trí và thời gian được ghi nhận. Cộng vận tốc trong Cơ học cổ điển [ sửa  |  sửa mã nguồn ] Như đã nói ở trên, vận tốc có tính tương đối và, do đó, có thể nhận các giá trị khác nhau đối với các hệ quy chiếu khác nhau. Để \"chuyển đổi\" vận tốc từ hệ quy chiếu này sang hệ quy chiếu khác, người ta sử dụng  phép cộng vận tốc . Trong Cơ học cổ điển, công thức cộng vận tốc đơn giản là  phép cộng véctơ  được thể hiện như sau: v A B = v A C + v C B {\\displaystyle \\mathbf {v} _{AB}=\\mathbf {v} _{AC}+\\mathbf {v} _{CB}} Trong đó: v A B {\\displaystyle \\mathbf {v} _{AB}}  là vận tốc của A đối với B v A C {\\displaystyle \\mathbf {v} _{AC}}  là vận tốc của A đối với C v C B {\\displaystyle \\mathbf {v} _{CB}}  là vận tốc của C đối với B Như vậy,  vận tốc của một vật A đối với hệ quy chiếu B bằng vận tốc của A đối với một hệ quy chiếu trung gian C cộng với vận tốc của hệ quy chiếu trung gian đó đối với hệ quy chiếu B . Cộng vận tốc trong Cơ học tương đối tính [ sửa  |  sửa mã nguồn ] Vận tốc góc [ sửa  |  sửa mã nguồn ] Bài chi tiết:  Vận tốc góc Thuyết tương đối hẹp [ sửa  |  sửa mã nguồn ] Trong thuyết tương đối hẹp, vận tốc được mở rộng ra thành  vận tốc-4  trong  không-thời gian . Nó là đạo hàm theo thời gian của  véctơ vị trí-4 : U a := d x a d τ = d x a d t d t d τ = ( γ c , γ u ) {\\displaystyle U^{a}:={\\frac {dx^{a}}{d\\tau }}={\\frac {dx^{a}}{dt}}{\\frac {dt}{d\\tau }}=\\left(\\gamma c,\\gamma \\mathbf {u} \\right)} với  u  là véctơ vận tốc trong không gian ba chiều thông thường u i = d x i d t {\\displaystyle u^{i}={\\frac {dx^{i}}{dt}}} và  i  = 1, 2, 3. Chú ý rằng: U a U a = − c 2 {\\displaystyle U^{a}U_{a}=-c^{2}\\,} Xem thêm [ sửa  |  sửa mã nguồn ] Tốc độ Gia tốc Tốc độ phản ứng hóa học Vận tốc góc Vận tốc-4 Tham khảo [ sửa  |  sửa mã nguồn ] Sách giáo khoa Toán 5, Chương IV, Nhà xuất bản giáo dục Việt Nam Sách giáo khoa Vật lý 8, Chương I: Cơ học, Bài 2: Vận tốc, Nhà xuất bản giáo dục Việt Nam Sách giáo khoa Vật lý 10 và Sách giáo khoa Vật lý 10 nâng cao, Nhà xuất bản giáo dục Việt Nam Liên kết ngoài [ sửa  |  sửa mã nguồn ] Wikimedia Commons có thư viện hình ảnh và phương tiện truyền tải về  Vận tốc \n\t\t\t\t\t\tLấy từ “ https://vi.wikipedia.org/w/index.php?title=Vận_tốc&oldid=31823362 ”\t\t\t\t\t Thể loại :  Hoàn toàn không có nguồn tham khảo tháng 5 năm 2016 Vận tốc Đại lượng vật lý Cơ học Khái niệm vật lý Chuyển động Chuyển động học Thể loại ẩn:  Tham số bản mẫu ngày tháng không hợp lệ Bài cơ bản Bài cơ bản dài trung bình Trình đơn chuyển hướng Công cụ cá nhân Chưa đăng nhập Thảo luận cho địa chỉ IP này Đóng góp Mở tài khoản Đăng nhập Không gian tên Bài viết Thảo luận Biến thể Các hiển thị Đọc Sửa đổi Sửa mã nguồn Xem lịch sử Khác Tìm kiếm Xem nhanh Trang Chính Bài viết chọn lọc Tin tức Bài viết ngẫu nhiên Thay đổi gần đây Phản hồi lỗi Quyên góp Tương tác Hướng dẫn Giới thiệu Wikipedia Cộng đồng Thảo luận chung Giúp sử dụng Liên lạc Công cụ Các liên kết đến đây Thay đổi liên quan Các trang đặc biệt Liên kết thường trực Thông tin trang Khoản mục Wikidata Trích dẫn trang này In/xuất ra Tạo một quyển sách Tải về dưới dạng PDF Bản để in ra Tại dự án khác Wikimedia Commons Ngôn ngữ khác Afrikaans Alemannisch አማርኛ العربية Aragonés অসমীয়া Asturianu Bahasa Indonesia Bahasa Melayu বাংলা Bân-lâm-gú Basa Jawa Беларуская Беларуская (тарашкевіца)‎ Boarisch Bosanski Български Буряад Català Čeština ChiShona Cymraeg Dansk Deutsch Eesti Ελληνικά English Español Esperanto Euskara فارسی Français Frysk Gaeilge Gaelg Galego 한국어 Հայերեն हिन्दी Hrvatski Ido Interlingua IsiXhosa Íslenska Italiano עברית Қазақша Kiswahili Kreyòl ayisyen Latina Latviešu Limburgs Magyar Македонски മലയാളം मराठी Mìng-dĕ̤ng-ngṳ̄ မြန်မာဘာသာ Nederlands नेपाली 日本語 Nordfriisk Norsk Norsk nynorsk Occitan Oʻzbekcha/ўзбекча ਪੰਜਾਬੀ پنجابی ភាសាខ្មែរ Piemontèis Polski Português Română Runa Simi Русский Русиньскый Scots Shqip සිංහල Simple English Slovenčina Slovenščina Српски / srpski Srpskohrvatski / српскохрватски Suomi Svenska Tagalog தமிழ் Татарча/tatarça ไทย Тоҷикӣ Türkçe Українська اردو Võro 文言 Winaray ייִדיש 粵語 中文 Sửa liên kết  Trang này được sửa đổi lần cuối lúc 12:33 ngày 8 tháng 10 năm 2017. Văn bản được phát hành theo  Giấy phép Creative Commons Ghi công–Chia sẻ tương tự ; có thể áp dụng điều khoản bổ sung. Với việc sử dụng trang web này, bạn chấp nhận  Điều khoản Sử dụng  và  Quy định quyền riêng tư . \nWikipedia® là thương hiệu đã đăng ký của  Wikimedia Foundation, Inc. , một tổ chức phi lợi nhuận. Quy định quyền riêng tư Giới thiệu Wikipedia Lời phủ nhận Nhà phát triển Tuyên bố về cookie Phiên bản di động",
          "url": "https://vi.wikipedia.org/wiki/V%E1%BA%ADn_t%E1%BB%91c",
          "relevance": "0"
        },
        {
          "title": "Text Summarization",
          "content": "Thầy Nguyễn Minh Thành Search this site Giảng dạy An Toàn Bảo Mật HTTT Công Nghệ Phần Mềm Cấu Trúc Dữ Liệu Hệ thống phân tán Java căn bản Kỹ Thuật Lập Trình căn bản Lập Trình C++ nc Lập Trình Hướng Đối Tượng Lập Trình Mạng Java Công Nghệ Web Công Nghệ Web nâng cao Oracle Phân Tích Thiết Kế HTTT Quản Lý Dự Án Phần Mềm Tin Học Quản Lý (Access) Tin Học Đại Cương Xây Dựng Phần Mềm HDT Mã Nguồn Mở Sitemap Tôn Đức Thắng Tin học ứng dụng trong kinh doanh 2 Hồng Bàng Lập Trình Mạng - HB ITC CN WEB (Học Lại) HĐH Linux K13 Lớp ITE khóa 14 Liên Kế Website Microsoft MSDN Microsoft DreamSpark Nghiên cứu Information Retrieval Vector Space Model Text Summarization Outsourced Database Security Voice Recognization NLP Tools Master Thesis Site owners Thành Nguyễn Minh Text Summarization (Tổng hợp và báo cáo : Nguyễn Minh Thành) Link file gốc :  http://www.box.net/s/3i6voz9m887bayup17tm 1.\nĐịnh nghĩa Tóm tắt văn bản là quá trình rút trích những thông\ntin quan trọng nhất từ một hoặc nhiều nguồn để tạo ra phiên bản cô đọng, ngắn gọn\nphục vụ cho một hoặc nhiều người dùng cụ thể, hay một hoặc nhiều nhiệm vụ cụ thể. “The process of\ndistilling the most important information from a source (or sources) to produce\nan abridged version for a particular user (or users) and task (or tasks).”  [Mani    2001] 2.\nCác ứng dụng hiện tại - Tóm tắt tin tức  - Hỗ trợ bác sĩ trong điều trị - Tóm tắt kết quả tìm kiếm trong các search engine - Thu thập dữ liệu thông minh - Tóm tắt bài báo khoa học - Tóm tắt nội dung hội nghị, cuộc họp - Tóm tắt nội dung video, audio, … - Trả lời tự động 3.\nPhân loại tóm tắt 3.1\nTheo kết quả (output) - Tóm tắt rút trích (Extract) : là một bản tóm tắt    bao gồm các nội dung được rút trích từ văn bản\ngốc. - Tóm tắt tóm lược (Abstract) : là một bản tóm tắt\ncó chứa các nội dung không được thể hiện trong văn bản gốc. 3.2\nTheo mục đích hay chức năng tóm tắt (Function) - Tóm tắt chỉ thị (Indicative) : tóm tắt    nhằm cung cấp một chức năng tham khảo để chọn\ntài liệu đọc chi tiết hơn (ứng dụng trong tóm tắt kết quả tìm kiếm). Ví dụ : Trong tóm tắt tin tức,    tóm tắt đưa ra chi tiết chính của từng sự kiện. - Tóm tắt thông tin (Information) : tóm tắt bao gồm\ntất cả các thông tin nổi bật có trong văn bản nguồn tại nhiều mức độ chi tiết\nkhác nhau. - Tóm tắt đánh giá (Evaluation) : tóm tắt nhằm mục\nđích đánh giá vấn đề chính của văn bản nguồn, thể hiện quan điểm của tác giả đối\nvới công việc của họ. 3.3\nTheo nội dung : - Tóm tắt chung (Generalized) : tóm tắt nhằm mục\nđích đưa ra các nội dung quan trọng bao quát văn bản gốc. - Tóm tắt hướng truy vấn (Query-based) : tóm tắt nhằm\nmục đích đưa ra kết quả dựa vào câu truy vấn của người. Tóm tắt này thường được\nsử dụng trong quá trình tìm kiếm thông tin (information retreival). 3.4\nTheo miền dữ liệu - Tóm tắt trên 1 miền dữ liệu (Domain) : tóm tắt nhắm\nvào một miền nội dung nào đó, như tin tức khủng bố, tin tức tài chính… - Tóm tắt trên 1 thể loại (Genre) : tóm tắt nhắm vào\nmột thể loại văn bản nào đó, như báo chí, email, web, bài báo… - Tóm tắt độc lập (Independent) : tóm tắt cho nhiều\nthể loại và nhiều miền dữ liệu. 3.5\nTheo mức độ chi tiết - Tóm tắt tổng quan (overview) : tóm tắt miêu tả tổng\nquan tất cả các nội dung nổi bật trong văn bản nguồn. - Tóm tắt tập trung sự kiện (event) : tóm tắt miêu tả\nmột sự kiện cụ thể nào đó trong văn bản nguồn. 3.6\nTheo số lượng - Tóm tắt đơn văn bản - Tóm tắt đa văn bản. 3.7\nTheo ngôn ngữ - Tóm tắt đơn ngôn ngữ - Tóm tắt đa ngôn ngữ - Tóm tắt xuyên ngôn ngữ (cross-language) 4.\nCác thật ngữ  - Tỷ lệ nén (Compression Rate) : độ đo thể hiện bao\nnhiêu thông tin được cô đọng trong văn bản tóm tắt. Được tính như sau :  chiều dài tóm tắt / chiều dài văn bản gốc . - Độ nổi bật hay liên quan (Salient or Relevance) :\ntrọng số được gán cho thông tin trong văn bản thể hiện độ quan trọng của thông\ntin đó đối với toàn văn bản hay để chỉ sự liên quan của thông tin đó đối với\nyêu cầu truy vấn của người dùng. - Sự mạch lạc (coherence) : một văn bản được gọi là\nmạch lạc nếu tất cả các thành phần trong nó tuân theo một thể thống nhất về mặt\nnội dung và không có sự trùng lặp nào giữa các thành phần. 5.\nĐặc điểm của các bản tóm tắt  ·      \n Giảm nội dung\nthông tin : lượng nội dung trong bản tóm tắt phải ít hơn so với văn bản gốc,\nnhưng phải đảm bào vẫn còn những thông tin quan trọng, nổi bật. o    Mức độ giảm nội dung thông tin được đo bằng tỷ lệ\nnén. o    Hoặc các bản tóm tắt thường có một chiều dài nhất định\nđược mong muốn. ·       \n Nội dung thông\ntin  o    Phải trung thực hoặc tương đương với văn bản nguồn. o    Phải liên quan, phù hợp với yêu cầu người dùng ·       \n Định dạng tốt o    Phải có định dạng tốt về ngữ pháp và cấu trúc diễn\nngôn (cấu trúc nội dung của từng loại văn bản).  o    Phải có thể đọc và hiểu được đối với người dùng. 6.\nCác giai đoạn của hệ thống tóm tắt Theo [Hovy 1999, Mani 2001, Sparck Jones 1999], hệ\nthống tóm tắt văn bản tự động được chia thành 3 giai đoạn chính : ·      \n Phân tích\n(Analysis or Interpretation) : biểu diễn và hiểu văn bản nguồn  ·      \n Biến đổi\n(Transformation) : trích chọn những nội dung quan trọng ·      \n Tổng hợp\n(Synthesis or Realization) : tạo văn bản mới chứa những điểm chính, quan trọng\ncủa văn bản gốc. Các giai đoạn của hệ thống\ntóm tắt 7.\nCác yếu tố  Các yếu tố chính ảnh hưởng đến quá\ntrình tóm tắt văn bản, cũng như tác động đến quá trình phân loại : ·      \n Yếu tố đầu vào : o     Chuẩn văn bản (text form) : cấu trúc, kích thước, thể\nloại o     Loại chủ đề (Subject type) : phổ biến, đặc biệt hay\nhạn chế o     Số đơn vị  \n (unit) : đơn văn bản, đa văn bản ·      \n Yếu tố mục đích\n: o     Situation  o     Người đọc (audience) : chung hay chỉ định đặc biệt o     Mục đích sử dụng (use)  ·      \n Yếu tố đầu ra : o     Material o     Format o     Use 9.\nCác đặc trưng Tóm tắt văn bản là bài toán của thuộc\nlĩnh vực xử lý ngôn ngữ tự nhiên. Trong quá trình phân tích xử lý ngôn ngữ tự\nnhiên, văn bản phải được biểu diễn giúp cho máy tính có thể hiểu được văn bản\nnguồn. Quá trình phân tích xử lý ngôn ngữ tự nhiên có các mức độ sâu xử lý khác\nnhau như : mức hình thái, mức cú pháp và mức ngữ nghĩa. Tương tự như bài toán xử\nlý ngôn ngữ tự nhiên, tóm tắt văn bản cũng xử lý văn bản ở 3 mức độ tương tự\ntrong quá trình tóm tắt. Tại mỗi mức độ, các đặc trưng được xử dụng để xử lý\nvăn bản hoàn toàn khác nhau. 9.1 Mức độ hình thái ·         \n Đặc trưng về chủ\nđề (Thematic) : thống kê từ, tần suất từ, stop words, TF.IDF. ·         \n Đặc trưng về vị\ntrí (Location) : vị trí câu trong văn bản hay đoạn văn (câu đầu tiên trong mỗi\nđoạn, n câu đầu tiên của văn bản), phương pháp tiêu đề (câu chứa từ có trong\ntiêu đề), cue-words hay fixed-phrased (câu chứa những ngữ cố định). ·         \n Đặc trưng về sự\ntương đồng (similarity) : các từ có cùng dẫn xuất (common stem), độ tương đồng\ngiữa các câu. ·         \n Đặc trưng về khoảng\ncách gần (Proximity) : khoảng cách giữa các đơn vị văn bản phải gần nhau. ·         \n Đặc trưng về đồng\nxuất hiện (co-ocurrence) : các từ cùng xuất hiện trong nhiều văn bản. ·         \n Đặc trưng về tên\nriêng (proper name). ·         \n Đặc trưng về chiều\ndài câu (short-length cutoff) : bỏ nhưng câu ngắn. 9.2 Mức độ cú pháp ·         \n Đặc trưng về định\ndạng (format). ·         \n Đặc trưng về chủ\nđề trong văn bản (Threads of topic). ·         \n Đặc trưng về cấu\ntrúc chuỗi từ vựng (Lexical chains). ·         \n Đặc trưng về cấu\ntrúc lý luận (Rhetorical structure). 9.3 Mức độ ngữ nghĩa ·         \n Đặc trưng về mối\nliên quan giữa các từ theo từ điển (lexical cohension) : đồng nghĩa (synonymy),\nbao hàm (hypernymy), lặp lại (repetition). ·         \n Đặc trưng về đồng\ntham chiếu (coreference) ·         \n Đặc trưng về mối\nquan hệ logic. ·         \n Đặc trưng về mối\nquan hệ biểu diễn ngữ nghĩa. ·         \n Đặc trưng về mối\nquan hệ cú pháp (grammatical cohesion) : trùng lặp (anaphora), tĩnh lược\n(ellipsis), liên từ (conjuction). 10.\nCác hướng tiếp cận Mặc dù có 2 loại tóm tắt là tóm tắt\nrút trích (extraction) và tóm tắt tóm lược (abstraction), tuy nhiên để thực hiện\ntóm lược cần có một lượng tri thức đất đủ về lĩnh vực cần tóm tắt. Điều này hiện\nnay còn hạn chế nhiều, do đó các hướng tiếp cận đa số tập trung vào dạng tóm tắt\nrút trích câu. 10.1 Các hướng\ntiếp cận cho tóm tắt đơn văn bản 10.1.1 Phương\npháp thống kê Hầu hết các nghiên cứu đầu tiên cho\ntóm tắt đơn văn bản đều tập trung trên những văn bản kỹ thuật (các bài báo khoa\nhọc).    Các phương pháp cổ điển thường tập\ntrung vào các đặc trưng hình thái để tính điểm cho các câu và rút trích các câu\nquan trọng để đưa vào tóm tắt. Ý tưởng chính của hướng tiếp cận : ·         \n Thu tập ngữ liệu ·         \n Tạo các bản tóm\ntắt thủ công ·         \n Thiết kế các\ncông thức toán hay logic để tính điểm cho các câu ·         \n Lặp cho đến khi\ntóm tắt tự động đạt được tính tương đương với tóm tắt thủ công : o     Tính điểm cho từng câu để tạo ra bản tóm tắt cho từng\nvăn bản trong ngữ liệu dựa vào các đặc trưng về hình thái. o     So sánh tóm tắt được tạo tự động với tóm tắt được tạo\nthủ công o     Cải thiện lại phương thức tính điểm cho câu Các nghiên cứu đại\nđiện cho phương pháp này : ·         \n Luhn (1958) (cited 1552 , citeulike.org) download o     Sử dụng các đặc trưng như : word frequency, stop\nwords, word distance. o     Dùng phương pháp so khớp từng ký tự để giải quyết\nstemming. ·         \n Baxendale (1958) (cited 282 o     Sử dụng các đặc trưng như : sentence position. o     Thử nghiệm 200 đoạn văn, 85% các câu đầu là câu\nchính và 7% các câu cuối và câu chính. o     Phương pháp khá chính xác nhưng quá chủ quan và ngây\nngô. Phương pháp này được xử dụng khá nhiều vào các hệ thống máy học sau này. ·         \n Edmundson (1969) (cited 831 , citeseer.ist.psu.edu)\ndownload o     Điển hình nhất trong phương pháp cổ điển o     Sử dụng các đặc trưng như : word frequency, stop\nwords, position, cue words, title . o     Sử dụng phương pháp kết nối tuyến tính để kết hợp\ncác điểm đặc trưng lại với nhau : Si = w1*Ci + w2*Ki + w3*Ti + w4*Li o     Thử nghiệm với 400 văn bản kỹ thuật và kết quả đạt\n44%. 10.1.2 Phương\npháp thống kê trên TF.IDF Phương pháp này còn gọi là mô hình\ntúi từ (bag-of-words), sử dụng mô hình trọng số TF.IDF (term frequency và\ninverse sentence frequence). Ở mô hình này, giá trị IDF được tính trên câu.\nTrong đó, TF là số lần xuất hiện của term trong 1 câu. Và DF là số câu có chứa\nterm. Cùng với phương pháp tính độ đo\nTF.IDF và phương pháp biểu diễn văn bản bằng vector không gian sử dụng Vector\nSpace Model (Saton 1975). Tuy nhiên, phương pháp dùng độ đo\nTF.IDF không được dùng độc lập, mà thường được kết hợp với các phương pháp khác\nnhư máy học, đồ thị… để đạt được hiệu quả cao hơn. 10.1.3 Phương\npháp máy học Năm 1990, với sự phát triển của nhiều\nkỹ thuật máy học trong xử lý ngôn ngữ, một số nhà nghiên cứu đã ứng dụng các kỹ\nthuật này vào trong tóm tắt văn bản tự động. Một số nghiên cứu điển hiển của\nphương phát này là : Naïve-Bayes, Decision Tree, Hidden Makov Model,\nLog-Linear, Neural Network, SVM. Framework chung cho hệ thống tóm tắt\nvăn bản bằng phương pháp máy học  10.1.3.1 Phương\npháp Naïve-Bayes Các hướng tiếp cận theo phương pháp\nnày giả định rằng các đặc trưng của văn bản độc lập nhau. Sử dụng bộ phân lớp\nNaïve-Bayes để xác định câu nào thuộc về tóm tắt và ngược lại : Cho s là các câu cần xác định. F 1 …F k \nlà các đặc trưng đã được chọn, và giả định các thuộc tính độc lập nhau. Xác suất\ncủa câu s thuộc về tóm tắt được tính như sau : Sau khi tính xác\nsuất các câu, n câu có xác suất cao nhất sẽ được rút trích. Các nghiên cứu đại\nđiện cho phương pháp này : ·         \n Kupiec (1995) o     Các đặc trưng sử dụng : word frequency, location,\ncue word, title & leading, sentence length, uppercase words. o     Ngữ liệu : 188 cặp văn bản khoa học và tóm tắt. Tổng\nsố câu :    568 câu. Số câu khớp trực tiếp\nvới tóm tắt 451 (79%). ·         \n Aone (1999) o     Kết hợp thêm nhiều đặc trưng phong phú hơn : tf.idf\n(single word, two-noun word, named-entities), discourse (cohension) (sử dụng\nWordnet và kỹ thuật sử lý ngôn ngữ tự nhiên để phân tích sự tham chiếu đối với\ncác thực thể). o     Ngữ liệu : sử dụng ngữ liệu của TREC. o     Hệ thống : DimSum. 10.1.3.1a    Phương pháp OPP (Optimal Position Policy) Lin\nvà Hovy (1997)  đã nghiên cứu tính\nquan trọng của đặc trưng vị trí câu (sentence position) và cho rằng các câu\ntrong văn bản tuân theo một cấu trúc diễn ngôn (diễn giải) có thể dự đoán được.\nVà do cấu trúc trong các loại văn bản khác nhau, nên đặc trưng về vị trí câu\nkhông thể định nghĩa đơn giản như trong phương pháp Naïve Bayes. Lin và Hovy đã đề ra phương pháp  Optimal Position Policy  cho một thể loại\nvăn bản (văn bản tin tức của Ziff-Davis về máy tính và phần cứng). Phương pháp\nthực hiện ·         \n Với mỗi văn bản,\ntính năng suất của mỗi vị trí câu với các từ khoá chủ đề. ·         \n Xếp hạng các vị\ntrí câu với năng suất trung bình bằng thủ tục OPP. ·         \n Lấy ra n vị trí\ncâu trong bảng xếp hạng làm tóm tắt. 10.1.3.2 Phương\npháp Decision Tree Lin\n& Hovy (1999)  đại diện của\nphương pháp này giả định rằng, các đặc trưng không độc lập với nhau. Tác giả đã\nkiểm tra nhiều đặc trưng và ảnh hưởng của chúng lên quá trình rút trích. Hệ thống\ntóm tắt của Lin là loại tóm tắt hướng về truy vấn (query-based). Các đặc trưng : position(OOP), numeric\ndata, proper name, pronoun & adjective, weekday hoặc month. Cùng với 2 đăc\ntrưng mới : query signature (số từ truy vấn có trong câu) và IR signature (những\ntừ nổi bật, quan trọng ~ tf*idf).  Hệ thống  Summarist    của  Lin và Hovy  sử dụng thuật toán C4.5 để\nhuấn luyện cây quyết định. Hệ thống sử dụng tập ngữ liệu của TIPSTER-SUMMAC. 10.1.3.3 Phương\npháp Hidden Makov Model Những hướng tiếp cận trước đều dựa\ntrên những đặc trưng và không tuần tự.  Conroy\nvà O’leary (2001)  đã đưa ra hướng tiếp cận dựa trên mô hình HMM với ý tưởng\ncơ bản là sử dụng một chuỗi tuần tự các câu. Tác giả đưa ra khái niệm về sự phụ\nthuộc cục bộ (local dependencies) giữa các câu và sử dụng mô hình HMM để xác định\nsự phụ thuộc này. Các đặc trưng sử dụng : position,\nnumber of term, likelihood of sentence. Mô hình HMM bao gồm 2s + 1 trạng\nthái, trong đó s là số trạng thái tóm tắt (câu thuộc tóm tắt) và s+1 là câu\nkhông thuộc tóm tắt. ví dụ về mô hình Hidden\nMakov Model Mô hình HMM xây dựng ma trận chuyển\nvị M, coi các đặc trưng là đa biến và tính xác suất của các câu qua từng trạng\nthái. Sử dụng tập ngữ liệu của TREC và được\nđánh giá với 2 hệ thống khác là DimSum và QR, kết quả đều cho độ đo Precision\ncao hơn. 10.1.3.4 Phương\npháp Log-Linear              Osborne (2002)  đại diện cho mô hình này\ncũng coi các đặc trưng là không độc lập với nhau và sử dụng mô hình Log-Linear\nkhắc phục giả định này.              Các\nđặc trưng sử dụng : word pair, sentence length, sentence position và discourse\nfeatures (nằm trong introduction, hay conclusion).              Mô\nhình huấn luyện của Log-Linear được thực hiện như sau :              Trong đó, c là nhãn muốn gán cho câu\ns, f i  là đặc trưng thứ i và λ i  là trọng số kết nối các đặc\ntrưng. Nhãn c có 2 khả năng : thuộc tóm tắt hoặc không thuộc tóm tắt.              Giai đoạn phân lớp câu mới đươc thực\nhiện như sau :              Kết quả được đo bằng độ đo\nf2=2pr/(p+r). Tác giả đã đánh giá với hướng tiếp cận Bayes và kết quả luôn cho\nđộ đo f2 cao hơn. 10.1.3.5 Phương\npháp mạng Neural và đặc trưng của hãng thứ ba              DUC 2002 đã đưa ra một baseline rất\nmạnh cho tóm tắt đơn văn bản bằng phương pháp rút trích n câu đầu tiên của các\nbáo tin tức và dường như kết thúc hướng nghiên cứu này.              Nhưng  Svore (2007)  đã đưa ra một hướng tiếp cận mới sử dụng mạng Neural để\nhuấn luyện, kết qủa    cho thấy đã vượt qua\nbaseline của DUC 2002.              Các đặc trưng sử dụng : position,\nn-grams frequency. Ngoài ra, còn sử dụng thêm nhật ký truy vấn của bộ máy tìm\nkiêm Microsoft và Wordnet. Tác giả cho rằng, những câu có chứa từ khoá trong\ncác các câu truy vấn thì sẽ có kết quả tốt hơn, và tìm từ khoá đó trên Wordnet.              Mô hình được huấn luyện từ các đặc\ntrưng và các nhãn trong các bài báo. Sau đó được xếp hạng bằng hệ thống\nRankNet. Ngữ liệu được lấy từ CNN.com và được đánh giá bằng độ đo ROUGE-1 và\nROUGE-2 (hai độ đo phổ biến hiện tại cho tóm tắt văn bản). 10.1.4 Phương\npháp phân tích ngôn ngữ tự nhiên              Phương\npháp tiếp theo xử dụng các kỹ thuật phân tích ngôn ngữ tự nhiên phức tạp. Không\nphải tất cả các phương pháp phân tích ngôn ngữ tự nhiên đều xử dụng máy học,\nđôi khi phương pháp chỉ sử dụng một số các heuristic để tạo rút trích.  Hầu hết các phương pháp này đều dựa\ntrên cấu trúc diễn ngôn ( discourse\ntructure ) hay cấu trúc diễn đạt (thể hiện) của văn bản, như : cấu trúc các\nsection của văn bản, liên kết ngữ pháp (trùng lặp, tĩnh lược, liên hợp), liên kết\ntừ vựng (đồng nghĩa, bao hàm, lặp lại), cấu trúc chính phụ. Các nghiên cứu đại điện cho phương\npháp này : ·         \n Ono (1994) o     Xây dựng một thủ tục để rút trích các cấu trúc chính\nphụ (rhetorical structure) từ các văn bản tiếng Nhật, và xây dựng một cây nhị\nphân để thể hiện. o     Các bước để rút trích cấu trúc : phân tích câu, rút\ntrích một quan hệ chính phụ, phân đoạn, tạo ứng viên và đánh giá độ ưu tiên. o     Sau khi xây dựng cây, sẽ thực hiện tỉa nhánh để giảm\nbớt câu và tạo tóm tắt. o     Kết quả đạt được 51% các câu chính được xác định, và\n74% các câu quan trọng nhất được xác định. ·         \n Barzilay và Elhadad (1997) o     Hai tác giả cũng đã sử dụng một lượng đáng kể những\nphân tích ngôn ngữ trong tóm tắt văn bản dựa trên chuỗi từ vựng (lexical\nchain). Chuỗi từ vựng là chuỗi các từ liên quan trong văn bản. o     Các bước thực hiện : phân đoạn văn bản, xác định các\nchuỗi từ vựng và sử dụng các chuỗi từ vựng tốt nhất để xác định câu được chèn\nvào tóm tắt. o     Để tìm các chuỗi từ vựng, tác giả sử dụng Wordnet.\nCác từ có liên quan với nhau sẽ được đưa vào chuỗi. Sự liên quan được tính bằng\nkhoảng cách trong Wordnet. Chuỗi sẽ được tính điểm dựa vào chiều dài và sự đồng\nnhất của nó. o     Kết quả đạt được tốt hơn hệ thống tóm tắt của\nMicrosoft. Với độ Precious là 61 và độ recall 67 (Microsoft là 33 và 27). o     Hạn chế : không thể kiểm được chiều dài và mức độ\nchi tiết của tóm tắt do số chuỗi còn ít. Tóm tắt thiếu sự kết dính và chưa chi\nchiết do chọn cả câu. ·         \n Marcu (1998) o     Sử dụng các heuristic dựa trên cấu trúc diễn đạt với\ncác đặc trưng truyền thống. Lý thuyết về cấu trúc diễn đạt được tác giả thể hiện\nthông qua lý thuyết cấu trúc chính phụ (Rhetorical Structure Theory). Lý thuyết\ncho rằng hai khoảng văn bản không trùng lắp có quan hệ  trung tâm (nucleus) và vệ tinh (satellite ).    Trong đó trung tâm quan trọng hơn vệ tinh và\nđộc lập hoàn toàn trong cấu trúc chính phụ. Cấu trúc trọng tâm và vệ tinh được\nbiểu diễn thành cây nhị phân. o     Để tính điểm cho các cấu trúc, tác giả sử dụng nhiều\nđộ đo khác nhau như : clustering based metric, marker based metric, rhetorical\nclustering based technique, shape based metric, title based metric, position\nbased metric, connectedness based metric và sử dụng phương pháp kết hợp tuyến\ntính. Lấy ra n câu chứa cấu trúc có điểm cao nhất. o     Hệ thống đat được kết quả độ đo F 75.42% cao hơn\n3.5% so với baseline bằng phương pháp lấy n câu đầu. Ngữ liệu được sử dụng là từ\nTREC. 10.2 Các hướng\ntiếp cận cho tóm tắt đa văn bản Các vấn đề phát sinh trong tóm tắt\nđa văn bản là tính trùng lắp và bổ sung thông tin trong các nguồn văn bản. Do\nđó, nhiệm vụ trong tóm tắt đa văn bản không chỉ bao gồm việc sao chép dữ liệu từ\nnhững văn bản gốc sang bản tóm tắt mà còn đảm bảo tính mới, không dữ thừa của\nthông tin, cũng như đảm bảo tóm tắt có tính kết dính và hoàn chỉnh. 10.2.1 Phương\npháp dùng template              McKeown và Radev (1995,1998)  đã xây dựng\nhệ thống SUMMONS dựa trên hướng tiếp cận dùng template cho tóm tắt đa văn bản. Hệ thống đọc trong CSDL các tập mẫu\nđược xây dựng sẵn bởi một hệ thống khác. Trước tiên, hệ thống sẽ điền vào các\nchỗ trống trong template các thông tin từ các văn bản nguồn. Sau dó, hệ thống tổng\nhợp thành một bản tóm tắt và cho ra kết quả.  Hệ thống SUMMONS bao gồm 2 thành phần\nchính : content planner (lập nội dung) và linguistic generator (tạo ngôn ngữ).\nTuy nhiên, hệ thống chỉ có thể phục vụ cho một miền dữ liệu nhỏ. 10.2.2 Phương\npháp gom cụm chủ đề và hợp nhất thông tin McKeown  (1999) đã cải tiến hệ thống SUMMONS cũng như  Barzilay  (1999) bằng một hướng tiếp cận\ndựa trên gom cụm và hợp nhất thông tin. Hướng tiếp cận mới bao gồm 2 giai\nđoạn : - Gom cụm các đơn vị văn bản\n(clustering) : các đơn vị văn bản được biểu diễn bằng vector với các đặc trưng\nnhư TF-IDF, noun-phrase, proper noun, synset (tập đồng nghĩa từ Wordnet). Từng\ncặp đơn vị văn bản sẽ được tính độ tương đồng với nhau để phân loại cho các các\ncụm theo từng chủ đề (themes). - Hợp nhất thông tin : sau khi phân\ncụm,các cụm sẽ được so sánh với nhau bằng một giải thuật để tìm sự trùng lặp\nthông tin. Sau cùng hệ thống rút trích câu nổi bật trong từng cụm để làm tóm tắt,\nnếu trùng lắp thì câu suất hiện ở văn bản mới hơn sẽ dược rút trích. - Giải thuật để tìm sự trùng lắp là\nsử dụng bộ phân tích thống kê của Collin (1999) xây dựng cây phụ thuộc\n(dependency tree). 10.2.2 Phương\npháp gom cụm (cluster-based) với MMR Các văn bản thường được viết để giải\nquyết nhiều chủ đề khác nhau, mỗi chủ đề sẽ được viết sau chủ đề khác theo một\ncách có tổ chức. Mỗi chủ đề cũng có thể được viết vào những phần riêng biệt hoặc\nkhông. Các chủ đề này cũng cần được viết vào tóm tắt theo thứ tự như trong văn\nbản.Vấn đề đặt ra là làm thế nào xác định được các cụm chủ đề của văn bản. Phương pháp gom cụm là nhằm phân loại\ncác câu vào các cụm theo từng chủ đề mà chúng nói đến. Có nhiều phương pháp gom\ncụm khác nhau. Một trong số đó là phương pháp MMR (Maximal Marginal Relevance”\ncủa  Carbonell và Jade Goldstein (1998) . a. Phương pháp MMR cho IR Phương pháp MMR ban đầu được đề xuất\ncho lĩnh vực IR. Ý tưởng của phương pháp là kết nối tính liên quan đến câu truy\nvấn và tính mới của văn bản tìm kiếm. Độ đo MMR của một văn bản đối với một câu\ntruy vấn truy vấn như sau : Trong đó, Q là câu truy vấn, R là danh sách đã được\nsắp xếp các văn bản của quá trình IR, S là tập các văn bản được chọn, D i \nlà văn bản đang xét. Sim 1  là độ liên quan của D i  với Q,\nSim 2  là độ mới thông tin của D i .    λ là trọng số kết nối tuyến tính. Theo phương pháp này, thì một văn bản được chọn phải\ngần nhất với câu truy vấn và phải có tính mới đối với những văn bản đã chọn, tức\nlà có độ đo MMR cao nhất Thủ tục chọn lặp lại nhiều lần cho đến khi văn bản\ncó độ đo MMR cao nhất thấp hơn một ngưỡng  Q . Mô hình này có thể sử bất kỳ môt hình biểu diễn văn\nbản nào như mô hình vector, mô hình chủ đề ẩn hay một mô hình tương tự. Độ đo\ntương đồng Sim 1  và Sim 2  có thể sử dụng bất kì độ đo nào\nnhư Cosin, Log… b. Phương pháp MMR cho Text\nSummarization Mô hình MMR đã được áp dụng cho Text Summarization bởi\n Ganapathiraju  (2002), như sau : Trong đó, P là các câu từ các văn bản, D là tập các\nvăn bản, Q là câu truy vấn, C là tập các cụm câu, S tập các câu được chọn. Hệ thống tóm tắt với MMR bao gồm các chức năng : - Phân đoạn văn bản thành các câu. - Phân cụm các câu. - Tính điềm MMR cho các câu để chọn câu thích hợp được\nđưa vào tóm tắt. Phân cụm câu            - Các\ncâu được biểu diễn dưới dạng vector.            - Do\ncác vector câu rất thưa, nêu việc sử dụng giải thuật gom cụm dựa trên độ tương\nđồng của câu sẽ không chính xác. Do đó sử dụng giải thuật gom cụ tích tụ\n(agglomerative clustering) là lựa chọn tốt hơn.            - Các\nbước gom cụm :                Cho P1, P2,…PN là tập hợp các câu trong văn\nbản.                Định nghĩa các cluster C1, C2,..CN như sau P1\n ∈ C1 , P2  ∈ C2\n,...,PN  ∈ CN                Lặp để trộn\nCluster : cho Sim(Ci,Cj) là độ tương đồng giữa 2 trọng tâm của cluster, trộn\nhai cluster khi thoả :                Dừng trộn khi :  Tính điểm cho\ncác câu            - Các\ncâu được tính điểm theo công thức MMR ở trên.            - Độ\nđo Sim 1  của câu được tính bởi 4 độ đo :                 + Độ tương đồng với query                 + Độ bao phủ cluster                 + Nội dung của câu (có chứa tên riêng,\nngày…)                 + Nhãn thời gian : câu gần đây nhất sẽ quan\ntrọng hơn.            - Độ\nđo Sim2 được tính bởi 3 độ đo :                 + Độ tương đồng với các câu đã chọn                 + Thuộc vào cluster    đã đóng góp câu vào tóm tắt                 + Thuộc vào văn bản đã đóng góp câu vào tóm\ntắt Tập dữ liệu huấn luyện\nđược lấy từ DUC 2002. 10.2.3 Phương\npháp gom cụm với lý thuyết đồ thị Vấn đề chủ yếu của tóm tắt đa văn bản\nlà gom cụm các câu để tránh sự dư thừa trong tóm tắt. Một hướng tiếp cận mới là\nứng dụng lý thuyết đồ thị. Sau khi đã tiền xử lý văn bản như : loại bỏ stop\nword, xử lý dẫn suất (stemming), các câu được thể hiện thành các node trong một\nđồ thị vô hướng. Mỗi câu một node. Hai câu có những từ chung sẽ được nối một cạnh.\nHoặc sử dụng độ đo cosin, tính độ tương đồng giữa 2 câu, nếu lớn hơn một ngưỡng\n Q  thì sẽ có một cạnh nối\ngiữa 2 node. Sau khi biểu diễn thành đồ thị, đồ\nthị sẽ được phân hoạch thành các đồ thị con dựa vào các cạnh nối. Nếu tóm tắt cần\ntạo là tóm tắt dựa vào truy vấn, thì đồ thị nào gần với truy vấn nhất sẽ được\ndùng để tạo tóm tắt. Nếu tóm tắt là chung, thì tất cả các đồ thị con đều tham\ngia vào tóm tắt. Để tạo tóm tắt, mỗi đồ thị con sẽ\nđưa ra các node (câu) ứng viên cao nhất (các node có nhiều cạnh nối nhất) để\ninclude vào truy vấn. 10.2.4 Phương\npháp kích hoạt lan truyền trên đồ thị Mani\nvà Bloedorn (1997)  đã đề xuất một\nframework cho tóm tắt văn bản dựa trên đồ thị để tim sự tương đồng hoặc không\ntương đồng giữa các cặp văn bản. Phương pháp này không tạo ra văn bản tóm tắt\nnhưng highlight thông tin trên văn bản gốc. Văn bản được thể hiện thành đồ thị\nnhư sau : mỗi node biểu diễn sự xuất hiện của một từ đơn trong văn bản, mỗi\nnode có nhiều loại liên kết với các node khác như : SAME, ALPHA, PHRASE, NAME,\nCOREF. Sau khi tạo đồ thị, các node chủ đề\nsẽ được xác định bằng phương pháp phân tích dẫn xuất (stemming) và trở thành\nentry node. Các node sẽ được đánh trọng số bằng phương pháp TF*IDF.    Một sự tìm kiếm văn bản liên quan ngữ nghĩa\ngiữa các node sẽ được lan truyền. Trọng số của các node sẽ được thay đổi trong\nquá trình lan truyền. Các cặp đồ thị (các văn bản) sẽ được so sánh với nhau để\ntim những node chung dựa vào dẫn xuất cũng như đồng ngữ nghĩa. Điểm của các\nnode chung cũng sẽ được tính lại. Quá trình lan truyền lại tiếp tục cho đến khi\nkhông còn cập nhật trọng số điểm cho các node.  Sau cùng những câu chứa các node có\nđiểm chung và riêng cao sẽ được đánh dấu. 10.2.5 Phương\npháp dựa trên trọng tâm Radev (2004) đã đề xuất phương pháp\nsử dụng các trọng tâm của cụm đề làm trung tâm cho tóm tắt. Hướng tiếp cận này\nđã được phát triển trong hệ thống MEAD. Các văn bản trong hướng tiếp cận\nnày sử dụng mô hình túi từ. Bao gồm 2 giai đoạn : a. Giai đoạn xác định chủ đề : Các văn bản được biểu diễn dưới\nvector và một thuật toán gom cụm tích tụ (an agglomerative clustering) được sử\ndụng để thực hiện nhiệm vụ này. Văn bản liên quan nhất với trọng\ntâm cluster sẽ được thêm vào, và cluster sẽ được tính lại trọng tâm như sau  b. Giai đoạn chọn câu Trong mỗi cluster, chọn các câu là\ntrung tâm của chủ đề trong cụm. Hai độ đo tương tự MMR là cluster-based\nrelative utility (CBRU) and cross-sentence informational subsumption (CSIS) được\nsử dụng để độ liên quan của câu với chủ đề của cụm và độ dư thừa giũa các câu.\nTuy nhiên, hai độ đo này không phục thuộc truy vấn như MMR. Đề tính độ tương đồng, mỗi câu được\nbiểu diễn bởi các đặc trưng : centroid value, positional value, first-sentence\noverlap. Comments Bản quyền thuộc về Nguyễn Minh Thành Mọi thông tin lấy trên trang này vui lòng ghi chú tác giả. Sign in | Recent Site Activity | Report Abuse | Print Page | Powered By Google Sites",
          "url": "https://sites.google.com/site/trangmonhocitc/text-summarization",
          "relevance": "0"
        }
      ]
    }
  ]
}