{
  "authors": [
    "Phùng Đức Nhật",
    "Đặng Minh Hiếu",
    "Lưu Hữu Phước",
    "Trần Ngọc Tuyền"
  ],
  "modified": "21/11/2017 - 22:00:00",
  "collection": [
    {
      "query": "hồi quy tuyến tính",
      "description": "Muốn tìm hiểu về giải thuật hồi quy tuyến tính, bất kỳ trang web nào cung cấp thông tin về giải thuật hồi quy tuyến tính trong học máy: giải thích về một giải thuật sắp cụ thể, các ví dụ liên quan, hướng dẫn cài đặt giải thuật trên một ngôn ngữ lập trình đều là kết quả phù hợp.",
      "sites": [
        {
          "url": "https://vi.wikipedia.org/wiki/H%E1%BB%93i_quy_tuy%E1%BA%BFn_t%C3%ADnh",
          "title": "Hồi quy tuyến tính – Wikipedia tiếng Việt",
          "content": "Phân tích hồi quy tuyến tính là một phương pháp phân tích quan hệ giữa biến phụ thuộc Y với một hay nhiều biến độc lập X. Mô hình hóa sử dụng hàm tuyến tính (bậc 1). Các tham số của mô hình (hay hàm số) được ước lượng từ dữ liệu.Hồi quy tuyến tính được sử dụng rộng rãi trong thực tế do tính chất đơn giản hóa của hồi quy. Nó cũng dễ ước lượng.Hàm ước lượng thống kê được sử dụng phổ biến nhất là phương pháp bình phương nhỏ nhất. Khi mô hình có các phần sai số thỏa mãn bốn Giả thuyết Gauss-Markov, thì phương pháp ước lượng đó được coi là không chệch.Tiếp đó là phương pháp bình phương tối thiểu tổng quát hóa. Phương pháp được sử dụng khi sai số hoặc bị tương quan với nhau hoặc có hiệp phương sai không đồng nhất hoặc cả hai. Khi đó, phương pháp này sẽ hiệu quả để ước lượng (tính) các tham số beta.Kiểm đinh t được sử dụng để kiểm tra xem các tham số beta đã được lượng ở trên có khác không hoặc lớn (nhỏ hơn) một giá trị nhất định.",
          "relevence": "yes"
        },
        {
          "url": "https://www.slideshare.net/camthuninh/m-hnh-hi-qui-n-bin",
          "title": "Mô hình hổi qui đơn biến",
          "content": "\n      Slideshare uses cookies to improve functionality and performance, and to provide you with relevant advertising. If you continue browsing the site, you agree to the use of cookies on this website. See our User Agreement and Privacy Policy.\n    \n      Slideshare uses cookies to improve functionality and performance, and to provide you with relevant advertising. If you continue browsing the site, you agree to the use of cookies on this website. See our Privacy Policy and User Agreement for details.\n    \n            \n              Published on Apr 11, 2014\n            \n          \n                    Mô hình hổi qui đơn biến\rSlides ò Ms.kim Dung\n                  LinkedIn Corporation © 2017Clipping is a handy way to collect and organize the most important slides from a presentation. You can keep your great finds in clipboards organized around topics.Looks like you’ve clipped this slide to  already.",
          "relevence": "no"
        },
        {
          "url": "https://www.youtube.com/watch?v=bTzGlI2Kjtg",
          "title": "Bài giảng 30: Giới thiệu mô hình hồi qui tuyến tính (linear regression model), phần 1 - YouTube",
          "content": "\n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \n        Đang hoạt động...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \n        Đang hoạt động...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  Bài giảng 30, giới thiệu mô hình hồi qui tuyến tính (simple linear regression model), giả định, cách ước tính tham số, và ví dụ bằng R.\n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \n        Đang hoạt động...\n    \n  \n        \n\n    \n        Đang tải danh sách phát...\n    \n  ",
          "relevence": "yes"
        },
        {
          "url": "http://bis.net.vn/forums/t/722.aspx",
          "title": "\r\n\tPhân tích hồi qui tuyến tính với SPSS - BIS\r\n",
          "content": "Phân tích hồi qui tuyến tính với SPSSNguyễn Văn Chức – chuc1803@gmail.comHồi qui là một mô hình thống kê được sử dụng để dự đoán giá trị của biến phụ thuộc (dependence variable) hay còn gọi là biến kết quả dựa vào những giá trị của ít nhất 1 biến độc lập (independence variable) hay còn gọi là biến nguyên nhân. Nếu mô hình hồi qui phân tích sự phụ thuộc của 1 biến phụ thuộc vào 1 biến độc lập gọi là hồi qui đơn, nếu có nhiều biến độc lập gọi là hồi qui bội. Hồi qui tuyến tính là mô hình hồi qui trong đó mối quan hệ giữa các biến được biểu diễn bởi một đường thẳng (đường thẳng là đường phù hợp nhất với dữ liệu).Bài viết này giới thiệu sơ lược về mô hình hồi qui tuyến tính đơn và cách thực hiện hồi qui trên phần mềm SPSS v18Hồi qui tuyến tính đơnHồi qui tuyến tính đơn biểu diễn mối quan hệ giữa biến phụ thuộc vào 1 biến độc lập. Mô hình hồi qui được biểu diễn như sau:Ví dụ: Có dữ liệu về diện tích của 7 cửa hàng bán trái cây và doanh thu hàng năm  như sau:Yêu cầu: Lập mô hình hồi qui tuyến tính thể hiện mối quan hệ giữa doanh thu và diện tích của cửa hàng.Sử dụng SPSS1.     Mở SPSS, tạo một file dữ liệu mới với tên và định dạng của các biến trong tab Variable view như sau:Nhập dữ liệu trong tab : Data view như sau:2.Thực hiện hồi qui như sau:Trong menu Analyze, chọn Regression, chọn Linear, xuất hiện hộp thoại, chỉ định biến phụ thuộc trong ô Dependent và các biến độc lập trong ô Independent(s)Thiết lập thêm các tham số tùy chọn (nếu cần thiết) để giải thích mô hìnhBấm OK để thực hiện hồi qui. Kết quả như sau:Các hệ số hồi qui được cho trong bảng CoefficientsPhương trình hồi qui tương ứng Khi diện tích tăng lên 1 đơn vị, mô hình dự đoán doanh thu hàng năm tăng trung bình 1487$.Một số tham số quan trọng để đánh giá mô hình hồi qui.Tham số R bình phương hiệu chỉnh (Adjusted R Square) cho biết mức độ (%) sự biến thiên của biến phụ thuộc được giải thích bởi biến độc lập. Trong ví dụ này, có thể nói 93% sự biến đổi doanh thu hàng năm có thể được giải thích bằng sự biến đổi về qui mô của cửa hàng (đo bằng diện tích).Bảng ANOVAGiá trị của Sig( P-value) của bảng ANOVA dùng để đánh giá sự phù hợp (tồn tại) của mô hình. Giá trị Sig nhỏ (thường <5%) thì mô hình tồn tại.Giá trị Sig trong bảng Coefficients cho biết các tham số hồi qui có ý nghĩa hay không (với độ tin cậy 95% thì Sig<5% có ý nghĩa).Hệ số tương qua cho biết mức độ tương quan giữa biến phụ thuộc và biến độc lập (thường sử dụng hệ số tương quan Pearson)Trong ví dụ này, hệ số tương quan giữa biến phụ thuộc (Doanh thu) và biến độc lập (Diện tích) là 0.97, cho biết mối tương quan giữa Doanh thu và Diện tích là rất chặt.Chạy đồng thời nhiều dạng phương trình hồi qui để tìm dạng đường phù hợp nhất.Trong trường hợp chưa biết dạng đường nào (Linear, Quadratic, Logarithmic, Cubic, Power…) phù hợp với dữ liệu mẫu, ta chạy hồi qui nhiều dạng đường để chọn ra dạng đường phù hợp nhất.Trong menu Analyze, chọn Regression, chọn Cure Estimation, chọn dạng đường muốn thực hiện hồi qui:Kết quả mô hình hồi qui theo từng dạng đường được mô tả đầy đủ trong các bảng dữ liệu, ý nghĩa các tham số đã được giải thích như trong ví dụ hồi qui tuyến tính, từ đó giúp ta chọn được dạng đường phù hợp nhất với dữ liệu đã có. ",
          "relevence": "yes"
        },
        {
          "url": "http://phantichspss.com/cach-viet-phuong-trinh-hoi-quy-tuyen-tinh-da-bien.html",
          "title": "Cách viết phương trình hồi quy tuyến tính đa biến - Hỗ Trợ SPSS",
          "content": "\n\tSau khi phân tích hồi quy, sẽ ra được bảng kết quả. Vấn đề còn lại là thể hiện thành phương trình hồi quy như thế nào cho đúng. Hôm nay nhóm thạc sỹ QTDK ĐH Bách Khoa sẽ bàn chi tiết vấn đề này nhé.\n\n\t\t\t\t\tCoefficientsa\n\t\t\t\t\n\t\t\t\t\tModel\n\t\t\t\t\n\t\t\t\t\tUnstandardized Coefficients\n\t\t\t\t\n\t\t\t\t\tStandardized Coefficients\n\t\t\t\t\n\t\t\t\t\tt\n\t\t\t\t\n\t\t\t\t\tSig.\n\t\t\t\t\n\t\t\t\t\tCollinearity Statistics\n\t\t\t\t\n\t\t\t\t\tB\n\t\t\t\t\n\t\t\t\t\tStd. Error\n\t\t\t\t\n\t\t\t\t\tBeta\n\t\t\t\t\n\t\t\t\t\tTolerance\n\t\t\t\t\n\t\t\t\t\tVIF\n\t\t\t\t\n\t\t\t\t\t1\n\t\t\t\t\n\t\t\t\t\t(Constant)\n\t\t\t\t\n\t\t\t\t\t-1.005\n\t\t\t\t\n\t\t\t\t\t.230\n\t\t\t\t\n\t\t\t\t\t \n\t\t\t\t\n\t\t\t\t\t-4.376\n\t\t\t\t\n\t\t\t\t\t.000\n\t\t\t\t\n\t\t\t\t\t \n\t\t\t\t\n\t\t\t\t\t \n\t\t\t\t\n\t\t\t\t\tTINCAY\n\t\t\t\t\n\t\t\t\t\t.158\n\t\t\t\t\n\t\t\t\t\t.035\n\t\t\t\t\n\t\t\t\t\t.216\n\t\t\t\t\n\t\t\t\t\t4.550\n\t\t\t\t\n\t\t\t\t\t.000\n\t\t\t\t\n\t\t\t\t\t.741\n\t\t\t\t\n\t\t\t\t\t1.349\n\t\t\t\t\n\t\t\t\t\tDAPUNG\n\t\t\t\t\n\t\t\t\t\t.186\n\t\t\t\t\n\t\t\t\t\t.033\n\t\t\t\t\n\t\t\t\t\t.257\n\t\t\t\t\n\t\t\t\t\t5.688\n\t\t\t\t\n\t\t\t\t\t.000\n\t\t\t\t\n\t\t\t\t\t.820\n\t\t\t\t\n\t\t\t\t\t1.220\n\t\t\t\t\n\t\t\t\t\tDAMBAO\n\t\t\t\t\n\t\t\t\t\t.217\n\t\t\t\t\n\t\t\t\t\t.052\n\t\t\t\t\n\t\t\t\t\t.189\n\t\t\t\t\n\t\t\t\t\t4.191\n\t\t\t\t\n\t\t\t\t\t.000\n\t\t\t\t\n\t\t\t\t\t.822\n\t\t\t\t\n\t\t\t\t\t1.217\n\t\t\t\t\n\t\t\t\t\tCAMTHONG\n\t\t\t\t\n\t\t\t\t\t.295\n\t\t\t\t\n\t\t\t\t\t.053\n\t\t\t\t\n\t\t\t\t\t.271\n\t\t\t\t\n\t\t\t\t\t5.526\n\t\t\t\t\n\t\t\t\t\t.000\n\t\t\t\t\n\t\t\t\t\t.691\n\t\t\t\t\n\t\t\t\t\t1.446\n\t\t\t\t\n\t\t\t\t\tHUUHINH\n\t\t\t\t\n\t\t\t\t\t.226\n\t\t\t\t\n\t\t\t\t\t.045\n\t\t\t\t\n\t\t\t\t\t.224\n\t\t\t\t\n\t\t\t\t\t5.011\n\t\t\t\t\n\t\t\t\t\t.000\n\t\t\t\t\n\t\t\t\t\t.836\n\t\t\t\t\n\t\t\t\t\t1.196\n\t\t\t\t\n\t\t\t\t\tMINHBACH\n\t\t\t\t\n\t\t\t\t\t.223\n\t\t\t\t\n\t\t\t\t\t.044\n\t\t\t\t\n\t\t\t\t\t.224\n\t\t\t\t\n\t\t\t\t\t5.074\n\t\t\t\t\n\t\t\t\t\t.000\n\t\t\t\t\n\t\t\t\t\t.857\n\t\t\t\t\n\t\t\t\t\t1.167\n\t\t\t\t\n\t\t\t\t\ta. Dependent Variable: HAILONG\n\t\t\t\t\n\t\t\t\t\t \n\t\t\t\t\n\t Biến phụ thuộc ở đây là HAILONG, có 6 biến độc lập đều có ý nghĩa thống kê.\n\n\tPhương trình hồi quy có hai dạng, dạng thứ nhất là phương trình hồi quy viết theo hệ số chưa chuẩn hóa, dạng thứ nhì là phương trình hồi quy viết theo hệ số chuẩn hóa.\n\n\tHAILONG = -1.005+ 0.158*TINCAY + 0.186*DAPUNG + 0.217*DAMBAO + 0.295*CAMTHONG + 0.226*HUUHINH + 0.223*MINHBACH\n\n\tHAILONG = 0.216*TINCAY + 0.257*DAPUNG + 0.189*DAMBAO + 0.271*CAMTHONG + 0.224*HUUHINH + 0.224*MINHBACH\n\n\tXét biến TINCAY, nếu giải thích theo phương trình hồi quy chưa chuẩn hóa, nếu biến TINCAY tăng 1 đơn vị, thì biến HAILONG tăng 0.158 đơn vị. Nếu giải thích theo phương trình hồi quy đã chuẩn hóa, khi biến TINCAY tăng 1 đơn vị độ lệch chuẩn (standard deviation), thì biến HAILONG tăng 0.216 đơn vị độ lệch chuẩn. Lưu ý trong phương trình đã chuẩn hóa, hằng số constant = 0, nên không thể hiện ra.\n\n\tVậy tóm lại khi viết phương trình hồi quy nên dùng hệ số nào? Đã chuẩn hóa hay chưa chuẩn hóa. Vấn đề này gây ra rắc rối trong suy nghĩ đây :), và một số bạn suy nghĩ đơn giản, dạng như chuẩn hóa thì tốt hơn chưa chuẩn hóa, vì đã CHUẨN rồi mà. Như vậy chưa chính xác nhé. Thực tế khi làm luận văn, mà các biến độc lập có cùng đơn vị đo lường( như là thang đo likert) thì sử dụng phương trình viết theo hệ số chưa chuẩn hóa, vì lúc đó sẽ dễ dàng giải thích ý nghĩa kết quả hơn nhiều.\n\n\tGiới thiệu liên hệ nhóm ở đây, các bạn liên hệ khi có chỗ nào chưa hiểu rõ nhé: http://phantichspss.com/lien-he-gioi-thieu",
          "relevence": "yes"
        },
        {
          "url": "http://vietsciences.free.fr/khaocuu/nguyenvantuan/bieudoR/ch10-phantichoiqui.htm",
          "title": "Vietsciences; Nguyễn Văn Tuấn; Nguyen Van Tuan;Hướng dẫn phân tích số \nliệu và vẽ biểu đồ bằng R ;Phân tích hồi qui tuyến tính ; science, khoa hoc, \nkhoahoc, tin hoc,  informatique;computer; vat ly; physics, physique, \nchimie, chemistry, hoa hoc, sinh vat, b ",
          "content": "\n\t\n\t\t\t\t\t\t \n\t\n\t\t\t\t\t\t\n                            \n                              \n                                10\n                                    \n                                    Bảng 1.  Độ tuổi, tỉ trọng cơ thể và \n                                    cholesterol \n                                    > age \n                                    <- c(46,20,52,30,57,25,28,36,22,43,57,33,\n                                               22,63,40,48,28,49) \n > bmi \n                                    <-c(25.4,20.6,26.2,22.6,25.4,23.1,22.7,24.9,\n                                              19.8,25.3,23.2,21.8,20.9,26.7,26.4,21.2,\n                                              21.2,22.8) \n > chol \n                                    <- c(3.5,1.9,4.0,2.6,4.5,3.0,2.9,3.8,\n                                                2.1,3.8,4.1,3.0, 2.5,4.6,3.2,\n                                    \n                                                4.2,2.3,4.0) \n > data \n                                    <- data.frame(age, bmi, chol)> \n                                    plot(chol ~ age, pch=16) \n                                     \n\t\n\t\n                                          \n                                          Biểu đồ 10.1.  Liên hệ giữa độ \n                                          tuổi và cholesterol.Biểu đồ 10.1 trên \n                                    cho thấy mối liên hệ giữa độ tuổi (age) \n                                    và cholesterol là một đường thẳng (tuyến \n                                    tính). Để “đo lường” mối liên hệ này, chúng \n                                    ta có thể sử dụng hệ số tương quan \n                                    (coefficient of correlation).  \n \n                                    10.1  Hệ số \n                                    tương quan  \n \n                                    \n \n                                    \n                                    10.1.1  Hệ số tương quan Pearson\n                                    \n                                    \n                                     Trong đó, như \n                                    định nghĩa phần trên,\n                                     và\n                                     là \n                                    giá trị trung bình của biến số x và\n                                    y. Để ước tính hệ số tương quan giữa \n                                    độ tuổi\n                                    \n                                    \n                                    age \n                                    và cholesterol, chúng ta có thể sử dụng hàm\n                                    \n                                    \n                                    cor(x,y) \n                                    như sau: \n                                    Chúng ta có thể kiểm định giả thiết hệ số \n                                    tương quan bằng 0 (tức hai biến x và\n                                    y không có liên hệ). Phương pháp kiểm \n                                    định này thường dựa vào phép biến đổi Fisher \n                                    mà \n                                    \n                                    R \n                                    đã có sẵn một hàm \n                                    \n                                    cor.test \n                                    để tiến hành việc tính toán.\n                                    > cor.test(age, chol) Kết quả phân \n                                    tích cho thấy kiểm định t = 10.70 với trị số \n                                    p=1.058e-08; do đó, chúng ta có bằng chứng \n                                    để kết luận rằng mối liên hệ giữa độ tuổi và \n                                    cholesterol có ý nghĩa thống kê.  Kết luận \n                                    này cũng chính là kết luận chúng ta đã đi \n                                    đến trong phần phân tích hồi qui tuyến tính \n                                    trên. \n                                                                         \n                                                                        10.1.2  Hệ số tương quan Spearman \n                                    \n                                    r\n                                    \n                                    Cannot compute exact p-values with ties in: \n                                    cor.test.default(age, chol, method = \n                                    \"spearman\") \n                                    Kết quả phân tích cho thấy giá trị \n                                    rho=0.947, và trị số p=0.00000000257. Kết \n                                    quả từ phân tích này cũng không khác với \n                                    phân tích hồi qui tuyến tính: mối liên hệ \n                                    giữa độ tuổi và cholesterol rất cao và có ý \n                                    nghĩa thống kê. Hệ số tương \n                                    quan Kendall (cũng là một phương pháp phân \n                                    tích phi tham số) được ước tính bằng cách \n                                    tìm các cặp số (x, y) “song hành\" với \n                                    nhau. Một cặp (x, y) song hành ở đây \n                                    được định nghĩa là hiệu (độ khác biệt) trên \n                                    trục hoành có cùng dấu hiệu (dương hay âm) \n                                    với hiệu trên trục tung. Nếu hai biến số \n                                    x và y không có liên hệ với nhau, \n                                    thì số cặp song hành bằng hay tương đương \n                                    với số cặp không song hành.  Bởi vì có nhiều \n                                    cặp phải kiểm định, phương pháp tính toán hệ \n                                    số tương quan Kendall đòi hỏi thời gian của \n                                    máy tính khá cao. Tuy nhiên, nếu một dữ liệu \n                                    dưới 5000 đối tượng thì một máy vi tính có \n                                    thể tính toán khá dễ dàng. \n                                    \n                                    R \n                                    dùng hàm \n                                    \n                                    cor.test \n                                    với thông số \n                                    \n                                    method=”kendall” \n                                    để ước tính hệ số tương quan Kendall: Kết quả phân tích hệ số tương quan \n                                    Kendall một lần nữa khẳng định mối liên hệ \n                                    giữa độ tuổi và cholesterol có ý nghĩa thống \n                                    kê, vì hệ số tau = 0.833 và trị số p = \n                                    1.98e-06. Các hệ số \n                                    tương quan trên đây đo mức độ tương quan \n                                    giữa hai biến số, nhưng không cho chúng ta \n                                    một phương trình để nối hai biến số đó với \n                                    nhau. Do đó, vấn đề đặt ra là chúng ta tìm \n                                    một phương trình tuyến tính để mô tả mối \n                                    liên hệ này. Chúng ta sẽ ứng dụng mô hình \n                                    hồi qui tuyến tính.  \n                                    \n                                             \n                                    [1]Nói cách khác, \n                                    phương trình trên giả định rằng độ \n                                    cholesterol của một cá nhân bằng một hằng số\n                                    \n                                    \n                                    a \n                                    cộng với một hệ số \n                                    \n                                    b \n                                    liên quan đến độ tuổi, và một sai số \n                                    \n                                    ei. \n                                    Trong phương trình trên, \n                                    \n                                    a \n                                    là chặn (intercept, tức giá trị lúc\n                                    xi =0), và  \n                                    \n                                    b \n                                    là độ dốc (slope hay gradient). Trong thực \n                                    tế, \n                                    \n                                    a \n                                    và \n                                    \n                                    b \n                                    là hai thông số (paramater, còn gọi là \n                                    regression coefficient hay hệ số hồi \n                                    qui), và \n                                    \n                                    ei \n                                    là một biến số theo luật phân phối chuẩn với \n                                    trung bình 0 và phương sai \n                                    \n                                    s2.\n                                    Các thông \n                                    số \n                                    \n                                    a,\n                                    \n                                    \n                                    b \n                                    và \n                                    \n                                    s2 \n                                    phải được ước tính từ dữ liệu. Phương pháp \n                                    để ước tính các thông số này là phương pháp\n                                    bình phương nhỏ nhất (least squares \n                                    method). Như tên gọi, phương pháp bình \n                                    phương nhỏ nhất tìm giá trị \n                                    \n                                    a,\n                                    \n                                    \n                                    b \n                                    sao cho\n                                     nhỏ nhất. Sau vài \n                                    thao tác toán, có thể chứng minh dễ dàng \n                                    rằng, ước số cho \n                                    \n                                    a \n                                    và \n                                    \n                                    b \n                                    đáp ứng điều kiện đó là: \n                                    \n                                             \n                                    [2]\n                                    và\n                                    \n                                                    \n                                    [3]\n                                    \n                                    \n                                            \n                                    [4]\n                                    \n                                               \n                                    [5]\n                                    \n                                    > lm(chol ~ age)Trong lệnh trên,\n                                    \n                                    \n                                    “chol ~ age” \n                                    có nghĩa là mô tả \n                                    \n                                    chol \n                                    là một hàm số của \n                                    \n                                    age.  \n                                    Kết quả tính toán của \n                                    \n                                    lm \n                                    \n                                    cho thấy\n                                    =1.0892 \n                                    và=0.05779. \n                                     Nói cách khác, với hai thông số này, chúng \n                                    ta có thể ước tính độ cholesterol cho bất cứ \n                                    độ tuổi nào trong khoảng tuổi của mẫu bằng \n                                    phương trình tuyến tính: \n                                    \n                                    =\n                                    \n                                    \n                                    1.08922 + 0.05779 \n                                    \n                                    x \n                                    agePhương trình này \n                                    có nghĩa là khi độ tuổi tăng 1 năm thì độ \n                                    cholesterol tăng khoảng 0.058 mmol/L. \n                                    \n                                    \n                                    > reg <- lm(chol ~ age)\n                                    \n                                    > summary(reg)\n                                    \n                                     \n                                    \n                                    Call: lm(formula = chol ~ age)\n                                    \n                                     \n                                    \n                                    Residuals:\n                                    \n                                         Min       1Q   Median       3Q      Max\n                                    \n                                    \n                                    -0.40729 -0.24133 -0.04522  0.17939  0.63040\n                                    \n                                    \n                                     \n                                    \n                                    Coefficients:\n                                    \n                                                Estimate Std. Error t value \n                                    Pr(>|t|)    \n                                    \n                                    (Intercept) 1.089218   0.221466   4.918 \n                                    0.000154 ***\n                                    \n                                    age         0.057788   0.005399  10.704 \n                                    1.06e-08 ***\n                                    \n                                    ---\n                                    \n                                    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' \n                                    0.05 '.' 0.1 ' ' 1 \n                                    \n                                     \n                                    \n                                    Residual standard error: 0.3027 on 16 \n                                    degrees of freedom\n                                    \n                                    Multiple R-Squared: 0.8775,     Adjusted \n                                    R-squared: 0.8698 \n                                    \n                                    F-statistic: 114.6 on 1 and 16 DF,  p-value: \n                                    1.058e-08\n                                    \n                                    Residuals:\n                                    \n                                         Min       1Q   Median       3Q      Max\n                                    \n                                    \n                                    -0.40729 -0.24133 -0.04522  0.17939  0.63040\n                                    (b) Phần hai trình \n                                    bày ước số của\n                                    và cùng \n                                    với sai số chuẩn và giá trị của kiểm định \n                                    t.  Giá trị kiểm định t cho\n                                     là \n                                    10.74 với trị số p=0.0000000106, cho thấy \n                                    \n                                    \n                                    b \n                                    \n                                    không phải bằng 0.  Nói cách khác, chúng ta \n                                    có bằng chứng để cho rằng có một mối liên hệ \n                                    giữa cholesterol và độ tuổi, và mối liên hệ \n                                    này có ý nghĩa thống kê.  \n                                    \n                                     \n                                    \n                                    Coefficients:\n                                    \n                                                Estimate Std. Error t value \n                                    Pr(>|t|)    \n                                    \n                                    (Intercept) 1.089218   0.221466   4.918 \n                                    0.000154 ***\n                                    \n                                    age         0.057788   0.005399  10.704 \n                                    1.06e-08 ***\n                                    \n                                    ---\n                                    \n                                    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' \n                                    0.05 '.' 0.1 ' ' 1 \n                                    \n                                    Residual standard error: 0.3027 on 16 \n                                    degrees of freedom\n                                    \n                                    Multiple R-Squared: 0.8775,     Adjusted \n                                    R-squared: 0.8698 \n                                    \n                                    F-statistic: 114.6 on 1 and 16 DF,  p-value: \n                                    1.058e-08\n                                    \n                                                           \n                                    [6]Tức là bằng tổng \n                                    bình phương giữa số ước tính và trung bình \n                                    chia cho tổng bình phương số quan sát và \n                                    trung bình. Trị số R2 trong ví dụ \n                                    này là 0.8775, có nghĩa là phương trình \n                                    tuyến tính (với độ tuổi là một yếu tố) giải \n                                    thích khoảng 88% các khác biệt về độ \n                                    cholesterol giữa các cá nhân. Tất nhiên trị \n                                    số R2 có giá trị từ 0 đến 100% \n                                    (hay 1). Giá trị R2 càng cao là \n                                    một dấu hiệu cho thấy mối liên hệ giữa hai \n                                    biến số độ tuổi và cholesterol càng chặt \n                                    chẽ. \n                                                                        10.2.3  Giả định của phân tích hồi qui tuyến \n                                    tính(a) x là \n                                    một biến số cố định hay fixed, (“cố định” ở \n                                    đây có nghĩa là không có sai sót ngẫu nhiên \n                                    trong đo lường);\n                                    \n                                    (b) \n                                    \n                                    ei \n                                    phân phối theo luật phân phối chuẩn; \n                                    (c) \n                                    \n                                    ei \n                                    có giá trị trung bình (mean) là 0; \n                                    \n                                    \n                                    (d) \n                                    \n                                    ei \n                                    có phương sai \n                                    \n                                    s2 \n                                    cố định cho tất cả xi; và\n                                    (e) các giá trị \n                                    liên tục của \n                                    \n                                    ei \n                                    không có liên hệ tương quan với nhau (nói \n                                    cách khác, \n                                    \n                                    e1\n                                    \n                                    \n                                    và \n                                    \n                                    e2 \n                                    không có liên hệ với nhau). \n                                    Nếu các giả định này không được đáp ứng thì \n                                    mô hình mà chúng ta ước tính có vấn đề hợp \n                                    lí (validity). Do đó, trước khi trình bày và \n                                    diễn dịch mô hình trên, chúng ta cần phải \n                                    kiểm tra xem các giả định trên có đáp ứng \n                                    được hay không. Trong trường hợp này, giả \n                                    định (a) không phải là vấn đề, vì độ tuổi \n                                    không phải là một biến số ngẫu nhiên, và \n                                    không có sai số khi tính độ tuổi của một cá \n                                    nhân.   Đối với các giả định (b) \n                                    đến (e), cách kiểm tra đơn giản nhưng hữu \n                                    hiệu nhất là bằng cách xem xét mối liên hệ \n                                    giữa,\n                                    , \n                                    và phần dư\n                                     () \n                                    bằng những đồ thị tán xạ.  Với lệnh \n                                    \n                                    fitted() \n                                    chúng ta có thể tính toán\n                                     cho \n                                    từng cá nhân như sau (ví dụ đối với đối \n                                    tượng số 1, 46 tuổi, độ cholestrol có thể \n                                    tiên đoán như sau: \n                                    \n                                    1.08922 + 0.05779 \n                                    \n                                    x \n                                    46 = 3.747). \n                                    Với lệnh \n                                    \n                                    resid() \n                                    chúng ta có thể tính toán  phần dư\n                                     cho \n                                    từng cá nhân như sau (với đối tượng 1, e1 \n                                    = \n                                    \n                                    3.5 – 3.74748 = -0.24748):\n                                    \n                                    \n\t\n\t\n                                          \n                                          Biểu đồ 10.2.  Phân tích phần \n                                          dư để kiểm tra các giả định trong phân \n                                          tích hồi qui tuyến tính.\n                                    \n                                    Để kiểm tra các giả định trên, chúng ta có \n                                    thể vẽ một loạt 4 đồ thị \n                                    \n                                    treân như\n                                    \n                                    \n                                    sau: (a) Đồ thị bên \n                                    trái dòng 1 vẽ phần dư\n                                    và \n                                    giá trị tiên đoán cholesterol. \n                                    Đồ thị này cho thấy các giá trị phần dư tập \n                                    chung quanh đường y = 0, cho nên giả định \n                                    (c), hay \n                                    \n                                    ei \n                                    có giá trị trung bình 0, là có thể chấp nhận \n                                    được.  \n                                    \n                                    (b) Đồ thị bên phải dòng 1 vẽ giá trị phần \n                                    dư và giá trị kì vọng dựa vào phân phối \n                                    chuẩn. Chúng ta thấy các số phần dư tập \n                                    trung rất gần các giá trị trên đường chuẩn, \n                                    và do đó, giả định (b), tức  \n                                    \n                                    ei \n                                    phân phối theo luật phân phối chuẩn, cũng có \n                                    thể đáp ứng. \n                                    \n                                                                        10.2.4    \n                                    \n                                                                        Mô hình tiên đoán\n                                    Sau khi mô hình tiên đoán cholesterol đã \n                                    được kiểm tra và tính hợp lí đã được thiết \n                                    lập, chúng ta có thể vẽ đường biểu diễn của \n                                    mối liên hệ giữa độ tuổi và cholesterol bằng \n                                    lệnh \n                                    \n                                    abline \n                                    như sau (xin nhắc lại object của phân tích \n                                    là \n                                    \n                                    reg):\n                                    \n                                    \n                                    > plot(chol ~ age, pch=16)\n                                     \n                                    \n\t\n\t\n                                          \n                                          Biểu đồ 10.3.  Đường biểu diễn \n                                          mối liên hệ giữa độ tuổi (age) và \n                                          cholesterol. \n\t\t\t\t\t\n                                    \n                                    Nhưng mỗi giá trị\n                                     được tính \n                                    từ ước số \n                                    và, \n                                    mà các ước số này đều có sai số chuẩn, cho \n                                    nên giá trị tiên đoán\n                                     cũng \n                                    có sai số.  Nói cách khác,\n                                     chỉ \n                                    là trung bình, nhưng trong thực tế có thể \n                                    cao hơn hay thấp hơn tùy theo chọn mẫu.  \n                                    Khoảng tin cậy 95% này có thể ước tính qua\n                                    \n                                    \n                                    R \n                                    bằng các lệnh sau đây:  \n                                    \n \n                                    \n\t\n\t\n                                          \n                                          Biểu đồ 10.4.  Giá trị \n                                    tiên đoán và khoảng tin cậy 95% \n \n                                    \n                                    \n                                    10.3  Mô hình hồi qui tuyến tính đa biến \n                                    (multiple linear regression) \n                                    \n                                     \n                                    [7]\n                                    y1 \n                                    = \n                                    \n                                    a + \n                                    \n                                    b1x11 \n                                    + \n                                    \n                                    b2x21 \n                                    + …+ \n                                    \n                                    bkxk1 \n                                    +  \n                                    \n                                    e1\n                                    y2 \n                                    = \n                                    \n                                    a + \n                                    \n                                    b1x12 \n                                    + \n                                    \n                                    b2x22 \n                                    + …+ \n                                    \n                                    bkxk2 \n                                    +  \n                                    \n                                    e2\n                                    y3 \n                                    = \n                                    \n                                    a + \n                                    \n                                    b1x13 \n                                    + \n                                    \n                                    b2x23 \n                                    + …+ \n                                    \n                                    bkxk3 \n                                    +  \n                                    \n                                    e3\n                                    …\n                                    yn \n                                    = \n                                    \n                                    a + \n                                    \n                                    b1x1n \n                                    + \n                                    \n                                    b2x2n \n                                    + …+ \n                                    \n                                    bkxkn \n                                    +  \n                                    \n                                    en\n                                     \n                                    Phương pháp ước tính\n                                     cũng chủ yếu dựa vào phương \n                                    pháp bình phương nhỏ nhất. Gọi\n                                     là \n                                    ước tính của yi , \n                                    phương pháp bình phương nhỏ nhất tìm giá trị\n                                     sao \n                                    cho\n                                     nhỏ \n                                    nhất. Đối với mô hình \n                                    hồi qui tuyến tính đa biến, cách viết và mô \n                                    tả mô hình gọn nhất là dùng kí hiệu ma trận. \n                                    Mô hình [7] có thể thể hiện bằng kí hiệu ma \n                                    trận như sau: Y \n                                    = Xb\n                                    + \n                                    \n                                    e\n                                    \n                                    \n                                    \n                                    \n                                    Ví dụ 2.  \n                                    Chúng ta quay lại nghiên cứu về mối liên hệ \n                                    giữa độ tuổi, \n                                    \n                                    bmi \n                                    \n                                    và cholesterol. Trong ví dụ, chúng ta chỉ \n                                    mới xét mối liên hệ giữa độ tuổi và \n                                    cholesterol, mà chưa xem đến mối liên hệ \n                                    giữa  cả hai yếu tố độ tuổi và \n                                    \n                                    bmi \n                                    \n                                    và cholesterol. Biểu đồ sau đây cho chúng ta \n                                    thấy mối liên hệ giữa ba biến số này: \n                                    \n                                    \n                                    > pairs(data) \n                                    \n\t\n\t\n                                          \n                                          Biểu đồ 10.5.  Giá trị tiên \n                                          đoán và khoảng tin cậy 95%.Cũng như giữa độ \n                                    tuổi và cholesterol, mối liên hệ giữa bmi và \n                                    cholesterol cũng gần tuân theo một đường \n                                    thằng.  Biểu đồ trên còn cho chúng ta thấy \n                                    độ tuổi và bmi có liên hệ với nhau.  Thật \n                                    vậy, phân tích hồi qui tuyến tính đơn giản \n                                    giữa bmi và cholesterol cho thấy như mối \n                                    liên hệ này có ý nghĩa thống kê: \n                                    > summary(lm(chol ~  bmi))\n                                    \n                                    Call: lm(formula = chol ~ bmi)\n                                    Coefficients:\n                                    \n                                    hay phương trình \n                                    cũng có thể mô tả bằng kí hiệu ma trận: Y \n                                    = Xb\n                                    + \n                                    \n                                    e \n                                    \n                                     vừa trình bày ở trên. Ở đây, Y là \n                                    một vector vector 18 \n                                    \n                                    x \n                                    \n                                    1, X là một matrix 18 \n                                    \n                                    x \n                                    \n                                    2 phần tử, \n                                    \n                                    b \n                                    và một vector 2 \n                                    \n                                    x \n                                    \n                                    1, và \n                                    \n                                    e \n                                    \n                                    là vector gồm 18 \n                                    \n                                    x \n                                    \n                                    1 phần tử. Để ước tính hai hệ số hồi qui,\n                                    \n                                    \n                                    b1 \n                                    \n                                    và \n                                    \n                                    b2 \n                                    \n                                    chúng ta cũng ứng dụng hàm \n                                    \n                                    lm()trong\n                                    \n                                    \n                                    R \n                                    như sau: \n                                    \n                                    Cholesterol = 0.455 + 0.054(age) + \n                                    0.0333(bmi) \n                                    Chúng ta chú ý phương trình với độ tuổi \n                                    (trong phân tích phần trước) giải thích \n                                    khoảng 87.7% độ dao động cholesterol giữa \n                                    các cá nhân. Khi chúng ta thêm yếu tố BMI, \n                                    hệ số này tăng lên 88.2%, tức chỉ 0.5%.  Câu \n                                    hỏi đặt ra là 0.5% tăng trưởng này có ý \n                                    nghĩa thống kê hay không. Câu trả lời có thể \n                                    xem qua kết quả kiểm định yếu tố bmi với trị \n                                    số p = 0.487. Như vậy, bmi không cung cấp \n                                    cho chúng thêm thông tin hay tiên đoán \n                                    cholesterol hơn những gì chúng ta đã có từ \n                                    độ tuổi. Nói cách khác, khi độ tuổi đã được \n                                    xem xét, thì ảnh hưởng của bmi không còn ý \n                                    nghĩa thống kê. Điều này có thể hiểu được, \n                                    bởi vì qua biểu đồ 10.5 chúng ta thấy độ \n                                    tuổi và bmi có một mối liên hệ khá cao. Vì \n                                    hai biến này có tương quan với nhau, chúng \n                                    ta không cần cả hai trong phương trình. (Tuy \n                                    nhiên, ví dụ này chỉ có tính cách minh họa \n                                    cho việc tiến hành phân tích hồi qui tuyến \n                                    tính đa biến bằng \n                                    \n                                    R, \n                                    chứ không có ý định mô phỏng dữ liệu theo \n                                    định hướng sinh học).\n                                    \n\t\n\t\n                                          \n                                          Biểu đồ 10.6.  Phân tích phần \n                                          dư để kiểm tra các giả định trong phân \n                                          tích hồi qui tuyến tính đa biến.Tuy \n                                    BMI không có ý nghĩa thống kê trong trường \n                                    hợp này, Biểu đồ 10.6 cho thấy các \n                                    giả định về mô hình hồi qui tuyến tính có \n                                    thể đáp ứng.\n                                    yi \n                                    = \n                                    a \n                                    + \n                                    b1x \n                                    + \n                                    b2x2 \n                                    + \n                                    b3x3 \n                                    + .. + \n                                    bpxp \n                                    + \n                                    ei.\n                                    Trong \n                                    đó các thông số \n                                    bj\n                                    (j = 1, 2, 3, … p)\n                                    là hệ số đo lường mối liên hệ giữa y\n                                    và x; và \n                                    ei\n                                    là phần dư của mô hình, với giả định\n                                    ei\n                                    tuân theo luật phân phối chuẩn \n                                    với trung bình 0 và phương sai \n                                    s2. \n                                    Cho một dãy cặp số (y1, \n                                    x1), (y2,\n                                    x2), (y3,\n                                    x3), …, (yn,\n                                    xn), chúng ta có thể áp \n                                    dụng phương pháp bình phương nhỏ nhất để ước \n                                    tính \n                                    bj\n                                    và \n                                    s2.\n                                      \n                                    Trước \n                                    khi phân tích các số liệu này, chúng ta cần \n                                    nhập số liệu vào \n                                    R \n                                    với những lệnh thông thường như sau: \n                                     \n > id <- \n                                    1:19 \n > conc \n                                    <- c(1.0, 1.5, 2.0, 3.0, 4.0, 4.5,  5.0,  \n                                    5.5,\n                                                6.0, 6.5, 7.0, 8.0, 9.0, 10.0, \n                                    11.0, 12.0,\n                                               13.0, 14.0, 15.0)  \n > \n                                    strength <- c(6.3, 11.1, 20.0, 24.0, 26.1, \n                                    30.0, \n                                                    33.8, 34.0, 38.1, 39.9, \n                                    42.0, 46.1, \n                                                    53.1, 52.0, 52.5, 48.0, \n                                    42.8, 27.8, \n                                                    21.9) \n > data \n                                    <- data.frame(id, conc, strength) \n \n                                    Chúng \n                                    ta thử xem mô hình hồi qui tuyến tính đơn \n                                    giản bằng lệnh:  \n > \n                                    simple.model <- lm(strength ~ conc)> \n                                    summary(simple.model) \n Call: \n                                    lm(formula = strength ~ conc) \n \n                                    Residuals:    \n                                    Min      1Q  Median      3Q     Max \n                                    \n                                    -25.986  -3.749   2.938   7.675  15.840\n                                     \n \n                                    Coefficients:        \n                                        Estimate Std. Error t value Pr(>|t|)  \n                                    \n                                    (Intercept)  21.3213     5.4302   3.926  \n                                    0.00109 **\n                                    conc          1.7710     0.6478   2.734  \n                                    0.01414 * ---Signif. \n                                    codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' \n                                    0.1 ' ' 1  \n \n                                    Residual standard error: 11.82 on 17 degrees \n                                    of freedom\n                                    Multiple R-Squared: 0.3054,     Adjusted \n                                    R-squared: 0.2645 \n                                    F-statistic: 7.474 on 1 and 17 DF,  p-value: \n                                    0.01414 \n \n                                    Kết \n                                    quả trên cho thấy mô hình hồi qui tuyến tính \n                                    đơn giản này (strength \n                                    = 21.32 + 1.77*conc) \n                                    giải thích khoảng 31% phương sai của \n                                    strength. \n                                    Ước số phương sai của mô hình này là:   s2= \n                                    (11.82)2 = 139.7.   \n \n                                    Bây \n                                    giờ chúng ta xem qua biểu đồ và đường biểu \n                                    diễn của mô hình trên: \n                                    > \n                                    plot(strength ~ conc, \n                                           \n                                    xlab=\"Concentration of hardwood\", \n                                    \n                                           \n                                    ylab=\"Tensile strength\",      \n                                    main=\"Relationship between hardwood \n                                    concentration \n                                                 \\n and tensile strengt\", \n                                    pch=16) \n \n                                    \n\t\n\t\n\t\t\t\t\t                Biểu đồ 10.7. Mối liên hệ giữa hàm \n                                          lượng gỗ cứng và độ căng mạnh của vật \n                                          liệu. Đường thẳng là đường biểu diễn \n                                          của mô hình hồi qui tuyến tính đơn \n                                          giản. \n\t\t\t\t\t \n Qua biểu đồ này, \n                                    chúng ta thấy rõ ràng mô hình hồi qui tuyến \n                                    tính không thích hợp cho số liệu, bởi vì mối \n                                    liên hệ giữa hai biến này không tuân theo \n                                    một phương trình đường thẳng, mà là một \n                                    đường cong.  Nói cách khác, một mô hình \n                                    phương trình bậc hai có lẽ thích hợp hơn.  \n                                    Gọi y là strength và x là \n                                    conc, chúng ta có thể viết mô hình đó như \n                                    sau:  \n \n                                    yi \n                                    = \n                                    a \n                                    + \n                                    b1x \n                                    + \n                                    b2x2 \n \n                                    Bây \n                                    giờ chúng ta sẽ sử dùng \n                                    R \n                                    để ước tính ba thông số trên. \n \n                                    > \n                                    quadratic <- lm(strength ~ poly(conc, 2))\n                                    > \n                                    summary(quadratic) \n \n                                    Call:\n                                    lm(formula \n                                    = strength ~ poly(conc, 2)) \n \n                                    Residuals:\n                                        \n                                    Min      1Q  Median      3Q     Max \n                                    \n                                    -5.8503 \n                                    -3.2482 -0.7267  4.1350  6.5506  \n \n                                    \n                                    Coefficients:\n                                    \n                                                   Estimate Std. Error t value \n                                    Pr(>|t|)    \n                                    \n                                    (Intercept)      34.184      1.014  33.709 \n                                    2.73e-16 ***\n                                    poly(conc, \n                                    2)1   32.302      4.420   7.308 1.76e-06 ***\n                                    poly(conc, \n                                    2)2  -45.396      4.420 -10.270 1.89e-08 ***\n                                    ---\n                                    Signif. \n                                    codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' \n                                    0.1 ' ' 1  \n \n                                    Residual \n                                    standard error: 4.42 on 16 degrees of \n                                    freedom\n                                    Multiple \n                                    R-Squared: 0.9085,     Adjusted R-squared: \n                                    0.8971 \n                                    \n                                    F-statistic: 79.43 on 2 and 16 DF,  p-value: \n                                    4.912e-09 \n       y \n                                    = 34.18 + 32.30*x – 45.4*x2 \n                                     \n \n                                    giải thích khoảng 91% phương sai của y.  \n                                    Phương sai của y bây giờ là   s2 \n                                    = (4.42)2 = 19.5.  So với mô hình \n                                    tuyến tính, mô hình này rõ ràng là tốt hơn \n                                    rất nhiều. \n       Chúng \n                                    ta thử xét một mô hình cubic (bậc ba): \n       yi \n                                    = \n                                    a \n                                    + \n                                    b1x \n                                    + \n                                    b2x2\n                                    + \n                                    b3x3 \n                                     \n Xem \n                                    có mô tả y tốt hơn mô hình phương \n                                    trình bậc hai hay không. \n > cubic \n                                    <- lm(strength ~ poly(conc, 3))> \n                                    summary(cubic)Call: \n                                    lm(formula = strength ~ poly(conc, 3)) \n \n                                    Residuals:     \n                                    Min       1Q   Median       3Q      Max\n                                    \n                                    -4.62503 -1.61085  0.04125  1.58922  5.02159\n                                     \n \n                                    Coefficients:\n                                                   Estimate Std. Error t value \n                                    Pr(>|t|)    \n                                    (Intercept)     34.1842     0.5931  57.641  \n                                    < 2e-16 ***\n                                    poly(conc, 3)1  32.3021     2.5850  12.496 \n                                    2.48e-09 ***\n                                    poly(conc, 3)2 -45.3963     2.5850 -17.561 \n                                    2.06e-11 ***\n                                    poly(conc, 3)3 -14.5740     2.5850  -5.638 \n                                    4.72e-05 ***---Signif. \n                                    codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' \n                                    0.1 ' ' 1  \n \n                                    Residual standard error: 2.585 on 15 degrees \n                                    of freedom\n                                    Multiple R-Squared: 0.9707,     Adjusted \n                                    R-squared: 0.9648 \n                                    F-statistic: 165.4 on 3 and 15 DF,  p-value: \n                                    1.025e-11 \n # \n                                    lặp lại các mô hình trên: > \n                                    linear <- lm(strength ~ conc)> \n                                    quadratic <- lm(strength ~ poly(conc, 2))> cubic \n                                    <- lm(strength ~ poly(conc, 3)) \n # \n                                    tạo nên một biến x với nhiều số gần nhau> xnew \n                                    <- (0:160)/10  \n # \n                                    Tính giá trị tiên đoán (predictive values) \n                                    của y > y2 = \n                                    predict(quadratic, data.frame(conc=xnew)) \n                                    > y3 = \n                                    predict(cubic, data.frame(conc=xnew)) \n                                     \n # Vẽ \n                                    3 đường thẳng, bậc hai và bậc 3  \n                                    > \n                                    plot(strength ~ conc, pch=16,       \n                                    main=”Hardwood concentration and tensile \n                                    strength”,       \n                                    sub=”Linear, quadratic, and cubic fits”)> \n                                    abline(linear, col=”black”)> \n                                    lines(xnew, y2, col=”blue”, lwd=3)> \n                                    lines(xnew, y3, col=”red”, lwd=4) \n                                     \n                                    10.5  Xây \n                                    dựng mô hình tuyến tính từ nhiều biến \n      Cho một mô hình \n                                    hồi qui tuyến tính\n                                    , \n                                    chúng ta có k+1 thông số\n                                    ), \n                                    và có thể tính tổng bình phương phần dư \n                                    (residual sum of squares, RSS):\n                                    \n                                    \n                                    \n                                    \n                                    Ví dụ 4. \n                                    Để nghiên cứu ảnh hưởng của các yếu tố như \n                                    nhiệt độ, thời gian, và thành phần hóa học \n                                    đến sản lượng CO2. Số liệu của \n                                    nghiên cứu này có thể tóm lược trong bảng số \n                                    2. Mục tiêu chính của nghiên cứu là tìm một \n                                    mô hình hồi qui tuyến tính để tiên đoán sản \n                                    lượng CO2, cũng như đánh giá độ \n                                    ảnh hưởng của các yếu tố này. Bảng 2.  Sản \n                                    lượng CO2 và một số yếu tố có thể \n                                    ảnh hưởng đến CO2\n                                    \n                                    Chú thích:  y = sản lượng CO2;  \n                                    X1 = thời gian (phút); X2 = nhiệt độ (C); X3 \n                                    = phần trăm hòa tan; X4 = lượng dầu \n                                    (g/100g); X5 = lượng than đá; X6 = tổng số \n                                    lượng hòa tan; X7 = số hydrogen tiêu thụ.\n                                    \n                                    Trước \n                                    khi phân tích số liệu, chúng ta cần nhập số \n                                    liệu vào \n                                    R \n                                    bằng các lệnh thông thường.  Số liệu sẽ chứa \n                                    trong đối tượng \n                                    REGdata.\n                                    > y <- \n                                    c(36.98,13.74,10.08, 8.53,36.42,26.59,19.07,        \n                                    5.96,15.52,56.61,26.72,20.80,6.99,45.93,\n                                             43.09,15.79,21.60,35.19,26.14, \n                                    8.60,  \n                                           11.63, 9.59, \n                                    4.42,38.89,11.19,75.62,36.03) \n > x1 <- \n                                    c(5.1,26.4,23.8,46.4, 7.0,12.6,18.9,30.2,\n                                              53.8,5.6,15.1,20.3,48.4,5.8,11.2,27.9,5.1,\n                                              11.7,16.7,24.8,24.9,39.5,29.0, \n                                    5.5, 11.5,   \n                                            5.2,10.6) \n > x2 <- \n                                    c(400,400, 400, 400, 450, 450, 450, 450, \n                                    450,     \n                                          400, 400, 400, 400, 425, 425, 425, \n                                    450, 450, \n                                              450, 450, 450, 450, 450, 460, 450, \n                                    470, 470) \n > x3 <- \n                                    c(51.37,72.33,71.44,79.15,80.47,89.90,91.48,\n                                              98.60,98.05,55.69, \n                                    66.29,58.94,74.74,63.71,\n                                              67.14,77.65,67.22,81.48,83.88,89.38,79.77,\n                                              87.93, \n                                    79.50,72.73,77.88,75.50,83.15) \n > x4 <- \n                                    c(4.24,30.87,33.01,44.61,33.84,41.26,41.88,\n                                              70.79,66.82,8.92,17.98,17.79,33.94,11.95,\n                                              14.73,34.49,14.48,29.69,26.33, \n                                    37.98,25.66,\n                                              22.36,31.52,17.86,25.20, \n                                    8.66,22.39) \n > x5 <- \n                                    c(1484.83, 289.94, 320.79, 164.76, 1097.26,\n                                    \n                                              605.06, 405.37, 253.70, \n                                    142.27,1362.24, 507.65,\n                                             377.60,  158.05, 130.66, 682.59, \n                                    274.20, \n                                              1496.51, 652.43,  458.42, 312.25, \n                                    307.08, \n                                              193.61,155.96,1392.08, \n                                    663.09,1464.11, 720.07) \n > x6 <- \n                                    c(2227.25, 434.90, 481.19, 247.14,1645.89,\n                                    \n                                              907.59,608.05, 380.55, \n                                    213.40,2043.36, 761.48,\n                                              566.40,237.08,1961.49,1023.89, \n                                    411.30,2244.77,\n                                              978.64,687.62, 468.38, 460.62, \n                                    290.42,233.95,     \n                                         2088.12,994.63,2196.17,1080.11) \n > x7 <- \n                                    c(2.06,1.33,0.97,0.62,0.22,0.76,1.71,3.93,1.97,\n                                              5.08,0.60,0.90, \n                                    0.63,2.04,1.57,2.38,0.32,\n                                              0.44,8.82,0.02,1.72,1.88,1.43,1.35,1.61,\n                                              4.78,5.88) \n > \n                                    REGdata <- data.frame(y, \n                                    x1,x2,x3,x4,x5,x6,x7)  \n \n                                    Trước \n                                    khi phân tích số liệu, chúng ta cần nhập số \n                                    liệu vào \n                                    R \n                                    bằng các lệnh thông thường.  Số liệu sẽ chứa \n                                    trong đối tượng \n                                    REGdata.\n                                     \n \n                                    Bây \n                                    giờ chúng ta bắt đầu phân tích. Mô hình đầu \n                                    tiên là mô hình gồm tất cả 7 biến độc lập \n                                    như sau:  \n                                     \n \n                                    > reg <- \n                                    lm(y ~ x1+x2+x3+x4+x5+x6+x7, data=REGdata)\n                                    > \n                                    summary(reg) \n \n                                    Call: \n                                    lm(formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 \n                                    + x7, data = REGdata) \n \n                                    Residuals:\n                                        \n                                    Min      1Q  Median      3Q     Max \n                                    \n                                    -20.035  \n                                    -4.681  -1.144   4.072  21.214  \n \n                                    \n                                    Coefficients:\n                                    \n                                                 Estimate Std. Error t value \n                                    Pr(>|t|) \n                                    \n                                    (Intercept) 53.937016  57.428952   0.939   \n                                    0.3594  \n                                    \n                                    x1          -0.127653   0.281498  -0.453   \n                                    0.6553  \n                                    \n                                    x2          -0.229179   0.232643  -0.985   \n                                    0.3370  \n                                    \n                                    x3           0.824853   0.765271   1.078   \n                                    0.2946  \n                                    \n                                    x4          -0.438222   0.358551  -1.222   \n                                    0.2366  \n                                    \n                                    x5          -0.001937   0.009654  -0.201   \n                                    0.8431  \n                                    \n                                    x6           0.019886   0.008088   2.459   \n                                    0.0237 *\n                                    \n                                    x7           1.993486   1.089701   1.829   \n                                    0.0831 .\n                                    ---\n                                    Signif. \n                                    codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' \n                                    0.1 ' ' 1  \n \n                                    Residual \n                                    standard error: 10.61 on 19 degrees of \n                                    freedom\n                                    Multiple \n                                    R-Squared: 0.728,      Adjusted R-squared: \n                                    0.6278 \n                                    \n                                    F-statistic: 7.264 on 7 and 19 DF,  p-value: \n                                    0.0002674 \n \n                                    Kết \n                                    quả trên cho thấy tất cả 7 biến số “giải \n                                    thích” khoảng 73% phương sai của y. \n                                    Nhưng trong 7 biến đó, chỉ có \n                                    x6 \n                                    là có \n                                    ý nghĩa thống kê (p=0.024). Chúng ta thử \n                                    giảm mô hình thành một mô hình hồi qui tuyến \n                                    tính đơn giản với chỉ biến \n                                    x6. \n \n                                    > \n                                    summary(lm(y ~ x6, data=REGdata)) \n \n                                    Call: \n                                    lm(formula = y ~ x6, data = REGdata) \n \n                                    Residuals:\n                                        \n                                    Min      1Q  Median      3Q     Max \n                                    \n                                    -28.081  \n                                    -5.829  -0.839   5.522  26.882  \n \n                                    \n                                    Coefficients:\n                                    \n                                                Estimate Std. Error t value \n                                    Pr(>|t|)    \n                                    \n                                    (Intercept) 6.144181   3.483064   1.764     \n                                    0.09 .  \n                                    \n                                    x6          0.019395   0.002932   6.616 \n                                    6.24e-07 ***\n                                    ---\n                                    Signif. \n                                    codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' \n                                    0.1 ' ' 1  \n \n                                    Residual \n                                    standard error: 10.7 on 25 degrees of \n                                    freedom\n                                    Multiple \n                                    R-Squared: 0.6365,     Adjusted R-squared: \n                                    0.6219 \n                                    \n                                    F-statistic: 43.77 on 1 and 25 DF,  p-value: \n                                    6.238e-07\n                                    Chỉ \n                                    với một biến \n                                    x6 \n                                    mà mô hình có thể giải thích khoảng 64% \n                                    phương sai của y.  Chúng ta chấp nhận \n                                    mô hình này? Trước khi chấp nhận mô hình \n                                    này, chúng ta phải xem xét độ tương quan \n                                    giữa các biến độc lập:  \n \n                                    > \n                                    pairs(REGdata) \n                                    \n                                    \n                                     \n                                    Kết \n                                    quả trên cho thấy y có liên hệ với \n                                    các biến như \n                                    x1,\n                                    x5 \n                                    và \n                                    x6. \n                                    Ngoài ra, biến \n                                    x5 \n                                    và \n                                    x6 \n                                    có một mối liên hệ rất mật thiết (gần như là \n                                    một đường thẳng) với hệ số tương quan là \n                                    0.88.  Ngoài ra, \n                                    x5 \n                                    và \n                                    x1 \n                                    hay \n                                    x6 \n                                    và \n                                    x5 \n                                    cũng có liên hệ với nhau nhưng theo một hàm \n                                    số nghịch đảo. Điều này có nghĩa là biến\n                                    x5 \n                                    và \n                                    x6 \n                                    cung cấp một lượng thông tin như nhau để \n                                    tiên đoán y, tức là chúng ta không \n                                    cần cả hai trong một mô hình. \n                                    Để tìm \n                                    một mô hình tối ưu trong bối cảnh có nhiều \n                                    mối tương quan như thế, chúng ta ứng dụng\n                                    \n                                    step \n                                    như sau. Chú ý cách cung cấp thông số \n                                    lm(y ~ .),dấu \n                                    “.” có nghĩa là yêu cầu \n                                    R \n                                    xem xét tất cả biến trong đối tượng \n                                    REGdata. \n                                     \n > reg \n                                    <- lm(y ~ ., data=REGdata) \n                                    > step(reg, direction=”both”)y \n                                          ~ x1 + x2 + x3 + x4 + x5 + x6 + x7\n                                          \n                                                 Df Sum of Sq     RSS     AIC- \n                                          x5    1      4.54 2145.37  132.13- \n                                          x1    1     23.17 2164.00  132.36- \n                                          x2    1    109.34 2250.18  133.42- \n                                          x3    1    130.90 2271.74  133.68\n                                          <none>              2140.83  134.07- \n                                          x4    1    168.31 2309.14  134.12- \n                                          x7    1    377.09 2517.92  136.45- \n                                          x6    1    681.09 2821.92  139.53y \n                                          ~ x1 + x2 + x3 + x4 + x6 + x7  \n \n                                                 Df Sum of Sq    RSS    AIC- \n                                          x1    1      22.7 2168.1  130.4- \n                                          x2    1     113.8 2259.1  131.5- \n                                          x3    1     133.5 2278.9  131.8\n                                          <none>              2145.4  132.1- \n                                          x4    1     170.8 2316.2  132.2+ \n                                          x5    1       4.5 2140.8  134.1- \n                                          x7    1     375.7 2521.1  134.5- \n                                          x6    1    1058.5 3203.8  141.0y \n                                          ~ x2 + x3 + x4 + x6 + x7  \n \n                                                 Df Sum of Sq    RSS    AIC- \n                                          x2    1      96.8 2264.9  129.6- \n                                          x3    1     122.0 2290.0  129.9\n                                          <none>              2168.1  130.4- \n                                          x4    1     187.4 2355.5  130.7+ \n                                          x1    1      22.7 2145.4  132.1+ \n                                          x5    1       4.1 2164.0  132.4- \n                                          x7    1     385.0 2553.1  132.8- \n                                          x6    1    1526.2 3694.3  142.8y \n                                          ~ x3 + x4 + x6 + x7  \n \n                                                 Df Sum of Sq    RSS    AIC- \n                                          x3    1      25.4 2290.3  127.9- \n                                          x4    1      90.9 2355.8  128.7\n                                          <none>              2264.9  129.6+ \n                                          x2    1      96.8 2168.1  130.4+ \n                                          x5    1       8.3 2256.5  131.5+ \n                                          x1    1       5.7 2259.1  131.5- \n                                          x7    1     384.9 2649.7  131.8- \n                                          x6    1    2015.6 4280.5  144.8y \n                                          ~ x4 + x6 + x7  \n \n                                                Df Sum of Sq    RSS    AIC- \n                                          x4    1      73.5 2363.8  126.7\n                                          <none>              2290.3  127.9+ \n                                          x3    1      25.4 2264.9  129.6+ \n                                          x1    1      11.3 2279.0  129.8+ \n                                          x5    1       6.3 2284.0  129.8+ \n                                          x2    1       0.3 2290.0  129.9- \n                                          x7    1     486.6 2776.9  131.1- \n                                          x6    1    1993.8 4284.1  142.8y \n                                          ~ x6 + x7  \n \n                                                 Df Sum of Sq    RSS    AIC\n                                          <none>              2363.8  126.7+ \n                                          x4    1      73.5 2290.3  127.9+ \n                                          x1    1      33.4 2330.4  128.4+ \n                                          x3    1       8.1 2355.8  128.7+ \n                                          x5    1       7.7 2356.1  128.7+ \n                                          x2    1       7.3 2356.6  128.7- \n                                          x7    1     497.3 2861.2  129.9- \n                                          x6    1    4477.0 6840.8  153.4\n                                          lm(formula = y ~ x6 + x7, data = \n                                          REGdata) \n \n                                          Coefficients:\n                                          (Intercept)           x6           x7 \n                                          \n                                              2.52646      0.01852      2.18575 \n                                          Quá trình tìm mô \n                                    hình tối ưu dừng ở mô hình với hai biến\n                                    x6 \n                                    và \n                                    x7, \n                                    vì mô hình này có giá trị AIC thấp nhất.  \n                                    Phương trình tuyến tính tiên đoán y \n                                    là:  y = 2.526 + 0.0185(x6) \n                                    + 2.186(x7).  \n                                     \n \n                                    > \n                                    summary(lm(y ~ x6+x7, data=REGdata))\n                                    Call: \n                                    lm(formula = y ~ x6 + x7, data = REGdata) \n \n                                    Residuals:\n                                         \n                                    Min       1Q   Median       3Q      Max\n                                    \n                                    -23.2035  \n                                    -4.3713   0.2513   4.9339  21.9682  \n \n                                    \n                                    Coefficients:\n                                    \n                                                Estimate Std. Error t value \n                                    Pr(>|t|)    \n                                    \n                                    (Intercept) 2.526460   3.610055   0.700   \n                                    0.4908    \n                                    x6       \n                                       0.018522   0.002747   6.742 5.66e-07 ***\n                                    \n                                    x7          2.185753   0.972696   2.247   \n                                    0.0341 *  \n                                    ---\n                                    Signif. \n                                    codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' \n                                    0.1 ' ' 1  \n \n                                    Residual \n                                    standard error: 9.924 on 24 degrees of \n                                    freedom\n                                    Multiple \n                                    R-Squared: 0.6996,     Adjusted R-squared: \n                                    0.6746 \n                                    \n                                    F-statistic: 27.95 on 2 and 24 DF,  p-value: \n                                    5.391e-07 \n \n                                    Phân \n                                    tích chi tiết (kết quả trên) cho thấy hai \n                                    biến này giải thích khoảng 70% phương sai \n                                    của y.   \n 10.6  Xây \n                                    dựng mô hình tuyến tính bằng Bayesian Model \n                                    Average (BMA) \n Sau đó, tạo ra \n                                    một ma trận chỉ gồm các biến độc lập. Trong \n                                    data frame chúng ta biết \n                                    REGdata \n                                    có 8 biến, với biến số 1 là y. Do đó, \n                                    lệnh \n                                    REGdata[, \n                                    -1] \n                                    có nghĩa là tạo ra một data frame mới ngoại \n                                    trừ cột thứ nhất (tức y).  \n > xvars \n                                    <- REGdata[,-1]\n                                    Kế \n                                    tiếp, chúng ta định nghĩa biến phụ thuộc tên\n                                    co2 \n                                    từ \n                                    REGdata:\n                                    > co2 \n                                    <- REGdata[,1] \n \n                                    Bây \n                                    giờ chúng ta đã sẵn sàng phân tích bằng phép \n                                    tính BMA. Hàm \n                                    bicreg  \n                                    được viết đặc biệt cho phân tích hồi qui \n                                    tuyến tính. Cách áp dụng hàm \n                                    bicreg\n                                    \n                                    như \n                                    sau: \n > bma \n                                    <- bicreg(xvars, co2, strict=FALSE, OR=20) \n \n                                    Chúng \n                                    ta sử dụng hàm \n                                    summary \n                                    để biết kết quả: \n > \n                                    summary(bma)Call:\n                                    bicreg(x = xvars, y = co2, strict = FALSE, \n                                    OR = 20) \n 16  \n                                    models were selectedBest  \n                                    5  models (cumulative posterior probability \n                                    =  0.6599 ):  \n \n                                               p!=0    EV       SD       model \n                                    1   model 2   model 3  \n                                    Intercept  100.0   5.75672  14.6244    \n                                    2.5264    6.1441    8.6120 \n                                    x1          12.4  -0.01807   0.1008      \n                                    .         .         .    \n                                    x2          10.4  -0.00075   0.0282      \n                                    .         .         .    \n                                    x3          10.7   0.00011   0.0791      \n                                    .         .         .    \n                                    x4          20.2  -0.03059   0.1020      \n                                    .         .      -0.1419 \n                                    x5          10.5  -0.00023   0.0030      \n                                    .         .         .    \n                                    x6         100.0   0.01815   0.0040    \n                                    0.0185    0.0193    0.0164 \n                                    x7          73.7   1.60766   1.2821    \n                                    2.1857      .       2.1628 \n                                                                                                     \n                                    \n                                    nVar                                     \n                                    2         1         3    \n                                    r2                                     \n                                    0.700     0.636     0.709  \n                                    BIC                                  \n                                    -25.8832  -24.0238  -23.4412 post \n                                    prob                              \n                                    0.311     0.123     0.092   \n \n                                    \n                                                model 4   model 5 \n                                    \n                                    \n                                    Intercept     7.5936    7.3537\n                                    \n                                    x1           -0.1393      .   \n                                    \n                                    x2              .         .   \n                                    \n                                    x3              .      -0.0572\n                                    \n                                    x4              .         .   \n                                    \n                                    x5              .         .   \n                                    \n                                    x6            0.0162    0.0179\n                                    \n                                    x7            2.1233    2.2382\n                                    \n                                                                   \n                                     \n                                     \n \n                                    \n                                    nVar            3         3   \n                                    \n                                    r2            0.704     0.701 \n                                    \n                                    BIC         -22.9721  -22.6801\n                                    post \n                                    prob     0.072     0.063  \n \n                                    BMA \n                                    trình bày kết quả của 5 mô hình được đánh \n                                    giá là tối ưu nhất cho tiên đoán y (model \n                                    1, model 2, … model 5). \n                                     \n Một cách thể hiện kết quả trên là \n                                    qua một biểu đồ như sau:  \n > \n                                    imageplot.bma(bma) \n                                     \n                                     \n \n                                    \n                                     \n                                     \n  \n                                    Tài \n                                    liệu tham khảo cho BMA \n \n                                    \n                                    Raftery, Adrian E. (1995). Bayesian model \n                                    selection in social research (with \n                                    Discussion). Sociological Methodology 1995 \n                                    (Peter V. Marsden, ed.), pp. 111-196, \n                                    Cambridge, Mass.: Blackwells. \n \n                                    Một số \n                                    bài báo liên quan đến BMA có thể tải từ \n                                    trang web sau đây: \n                                    \n                                    \n                                    www.stat.colostate.edu/~jah/papers.\n                                                        \n                            \n                            _____________________________________________________________\n\t\t\t\t\t\t\tTrích từ quyển\n                            \n                            Phân Tích Số Liệu và Tạo Biểu \n                            Đồ bằng \n                             \n                            - Hướng Dẫn Thực Hành\n\t\t\t\t\t\t\tNhà xuất bản \n                            Đại Học Quốc gia \n                            \n                            \n\t\t\t©                     \n    \n\t\t\thttp://vietsciences.org      \n\t\t\tvà                     \n                    \n    http://vietsciences.free.fr Nguyễn \n\tVăn Tuấn           \n             \n           \n               \n             \n             \n           \n",
          "relevence": "yes"
        },
        {
          "url": "http://tailieu.vn/tag/ham-hoi-quy-tuyen-tinh.html",
          "title": "Tài liệu Hàm Hồi Quy Tuyến Tính chọn lọc - TaiLieu.VN",
          "content": "\n\t\t\t\t\t\tMÔ HÌNH HỒI QUY \u000bTUYẾN TÍNH 3 BiẾN\r\nHàm hồi quy tổng thể:\r\nE(Y/X2 , X3) = β1 + β2X2 + β3X3 \r\nY: Biến phụ thuộc (Biến được giải thích)\r\nX2 , X3 : Các biến độc lập (Biến giải thích)  \r\n β1 : Hệ số tự do\r\n β2, β3 : Hệ số hồi quy riêng. \r\n β2, β3  cho biết ảnh hưởng từng biến độc lập lên giá trị trung bình của biến phụ thuộc khi các biến còn lại được giữ không đổi\t\t\t\t\t\t\n\t\t\t\t\t\t\t39p   anhtaisaigonmekong\n\t\t\t\t\t\t\t 07-09-2012\n\t\t\t\t\t\t\t 295\n\t\t\t\t\t\t\t 60\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\t\n\t\t\t\t\t\tBài giảng \"Kinh tế lượng - Chương 1: Mô hình hồi quy tuyến tính hai biến\" cung cấp cho người học các kiến thức: Mô hình và một số khái niệm, phương pháp ước lượng OLS, tính không chệch và độ chính xác của ước lượng OLS, độ phù hợp của hàm hồi quy - hệ số xác định R2,... Mời các bạn cùng tham khảo nội dung chi tiết.\t\t\t\t\t\t\n\t\t\t\t\t\t\t36p   doinhugiobay_14\n\t\t\t\t\t\t\t 16-02-2016\n\t\t\t\t\t\t\t 53\n\t\t\t\t\t\t\t 7\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\t\n\t\t\t\t\t\tMời các bạn cùng tìm hiểu \"Bài giảng Chương 3: Hồi quy tuyến tính bội\" để nắm bắt một số thông tin cơ bản về hàm hồi quy tổng thể; hàm hồi quy mẫu; các giả thuyết cho mô hình hồi quy k biến;... Hy vọng tài liệu làn nguồn thông tin hữu ích cho quá trình học tập và nghiên cứu của các bạn.\t\t\t\t\t\t\n\t\t\t\t\t\t\t4p   codon_011\n\t\t\t\t\t\t\t 22-02-2016\n\t\t\t\t\t\t\t 21\n\t\t\t\t\t\t\t 1\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\t\n\t\t\t\t\t\t1.1. Hãy giải thích các khái niệm\na) Hàm hồi quy tổng thể và hàm hồi quy mẫu\nb) Yếu tố ngẫu nhiên và phần dư\nc) Các hệ số hồi quy, ước lượng các hệ số hồi quy\nd) Hàm hồi quy tuyến tính\n1.2. a) Những môn học nào cần biết để nắm vững kinh tế lượng\nb) Các bước giải bài toán kinh tế lượng\nc) Có những cách nào để viết hàm hồi quy tổng thể\t\t\t\t\t\t\n\t\t\t\t\t\t\t21p   lengan2706\n\t\t\t\t\t\t\t 13-05-2011\n\t\t\t\t\t\t\t 2695\n\t\t\t\t\t\t\t 689\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\t\n\t\t\t\t\t\t Sau khi kết thúc bài, học viên sẽ hiểu được những vấn đề sau đây:\n• Ý tưởng của phương pháp bình phương\n\n• • • • • •\n\ntối thiểu (OLS) và cách sử dụng OLS để ước lượng các hệ số hồi quy. Ý nghĩa của các hệ số hồi quy ước lượng. Các giả thiết cơ bản của phương pháp OLS. Hệ số xác định r2 đo độ phù hợp của hàm hồi quy. Khoảng tin cậy và kiểm định giả thuyết cho các hệ số hồi quy. Phân tích phương sai – kiểm định về sự phù hợp...\t\t\t\t\t\t\n\t\t\t\t\t\t\t24p   thoabar3\n\t\t\t\t\t\t\t 28-11-2010\n\t\t\t\t\t\t\t 905\n\t\t\t\t\t\t\t 150\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\t\n\t\t\t\t\t\tTrong chương này trình bầy về cách vẽ các đường cong dựa theo các điểm dữ liệu. Các phương pháp đơn giản nhất  là sử dụng  các hàm hồi quy tuyến tính của  EXCEL: các hàm LINEST và LOGEST, hai hàm này thực hiện phép hồi quy tuyến tính bội. Bằng cách chuyển đổi một cách thích hợp các phương trình, bạn có thể  làm xấp xỉ nhiều phương trình phi tuyến bằng các hàm này. Ngoài ra, có thể sử dụng hàm LINEST để thực hiện phép hồi quy đa thức cho việc làm xấp...\t\t\t\t\t\t\n\t\t\t\t\t\t\t10p   locmdc\n\t\t\t\t\t\t\t 30-10-2010\n\t\t\t\t\t\t\t 257\n\t\t\t\t\t\t\t 66\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\t\n\t\t\t\t\t\t\nHàm CORREL() Trả về hệ số tương quan của hai mảng array1 và array2. Thường được dùng để xác định mối quan hệ của hai đặc tính. \t\t\t\t\t\t\n\t\t\t\t\t\t\t18p   vudung75\n\t\t\t\t\t\t\t 20-04-2011\n\t\t\t\t\t\t\t 521\n\t\t\t\t\t\t\t 69\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\t\n\t\t\t\t\t\tTrong Bài giảng Phân tích số liệu  Bài 8: Phân tích hồi quy bội nhằm trình bày về hồi quy tuyến tính bội, phân tích hồi quy bội, mô hình hồi quy bội. Mô hình hồi quy bội gồm 1 biến phụ thuộc Y (biến định lượng) và nhiều biến độc lập X1, …, Xp. Phân tích hồi quy của Y theo Xi là tìm dạng phụ thuộc hàm giữa chúng.\t\t\t\t\t\t\n\t\t\t\t\t\t\t11p   sms_12\n\t\t\t\t\t\t\t 06-05-2014\n\t\t\t\t\t\t\t 77\n\t\t\t\t\t\t\t 24\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\t\n\t\t\t\t\t\tSử dụng mô hình hồi quy.\nĐổi đơn vị tính trong hàm hồi quy.\nDự báo.\nHồi quy qua gốc toạ độ.\nMô hình tuyến tính Logarit\nMô hình log-lin\t\t\t\t\t\t\n\t\t\t\t\t\t\t19p   vutrungtam\n\t\t\t\t\t\t\t 12-06-2009\n\t\t\t\t\t\t\t 1397\n\t\t\t\t\t\t\t 240\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\t\n\t\t\t\t\t\tChúng ta có thế xây dựng được mô hình hồi quy bội dù chúng ta có đưa vào bao\r\nnhiêu biến đi chăng nữa thì yếu tố phần dư vẫn tồn tại vì yếu tố hiễn nhiên của\r\nchúng ,ngaycả khi các biến bị loại bỏ khỏi mô hình\r\n- ei được sử dụng như một yếu tố đại diienj cho tất cả các biến không có trong\r\nmô hình ngay cả khi các biến bị loại bỏ khỏi mô hình là biến nào đi chăng nữa khi\r\nđó quá trình chuyển đổi mô hình hồi quy tổng thể PRF sang mô hình hồi quy...\t\t\t\t\t\t\n\t\t\t\t\t\t\t28p   huonghieuphuong\n\t\t\t\t\t\t\t 12-09-2011\n\t\t\t\t\t\t\t 173\n\t\t\t\t\t\t\t 45\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\t\n\t\t\t\t\t\tChương 1 Mô hình hồi quy tuyến tính hai biến, nội dung chương học này gồm: Mô hình hồi quy và một số khái niệm; Phương pháp ước lượng OLS; Tính không chệch và độ chính xác của các ước lượng OLS; Độ phù hợp của hàm hồi quy mẫu – Hệ số xác định R2; Một số vấn đề bổ sung.\t\t\t\t\t\t\n\t\t\t\t\t\t\t21p   canhdangxuan\n\t\t\t\t\t\t\t 11-04-2014\n\t\t\t\t\t\t\t 74\n\t\t\t\t\t\t\t 11\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\t\n\t\t\t\t\t\tCùng nắm kiến thức trong bài thực hành 1 \"Hồi quy đơn giản với Excel\" thông qua việc tìm hiểu các nội dung sau: vẽ tập dữ liệu food.xls, hồi quy tuyến tính đơn giản, vẽ đường hồi quy đơn giản, vẽ đồ thị phần dư của hàm số hồi quy, dự báo bằng hàm hồi quy đơn giản.\t\t\t\t\t\t\n\t\t\t\t\t\t\t11p   ngoccuong93\n\t\t\t\t\t\t\t 09-09-2014\n\t\t\t\t\t\t\t 45\n\t\t\t\t\t\t\t 10\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\t\n\t\t\t\t\t\tDưới đây là bài giảng Kinh tế lượng: Chương 3 - Hồi quy tuyến tính bội. Mời các bạn tham khảo bài giảng để hiểu rõ hơn những nội dung về mô hình hồi quy tuyến tính 3 biến; một số dạng hàm (hàm sản xuất cobb - douglas; hàm hồi quy đa thức bậc 2); hồi quy tuyến tính k biến.\t\t\t\t\t\t\n\t\t\t\t\t\t\t81p   cocacola_07\n\t\t\t\t\t\t\t 07-11-2015\n\t\t\t\t\t\t\t 39\n\t\t\t\t\t\t\t 16\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\t\n\t\t\t\t\t\tBài báo trình bày kết quả xác lập các hàm tương quan hồi quy tuyến tính giữa các tính chất cơ lý của đất có nguồn gốc sông - biển (amQ13) và nguồn gốc biển - đầm lầy (mbQ21-2) ở thừa Thiên Thiên Huế. \t\t\t\t\t\t\n\t\t\t\t\t\t\t6p   phalinh15\n\t\t\t\t\t\t\t 12-08-2011\n\t\t\t\t\t\t\t 61\n\t\t\t\t\t\t\t 6\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\t\n\t\t\t\t\t\tBài giảng Tin học chuyên ngành dành cho khối B trình bày về cơ sở dữ liệu, các hàm thống kê, thống kê mô tả, tương quan và hồi quy, hồi quy tuyến tính; phân tích phương sai, so sánh và kiểm định, phân tích phương sai 2 nhân tố, phân tích phương sai 1 nhân tố.\t\t\t\t\t\t\n\t\t\t\t\t\t\t53p   lehuyhieu91vp\n\t\t\t\t\t\t\t 24-05-2014\n\t\t\t\t\t\t\t 39\n\t\t\t\t\t\t\t 4\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\t\n\t\t\t\t\t\tDưới đây là bài giảng Kinh tế lượng: Chương 3 - Hồi quy tuyến tính bội. Mời các bạn tham khảo bài giảng để hiểu rõ hơn những nội dung về mô hình hồi quy tuyến tính 3 biến; một số dạng hàm (hàm sản xuất cobb - douglas; hàm hồi quy đa thức bậc 2); hồi quy tuyến tính k biến.\t\t\t\t\t\t\n\t\t\t\t\t\t\t14p   youcanletgo_03\n\t\t\t\t\t\t\t 13-01-2016\n\t\t\t\t\t\t\t 19\n\t\t\t\t\t\t\t 3\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\t\n\t\t\t\t\t\tTính hệ số hồi quy (Coefficient)Tính phương sai (Variance)Tính độ lệch chuẩn (Standard Deviation)SDY và SDX Tính đồng phương sai hay hiệp phương sai (Covariance SXY = cov(X,Y),Tính tổng bình phương độ lệch\t\t\t\t\t\t\n\t\t\t\t\t\t\t10p   meomeoten\n\t\t\t\t\t\t\t 02-12-2013\n\t\t\t\t\t\t\t 45\n\t\t\t\t\t\t\t 2\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\t\n\t\t\t\t\t\tBài giảng \"Học máy - Chương 4: Các phương pháp học có giám sát\" trình bày các ví dụ về các phương pháp học có giám sát, hàm đánh giá lỗi, hồi quy tuyến tính, quy tắc delta, cập nhật theo đợt, các điều kiện kết thúc học. Mời các bạn cùng tham khảo nội dung chi tiết.\t\t\t\t\t\t\n\t\t\t\t\t\t\t12p   nhasinhaoanh_09\n\t\t\t\t\t\t\t 13-10-2015\n\t\t\t\t\t\t\t 28\n\t\t\t\t\t\t\t 2\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\t\n\t\t\t\t\t\tBài giảng Chương 4: Hồi quy với biến giả\t\t\t\t\t\t\n\t\t\t\t\t\t\t4p   codon_011\n\t\t\t\t\t\t\t 22-02-2016\n\t\t\t\t\t\t\t 12\n\t\t\t\t\t\t\t 1\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\t\n\t\t\t\t\t\tBài giảng \"Kinh tế lượng - Bài 4: Một số mô hình hồi quy thường gặp trong thực tế\" cung cấp cho người học các kiến thức: Một số dạng của hàm hồi quy, hồi quy với biến giả. Mời các bạn cùng tham khảo.\t\t\t\t\t\t\n\t\t\t\t\t\t\t122p   doinhugiobay_13\n\t\t\t\t\t\t\t 26-01-2016\n\t\t\t\t\t\t\t 18\n\t\t\t\t\t\t\t 1\n\t\t\t\t\t\t\t  Download\n\t\t\t\t\t\tGiấy phép ICP số: 670/GP-BTTTT cấp ngày 30/11/2015 Copyright © 2009-2015 TaiLieu.VN. All rights reserved. TaiLieu.VN hiển thị tốt nhất với trình duyệt Chrome, Firefox, Internet Explorer 8.",
          "relevence": "yes"
        },
        {
          "url": "https://viblo.asia/p/linear-regression-hoi-quy-tuyen-tinh-trong-machine-learning-4P856akRlY3",
          "title": "Linear Regression - Hồi quy tuyến tính trong Machine Learning - Viblo",
          "content": "Trong bài viết, mình sẽ giới thiệu một trong những thuật toán cơ bản nhất của Machine Learning. Đây là thuật toán Linear Regression (Hồi Quy Tuyến Tính) thuộc nhóm Supervised learning ( Học có giám sát ). Hồi quy tuyến tính là một phương pháp rất đơn giản nhưng đã được chứng minh được tính hữu ích cho một số lượng lớn các tình huống. Trong bài viết này, bạn sẽ khám phá ra chính xác cách thức tuyến tính làm việc như thế nào.\nTrong việc phân tích dữ liệu, bạn sẽ tiếp xúc với thuật ngữ \"Regression\" ( Hồi quy ) rất thường xuyên. Trước khi đi sâu vào Hồi quy tuyến tính, hãy tìm hiểu khái niệm Hồi quy trước đã. Hồi quy chính là một phương pháp thống kê để thiết lập mối quan hệ giữa một biến phụ thuộc và một nhóm tập hợp các biến độc lập. Ví dụ :Ở đây chính ta đang thiết lập mối quan hệ giữa Chiều cao & Trọng lượng của một người với Tuổi của anh/cô ta. Đây là một ví dụ rất cơ bản của Hồi quy.\"Hồi quy tuyến tính\" là một phương pháp thống kê để hồi quy dữ liệu với biến phụ thuộc có giá trị liên tục trong khi các biến độc lập có thể có một trong hai giá trị liên tục hoặc là giá trị phân loại. Nói cách khác \"Hồi quy tuyến tính\" là một phương pháp để dự đoán biến phụ thuộc (Y) dựa trên giá trị của biến độc lập (X). Nó có thể được sử dụng cho các trường hợp chúng ta muốn dự đoán một số lượng liên tục. Ví dụ, dự đoán giao thông ở một cửa hàng bán lẻ, dự đoán thời gian người dùng dừng lại một trang nào đó hoặc số trang đã truy cập vào một website nào đó v.v...Để bắt đầu với Hồi quy tuyến tính, chúng ta hãy đi lướt qua một số khái niệm toán học về thống kê.Không một kích thước nào phù hợp cho tất cả, điều này cũng đúng đối với Hồi quy tuyến tính. Để thoả mãn hồi quy tuyến tính, dữ liệu nên thoả mãn một vài giả định quan trọng. Nếu dữ liệu của bạn không làm theo các giả định, kết quả của bạn có thể sai cũng như gây hiểu nhầm.Trong khi sử dụng hồi quy tuyến tính, mục tiêu của chúng ta là để làm sao một đường thẳng có thể tạo được sự phân bố gần nhất với hầu hết các điểm. Do đó làm giảm khoảng cách (sai số) của các điểm dữ liệu cho đến đường đó.Ví dụ, ở các điểm ở hình trên (trái) biểu diễn các điểm dữ liệu khác nhau và đường thẳng (bên phải) đại diện cho một đường gần đúng có thể giải thích mối quan hệ giữa các trục x & y. Thông qua, hồi quy tuyến tính chúng ta cố gắng tìm ra một đường như vậy. Ví dụ, nếu chúng ta có một biến phụ thuộc Y và một biến độc lập X - mối quan hệ giữa X và Y có thể được biểu diễn dưới dạng phương trình sau:Ở đây,Sử dụng công cụ thống kê ví dụ như Excel, R, SAS ... bạn sẽ trực tiếp tìm hằng số (B0 và B1) như là kết quả của hàm hồi quy tuyến tính. Như lý thuyết ở trên, nó hoạt động trên khái niệm OLS và cố gắng giảm bớt diện tích sai số,  các công cụ này sử dụng các gói phần mềm tính các hằng số này.Ví dụ, giả sử chúng ta muốn dự đoán y từ x trong bảng sau và giả sử rằng phương trình hồi quy của chúng ta sẽ giống như y = B0 + B1 * xỞ đây,Nếu chúng ta phân biệt các Tổng còn lại của diện tích sai số (RSS) tương ứng với B0 & B1 và tương đương với các kết quả bằng không, chúng ta có được các phương trình sau đây như là một kết quả:Đưa giá trị từ bảng 1 vào các phương trình trên,Do đó, phương trình hồi quy nhất sẽ trở thành -Hãy xem, dự đoán của chúng ta như thế nào bằng cách sử dụng phương trình nàyChỉ với 10 điểm dữ liệu để phù hợp với một đường thẳng thì dự đoán của chúng ta sẽ chính xác lắm, nhưng nếu chúng ta thấy sự tương quan giữa 'Y-Thưc tế' và 'Y - Dự đoán' thì triển vọng sẽ rất cao do đó cả hai series đang di chuyển cùng nhau và đây là biểu đồ để hiển thị giá trị dự đoán:Một khi bạn xây dựng mô hình, câu hỏi tiếp theo đến trong đầu là để biết liệu mô hình của bạn có đủ để dự đoán trong tương lai hoặc là mối quan hệ mà bạn đã xây dựng giữa các biến phụ thuộc và độc lập là đủ hay không.Vì mục đích này có nhiều chỉ số mà chúng ta cần tham khảoCông thức tính R^2 sẽ bằng :Trong đó N là số quan sát được sử dụng để phù hợp với mô hình, σx là độ lệch chuẩn của x, và σy là độ lệch chuẩn của y.Root Mean Square Error (RMSE)\nRMSE cho biết mức độ phân tán các giá trị dự đoán từ các giá trị thực tế. Công thức tính RMSE là\nN: Tổng số quan sátMặc dù RMSE là một đánh giá tốt cho các sai số nhưng vấn đề với nó là nó rất dễ bị ảnh hưởng bởi phạm vi của biến phụ thuộc của bạn. Nếu biến phụ thuộc của bạn có dải biến thiên hẹp, RMSE của bạn sẽ thấp và nếu biến phụ thuộc có phạm vi rộng RMSE sẽ cao. Do đó, RMSE là một số liệu tốt để so sánh giữa các lần lặp lại khác nhau của mô hìnhMean Absolute Percentage Error (MAPE)Để khắc phục những hạn chế của RMSE, các nhà phân tích thích sử dụng MAPE so với RMSE. MAPE cho sai số trong tỷ lệ phần trăm và do đó so sánh được giữa các mô hình. Công thức tính MAPE có thể được viết như sau:\nN: Tổng số quan sátCho đến hiện tại, chúng ta đã thảo luận về kịch bản mà chúng ta chỉ có một biến độc lập. Nếu chúng ta có nhiều hơn một biến độc lập, phương pháp phù hợp nhất là \"Multiple Regression Linear\" - Hồi quy tuyến tính đa biếnVề cơ bản không có sự khác biệt giữa hồi quy tuyến tính 'giản đơn' và 'đa biến'. Cả hai đều làm việc tuân theo nguyên tắc OLS và thuật toán để có được đường hồi quy tối ưu nhất cũng tương tự. Trong trường hợp sau, phương trình hồi quy sẽ có một hình dạng như sau:Ở đây,Bi: Các hệ số khác nhau\nXi:  Các biến độc lập khác nhauỞ trên, bạn đã biết rằng hồi quy tuyến tính là một kỹ thuật phổ biến và bạn cũng có thể thấy các phương trình toán học của hồi quy tuyến tính. Nhưng bạn có biết làm thế nào để thực hiện một hồi quy tuyến tính trong Python ?? Có một số cách để có thể làm điều đó, bạn có thể thực hiện hồi quy tuyến tính bằng cách sử dụng các mô hình thống kê, numpy, scipy và sckit learn. Nhưng trong bài này chúng ta sẽ sử dụng sckit learn để thực hiện hồi quy tuyến tính.Scikit-learn là một module Python mạnh mẽ cho việc học máy. Nó chứa hàm cho hồi quy, phân loại, phân cụm, lựa chọn mô hình và giảm kích chiều. Chúng ta sẽ khám phá module sklearn.linear_model có chứa \"các method để thực hiện hồi quy, trong đó giá trị mục tiêu sẽ là sự kết hợp tuyến tính của các biến đầu vào\".Trong bài đăng này, chúng ta sẽ sử dụng bộ dữ liệu Nhà ở Boston, bộ dữ liệu chứa thông tin về giá trị nhà cửa ở ngoại ô thành phố Boston. Tập dữ liệu này ban đầu được lấy từ thư viện StatLib được duy trì tại Đại học Carnegie Mellon và bây giờ đã có trên UCI Machine Learning Repository.Bộ Dữ liệu Nhà ở Boston bao gồm giá nhà ở những nơi khác nhau ở Boston. Cùng với giá cả, tập dữ liệu cũng cung cấp thông tin như Tội phạm (CRIM), các khu vực kinh doanh không-bán-lẻ ở thị trấn (INDUS), tuổi chủ sở hữu ngôi nhà (AGE) và có nhiều thuộc tính khác có sẵn ở đây .\nBộ dữ liệu chính nó có thể down từ đây . Tuy nhiên, vì chúng ta sử dụng scikit-learn, chúng ta có thể import nó từ scikit-learn.Trước hết, chúng ta sẽ import bộ dữ liệu Boston Housing và lưu trữ nó trong một biến gọi là boston. Để import nó từ scikit-learn, chúng ta sẽ cần phải chạy đoạn mã này.Biến boston là một dạng từ điển, vì vậy chúng ta có thể kiểm tra key của nó sử dụng đoạn mã bên dưới.Nó sẽ trả về như sau\nTiếp,Trước tiên, chúng ta có thể dễ dàng kiểm tra shape của nó bằng cách gọi boston.data.shape và nó sẽ trả lại kích thước của tập dữ liệu với kích thước column.Như chúng ta có thể thấy nó trả về (506, 13), có nghĩa là có 506 hàng dữ liệu với 13 cột. Bây giờ chúng ta muốn biết 13 cột là gì. Chúng ta sẽ chạy đoạn code sau :Bạn có thể dùng lệnh print(boston.DESCR) để kiểm tra description của dữ liệu thay vì mở web để đọc.Tiếp, convert dữ liệu về dạng pandas! Rất đơn giản, gọi hàm pd.DataFrame() và truyền boston.data. Chúng ta có thể kiểm tra 5 dữ liệu đầu tiên bằng bos.head().Hoặc bạn co thể dùng đoạn lệnh sau để show được tên cộtCó vẻ vẫn chưa có column tên là PRICE.Ta sẽ add nó vào sử dụng đoạn mã trênNếu bạn muốn nhìn các số liệu tổng hợp thống kê, hãy chạy đoạn mã sau .Về cơ bản, trước khi chia dữ liệu thành tập dữ liệu để train - test, chúng ta cần chia dữ liệu thành hai giá trị : giá trị đích và giá trị dự báo. Hãy gọi giá trị đích Y và các giá trị dự báo X.\nNhư vậy,Bây giờ chúng ta có thể split dữ liệu để train và test với snippet như sau.Nếu chúng ta kiểm tra shape của mỗi biến, chúng ta đã có được bộ dữ liệu với tập dữ liệu thử nghiệm có tỷ lệ 66,66% đối với dữ liệu train và 33,33% đối với dữ liệu test.Tiếp, chúng ta sẽ chạy hồi quy tuyến tính.Đoạn mã trên sẽ phù hợp với một mô hình dựa trên X_train và Y_train. Bây giờ chúng tôi đã có mô hình tuyến tính, chúng ta sẽ cố gắng dự đoán nó cho X_test và các giá trị dự đoán sẽ được lưu trong Y_pred. Để hình dung sự khác biệt giữa giá thực tế và giá trị dự đoán, chúng tôi cũng tạo ra một bảng biểu .Thực tế thì đáng lẽ đồ thị ở trên phải tạo một đường tuyến tính như chúng ta đã thảo luận lý thuyết ở trên. Tuy nhiên, model không thích hợp 100%, cho nên nó đã ko thể tạo được đường tuyến tính.Để kiểm tra mức độ lỗi của một mô hình, chúng ta có thể sử dụng Mean Squared Error. Đây là một trong các phương pháp để đo trung bình của ô vuông của sai số. Về cơ bản, nó sẽ kiểm tra sự khác biệt giữa giá trị thực tế và giá trị dự đoán. Để sử dụng nó, chúng ta có thể sử dụng hàm bình phương trung bình sai số của scikit-learn bằng cách chạy đoạn mã nàykết quả nhận đượchttp://bigdata-madesimple.com/how-to-run-linear-regression-in-python-scikit-learn/http://aimotion.blogspot.com/2011/10/machine-learning-with-python-linear.htmlhttp://machinelearningmastery.com/simple-linear-regression-tutorial-for-machine-learning/http://machinelearningmastery.com/implement-simple-linear-regression-scratch-python/https://medium.com/@haydar_ai/learning-data-science-day-9-linear-regression-on-boston-housing-dataset-cd62a80775ef\n                    © 2017 Viblo. All rights reserved.\n                ",
          "relevence": "yes"
        },
        {
          "url": "http://rpubs.com/kiengsovn/hoiquy1",
          "title": "RPubs - Hồi quy tuyến tính",
          "content": "\n\nTwitter\n\n\nFacebook\n\n\nGoogle+\n\n",
          "relevence": "yes"
        },
        {
          "url": "http://quantri.vn/dict/details/13789-hoi-quy-tuyen-tinh-2-bien",
          "title": "Hồi quy tuyến tính 2 biến | Quantri.vn",
          "content": "Nếu bạn chưa là hội viên hãy trở thành hội viên của quantri.vn để được hỗ trợ và chia sẽ thông tin nhiều hơn. Click Like button để trở thành hội viên của quantri.vn trên facebook.\n\t\t\t\t\t\t\t\t\tLý thuyết Quản Trị là hệ thống mà Quantri.vn đã số hoá toàn bộ Sách giáo khoa của chương trình \n\t\t\t\t\t\t\t\t\t4 năm đại học và 2 năm sau đại học chuyên ngành Quản trị Kinh doanh.\n\t\t\t\t\t\t\t\t\tVới hệ thống này, bạn có thể truy xuất tất cả hệ thống lý thuyết chuyên ngành Quản trị Kinh doanh \n\t\t\t\t\t\t\t\t\ttrong quá trình nghe giảng, làm bài tập hoặc thi cử.\n\t\t\t\t\t\t\t\t\tHệ thống Lý Thuyết Quản Trị được phát triển bởi Viện MBA, thành viên của MBA Institute Global\n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\tNếu còn thắc mắc hoặc tìm hiểu chuyên sâu hơn về Quản trị Ứng dụng, bạn có thể \n\t\t\t\t\t\t\t\t\tđặt câu hỏi với Chuyên Gia Quantri.vn\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tHàm hồi quy tuyến tính 2 biến của tổng thể:Trong quan hệ hồi quy , một biến phụ thuộc có thể được giải thích bởi nhiều biến độc lập.Nếu chỉ nghiên cứu một biến phụ thuộc bị ảnh hưởng bởi một biến độc lập => Mô hình hồi quy hai biến.Nếu mối quan hệ giữa hai biến này là tuyến tính => Mô hình hồi quy tuyến tính hai biến.Hàm hồi quy tổng thể (PRF) của mô hình hồi quy hai biến: Hàm hồi quy mẫu của hồi quy 2 biến:Trong thực tế rất khó nghiên cứu trên tổng thể nên thông thường người ta nghiên cứu xây dựng hàm hồi quy trên một mẫu => Gọi là hàm hồi quy mẫu.Nếu bỏ qua sai số ngẫu nhiên ei , thì giá trị thực tế Yi sẽ trở thành giá trị ước lượng Quantri.vn- Biên tập và hệ thống hóa\n\t                \tGiấy phép Trang tin điện tử tổng hợp số 18/GP-ICP-STTTT\n\t\t\t\t\t\tSáng lập và chịu trách nhiệm nội dung: Tiến Sĩ Dũng Nguyễn\n\t\t\t\t\t\tCopyright © 2013 Quantri. All rights reserved.\n\t                \n                    \tQuanTri.vn là trang thông tin điện tử tổng hợp thuộc sở hữu của\n\t\t\t\t\t\tMBA Institute Vietnam\n\t\t\t\t\t\tA member of MBA Institute Global\n\t\t\t\t\t\t106 Nguyễn Văn Trỗi Quận Phú Nhuận Thành phố Hồ Chí Minh\n\t\t\t\t\t\tĐiện thoại: (08).22.49.49.49 \n\t\t\t\t\t\tEmail: info@mbainstitute.org\n                   ",
          "relevence": "yes"
        },
        {
          "url": "https://techtalk.vn/tag/linear-regression-hoi-quy-tuyen-tinh",
          "title": "Linear Regression (Hồi Quy Tuyến Tính) | Tech Talk",
          "content": "",
          "relevence": "yes"
        },
        {
          "url": "http://bis.net.vn/forums/t/722.aspx",
          "title": "\r\n\tPhân tích hồi qui tuyến tính với SPSS - BIS\r\n",
          "content": "Phân tích hồi qui tuyến tính với SPSSNguyễn Văn Chức – chuc1803@gmail.comHồi qui là một mô hình thống kê được sử dụng để dự đoán giá trị của biến phụ thuộc (dependence variable) hay còn gọi là biến kết quả dựa vào những giá trị của ít nhất 1 biến độc lập (independence variable) hay còn gọi là biến nguyên nhân. Nếu mô hình hồi qui phân tích sự phụ thuộc của 1 biến phụ thuộc vào 1 biến độc lập gọi là hồi qui đơn, nếu có nhiều biến độc lập gọi là hồi qui bội. Hồi qui tuyến tính là mô hình hồi qui trong đó mối quan hệ giữa các biến được biểu diễn bởi một đường thẳng (đường thẳng là đường phù hợp nhất với dữ liệu).Bài viết này giới thiệu sơ lược về mô hình hồi qui tuyến tính đơn và cách thực hiện hồi qui trên phần mềm SPSS v18Hồi qui tuyến tính đơnHồi qui tuyến tính đơn biểu diễn mối quan hệ giữa biến phụ thuộc vào 1 biến độc lập. Mô hình hồi qui được biểu diễn như sau:Ví dụ: Có dữ liệu về diện tích của 7 cửa hàng bán trái cây và doanh thu hàng năm  như sau:Yêu cầu: Lập mô hình hồi qui tuyến tính thể hiện mối quan hệ giữa doanh thu và diện tích của cửa hàng.Sử dụng SPSS1.     Mở SPSS, tạo một file dữ liệu mới với tên và định dạng của các biến trong tab Variable view như sau:Nhập dữ liệu trong tab : Data view như sau:2.Thực hiện hồi qui như sau:Trong menu Analyze, chọn Regression, chọn Linear, xuất hiện hộp thoại, chỉ định biến phụ thuộc trong ô Dependent và các biến độc lập trong ô Independent(s)Thiết lập thêm các tham số tùy chọn (nếu cần thiết) để giải thích mô hìnhBấm OK để thực hiện hồi qui. Kết quả như sau:Các hệ số hồi qui được cho trong bảng CoefficientsPhương trình hồi qui tương ứng Khi diện tích tăng lên 1 đơn vị, mô hình dự đoán doanh thu hàng năm tăng trung bình 1487$.Một số tham số quan trọng để đánh giá mô hình hồi qui.Tham số R bình phương hiệu chỉnh (Adjusted R Square) cho biết mức độ (%) sự biến thiên của biến phụ thuộc được giải thích bởi biến độc lập. Trong ví dụ này, có thể nói 93% sự biến đổi doanh thu hàng năm có thể được giải thích bằng sự biến đổi về qui mô của cửa hàng (đo bằng diện tích).Bảng ANOVAGiá trị của Sig( P-value) của bảng ANOVA dùng để đánh giá sự phù hợp (tồn tại) của mô hình. Giá trị Sig nhỏ (thường <5%) thì mô hình tồn tại.Giá trị Sig trong bảng Coefficients cho biết các tham số hồi qui có ý nghĩa hay không (với độ tin cậy 95% thì Sig<5% có ý nghĩa).Hệ số tương qua cho biết mức độ tương quan giữa biến phụ thuộc và biến độc lập (thường sử dụng hệ số tương quan Pearson)Trong ví dụ này, hệ số tương quan giữa biến phụ thuộc (Doanh thu) và biến độc lập (Diện tích) là 0.97, cho biết mối tương quan giữa Doanh thu và Diện tích là rất chặt.Chạy đồng thời nhiều dạng phương trình hồi qui để tìm dạng đường phù hợp nhất.Trong trường hợp chưa biết dạng đường nào (Linear, Quadratic, Logarithmic, Cubic, Power…) phù hợp với dữ liệu mẫu, ta chạy hồi qui nhiều dạng đường để chọn ra dạng đường phù hợp nhất.Trong menu Analyze, chọn Regression, chọn Cure Estimation, chọn dạng đường muốn thực hiện hồi qui:Kết quả mô hình hồi qui theo từng dạng đường được mô tả đầy đủ trong các bảng dữ liệu, ý nghĩa các tham số đã được giải thích như trong ví dụ hồi qui tuyến tính, từ đó giúp ta chọn được dạng đường phù hợp nhất với dữ liệu đã có. ",
          "relevence": "yes"
        },
        {
          "url": "http://utt.edu.vn/khcb/nghien-cuu-khoa-hoc/su-dung-phan-mem-r-tim-ham-hoi-quy-tuyen-tinh-don-a6454.html",
          "title": "Sử dụng phần mềm R tìm hàm hồi quy tuyến tính đơn",
          "content": "Phân tích hồi qui là nghiên cứu sự phụ thuộc của một biến (biến phụ thuộc hay còn gọi là biến được giải thích) vào một hay nhiều biến khác (biến độc lập hay còn gọi là biến giải thích) với ý tưởng cơ bản là ước lượng (hay dự đoán) giá trị trung bình của biến phụ thuộc trên cơ sở các giá trị đã biết của biến độc lập.• Biến phụ thuộc: là đại lượng ngẫu nhiên tuân theo các quy luật phân bố xác suất.• Biến độc lập: có giá trị xác định trướcBài viết này hướng dẫn cách sử dụng phần mềm R tìm hàm hồi quy tuyến tính đơn. Nội dung chi tiết được trình bày trong file đính kèm.ThS. Trần Thái Minh\n                                        Họ và tên:\n                                        \n                                    \n                                        Địa chỉ email:\n                                        \n                                    \n                                        Bình luận của bạn:\n                                        \n                                    Captcha * \n                                        \n                                    Bài giảngĐề tài vật lýLý thuyết ToánNgoai nguHóa họcBài giảnguttĐề tài vật lýLý thuyết ToánNgoai nguSố 54 Triều Khúc, Thanh Xuân, Hà Nội.Điện thoại: 043.854.4264 Email: infohn@utt.edu.vn - Website: utt.edu.vnĐịa chỉ: 278 Lam Sơn, Đồng Tâm, TP. Vĩnh Yên, Vĩnh Phúc.Điện thoại: 0211.386.7405 - Fax : 0211.386.7391 Email: infovy@utt.edu.vn - Website: vinhyen.utt.edu.vnĐịa chỉ: Phú Thái, Tân Thịnh, TP.Thái Nguyên, Thái Nguyên.Điện thoại: 0280.385.6545 - Fax : 0280.374.6975 Email: infotn@utt.edu.vn - Website: thainguyen.utt.edu.vn",
          "relevence": "yes"
        },
        {
          "url": "https://viblo.asia/p/linear-regression-hoi-quy-tuyen-tinh-trong-machine-learning-4P856akRlY3",
          "title": "Linear Regression - Hồi quy tuyến tính trong Machine Learning - Viblo",
          "content": "Trong bài viết, mình sẽ giới thiệu một trong những thuật toán cơ bản nhất của Machine Learning. Đây là thuật toán Linear Regression (Hồi Quy Tuyến Tính) thuộc nhóm Supervised learning ( Học có giám sát ). Hồi quy tuyến tính là một phương pháp rất đơn giản nhưng đã được chứng minh được tính hữu ích cho một số lượng lớn các tình huống. Trong bài viết này, bạn sẽ khám phá ra chính xác cách thức tuyến tính làm việc như thế nào.\nTrong việc phân tích dữ liệu, bạn sẽ tiếp xúc với thuật ngữ \"Regression\" ( Hồi quy ) rất thường xuyên. Trước khi đi sâu vào Hồi quy tuyến tính, hãy tìm hiểu khái niệm Hồi quy trước đã. Hồi quy chính là một phương pháp thống kê để thiết lập mối quan hệ giữa một biến phụ thuộc và một nhóm tập hợp các biến độc lập. Ví dụ :Ở đây chính ta đang thiết lập mối quan hệ giữa Chiều cao & Trọng lượng của một người với Tuổi của anh/cô ta. Đây là một ví dụ rất cơ bản của Hồi quy.\"Hồi quy tuyến tính\" là một phương pháp thống kê để hồi quy dữ liệu với biến phụ thuộc có giá trị liên tục trong khi các biến độc lập có thể có một trong hai giá trị liên tục hoặc là giá trị phân loại. Nói cách khác \"Hồi quy tuyến tính\" là một phương pháp để dự đoán biến phụ thuộc (Y) dựa trên giá trị của biến độc lập (X). Nó có thể được sử dụng cho các trường hợp chúng ta muốn dự đoán một số lượng liên tục. Ví dụ, dự đoán giao thông ở một cửa hàng bán lẻ, dự đoán thời gian người dùng dừng lại một trang nào đó hoặc số trang đã truy cập vào một website nào đó v.v...Để bắt đầu với Hồi quy tuyến tính, chúng ta hãy đi lướt qua một số khái niệm toán học về thống kê.Không một kích thước nào phù hợp cho tất cả, điều này cũng đúng đối với Hồi quy tuyến tính. Để thoả mãn hồi quy tuyến tính, dữ liệu nên thoả mãn một vài giả định quan trọng. Nếu dữ liệu của bạn không làm theo các giả định, kết quả của bạn có thể sai cũng như gây hiểu nhầm.Trong khi sử dụng hồi quy tuyến tính, mục tiêu của chúng ta là để làm sao một đường thẳng có thể tạo được sự phân bố gần nhất với hầu hết các điểm. Do đó làm giảm khoảng cách (sai số) của các điểm dữ liệu cho đến đường đó.Ví dụ, ở các điểm ở hình trên (trái) biểu diễn các điểm dữ liệu khác nhau và đường thẳng (bên phải) đại diện cho một đường gần đúng có thể giải thích mối quan hệ giữa các trục x & y. Thông qua, hồi quy tuyến tính chúng ta cố gắng tìm ra một đường như vậy. Ví dụ, nếu chúng ta có một biến phụ thuộc Y và một biến độc lập X - mối quan hệ giữa X và Y có thể được biểu diễn dưới dạng phương trình sau:Ở đây,Sử dụng công cụ thống kê ví dụ như Excel, R, SAS ... bạn sẽ trực tiếp tìm hằng số (B0 và B1) như là kết quả của hàm hồi quy tuyến tính. Như lý thuyết ở trên, nó hoạt động trên khái niệm OLS và cố gắng giảm bớt diện tích sai số,  các công cụ này sử dụng các gói phần mềm tính các hằng số này.Ví dụ, giả sử chúng ta muốn dự đoán y từ x trong bảng sau và giả sử rằng phương trình hồi quy của chúng ta sẽ giống như y = B0 + B1 * xỞ đây,Nếu chúng ta phân biệt các Tổng còn lại của diện tích sai số (RSS) tương ứng với B0 & B1 và tương đương với các kết quả bằng không, chúng ta có được các phương trình sau đây như là một kết quả:Đưa giá trị từ bảng 1 vào các phương trình trên,Do đó, phương trình hồi quy nhất sẽ trở thành -Hãy xem, dự đoán của chúng ta như thế nào bằng cách sử dụng phương trình nàyChỉ với 10 điểm dữ liệu để phù hợp với một đường thẳng thì dự đoán của chúng ta sẽ chính xác lắm, nhưng nếu chúng ta thấy sự tương quan giữa 'Y-Thưc tế' và 'Y - Dự đoán' thì triển vọng sẽ rất cao do đó cả hai series đang di chuyển cùng nhau và đây là biểu đồ để hiển thị giá trị dự đoán:Một khi bạn xây dựng mô hình, câu hỏi tiếp theo đến trong đầu là để biết liệu mô hình của bạn có đủ để dự đoán trong tương lai hoặc là mối quan hệ mà bạn đã xây dựng giữa các biến phụ thuộc và độc lập là đủ hay không.Vì mục đích này có nhiều chỉ số mà chúng ta cần tham khảoCông thức tính R^2 sẽ bằng :Trong đó N là số quan sát được sử dụng để phù hợp với mô hình, σx là độ lệch chuẩn của x, và σy là độ lệch chuẩn của y.Root Mean Square Error (RMSE)\nRMSE cho biết mức độ phân tán các giá trị dự đoán từ các giá trị thực tế. Công thức tính RMSE là\nN: Tổng số quan sátMặc dù RMSE là một đánh giá tốt cho các sai số nhưng vấn đề với nó là nó rất dễ bị ảnh hưởng bởi phạm vi của biến phụ thuộc của bạn. Nếu biến phụ thuộc của bạn có dải biến thiên hẹp, RMSE của bạn sẽ thấp và nếu biến phụ thuộc có phạm vi rộng RMSE sẽ cao. Do đó, RMSE là một số liệu tốt để so sánh giữa các lần lặp lại khác nhau của mô hìnhMean Absolute Percentage Error (MAPE)Để khắc phục những hạn chế của RMSE, các nhà phân tích thích sử dụng MAPE so với RMSE. MAPE cho sai số trong tỷ lệ phần trăm và do đó so sánh được giữa các mô hình. Công thức tính MAPE có thể được viết như sau:\nN: Tổng số quan sátCho đến hiện tại, chúng ta đã thảo luận về kịch bản mà chúng ta chỉ có một biến độc lập. Nếu chúng ta có nhiều hơn một biến độc lập, phương pháp phù hợp nhất là \"Multiple Regression Linear\" - Hồi quy tuyến tính đa biếnVề cơ bản không có sự khác biệt giữa hồi quy tuyến tính 'giản đơn' và 'đa biến'. Cả hai đều làm việc tuân theo nguyên tắc OLS và thuật toán để có được đường hồi quy tối ưu nhất cũng tương tự. Trong trường hợp sau, phương trình hồi quy sẽ có một hình dạng như sau:Ở đây,Bi: Các hệ số khác nhau\nXi:  Các biến độc lập khác nhauỞ trên, bạn đã biết rằng hồi quy tuyến tính là một kỹ thuật phổ biến và bạn cũng có thể thấy các phương trình toán học của hồi quy tuyến tính. Nhưng bạn có biết làm thế nào để thực hiện một hồi quy tuyến tính trong Python ?? Có một số cách để có thể làm điều đó, bạn có thể thực hiện hồi quy tuyến tính bằng cách sử dụng các mô hình thống kê, numpy, scipy và sckit learn. Nhưng trong bài này chúng ta sẽ sử dụng sckit learn để thực hiện hồi quy tuyến tính.Scikit-learn là một module Python mạnh mẽ cho việc học máy. Nó chứa hàm cho hồi quy, phân loại, phân cụm, lựa chọn mô hình và giảm kích chiều. Chúng ta sẽ khám phá module sklearn.linear_model có chứa \"các method để thực hiện hồi quy, trong đó giá trị mục tiêu sẽ là sự kết hợp tuyến tính của các biến đầu vào\".Trong bài đăng này, chúng ta sẽ sử dụng bộ dữ liệu Nhà ở Boston, bộ dữ liệu chứa thông tin về giá trị nhà cửa ở ngoại ô thành phố Boston. Tập dữ liệu này ban đầu được lấy từ thư viện StatLib được duy trì tại Đại học Carnegie Mellon và bây giờ đã có trên UCI Machine Learning Repository.Bộ Dữ liệu Nhà ở Boston bao gồm giá nhà ở những nơi khác nhau ở Boston. Cùng với giá cả, tập dữ liệu cũng cung cấp thông tin như Tội phạm (CRIM), các khu vực kinh doanh không-bán-lẻ ở thị trấn (INDUS), tuổi chủ sở hữu ngôi nhà (AGE) và có nhiều thuộc tính khác có sẵn ở đây .\nBộ dữ liệu chính nó có thể down từ đây . Tuy nhiên, vì chúng ta sử dụng scikit-learn, chúng ta có thể import nó từ scikit-learn.Trước hết, chúng ta sẽ import bộ dữ liệu Boston Housing và lưu trữ nó trong một biến gọi là boston. Để import nó từ scikit-learn, chúng ta sẽ cần phải chạy đoạn mã này.Biến boston là một dạng từ điển, vì vậy chúng ta có thể kiểm tra key của nó sử dụng đoạn mã bên dưới.Nó sẽ trả về như sau\nTiếp,Trước tiên, chúng ta có thể dễ dàng kiểm tra shape của nó bằng cách gọi boston.data.shape và nó sẽ trả lại kích thước của tập dữ liệu với kích thước column.Như chúng ta có thể thấy nó trả về (506, 13), có nghĩa là có 506 hàng dữ liệu với 13 cột. Bây giờ chúng ta muốn biết 13 cột là gì. Chúng ta sẽ chạy đoạn code sau :Bạn có thể dùng lệnh print(boston.DESCR) để kiểm tra description của dữ liệu thay vì mở web để đọc.Tiếp, convert dữ liệu về dạng pandas! Rất đơn giản, gọi hàm pd.DataFrame() và truyền boston.data. Chúng ta có thể kiểm tra 5 dữ liệu đầu tiên bằng bos.head().Hoặc bạn co thể dùng đoạn lệnh sau để show được tên cộtCó vẻ vẫn chưa có column tên là PRICE.Ta sẽ add nó vào sử dụng đoạn mã trênNếu bạn muốn nhìn các số liệu tổng hợp thống kê, hãy chạy đoạn mã sau .Về cơ bản, trước khi chia dữ liệu thành tập dữ liệu để train - test, chúng ta cần chia dữ liệu thành hai giá trị : giá trị đích và giá trị dự báo. Hãy gọi giá trị đích Y và các giá trị dự báo X.\nNhư vậy,Bây giờ chúng ta có thể split dữ liệu để train và test với snippet như sau.Nếu chúng ta kiểm tra shape của mỗi biến, chúng ta đã có được bộ dữ liệu với tập dữ liệu thử nghiệm có tỷ lệ 66,66% đối với dữ liệu train và 33,33% đối với dữ liệu test.Tiếp, chúng ta sẽ chạy hồi quy tuyến tính.Đoạn mã trên sẽ phù hợp với một mô hình dựa trên X_train và Y_train. Bây giờ chúng tôi đã có mô hình tuyến tính, chúng ta sẽ cố gắng dự đoán nó cho X_test và các giá trị dự đoán sẽ được lưu trong Y_pred. Để hình dung sự khác biệt giữa giá thực tế và giá trị dự đoán, chúng tôi cũng tạo ra một bảng biểu .Thực tế thì đáng lẽ đồ thị ở trên phải tạo một đường tuyến tính như chúng ta đã thảo luận lý thuyết ở trên. Tuy nhiên, model không thích hợp 100%, cho nên nó đã ko thể tạo được đường tuyến tính.Để kiểm tra mức độ lỗi của một mô hình, chúng ta có thể sử dụng Mean Squared Error. Đây là một trong các phương pháp để đo trung bình của ô vuông của sai số. Về cơ bản, nó sẽ kiểm tra sự khác biệt giữa giá trị thực tế và giá trị dự đoán. Để sử dụng nó, chúng ta có thể sử dụng hàm bình phương trung bình sai số của scikit-learn bằng cách chạy đoạn mã nàykết quả nhận đượchttp://bigdata-madesimple.com/how-to-run-linear-regression-in-python-scikit-learn/http://aimotion.blogspot.com/2011/10/machine-learning-with-python-linear.htmlhttp://machinelearningmastery.com/simple-linear-regression-tutorial-for-machine-learning/http://machinelearningmastery.com/implement-simple-linear-regression-scratch-python/https://medium.com/@haydar_ai/learning-data-science-day-9-linear-regression-on-boston-housing-dataset-cd62a80775ef\n                    © 2017 Viblo. All rights reserved.\n                ",
          "relevence": "yes"
        },
        {
          "url": "http://vietsciences.free.fr/khaocuu/nguyenvantuan/bieudoR/ch10-phantichoiqui.htm",
          "title": "Vietsciences; Nguyễn Văn Tuấn; Nguyen Van Tuan;Hướng dẫn phân tích số \nliệu và vẽ biểu đồ bằng R ;Phân tích hồi qui tuyến tính ; science, khoa hoc, \nkhoahoc, tin hoc,  informatique;computer; vat ly; physics, physique, \nchimie, chemistry, hoa hoc, sinh vat, b ",
          "content": "\n\t\n\t\t\t\t\t\t \n\t\n\t\t\t\t\t\t\n                            \n                              \n                                10\n                                    \n                                    Bảng 1.  Độ tuổi, tỉ trọng cơ thể và \n                                    cholesterol \n                                    > age \n                                    <- c(46,20,52,30,57,25,28,36,22,43,57,33,\n                                               22,63,40,48,28,49) \n > bmi \n                                    <-c(25.4,20.6,26.2,22.6,25.4,23.1,22.7,24.9,\n                                              19.8,25.3,23.2,21.8,20.9,26.7,26.4,21.2,\n                                              21.2,22.8) \n > chol \n                                    <- c(3.5,1.9,4.0,2.6,4.5,3.0,2.9,3.8,\n                                                2.1,3.8,4.1,3.0, 2.5,4.6,3.2,\n                                    \n                                                4.2,2.3,4.0) \n > data \n                                    <- data.frame(age, bmi, chol)> \n                                    plot(chol ~ age, pch=16) \n                                     \n\t\n\t\n                                          \n                                          Biểu đồ 10.1.  Liên hệ giữa độ \n                                          tuổi và cholesterol.Biểu đồ 10.1 trên \n                                    cho thấy mối liên hệ giữa độ tuổi (age) \n                                    và cholesterol là một đường thẳng (tuyến \n                                    tính). Để “đo lường” mối liên hệ này, chúng \n                                    ta có thể sử dụng hệ số tương quan \n                                    (coefficient of correlation).  \n \n                                    10.1  Hệ số \n                                    tương quan  \n \n                                    \n \n                                    \n                                    10.1.1  Hệ số tương quan Pearson\n                                    \n                                    \n                                     Trong đó, như \n                                    định nghĩa phần trên,\n                                     và\n                                     là \n                                    giá trị trung bình của biến số x và\n                                    y. Để ước tính hệ số tương quan giữa \n                                    độ tuổi\n                                    \n                                    \n                                    age \n                                    và cholesterol, chúng ta có thể sử dụng hàm\n                                    \n                                    \n                                    cor(x,y) \n                                    như sau: \n                                    Chúng ta có thể kiểm định giả thiết hệ số \n                                    tương quan bằng 0 (tức hai biến x và\n                                    y không có liên hệ). Phương pháp kiểm \n                                    định này thường dựa vào phép biến đổi Fisher \n                                    mà \n                                    \n                                    R \n                                    đã có sẵn một hàm \n                                    \n                                    cor.test \n                                    để tiến hành việc tính toán.\n                                    > cor.test(age, chol) Kết quả phân \n                                    tích cho thấy kiểm định t = 10.70 với trị số \n                                    p=1.058e-08; do đó, chúng ta có bằng chứng \n                                    để kết luận rằng mối liên hệ giữa độ tuổi và \n                                    cholesterol có ý nghĩa thống kê.  Kết luận \n                                    này cũng chính là kết luận chúng ta đã đi \n                                    đến trong phần phân tích hồi qui tuyến tính \n                                    trên. \n                                                                         \n                                                                        10.1.2  Hệ số tương quan Spearman \n                                    \n                                    r\n                                    \n                                    Cannot compute exact p-values with ties in: \n                                    cor.test.default(age, chol, method = \n                                    \"spearman\") \n                                    Kết quả phân tích cho thấy giá trị \n                                    rho=0.947, và trị số p=0.00000000257. Kết \n                                    quả từ phân tích này cũng không khác với \n                                    phân tích hồi qui tuyến tính: mối liên hệ \n                                    giữa độ tuổi và cholesterol rất cao và có ý \n                                    nghĩa thống kê. Hệ số tương \n                                    quan Kendall (cũng là một phương pháp phân \n                                    tích phi tham số) được ước tính bằng cách \n                                    tìm các cặp số (x, y) “song hành\" với \n                                    nhau. Một cặp (x, y) song hành ở đây \n                                    được định nghĩa là hiệu (độ khác biệt) trên \n                                    trục hoành có cùng dấu hiệu (dương hay âm) \n                                    với hiệu trên trục tung. Nếu hai biến số \n                                    x và y không có liên hệ với nhau, \n                                    thì số cặp song hành bằng hay tương đương \n                                    với số cặp không song hành.  Bởi vì có nhiều \n                                    cặp phải kiểm định, phương pháp tính toán hệ \n                                    số tương quan Kendall đòi hỏi thời gian của \n                                    máy tính khá cao. Tuy nhiên, nếu một dữ liệu \n                                    dưới 5000 đối tượng thì một máy vi tính có \n                                    thể tính toán khá dễ dàng. \n                                    \n                                    R \n                                    dùng hàm \n                                    \n                                    cor.test \n                                    với thông số \n                                    \n                                    method=”kendall” \n                                    để ước tính hệ số tương quan Kendall: Kết quả phân tích hệ số tương quan \n                                    Kendall một lần nữa khẳng định mối liên hệ \n                                    giữa độ tuổi và cholesterol có ý nghĩa thống \n                                    kê, vì hệ số tau = 0.833 và trị số p = \n                                    1.98e-06. Các hệ số \n                                    tương quan trên đây đo mức độ tương quan \n                                    giữa hai biến số, nhưng không cho chúng ta \n                                    một phương trình để nối hai biến số đó với \n                                    nhau. Do đó, vấn đề đặt ra là chúng ta tìm \n                                    một phương trình tuyến tính để mô tả mối \n                                    liên hệ này. Chúng ta sẽ ứng dụng mô hình \n                                    hồi qui tuyến tính.  \n                                    \n                                             \n                                    [1]Nói cách khác, \n                                    phương trình trên giả định rằng độ \n                                    cholesterol của một cá nhân bằng một hằng số\n                                    \n                                    \n                                    a \n                                    cộng với một hệ số \n                                    \n                                    b \n                                    liên quan đến độ tuổi, và một sai số \n                                    \n                                    ei. \n                                    Trong phương trình trên, \n                                    \n                                    a \n                                    là chặn (intercept, tức giá trị lúc\n                                    xi =0), và  \n                                    \n                                    b \n                                    là độ dốc (slope hay gradient). Trong thực \n                                    tế, \n                                    \n                                    a \n                                    và \n                                    \n                                    b \n                                    là hai thông số (paramater, còn gọi là \n                                    regression coefficient hay hệ số hồi \n                                    qui), và \n                                    \n                                    ei \n                                    là một biến số theo luật phân phối chuẩn với \n                                    trung bình 0 và phương sai \n                                    \n                                    s2.\n                                    Các thông \n                                    số \n                                    \n                                    a,\n                                    \n                                    \n                                    b \n                                    và \n                                    \n                                    s2 \n                                    phải được ước tính từ dữ liệu. Phương pháp \n                                    để ước tính các thông số này là phương pháp\n                                    bình phương nhỏ nhất (least squares \n                                    method). Như tên gọi, phương pháp bình \n                                    phương nhỏ nhất tìm giá trị \n                                    \n                                    a,\n                                    \n                                    \n                                    b \n                                    sao cho\n                                     nhỏ nhất. Sau vài \n                                    thao tác toán, có thể chứng minh dễ dàng \n                                    rằng, ước số cho \n                                    \n                                    a \n                                    và \n                                    \n                                    b \n                                    đáp ứng điều kiện đó là: \n                                    \n                                             \n                                    [2]\n                                    và\n                                    \n                                                    \n                                    [3]\n                                    \n                                    \n                                            \n                                    [4]\n                                    \n                                               \n                                    [5]\n                                    \n                                    > lm(chol ~ age)Trong lệnh trên,\n                                    \n                                    \n                                    “chol ~ age” \n                                    có nghĩa là mô tả \n                                    \n                                    chol \n                                    là một hàm số của \n                                    \n                                    age.  \n                                    Kết quả tính toán của \n                                    \n                                    lm \n                                    \n                                    cho thấy\n                                    =1.0892 \n                                    và=0.05779. \n                                     Nói cách khác, với hai thông số này, chúng \n                                    ta có thể ước tính độ cholesterol cho bất cứ \n                                    độ tuổi nào trong khoảng tuổi của mẫu bằng \n                                    phương trình tuyến tính: \n                                    \n                                    =\n                                    \n                                    \n                                    1.08922 + 0.05779 \n                                    \n                                    x \n                                    agePhương trình này \n                                    có nghĩa là khi độ tuổi tăng 1 năm thì độ \n                                    cholesterol tăng khoảng 0.058 mmol/L. \n                                    \n                                    \n                                    > reg <- lm(chol ~ age)\n                                    \n                                    > summary(reg)\n                                    \n                                     \n                                    \n                                    Call: lm(formula = chol ~ age)\n                                    \n                                     \n                                    \n                                    Residuals:\n                                    \n                                         Min       1Q   Median       3Q      Max\n                                    \n                                    \n                                    -0.40729 -0.24133 -0.04522  0.17939  0.63040\n                                    \n                                    \n                                     \n                                    \n                                    Coefficients:\n                                    \n                                                Estimate Std. Error t value \n                                    Pr(>|t|)    \n                                    \n                                    (Intercept) 1.089218   0.221466   4.918 \n                                    0.000154 ***\n                                    \n                                    age         0.057788   0.005399  10.704 \n                                    1.06e-08 ***\n                                    \n                                    ---\n                                    \n                                    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' \n                                    0.05 '.' 0.1 ' ' 1 \n                                    \n                                     \n                                    \n                                    Residual standard error: 0.3027 on 16 \n                                    degrees of freedom\n                                    \n                                    Multiple R-Squared: 0.8775,     Adjusted \n                                    R-squared: 0.8698 \n                                    \n                                    F-statistic: 114.6 on 1 and 16 DF,  p-value: \n                                    1.058e-08\n                                    \n                                    Residuals:\n                                    \n                                         Min       1Q   Median       3Q      Max\n                                    \n                                    \n                                    -0.40729 -0.24133 -0.04522  0.17939  0.63040\n                                    (b) Phần hai trình \n                                    bày ước số của\n                                    và cùng \n                                    với sai số chuẩn và giá trị của kiểm định \n                                    t.  Giá trị kiểm định t cho\n                                     là \n                                    10.74 với trị số p=0.0000000106, cho thấy \n                                    \n                                    \n                                    b \n                                    \n                                    không phải bằng 0.  Nói cách khác, chúng ta \n                                    có bằng chứng để cho rằng có một mối liên hệ \n                                    giữa cholesterol và độ tuổi, và mối liên hệ \n                                    này có ý nghĩa thống kê.  \n                                    \n                                     \n                                    \n                                    Coefficients:\n                                    \n                                                Estimate Std. Error t value \n                                    Pr(>|t|)    \n                                    \n                                    (Intercept) 1.089218   0.221466   4.918 \n                                    0.000154 ***\n                                    \n                                    age         0.057788   0.005399  10.704 \n                                    1.06e-08 ***\n                                    \n                                    ---\n                                    \n                                    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' \n                                    0.05 '.' 0.1 ' ' 1 \n                                    \n                                    Residual standard error: 0.3027 on 16 \n                                    degrees of freedom\n                                    \n                                    Multiple R-Squared: 0.8775,     Adjusted \n                                    R-squared: 0.8698 \n                                    \n                                    F-statistic: 114.6 on 1 and 16 DF,  p-value: \n                                    1.058e-08\n                                    \n                                                           \n                                    [6]Tức là bằng tổng \n                                    bình phương giữa số ước tính và trung bình \n                                    chia cho tổng bình phương số quan sát và \n                                    trung bình. Trị số R2 trong ví dụ \n                                    này là 0.8775, có nghĩa là phương trình \n                                    tuyến tính (với độ tuổi là một yếu tố) giải \n                                    thích khoảng 88% các khác biệt về độ \n                                    cholesterol giữa các cá nhân. Tất nhiên trị \n                                    số R2 có giá trị từ 0 đến 100% \n                                    (hay 1). Giá trị R2 càng cao là \n                                    một dấu hiệu cho thấy mối liên hệ giữa hai \n                                    biến số độ tuổi và cholesterol càng chặt \n                                    chẽ. \n                                                                        10.2.3  Giả định của phân tích hồi qui tuyến \n                                    tính(a) x là \n                                    một biến số cố định hay fixed, (“cố định” ở \n                                    đây có nghĩa là không có sai sót ngẫu nhiên \n                                    trong đo lường);\n                                    \n                                    (b) \n                                    \n                                    ei \n                                    phân phối theo luật phân phối chuẩn; \n                                    (c) \n                                    \n                                    ei \n                                    có giá trị trung bình (mean) là 0; \n                                    \n                                    \n                                    (d) \n                                    \n                                    ei \n                                    có phương sai \n                                    \n                                    s2 \n                                    cố định cho tất cả xi; và\n                                    (e) các giá trị \n                                    liên tục của \n                                    \n                                    ei \n                                    không có liên hệ tương quan với nhau (nói \n                                    cách khác, \n                                    \n                                    e1\n                                    \n                                    \n                                    và \n                                    \n                                    e2 \n                                    không có liên hệ với nhau). \n                                    Nếu các giả định này không được đáp ứng thì \n                                    mô hình mà chúng ta ước tính có vấn đề hợp \n                                    lí (validity). Do đó, trước khi trình bày và \n                                    diễn dịch mô hình trên, chúng ta cần phải \n                                    kiểm tra xem các giả định trên có đáp ứng \n                                    được hay không. Trong trường hợp này, giả \n                                    định (a) không phải là vấn đề, vì độ tuổi \n                                    không phải là một biến số ngẫu nhiên, và \n                                    không có sai số khi tính độ tuổi của một cá \n                                    nhân.   Đối với các giả định (b) \n                                    đến (e), cách kiểm tra đơn giản nhưng hữu \n                                    hiệu nhất là bằng cách xem xét mối liên hệ \n                                    giữa,\n                                    , \n                                    và phần dư\n                                     () \n                                    bằng những đồ thị tán xạ.  Với lệnh \n                                    \n                                    fitted() \n                                    chúng ta có thể tính toán\n                                     cho \n                                    từng cá nhân như sau (ví dụ đối với đối \n                                    tượng số 1, 46 tuổi, độ cholestrol có thể \n                                    tiên đoán như sau: \n                                    \n                                    1.08922 + 0.05779 \n                                    \n                                    x \n                                    46 = 3.747). \n                                    Với lệnh \n                                    \n                                    resid() \n                                    chúng ta có thể tính toán  phần dư\n                                     cho \n                                    từng cá nhân như sau (với đối tượng 1, e1 \n                                    = \n                                    \n                                    3.5 – 3.74748 = -0.24748):\n                                    \n                                    \n\t\n\t\n                                          \n                                          Biểu đồ 10.2.  Phân tích phần \n                                          dư để kiểm tra các giả định trong phân \n                                          tích hồi qui tuyến tính.\n                                    \n                                    Để kiểm tra các giả định trên, chúng ta có \n                                    thể vẽ một loạt 4 đồ thị \n                                    \n                                    treân như\n                                    \n                                    \n                                    sau: (a) Đồ thị bên \n                                    trái dòng 1 vẽ phần dư\n                                    và \n                                    giá trị tiên đoán cholesterol. \n                                    Đồ thị này cho thấy các giá trị phần dư tập \n                                    chung quanh đường y = 0, cho nên giả định \n                                    (c), hay \n                                    \n                                    ei \n                                    có giá trị trung bình 0, là có thể chấp nhận \n                                    được.  \n                                    \n                                    (b) Đồ thị bên phải dòng 1 vẽ giá trị phần \n                                    dư và giá trị kì vọng dựa vào phân phối \n                                    chuẩn. Chúng ta thấy các số phần dư tập \n                                    trung rất gần các giá trị trên đường chuẩn, \n                                    và do đó, giả định (b), tức  \n                                    \n                                    ei \n                                    phân phối theo luật phân phối chuẩn, cũng có \n                                    thể đáp ứng. \n                                    \n                                                                        10.2.4    \n                                    \n                                                                        Mô hình tiên đoán\n                                    Sau khi mô hình tiên đoán cholesterol đã \n                                    được kiểm tra và tính hợp lí đã được thiết \n                                    lập, chúng ta có thể vẽ đường biểu diễn của \n                                    mối liên hệ giữa độ tuổi và cholesterol bằng \n                                    lệnh \n                                    \n                                    abline \n                                    như sau (xin nhắc lại object của phân tích \n                                    là \n                                    \n                                    reg):\n                                    \n                                    \n                                    > plot(chol ~ age, pch=16)\n                                     \n                                    \n\t\n\t\n                                          \n                                          Biểu đồ 10.3.  Đường biểu diễn \n                                          mối liên hệ giữa độ tuổi (age) và \n                                          cholesterol. \n\t\t\t\t\t\n                                    \n                                    Nhưng mỗi giá trị\n                                     được tính \n                                    từ ước số \n                                    và, \n                                    mà các ước số này đều có sai số chuẩn, cho \n                                    nên giá trị tiên đoán\n                                     cũng \n                                    có sai số.  Nói cách khác,\n                                     chỉ \n                                    là trung bình, nhưng trong thực tế có thể \n                                    cao hơn hay thấp hơn tùy theo chọn mẫu.  \n                                    Khoảng tin cậy 95% này có thể ước tính qua\n                                    \n                                    \n                                    R \n                                    bằng các lệnh sau đây:  \n                                    \n \n                                    \n\t\n\t\n                                          \n                                          Biểu đồ 10.4.  Giá trị \n                                    tiên đoán và khoảng tin cậy 95% \n \n                                    \n                                    \n                                    10.3  Mô hình hồi qui tuyến tính đa biến \n                                    (multiple linear regression) \n                                    \n                                     \n                                    [7]\n                                    y1 \n                                    = \n                                    \n                                    a + \n                                    \n                                    b1x11 \n                                    + \n                                    \n                                    b2x21 \n                                    + …+ \n                                    \n                                    bkxk1 \n                                    +  \n                                    \n                                    e1\n                                    y2 \n                                    = \n                                    \n                                    a + \n                                    \n                                    b1x12 \n                                    + \n                                    \n                                    b2x22 \n                                    + …+ \n                                    \n                                    bkxk2 \n                                    +  \n                                    \n                                    e2\n                                    y3 \n                                    = \n                                    \n                                    a + \n                                    \n                                    b1x13 \n                                    + \n                                    \n                                    b2x23 \n                                    + …+ \n                                    \n                                    bkxk3 \n                                    +  \n                                    \n                                    e3\n                                    …\n                                    yn \n                                    = \n                                    \n                                    a + \n                                    \n                                    b1x1n \n                                    + \n                                    \n                                    b2x2n \n                                    + …+ \n                                    \n                                    bkxkn \n                                    +  \n                                    \n                                    en\n                                     \n                                    Phương pháp ước tính\n                                     cũng chủ yếu dựa vào phương \n                                    pháp bình phương nhỏ nhất. Gọi\n                                     là \n                                    ước tính của yi , \n                                    phương pháp bình phương nhỏ nhất tìm giá trị\n                                     sao \n                                    cho\n                                     nhỏ \n                                    nhất. Đối với mô hình \n                                    hồi qui tuyến tính đa biến, cách viết và mô \n                                    tả mô hình gọn nhất là dùng kí hiệu ma trận. \n                                    Mô hình [7] có thể thể hiện bằng kí hiệu ma \n                                    trận như sau: Y \n                                    = Xb\n                                    + \n                                    \n                                    e\n                                    \n                                    \n                                    \n                                    \n                                    Ví dụ 2.  \n                                    Chúng ta quay lại nghiên cứu về mối liên hệ \n                                    giữa độ tuổi, \n                                    \n                                    bmi \n                                    \n                                    và cholesterol. Trong ví dụ, chúng ta chỉ \n                                    mới xét mối liên hệ giữa độ tuổi và \n                                    cholesterol, mà chưa xem đến mối liên hệ \n                                    giữa  cả hai yếu tố độ tuổi và \n                                    \n                                    bmi \n                                    \n                                    và cholesterol. Biểu đồ sau đây cho chúng ta \n                                    thấy mối liên hệ giữa ba biến số này: \n                                    \n                                    \n                                    > pairs(data) \n                                    \n\t\n\t\n                                          \n                                          Biểu đồ 10.5.  Giá trị tiên \n                                          đoán và khoảng tin cậy 95%.Cũng như giữa độ \n                                    tuổi và cholesterol, mối liên hệ giữa bmi và \n                                    cholesterol cũng gần tuân theo một đường \n                                    thằng.  Biểu đồ trên còn cho chúng ta thấy \n                                    độ tuổi và bmi có liên hệ với nhau.  Thật \n                                    vậy, phân tích hồi qui tuyến tính đơn giản \n                                    giữa bmi và cholesterol cho thấy như mối \n                                    liên hệ này có ý nghĩa thống kê: \n                                    > summary(lm(chol ~  bmi))\n                                    \n                                    Call: lm(formula = chol ~ bmi)\n                                    Coefficients:\n                                    \n                                    hay phương trình \n                                    cũng có thể mô tả bằng kí hiệu ma trận: Y \n                                    = Xb\n                                    + \n                                    \n                                    e \n                                    \n                                     vừa trình bày ở trên. Ở đây, Y là \n                                    một vector vector 18 \n                                    \n                                    x \n                                    \n                                    1, X là một matrix 18 \n                                    \n                                    x \n                                    \n                                    2 phần tử, \n                                    \n                                    b \n                                    và một vector 2 \n                                    \n                                    x \n                                    \n                                    1, và \n                                    \n                                    e \n                                    \n                                    là vector gồm 18 \n                                    \n                                    x \n                                    \n                                    1 phần tử. Để ước tính hai hệ số hồi qui,\n                                    \n                                    \n                                    b1 \n                                    \n                                    và \n                                    \n                                    b2 \n                                    \n                                    chúng ta cũng ứng dụng hàm \n                                    \n                                    lm()trong\n                                    \n                                    \n                                    R \n                                    như sau: \n                                    \n                                    Cholesterol = 0.455 + 0.054(age) + \n                                    0.0333(bmi) \n                                    Chúng ta chú ý phương trình với độ tuổi \n                                    (trong phân tích phần trước) giải thích \n                                    khoảng 87.7% độ dao động cholesterol giữa \n                                    các cá nhân. Khi chúng ta thêm yếu tố BMI, \n                                    hệ số này tăng lên 88.2%, tức chỉ 0.5%.  Câu \n                                    hỏi đặt ra là 0.5% tăng trưởng này có ý \n                                    nghĩa thống kê hay không. Câu trả lời có thể \n                                    xem qua kết quả kiểm định yếu tố bmi với trị \n                                    số p = 0.487. Như vậy, bmi không cung cấp \n                                    cho chúng thêm thông tin hay tiên đoán \n                                    cholesterol hơn những gì chúng ta đã có từ \n                                    độ tuổi. Nói cách khác, khi độ tuổi đã được \n                                    xem xét, thì ảnh hưởng của bmi không còn ý \n                                    nghĩa thống kê. Điều này có thể hiểu được, \n                                    bởi vì qua biểu đồ 10.5 chúng ta thấy độ \n                                    tuổi và bmi có một mối liên hệ khá cao. Vì \n                                    hai biến này có tương quan với nhau, chúng \n                                    ta không cần cả hai trong phương trình. (Tuy \n                                    nhiên, ví dụ này chỉ có tính cách minh họa \n                                    cho việc tiến hành phân tích hồi qui tuyến \n                                    tính đa biến bằng \n                                    \n                                    R, \n                                    chứ không có ý định mô phỏng dữ liệu theo \n                                    định hướng sinh học).\n                                    \n\t\n\t\n                                          \n                                          Biểu đồ 10.6.  Phân tích phần \n                                          dư để kiểm tra các giả định trong phân \n                                          tích hồi qui tuyến tính đa biến.Tuy \n                                    BMI không có ý nghĩa thống kê trong trường \n                                    hợp này, Biểu đồ 10.6 cho thấy các \n                                    giả định về mô hình hồi qui tuyến tính có \n                                    thể đáp ứng.\n                                    yi \n                                    = \n                                    a \n                                    + \n                                    b1x \n                                    + \n                                    b2x2 \n                                    + \n                                    b3x3 \n                                    + .. + \n                                    bpxp \n                                    + \n                                    ei.\n                                    Trong \n                                    đó các thông số \n                                    bj\n                                    (j = 1, 2, 3, … p)\n                                    là hệ số đo lường mối liên hệ giữa y\n                                    và x; và \n                                    ei\n                                    là phần dư của mô hình, với giả định\n                                    ei\n                                    tuân theo luật phân phối chuẩn \n                                    với trung bình 0 và phương sai \n                                    s2. \n                                    Cho một dãy cặp số (y1, \n                                    x1), (y2,\n                                    x2), (y3,\n                                    x3), …, (yn,\n                                    xn), chúng ta có thể áp \n                                    dụng phương pháp bình phương nhỏ nhất để ước \n                                    tính \n                                    bj\n                                    và \n                                    s2.\n                                      \n                                    Trước \n                                    khi phân tích các số liệu này, chúng ta cần \n                                    nhập số liệu vào \n                                    R \n                                    với những lệnh thông thường như sau: \n                                     \n > id <- \n                                    1:19 \n > conc \n                                    <- c(1.0, 1.5, 2.0, 3.0, 4.0, 4.5,  5.0,  \n                                    5.5,\n                                                6.0, 6.5, 7.0, 8.0, 9.0, 10.0, \n                                    11.0, 12.0,\n                                               13.0, 14.0, 15.0)  \n > \n                                    strength <- c(6.3, 11.1, 20.0, 24.0, 26.1, \n                                    30.0, \n                                                    33.8, 34.0, 38.1, 39.9, \n                                    42.0, 46.1, \n                                                    53.1, 52.0, 52.5, 48.0, \n                                    42.8, 27.8, \n                                                    21.9) \n > data \n                                    <- data.frame(id, conc, strength) \n \n                                    Chúng \n                                    ta thử xem mô hình hồi qui tuyến tính đơn \n                                    giản bằng lệnh:  \n > \n                                    simple.model <- lm(strength ~ conc)> \n                                    summary(simple.model) \n Call: \n                                    lm(formula = strength ~ conc) \n \n                                    Residuals:    \n                                    Min      1Q  Median      3Q     Max \n                                    \n                                    -25.986  -3.749   2.938   7.675  15.840\n                                     \n \n                                    Coefficients:        \n                                        Estimate Std. Error t value Pr(>|t|)  \n                                    \n                                    (Intercept)  21.3213     5.4302   3.926  \n                                    0.00109 **\n                                    conc          1.7710     0.6478   2.734  \n                                    0.01414 * ---Signif. \n                                    codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' \n                                    0.1 ' ' 1  \n \n                                    Residual standard error: 11.82 on 17 degrees \n                                    of freedom\n                                    Multiple R-Squared: 0.3054,     Adjusted \n                                    R-squared: 0.2645 \n                                    F-statistic: 7.474 on 1 and 17 DF,  p-value: \n                                    0.01414 \n \n                                    Kết \n                                    quả trên cho thấy mô hình hồi qui tuyến tính \n                                    đơn giản này (strength \n                                    = 21.32 + 1.77*conc) \n                                    giải thích khoảng 31% phương sai của \n                                    strength. \n                                    Ước số phương sai của mô hình này là:   s2= \n                                    (11.82)2 = 139.7.   \n \n                                    Bây \n                                    giờ chúng ta xem qua biểu đồ và đường biểu \n                                    diễn của mô hình trên: \n                                    > \n                                    plot(strength ~ conc, \n                                           \n                                    xlab=\"Concentration of hardwood\", \n                                    \n                                           \n                                    ylab=\"Tensile strength\",      \n                                    main=\"Relationship between hardwood \n                                    concentration \n                                                 \\n and tensile strengt\", \n                                    pch=16) \n \n                                    \n\t\n\t\n\t\t\t\t\t                Biểu đồ 10.7. Mối liên hệ giữa hàm \n                                          lượng gỗ cứng và độ căng mạnh của vật \n                                          liệu. Đường thẳng là đường biểu diễn \n                                          của mô hình hồi qui tuyến tính đơn \n                                          giản. \n\t\t\t\t\t \n Qua biểu đồ này, \n                                    chúng ta thấy rõ ràng mô hình hồi qui tuyến \n                                    tính không thích hợp cho số liệu, bởi vì mối \n                                    liên hệ giữa hai biến này không tuân theo \n                                    một phương trình đường thẳng, mà là một \n                                    đường cong.  Nói cách khác, một mô hình \n                                    phương trình bậc hai có lẽ thích hợp hơn.  \n                                    Gọi y là strength và x là \n                                    conc, chúng ta có thể viết mô hình đó như \n                                    sau:  \n \n                                    yi \n                                    = \n                                    a \n                                    + \n                                    b1x \n                                    + \n                                    b2x2 \n \n                                    Bây \n                                    giờ chúng ta sẽ sử dùng \n                                    R \n                                    để ước tính ba thông số trên. \n \n                                    > \n                                    quadratic <- lm(strength ~ poly(conc, 2))\n                                    > \n                                    summary(quadratic) \n \n                                    Call:\n                                    lm(formula \n                                    = strength ~ poly(conc, 2)) \n \n                                    Residuals:\n                                        \n                                    Min      1Q  Median      3Q     Max \n                                    \n                                    -5.8503 \n                                    -3.2482 -0.7267  4.1350  6.5506  \n \n                                    \n                                    Coefficients:\n                                    \n                                                   Estimate Std. Error t value \n                                    Pr(>|t|)    \n                                    \n                                    (Intercept)      34.184      1.014  33.709 \n                                    2.73e-16 ***\n                                    poly(conc, \n                                    2)1   32.302      4.420   7.308 1.76e-06 ***\n                                    poly(conc, \n                                    2)2  -45.396      4.420 -10.270 1.89e-08 ***\n                                    ---\n                                    Signif. \n                                    codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' \n                                    0.1 ' ' 1  \n \n                                    Residual \n                                    standard error: 4.42 on 16 degrees of \n                                    freedom\n                                    Multiple \n                                    R-Squared: 0.9085,     Adjusted R-squared: \n                                    0.8971 \n                                    \n                                    F-statistic: 79.43 on 2 and 16 DF,  p-value: \n                                    4.912e-09 \n       y \n                                    = 34.18 + 32.30*x – 45.4*x2 \n                                     \n \n                                    giải thích khoảng 91% phương sai của y.  \n                                    Phương sai của y bây giờ là   s2 \n                                    = (4.42)2 = 19.5.  So với mô hình \n                                    tuyến tính, mô hình này rõ ràng là tốt hơn \n                                    rất nhiều. \n       Chúng \n                                    ta thử xét một mô hình cubic (bậc ba): \n       yi \n                                    = \n                                    a \n                                    + \n                                    b1x \n                                    + \n                                    b2x2\n                                    + \n                                    b3x3 \n                                     \n Xem \n                                    có mô tả y tốt hơn mô hình phương \n                                    trình bậc hai hay không. \n > cubic \n                                    <- lm(strength ~ poly(conc, 3))> \n                                    summary(cubic)Call: \n                                    lm(formula = strength ~ poly(conc, 3)) \n \n                                    Residuals:     \n                                    Min       1Q   Median       3Q      Max\n                                    \n                                    -4.62503 -1.61085  0.04125  1.58922  5.02159\n                                     \n \n                                    Coefficients:\n                                                   Estimate Std. Error t value \n                                    Pr(>|t|)    \n                                    (Intercept)     34.1842     0.5931  57.641  \n                                    < 2e-16 ***\n                                    poly(conc, 3)1  32.3021     2.5850  12.496 \n                                    2.48e-09 ***\n                                    poly(conc, 3)2 -45.3963     2.5850 -17.561 \n                                    2.06e-11 ***\n                                    poly(conc, 3)3 -14.5740     2.5850  -5.638 \n                                    4.72e-05 ***---Signif. \n                                    codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' \n                                    0.1 ' ' 1  \n \n                                    Residual standard error: 2.585 on 15 degrees \n                                    of freedom\n                                    Multiple R-Squared: 0.9707,     Adjusted \n                                    R-squared: 0.9648 \n                                    F-statistic: 165.4 on 3 and 15 DF,  p-value: \n                                    1.025e-11 \n # \n                                    lặp lại các mô hình trên: > \n                                    linear <- lm(strength ~ conc)> \n                                    quadratic <- lm(strength ~ poly(conc, 2))> cubic \n                                    <- lm(strength ~ poly(conc, 3)) \n # \n                                    tạo nên một biến x với nhiều số gần nhau> xnew \n                                    <- (0:160)/10  \n # \n                                    Tính giá trị tiên đoán (predictive values) \n                                    của y > y2 = \n                                    predict(quadratic, data.frame(conc=xnew)) \n                                    > y3 = \n                                    predict(cubic, data.frame(conc=xnew)) \n                                     \n # Vẽ \n                                    3 đường thẳng, bậc hai và bậc 3  \n                                    > \n                                    plot(strength ~ conc, pch=16,       \n                                    main=”Hardwood concentration and tensile \n                                    strength”,       \n                                    sub=”Linear, quadratic, and cubic fits”)> \n                                    abline(linear, col=”black”)> \n                                    lines(xnew, y2, col=”blue”, lwd=3)> \n                                    lines(xnew, y3, col=”red”, lwd=4) \n                                     \n                                    10.5  Xây \n                                    dựng mô hình tuyến tính từ nhiều biến \n      Cho một mô hình \n                                    hồi qui tuyến tính\n                                    , \n                                    chúng ta có k+1 thông số\n                                    ), \n                                    và có thể tính tổng bình phương phần dư \n                                    (residual sum of squares, RSS):\n                                    \n                                    \n                                    \n                                    \n                                    Ví dụ 4. \n                                    Để nghiên cứu ảnh hưởng của các yếu tố như \n                                    nhiệt độ, thời gian, và thành phần hóa học \n                                    đến sản lượng CO2. Số liệu của \n                                    nghiên cứu này có thể tóm lược trong bảng số \n                                    2. Mục tiêu chính của nghiên cứu là tìm một \n                                    mô hình hồi qui tuyến tính để tiên đoán sản \n                                    lượng CO2, cũng như đánh giá độ \n                                    ảnh hưởng của các yếu tố này. Bảng 2.  Sản \n                                    lượng CO2 và một số yếu tố có thể \n                                    ảnh hưởng đến CO2\n                                    \n                                    Chú thích:  y = sản lượng CO2;  \n                                    X1 = thời gian (phút); X2 = nhiệt độ (C); X3 \n                                    = phần trăm hòa tan; X4 = lượng dầu \n                                    (g/100g); X5 = lượng than đá; X6 = tổng số \n                                    lượng hòa tan; X7 = số hydrogen tiêu thụ.\n                                    \n                                    Trước \n                                    khi phân tích số liệu, chúng ta cần nhập số \n                                    liệu vào \n                                    R \n                                    bằng các lệnh thông thường.  Số liệu sẽ chứa \n                                    trong đối tượng \n                                    REGdata.\n                                    > y <- \n                                    c(36.98,13.74,10.08, 8.53,36.42,26.59,19.07,        \n                                    5.96,15.52,56.61,26.72,20.80,6.99,45.93,\n                                             43.09,15.79,21.60,35.19,26.14, \n                                    8.60,  \n                                           11.63, 9.59, \n                                    4.42,38.89,11.19,75.62,36.03) \n > x1 <- \n                                    c(5.1,26.4,23.8,46.4, 7.0,12.6,18.9,30.2,\n                                              53.8,5.6,15.1,20.3,48.4,5.8,11.2,27.9,5.1,\n                                              11.7,16.7,24.8,24.9,39.5,29.0, \n                                    5.5, 11.5,   \n                                            5.2,10.6) \n > x2 <- \n                                    c(400,400, 400, 400, 450, 450, 450, 450, \n                                    450,     \n                                          400, 400, 400, 400, 425, 425, 425, \n                                    450, 450, \n                                              450, 450, 450, 450, 450, 460, 450, \n                                    470, 470) \n > x3 <- \n                                    c(51.37,72.33,71.44,79.15,80.47,89.90,91.48,\n                                              98.60,98.05,55.69, \n                                    66.29,58.94,74.74,63.71,\n                                              67.14,77.65,67.22,81.48,83.88,89.38,79.77,\n                                              87.93, \n                                    79.50,72.73,77.88,75.50,83.15) \n > x4 <- \n                                    c(4.24,30.87,33.01,44.61,33.84,41.26,41.88,\n                                              70.79,66.82,8.92,17.98,17.79,33.94,11.95,\n                                              14.73,34.49,14.48,29.69,26.33, \n                                    37.98,25.66,\n                                              22.36,31.52,17.86,25.20, \n                                    8.66,22.39) \n > x5 <- \n                                    c(1484.83, 289.94, 320.79, 164.76, 1097.26,\n                                    \n                                              605.06, 405.37, 253.70, \n                                    142.27,1362.24, 507.65,\n                                             377.60,  158.05, 130.66, 682.59, \n                                    274.20, \n                                              1496.51, 652.43,  458.42, 312.25, \n                                    307.08, \n                                              193.61,155.96,1392.08, \n                                    663.09,1464.11, 720.07) \n > x6 <- \n                                    c(2227.25, 434.90, 481.19, 247.14,1645.89,\n                                    \n                                              907.59,608.05, 380.55, \n                                    213.40,2043.36, 761.48,\n                                              566.40,237.08,1961.49,1023.89, \n                                    411.30,2244.77,\n                                              978.64,687.62, 468.38, 460.62, \n                                    290.42,233.95,     \n                                         2088.12,994.63,2196.17,1080.11) \n > x7 <- \n                                    c(2.06,1.33,0.97,0.62,0.22,0.76,1.71,3.93,1.97,\n                                              5.08,0.60,0.90, \n                                    0.63,2.04,1.57,2.38,0.32,\n                                              0.44,8.82,0.02,1.72,1.88,1.43,1.35,1.61,\n                                              4.78,5.88) \n > \n                                    REGdata <- data.frame(y, \n                                    x1,x2,x3,x4,x5,x6,x7)  \n \n                                    Trước \n                                    khi phân tích số liệu, chúng ta cần nhập số \n                                    liệu vào \n                                    R \n                                    bằng các lệnh thông thường.  Số liệu sẽ chứa \n                                    trong đối tượng \n                                    REGdata.\n                                     \n \n                                    Bây \n                                    giờ chúng ta bắt đầu phân tích. Mô hình đầu \n                                    tiên là mô hình gồm tất cả 7 biến độc lập \n                                    như sau:  \n                                     \n \n                                    > reg <- \n                                    lm(y ~ x1+x2+x3+x4+x5+x6+x7, data=REGdata)\n                                    > \n                                    summary(reg) \n \n                                    Call: \n                                    lm(formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 \n                                    + x7, data = REGdata) \n \n                                    Residuals:\n                                        \n                                    Min      1Q  Median      3Q     Max \n                                    \n                                    -20.035  \n                                    -4.681  -1.144   4.072  21.214  \n \n                                    \n                                    Coefficients:\n                                    \n                                                 Estimate Std. Error t value \n                                    Pr(>|t|) \n                                    \n                                    (Intercept) 53.937016  57.428952   0.939   \n                                    0.3594  \n                                    \n                                    x1          -0.127653   0.281498  -0.453   \n                                    0.6553  \n                                    \n                                    x2          -0.229179   0.232643  -0.985   \n                                    0.3370  \n                                    \n                                    x3           0.824853   0.765271   1.078   \n                                    0.2946  \n                                    \n                                    x4          -0.438222   0.358551  -1.222   \n                                    0.2366  \n                                    \n                                    x5          -0.001937   0.009654  -0.201   \n                                    0.8431  \n                                    \n                                    x6           0.019886   0.008088   2.459   \n                                    0.0237 *\n                                    \n                                    x7           1.993486   1.089701   1.829   \n                                    0.0831 .\n                                    ---\n                                    Signif. \n                                    codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' \n                                    0.1 ' ' 1  \n \n                                    Residual \n                                    standard error: 10.61 on 19 degrees of \n                                    freedom\n                                    Multiple \n                                    R-Squared: 0.728,      Adjusted R-squared: \n                                    0.6278 \n                                    \n                                    F-statistic: 7.264 on 7 and 19 DF,  p-value: \n                                    0.0002674 \n \n                                    Kết \n                                    quả trên cho thấy tất cả 7 biến số “giải \n                                    thích” khoảng 73% phương sai của y. \n                                    Nhưng trong 7 biến đó, chỉ có \n                                    x6 \n                                    là có \n                                    ý nghĩa thống kê (p=0.024). Chúng ta thử \n                                    giảm mô hình thành một mô hình hồi qui tuyến \n                                    tính đơn giản với chỉ biến \n                                    x6. \n \n                                    > \n                                    summary(lm(y ~ x6, data=REGdata)) \n \n                                    Call: \n                                    lm(formula = y ~ x6, data = REGdata) \n \n                                    Residuals:\n                                        \n                                    Min      1Q  Median      3Q     Max \n                                    \n                                    -28.081  \n                                    -5.829  -0.839   5.522  26.882  \n \n                                    \n                                    Coefficients:\n                                    \n                                                Estimate Std. Error t value \n                                    Pr(>|t|)    \n                                    \n                                    (Intercept) 6.144181   3.483064   1.764     \n                                    0.09 .  \n                                    \n                                    x6          0.019395   0.002932   6.616 \n                                    6.24e-07 ***\n                                    ---\n                                    Signif. \n                                    codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' \n                                    0.1 ' ' 1  \n \n                                    Residual \n                                    standard error: 10.7 on 25 degrees of \n                                    freedom\n                                    Multiple \n                                    R-Squared: 0.6365,     Adjusted R-squared: \n                                    0.6219 \n                                    \n                                    F-statistic: 43.77 on 1 and 25 DF,  p-value: \n                                    6.238e-07\n                                    Chỉ \n                                    với một biến \n                                    x6 \n                                    mà mô hình có thể giải thích khoảng 64% \n                                    phương sai của y.  Chúng ta chấp nhận \n                                    mô hình này? Trước khi chấp nhận mô hình \n                                    này, chúng ta phải xem xét độ tương quan \n                                    giữa các biến độc lập:  \n \n                                    > \n                                    pairs(REGdata) \n                                    \n                                    \n                                     \n                                    Kết \n                                    quả trên cho thấy y có liên hệ với \n                                    các biến như \n                                    x1,\n                                    x5 \n                                    và \n                                    x6. \n                                    Ngoài ra, biến \n                                    x5 \n                                    và \n                                    x6 \n                                    có một mối liên hệ rất mật thiết (gần như là \n                                    một đường thẳng) với hệ số tương quan là \n                                    0.88.  Ngoài ra, \n                                    x5 \n                                    và \n                                    x1 \n                                    hay \n                                    x6 \n                                    và \n                                    x5 \n                                    cũng có liên hệ với nhau nhưng theo một hàm \n                                    số nghịch đảo. Điều này có nghĩa là biến\n                                    x5 \n                                    và \n                                    x6 \n                                    cung cấp một lượng thông tin như nhau để \n                                    tiên đoán y, tức là chúng ta không \n                                    cần cả hai trong một mô hình. \n                                    Để tìm \n                                    một mô hình tối ưu trong bối cảnh có nhiều \n                                    mối tương quan như thế, chúng ta ứng dụng\n                                    \n                                    step \n                                    như sau. Chú ý cách cung cấp thông số \n                                    lm(y ~ .),dấu \n                                    “.” có nghĩa là yêu cầu \n                                    R \n                                    xem xét tất cả biến trong đối tượng \n                                    REGdata. \n                                     \n > reg \n                                    <- lm(y ~ ., data=REGdata) \n                                    > step(reg, direction=”both”)y \n                                          ~ x1 + x2 + x3 + x4 + x5 + x6 + x7\n                                          \n                                                 Df Sum of Sq     RSS     AIC- \n                                          x5    1      4.54 2145.37  132.13- \n                                          x1    1     23.17 2164.00  132.36- \n                                          x2    1    109.34 2250.18  133.42- \n                                          x3    1    130.90 2271.74  133.68\n                                          <none>              2140.83  134.07- \n                                          x4    1    168.31 2309.14  134.12- \n                                          x7    1    377.09 2517.92  136.45- \n                                          x6    1    681.09 2821.92  139.53y \n                                          ~ x1 + x2 + x3 + x4 + x6 + x7  \n \n                                                 Df Sum of Sq    RSS    AIC- \n                                          x1    1      22.7 2168.1  130.4- \n                                          x2    1     113.8 2259.1  131.5- \n                                          x3    1     133.5 2278.9  131.8\n                                          <none>              2145.4  132.1- \n                                          x4    1     170.8 2316.2  132.2+ \n                                          x5    1       4.5 2140.8  134.1- \n                                          x7    1     375.7 2521.1  134.5- \n                                          x6    1    1058.5 3203.8  141.0y \n                                          ~ x2 + x3 + x4 + x6 + x7  \n \n                                                 Df Sum of Sq    RSS    AIC- \n                                          x2    1      96.8 2264.9  129.6- \n                                          x3    1     122.0 2290.0  129.9\n                                          <none>              2168.1  130.4- \n                                          x4    1     187.4 2355.5  130.7+ \n                                          x1    1      22.7 2145.4  132.1+ \n                                          x5    1       4.1 2164.0  132.4- \n                                          x7    1     385.0 2553.1  132.8- \n                                          x6    1    1526.2 3694.3  142.8y \n                                          ~ x3 + x4 + x6 + x7  \n \n                                                 Df Sum of Sq    RSS    AIC- \n                                          x3    1      25.4 2290.3  127.9- \n                                          x4    1      90.9 2355.8  128.7\n                                          <none>              2264.9  129.6+ \n                                          x2    1      96.8 2168.1  130.4+ \n                                          x5    1       8.3 2256.5  131.5+ \n                                          x1    1       5.7 2259.1  131.5- \n                                          x7    1     384.9 2649.7  131.8- \n                                          x6    1    2015.6 4280.5  144.8y \n                                          ~ x4 + x6 + x7  \n \n                                                Df Sum of Sq    RSS    AIC- \n                                          x4    1      73.5 2363.8  126.7\n                                          <none>              2290.3  127.9+ \n                                          x3    1      25.4 2264.9  129.6+ \n                                          x1    1      11.3 2279.0  129.8+ \n                                          x5    1       6.3 2284.0  129.8+ \n                                          x2    1       0.3 2290.0  129.9- \n                                          x7    1     486.6 2776.9  131.1- \n                                          x6    1    1993.8 4284.1  142.8y \n                                          ~ x6 + x7  \n \n                                                 Df Sum of Sq    RSS    AIC\n                                          <none>              2363.8  126.7+ \n                                          x4    1      73.5 2290.3  127.9+ \n                                          x1    1      33.4 2330.4  128.4+ \n                                          x3    1       8.1 2355.8  128.7+ \n                                          x5    1       7.7 2356.1  128.7+ \n                                          x2    1       7.3 2356.6  128.7- \n                                          x7    1     497.3 2861.2  129.9- \n                                          x6    1    4477.0 6840.8  153.4\n                                          lm(formula = y ~ x6 + x7, data = \n                                          REGdata) \n \n                                          Coefficients:\n                                          (Intercept)           x6           x7 \n                                          \n                                              2.52646      0.01852      2.18575 \n                                          Quá trình tìm mô \n                                    hình tối ưu dừng ở mô hình với hai biến\n                                    x6 \n                                    và \n                                    x7, \n                                    vì mô hình này có giá trị AIC thấp nhất.  \n                                    Phương trình tuyến tính tiên đoán y \n                                    là:  y = 2.526 + 0.0185(x6) \n                                    + 2.186(x7).  \n                                     \n \n                                    > \n                                    summary(lm(y ~ x6+x7, data=REGdata))\n                                    Call: \n                                    lm(formula = y ~ x6 + x7, data = REGdata) \n \n                                    Residuals:\n                                         \n                                    Min       1Q   Median       3Q      Max\n                                    \n                                    -23.2035  \n                                    -4.3713   0.2513   4.9339  21.9682  \n \n                                    \n                                    Coefficients:\n                                    \n                                                Estimate Std. Error t value \n                                    Pr(>|t|)    \n                                    \n                                    (Intercept) 2.526460   3.610055   0.700   \n                                    0.4908    \n                                    x6       \n                                       0.018522   0.002747   6.742 5.66e-07 ***\n                                    \n                                    x7          2.185753   0.972696   2.247   \n                                    0.0341 *  \n                                    ---\n                                    Signif. \n                                    codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' \n                                    0.1 ' ' 1  \n \n                                    Residual \n                                    standard error: 9.924 on 24 degrees of \n                                    freedom\n                                    Multiple \n                                    R-Squared: 0.6996,     Adjusted R-squared: \n                                    0.6746 \n                                    \n                                    F-statistic: 27.95 on 2 and 24 DF,  p-value: \n                                    5.391e-07 \n \n                                    Phân \n                                    tích chi tiết (kết quả trên) cho thấy hai \n                                    biến này giải thích khoảng 70% phương sai \n                                    của y.   \n 10.6  Xây \n                                    dựng mô hình tuyến tính bằng Bayesian Model \n                                    Average (BMA) \n Sau đó, tạo ra \n                                    một ma trận chỉ gồm các biến độc lập. Trong \n                                    data frame chúng ta biết \n                                    REGdata \n                                    có 8 biến, với biến số 1 là y. Do đó, \n                                    lệnh \n                                    REGdata[, \n                                    -1] \n                                    có nghĩa là tạo ra một data frame mới ngoại \n                                    trừ cột thứ nhất (tức y).  \n > xvars \n                                    <- REGdata[,-1]\n                                    Kế \n                                    tiếp, chúng ta định nghĩa biến phụ thuộc tên\n                                    co2 \n                                    từ \n                                    REGdata:\n                                    > co2 \n                                    <- REGdata[,1] \n \n                                    Bây \n                                    giờ chúng ta đã sẵn sàng phân tích bằng phép \n                                    tính BMA. Hàm \n                                    bicreg  \n                                    được viết đặc biệt cho phân tích hồi qui \n                                    tuyến tính. Cách áp dụng hàm \n                                    bicreg\n                                    \n                                    như \n                                    sau: \n > bma \n                                    <- bicreg(xvars, co2, strict=FALSE, OR=20) \n \n                                    Chúng \n                                    ta sử dụng hàm \n                                    summary \n                                    để biết kết quả: \n > \n                                    summary(bma)Call:\n                                    bicreg(x = xvars, y = co2, strict = FALSE, \n                                    OR = 20) \n 16  \n                                    models were selectedBest  \n                                    5  models (cumulative posterior probability \n                                    =  0.6599 ):  \n \n                                               p!=0    EV       SD       model \n                                    1   model 2   model 3  \n                                    Intercept  100.0   5.75672  14.6244    \n                                    2.5264    6.1441    8.6120 \n                                    x1          12.4  -0.01807   0.1008      \n                                    .         .         .    \n                                    x2          10.4  -0.00075   0.0282      \n                                    .         .         .    \n                                    x3          10.7   0.00011   0.0791      \n                                    .         .         .    \n                                    x4          20.2  -0.03059   0.1020      \n                                    .         .      -0.1419 \n                                    x5          10.5  -0.00023   0.0030      \n                                    .         .         .    \n                                    x6         100.0   0.01815   0.0040    \n                                    0.0185    0.0193    0.0164 \n                                    x7          73.7   1.60766   1.2821    \n                                    2.1857      .       2.1628 \n                                                                                                     \n                                    \n                                    nVar                                     \n                                    2         1         3    \n                                    r2                                     \n                                    0.700     0.636     0.709  \n                                    BIC                                  \n                                    -25.8832  -24.0238  -23.4412 post \n                                    prob                              \n                                    0.311     0.123     0.092   \n \n                                    \n                                                model 4   model 5 \n                                    \n                                    \n                                    Intercept     7.5936    7.3537\n                                    \n                                    x1           -0.1393      .   \n                                    \n                                    x2              .         .   \n                                    \n                                    x3              .      -0.0572\n                                    \n                                    x4              .         .   \n                                    \n                                    x5              .         .   \n                                    \n                                    x6            0.0162    0.0179\n                                    \n                                    x7            2.1233    2.2382\n                                    \n                                                                   \n                                     \n                                     \n \n                                    \n                                    nVar            3         3   \n                                    \n                                    r2            0.704     0.701 \n                                    \n                                    BIC         -22.9721  -22.6801\n                                    post \n                                    prob     0.072     0.063  \n \n                                    BMA \n                                    trình bày kết quả của 5 mô hình được đánh \n                                    giá là tối ưu nhất cho tiên đoán y (model \n                                    1, model 2, … model 5). \n                                     \n Một cách thể hiện kết quả trên là \n                                    qua một biểu đồ như sau:  \n > \n                                    imageplot.bma(bma) \n                                     \n                                     \n \n                                    \n                                     \n                                     \n  \n                                    Tài \n                                    liệu tham khảo cho BMA \n \n                                    \n                                    Raftery, Adrian E. (1995). Bayesian model \n                                    selection in social research (with \n                                    Discussion). Sociological Methodology 1995 \n                                    (Peter V. Marsden, ed.), pp. 111-196, \n                                    Cambridge, Mass.: Blackwells. \n \n                                    Một số \n                                    bài báo liên quan đến BMA có thể tải từ \n                                    trang web sau đây: \n                                    \n                                    \n                                    www.stat.colostate.edu/~jah/papers.\n                                                        \n                            \n                            _____________________________________________________________\n\t\t\t\t\t\t\tTrích từ quyển\n                            \n                            Phân Tích Số Liệu và Tạo Biểu \n                            Đồ bằng \n                             \n                            - Hướng Dẫn Thực Hành\n\t\t\t\t\t\t\tNhà xuất bản \n                            Đại Học Quốc gia \n                            \n                            \n\t\t\t©                     \n    \n\t\t\thttp://vietsciences.org      \n\t\t\tvà                     \n                    \n    http://vietsciences.free.fr Nguyễn \n\tVăn Tuấn           \n             \n           \n               \n             \n             \n           \n",
          "relevence": "yes"
        },
        {
          "url": "https://www.wattpad.com/story/2953081-nh%C3%B3m-h%C3%A0m-v%E1%BB%81-t%C6%B0%C6%A1ng-quan-v%C3%A0-h%E1%BB%93i-quy-tuy%E1%BA%BFn-t%C3%ADnh",
          "title": ". NHÓM HÀM VỀ TƯƠNG QUAN VÀ HỒI QUY TUYẾN TÍNH - kieutrangtn90 - Wattpad",
          "content": "No comments listed yet.",
          "relevence": "yes"
        },
        {
          "url": "http://baigiangtoanhoc.com/xem-bai-giang/647-Bai-giang-mo-hinh-hoi-quy-tuyen-tinh.html",
          "title": "Bài giảng mô hình hồi quy tuyến tính",
          "content": "\n                  \n                    \n                  \n                096.738.13.13Tài liệu bài giảng toán kinh tế, mô hình hồi quy tuyến tính tại đây Bạn đọc tham khảo tài liệu tại đây. \n                  Tổng hợp 40 bài toán thực tế luyện thi THPT Quốc gia 2017\n                \n                  Đề thi thử lần 2 môn toán của trường THPT Chuyên Phan Bội Châu Nghệ An năm 2017\n                \n                  Đề thi thử vào 10 các năm môn được download nhiều nhất\n                \n                  Đề thi thử THPT Quốc gia lần 3 chuyên Phan Bội Châu Nghệ An năm 2017 và đáp án\n                \n                  Đề luyện thi học sinh giỏi lớp 1 môn toán\n                \n                  Đề thi minh họa lần 3 môn toán THPT Quốc gia của Bộ giáo dục và đào tạo 2017 \n                 \n          Tổng hợp 40 bài toán thực tế luyện thi THPT Quốc gia 2017\n        Giáo viên: Đỗ Viết Tuân\n          Đề thi thử lần 2 môn toán của trường THPT Chuyên Phan Bội Châu Nghệ An năm 2017\n        Giáo viên: Đỗ Viết Tuân\n          Đề thi thử vào 10 các năm môn được download nhiều nhất\n        Giáo viên: Đỗ Viết Tuân\n          Đề thi thử THPT Quốc gia lần 3 chuyên Phan Bội Châu Nghệ An năm 2017 và đáp án\n        Giáo viên: Đỗ Viết Tuân\n          Đề luyện thi học sinh giỏi lớp 1 môn toán\n        Giáo viên: Đỗ Viết Tuân\n          Đề thi minh họa lần 3 môn toán THPT Quốc gia của Bộ giáo dục và đào tạo 2017 \n        Giáo viên: Đỗ Viết TuânTầng 8 - Tòa Toyota Thanh Xuân - Số 315 Trường Chinh - Thanh Xuân - Hà NộiSDT: 0868278566 - Email: info@hvt.vn096.738.13.13Để tiếp nhận những bài học lý thú và bổ ích từ những giáo viên chất lượng của chúng tôiBản quyền hệ thống thuộc về GCO GROUP. Mọi chi tiết xem tại  www.gco.vn",
          "relevence": "yes"
        },
        {
          "url": "http://www.bitex.com.vn/vn/kho-ung-dung/1447/hoi-quy-tuyen-tinh-y=-a-+-bx-tren-may-tinh-casio-fx-570ms.html",
          "title": "Hồi quy tuyến tính y= A + Bx trên máy tính Casio fx 570MS | Hoi quy tuyen tinh y= A + Bx tren may tinh Casio fx 570MS",
          "content": "\r\n\tHãy dùng Hồi quy tuyến tính y= A + Bx bđể tính A,B và hệ số tương quan r, ấp suất ở  C, tìm nhiệt độ khi áp suất 1000 hPa, hệ số tương ứng  trên máy tính Casio fx 570MS...\r\n\t\r\n\tCopyright © 2012 Công ty Cổ Phần XNK Bình TâyĐịa chỉ: Đường tỉnh 835, Ấp 3A, Xã Phước Lợi, Huyện Bến Lức, Tỉnh Long An. GPĐKKD số 0302562816 do Sở KHĐT Tp.HCM cấp ngày 25/03/2004Thiết kế web : TRUST.vn Bạn có thể thanh toán với",
          "relevence": "yes"
        },
        {
          "url": "https://www.esvn.vn/vi/tin-tuc/nhom-ham-ve-tuong-quan-va-hoi-quy-tuyen-tinh-4563",
          "title": "Nhóm hàm về tương quan và hồi quy tuyến tính",
          "content": "Nhóm hàm về tương quan và hồi quy tuyến tínhTiếp theo trong bài viết Nhóm hàm về phân phối xác suất trong excel 2010, bạn tìm hiểu chi tết nhóm hàm về tương quan và hồi quy tuyến tính.\n\nCORREL (array1, array2) : Tính hệ số tương quan giữa hai mảng để xác định mối quan hệ của hai đặc tính\nCOVAR (array1, array2) : Tính tích số các độ lệch của mỗi cặp điểm dữ liệu, rồi tính trung bình các tích số đó\nFORECAST (x, known_y's, known_x's) : Tính toán hay dự đoán một giá trị tương lai bằng cách sử dụng các giá trị hiện có, bằng phương pháp hồi quy tuyến tính\nGROWTH (known_y's, known_x's, new_x's, const) : Tính toán sự tăng trưởng dự kiến theo hàm mũ, bằng cách sử dụng các dữ kiện hiện có.\nINTERCEPT (known_y's, known_x's) : Tìm điểm giao nhau của một đường thẳng với trục y bằng cách sử dụng các trị x và y cho trước\nLINEST (known_y's, known_x's, const, stats) : Tính thống kê cho một đường bằng cách dùng phương pháp bình phương tối thiểu (least squares) để tính đường thẳng thích hợp nhất với dữ liệu, rồi trả về mảng mô tả đường thẳng đó. Luôn dùng hàm này ở dạng công thức mảng.\nLOGEST (known_y's, known_x's, const, stats) : Dùng trong phân tích hồi quy. Hàm sẽ tính đường cong hàm mũ phù hợp với dữ liệu được cung cấp, rồi trả về mảng gía trị mô tả đường cong đó. Luôn dùng hàm này ở dạng công thức mảng\nPEARSON (array1, array2) : Tính hệ số tương quan momen tích pearson (r), một chỉ mục không thứ nguyên, trong khoảng từ -1 đến 1, phản ánh sự mở rộng quan hệ tuyến tính giữa hai tập số liệu\nRSQ (known_y's, known_x's) : Tính bình phương hệ số tương quan momen tích Pearson (r), thông qua các điểm dữ liệu trong known_y's và known_x's\nSLOPE (known_y's, known_x's) : Tính hệ số góc của đường hồi quy tuyến tính thông qua các điềm dữ liệu\nSTEYX (known_y's, known_x's) : Trả về sai số chuẩn của trị dự đoán y đối với mỗi trị x trong hồi quy.\nTREND (known_y's, known_x's, new_x's, const) : Trả về các trị theo xu thế tuyến tính",
          "relevence": "yes"
        },
        {
          "url": "https://ongxuanhong.wordpress.com/2015/07/28/scikit-learn-linear-regression/",
          "title": "Scikit-learn: Linear regression – Ông Xuân Hồng",
          "content": "\nTrong bài viết này, ta sẽ tìm hiểu về mô hình hồi quy tuyến tính (linear regression) cũng như cài đặt một ví dụ đơn giản sử dụng thư viện scikit-learn.Phân tích hồi quy tuyến tính (linear regression) là một phương pháp phân tích quan hệ giữa biến phụ thuộc Y với một hay nhiều biến độc lập X. Mô hình hóa sử dụng hàm tuyến tính (bậc 1). Các tham số của mô hình (hay hàm số) được ước lượng từ dữ liệu.Biến Y phụ thuộc (dependence) vào biến X (independence), nghĩa là X và Y có quan hệ positive nếu giá trị biến X tăng/giảm thì giá trị biến Y cũng tăng/giảm. Ngược lại X và Y có quan hệ negative nếu giá trị biến X tăng/giảm nhưng giá trị biến Y giảm/tăng.Cho trước tập dữ liệu  (,  là số lượng mẫu) phân bố như hình sau.Mục tiêu của chúng ta là tìm được một đường thẳng (line – linear regression) thể hiện gần đúng nhất phân bố của tập dữ liệu trên.Regression lineĐường thẳng này được ước lượng dựa trên độ lỗi (least square error) giữa giá trị dự đoán (estimated value) so với giá trị thực tế (actual value). Mục tiêu của chúng ta là tìm được đường hồi quy có độ lỗi so với tập dữ liệu nhỏ nhất (minimize error).Như vậy, ta đi xây dựng mô hình hồi quy tuyến tính với phương trình đường thẳng . Sau đó, dựa vào giá trị  tìm được nhờ vào biến độc lập X trong tập huấn luyện ban đầu, ta tính được độ lỗi so với tập dữ liệu quan sát (observations). Để tối tiểu hóa độ lỗi này ta chỉ cần lấy đạo hàm bậc nhất. Từ đó, ta tìm được tham số  để thế vào mô hình hồi quy tuyến tính chúng ta cần tìm.Study lineTa có thể tham khảo thêm về cách tối tiểu hóa độ lỗi ở Wikipedia: https://en.wikipedia.org/wiki/Least_squaresĐầu tiên chúng ta sẽ tạo tập dữ liệu mẫu  gồm 20 phần tử. Sau đó, ta sẽ biểu diễn tập dữ liệu này ra biểu đồ.Simple dataTiếp theo, ta xây dựng mô hình hồi quy tuyến tính thông qua lớp LinearRegression và hàm fit() trong scikit-learn.Linear regressionHồi quy tuyến tính được sử dụng rộng rãi trong thực tế do tính chất đơn giản hóa của hồi quy. Hàm ước lượng thống kê được sử dụng phổ biến nhất là phương pháp bình phương nhỏ nhất (least square method).Tham khảo thêm:Mời bạn điền thông tin vào ô dưới đây hoặc kích vào một biểu tượng để đăng nhập: Bạn đang bình luận bằng tài khoản WordPress.com ( Đăng xuất / Thay đổi ) Bạn đang bình luận bằng tài khoản Twitter ( Đăng xuất / Thay đổi ) Bạn đang bình luận bằng tài khoản Facebook ( Đăng xuất / Thay đổi ) Bạn đang bình luận bằng tài khoản Google+ ( Đăng xuất / Thay đổi )Connecting to %s Notify me of new comments via email. Thông báo cho tôi bằng email khi có bài đăng mới. \n\nNếu ngày mai tớ trở thành người nổi tiếng. Gặp tớ giữa đường bạn đừng ngại tớ nghe. Cứ nhìn tớ như ngày xưa bạn thấy. Đừng nên nghĩ tớ quá xa vời. Nếu một ngày tớ là kẻ ăn xin. Thì bố thí cho tớ vài kỷ niệm. Dù rất nhỏ nhưng đừng nên khinh tớ. Vì tớ vẫn là tớ của ngày xưa. Nếu ngày mai tớ chẳng biết đường về. Hãy chỉ giùm tớ con đường đi đúng đắn. Đừng để tớ sa chân nơi bùn nhớp. Lúc rút chân lên thì đã bẩn mất rồi. Nếu ngày mai tớ gặp bạn giữa đường. Thì ta sẽ gặp nhau và cười nhé. Đừng nên trách và đừng nên đánh mất.Đôi khi có một số người lướt qua cuộc đời bạn và ngay tức khắc bạn nhận ra rằng sự có mặt của họ ý nghĩa như thế nào. Họ đã dạy bạn những bài học, đã giúp bạn nhận ra giá trị của chính mình hoặc trở thành con người mà bạn từng mơ ước. Có lẽ bạn sẽ không biết được những con người này từ đâu đến ( bạn cùng phòng, người hàng xóm, vị giáo sư, người bạn mất liên lạc từ lâu hay thậm chí là một người hoàn toàn xa lạ ). Nhưng khi bạn thờ ơ với họ, hãy nhớ rằng trong từng khoảnh khắc họ sẽ ảnh hưởng rất sâu sắc đến cuộc đời bạn. …Cuộc sống không hề làm khó bạn mà chính bạn đã tự làm khó mình bằng cách đặt ra những câu hỏi đại loại như: “Tại sao lại thế này?… Tại sao lại thế kia?…”. Không ai có thể trả lời giúp bạn mà chính bạn phải tự đi tìm lời giải đáp cho mình. Hãy luôn nghĩ về người khác trước khi nghĩ đến bản thân, chắc chắn lúc đó bạn sẽ hài lòng với câu trả lời của chính mình. Và đừng quên luôn đặt câu hỏi: “Mình đã làm được gì?” trước khi tự hỏi: “Mình đã nhận được gì?” nhé! Tôi tin là bạn sẽ thành công!  Thật ra trên đời, con người luôn cần có nhau để giải quyết rất nhiều vấn đề. Nhờ những lúc qua lại giúp đỡ đó mà hoá giải bao muộn phiền, nảy sinh các tình cảm, cùng trải nghiệm biết bao điều thú vị, hiểu biết thêm nhiều tri thức mới. Đây chính là cuộc sống!Vì thế hãy trân trọng những người đang “làm phiền” bên cạnh bạn nhé! Đừng để đến khi mất đi rồi mới nghẹn ngào lại bảo “giá như” …ĐĐ. GS. Thích Phước Tiến\n__(())__Namo Bụt SakyamuniNhận Email khi có bài viết mới\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\tThis slideshow requires JavaScript.",
          "relevence": "yes"
        },
        {
          "url": "https://nkvtruong.wordpress.com/2011/10/22/thiet-ke-thi-nghiem-va-hoi-quy-tuyen-tinh/",
          "title": "Thiết kế thí nghiệm và hồi quy tuyến tính | nkvtruong's Blog",
          "content": "Mục tiêu của bài viết này: Phân tích bằng ví dụ một cách dễ hiểu để thấy được vai trò của thiết kế thí nghiệm và hồi quy tuyến tính, cũng như các phân tích phương sai. Qua đó, vai trò của các nguồn biến thiên cùng với ý nghĩa của bậc tự do tương ứng cũng được giải thích.Ví dụ 1:Hãy bắt đầu bằng một ví dụ đơn giản nhất có thể: giả sử ta muốn ước lượng một mối quan hệ tuyến tính bậc nhất giữa x (gọi tên là yếu tố A) và y (biến chất lượng cần khảo sát): y= m0 + m1x và ta tiến hành thí nghiệm nhân tố (tạm dịch từ factorial experiment) với 1 yếu tố, 2 mức độ (levels), số lần lặp (replication) r là 1 như sau:Runsxy1-142+18Với số lần quan sát là n = 2 trong thí nghiệm trên, tác động của x (yếu tố A) lên biến chất lượng đầu ra y được sơ đồ hóa như sau:\"Sơ đồ tác động của yếu tố A, 1 lần lặp ở 2 mức thấp (-) và cao (+)\"Trong sơ đồ trên, ký hiệu (1) đại diện cho tác động yếu tố A ở mức thấp (-), còn a được hiểu là tác động của A ở mức cao (+). Xét về lượng, tác động của A được tính theo công thức:\ntrong đó p là số điểm tại mỗi mức (cao hay thấp) của A và r là số lần lặp lại của thí nghiệm. Trong trường hợp này p = 1 và r = 1.Sau khi có thông tin về tác động của yếu tố A lên biến chất lượng đầu ra, phương trình hồi quy được đề xuất như sau:\ntrong đó  là giá trị trung bình của 2 quan sát  và ; A là tác động của yếu tố A. Đường hồi quy được minh họa bằng hình vẽ sau:Phương trình hồi quy cho thiết kế thí nghiệm nhân tố (1 yếu tố, 2 mức, 1 lần lặp)Ta có thể diễn giải kết quả đơn giản này như sau: Nếu không có yếu tố A tác động vào, biến chất lượng đầu ra y sẽ không có gì thay đổi và nằm ngang như trên biểu đồ với giá trị là  (đây là giá trị ước lượng dựa trên thông tin ta có từ 2 điểm thí nghiệm chứ ta không biết giá trị thật). Khi có A, tác động này sẽ thể hiện khi thay đổi A từ điểm thấp (-1) lên điểm cao (+1) làm cho y thay đổi từ 4 đến 8. Trong phương trình hồi quy, tác động này thể hiện ở hệ số . Sở dĩ là A/2 là vì khi x thay đổi 2 đơn vị từ -1 đến +1, tác động của A = 4. Tác động này sẽ có tác dụng xoay đường  một góc anpha có .Một trong những công cụ để đánh giá phương trình hồi quy là phân tích phương sai. Trong phân tích phương sai, biến thiên sẽ được đo bằng các tổng bình phương và tổng biến thiên từ dữ liệu được chia làm hai thành phần chính khác nhau: biến thiên do hồi quy và biến thiên do nhiễu ngẫu nhiên. Mỗi loại biến thiên này còn có thể chia nhỏ ra nữa. Bây giờ ta thử phân tích phương sai cho ví dụ này.Biến thiên của do yếu tố A gây nên: .\nBiến thiên do nhiễu ngẫu nhiên: .\nBiến thiên tổng: .Có thể hiểu ý nghĩa của 3 loại biến thiên như sau: Biến thiên tổng  phản ánh biến thiên dữ liệu khi so sánh trường hợp có và không có yếu tố A tác động vào y. Về mặt toán học, biến thiên của dữ liệu xung quanh giá trị  thể hiện bằng tổng bình phương độ lệnh giữa các quan sát và y trung bình. Biến thiên tổng không phụ thuộc vào mô hình hồi quy được chọn. Trong khi đó, biến thiên do hồi quy  (trong trường hợp này ta chỉ có một yếu tố A, mô hình tuyến tính bậc nhất nên ) sẽ phụ thuộc vào mô hình hồi quy mình chọn, nhiệm vụ của nó là chuyển tải được càng nhiều phần của biến thiên tổng càng tốt. Phần còn lại của biến thiên tổng sau khi trừ đi biến thiên do hồi quy là biến thiên do nhiễu ngẫu nhiên , phần này thể hiện quá trình tương tác giữa số liệu quan trắc được và mô hình, do đó nó phản ánh độ khớp của mô hình ta chọn với dữ liệu thu thập được. Phân tích phương sai (ANOVA – Analysis of variance) được tóm tắt trong bảng sau:Nguồn\nbiến thiênSum of squaresBậc\ntự doASSA = 81ESSE = 00TổngSST = 81Từ đó, hệ số hồi quy R-squares được tính như sau: \nDo vậy, với mối quan hệ tuyến tính bậc nhất, nếu thiết kế thí nghiệm chỉ có 2 điểm, thông tin có được chỉ cho phép ta có một đường thằng hồi quy đi qua 2 điểm đó và yếu tố A lấy hết một bậc tự do đồng thời chiếm hết luôn tổng biến thiên nên hệ số tương quan đúng bằng 1. Không có nhiễu ngẫu nhiên hay nói đúng hơn là không ước lượng được nhiễu ngẫu nhiên trong trường hợp này vì ta thiếu thông tin. Tổng số bậc tự do là 1 vì ta chỉ có hai điểm quan sát và không có bậc tự do cho nhiễu ngẫu nhiên, điều này làm cho đường thằng là duy nhất, không có khả năng “xê dịch” trong quá trình ước lượng. Bây giờ ta sẽ mở rộng vấn đề sang ví dụ tiếp theo.Ví dụ 2:Cũng giống như ví dụ 1 nhưng ở đây ta giả sử thông tin ta có được nhiều hơn với 2 lần lặp (r = 2).Runsxy1y21-1432+187Với số lần quan sát lúc này cũng là n= 2, tác động của x (yếu tố A) lên biến chất lượng đầu ra y được sơ đồ hóa như sau:Sơ đồ tác động của yếu tố A, 2 lần lặp ở 2 mức thấp (-) và cao (+)Tác động của A cũng được tính theo công thức:\ntrong trường hợp này p = 1 và r = 2.Một cách tương tự, phương trình hồi quy được đề xuất như sau:\ntrong đó  là giá trị trung bình của 4 quan sát trong bảng thí nghiệm trên. Đường hồi quy được minh họa bằng hình vẽ sau:Phương trình hồi quy cho thiết kế thí nghiệm nhân tố (1 yếu tố, 2 mức, 2 lần lặp)Phân tích phương sai:Tổng biến thiên do hồi quy: .\nBiến thiên do nhiễu ngẫu nhiên: .\nBiến thiên tổng: .Sơ đồ minh họa cho tính toán các nguồn biến thiên cho phân tích phương sai được thể hiện trong hình bên dưới đây:Sơ đồ tính toán các nguồn biến thiênBảng phân tích hồi quy:Nguồn biến thiênSum of squaresBậc tự doASSA = 161ESSE = 12TổngSST = 173Nhận xét: CŨng với mối quan hệ tuyến tính bậc nhất, bây giờ tổng số điểm thí nghiệm là 4 nên số bậc tự do tổng cộng là 3 trong đó phương trình hồi quy vẫn chiếm 1, còn lại 2 cho nhiễu ngẫu nhiên (Error). Nhìn hình vẽ ta cũng có thể thấy ý nghĩa 2 bậc tự do cho Error nằm ở hai khoảng nhiễu (biên độ xê dịch) tại hai vị trí thí nghiệm mà đường thẳng hồi quy sẽ “xê dịch” trong đó. Việc xác định vị trí của đường hồi quy sẽ phụ thuộc vào hai khoảng nhiễu này và tiêu chuẩn tối ưu hóa để chọn đường hồi quy đó. Vấn đề này sẽ được bàn trong một bài riêng. Giả sử tại một trong hai điểm thí nghiệm trên ta chỉ có 1 giá trị quan sát (số lần lặp chỉ là 1), lúc này tổng bậc tự do sẽ giảm đi 1 (còn lại 2) và bậc tự do của Error sẽ bớt đi 1 (vì của hồi quy sẽ không đổi). Đường thẳng hồi quy sẽ bị cố định tại 1 đầu (không còn tự do) tại vị trí đó, đầu kia (2 lần lặp thí nghiệm) vẫn được xem là “tự do”. Tương tự nếu ta tăng số lượng quan sát, nghĩa là tăng bậc tự do tổng (và do đó bậc tự do của Error sẽ tăng lên), đường thằng hồi quy sẽ được xác định dựa trên nhiều thông tin hơn, nghĩa là “tự do” hơn. Khái niệm bậc tự do ở đây cũng tương tự như trong cơ học, có ý nghĩa là số chiều cho phép các điểm có thể xê dịch, chuyển động trong không gian.Fill in your details below or click an icon to log in: You are commenting using your WordPress.com account. ( Log Out / Change ) You are commenting using your Twitter account. ( Log Out / Change ) You are commenting using your Facebook account. ( Log Out / Change ) You are commenting using your Google+ account. ( Log Out / Change )Connecting to %s Notify me of new comments via email. \n\n\n\t\t\t\t\n\t\t\t\t\tnkvtruong's Blog\t\t\t\t\n\t\t\tBlog at WordPress.com. ",
          "relevence": "yes"
        }
      ]
    },
    {
      "query": "thuật toán k means",
      "description": "Muốn tìm hiểu về giải thuật k-means, bất kỳ trang web nào cung cấp thông tin về giải thuật k-means: tổng hợp giải thuật k means, giải thích về một giải thuật cụ thể, có ví dụ liên quan, hướng dẫn cài đặt giải thuật trên một ngôn ngữ lập trình đều là kết quả phù hợp.",
      "sites": [
        {
          "url": "http://bis.net.vn/forums/p/374/822.aspx",
          "title": "\r\n\tThuật toán K-Means với bài toán phân cụm dữ liệu - BIS\r\n",
          "content": "Thuật toán K-Means với bài toán phân cụm dữ liệuNguyễn Văn Chức – chuc1803@gmail.com 1.Giới thiệu về kỹ thuật phân cụm trong Khai phá dữ liệu (Clustering Techniques in Data Mining) Phân cụm là kỹ thuật rất quan trọng trong khai phá dữ liệu, nó thuộc lớp các phương pháp Unsupervised Learning trong Machine Learning. Có rất nhiều định nghĩa khác nhau về kỹ thuật này, nhưng về bản chất ta có thể hiểu phân cụm là các qui trình tìm cách nhóm các đối tượng đã cho vào các cụm (clusters), sao cho các đối tượng trong cùng 1 cụm tương tự (similar) nhau và các đối tượng khác cụm thì không tương tự (Dissimilar) nhau.Mục đích của phân cụm là tìm ra bản chất bên trong các nhóm của dữ liệu. Các thuật toán phân cụm (Clustering Algorithms) đều sinh ra các cụm (clusters). Tuy nhiên, không có tiêu chí nào là được xem là tốt nhất để đánh hiệu của của phân tích phân cụm, điều này phụ thuộc vào mục đích của phân cụm như: data reduction, “natural clusters”, “useful” clusters, outlier detection Kỹ thuật phân cụm có thể áp dụng trong rất nhiều lĩnh vực như: Các kỹ thuật phân cụm được phân loại như sau (xem hình)2. Thuật Toán K-MeansK-Means là thuật toán rất quan trọng và được sử dụng phổ biến trong kỹ thuật phân cụm. Tư tưởng chính của thuật toán K-Means là tìm cách phân nhóm các đối tượng (objects) đã cho vào K cụm (K là số các cụm được xác đinh trước, K nguyên dương) sao cho tổng bình phương khoảng cách giữa các đối tượng đến tâm nhóm (centroid ) là nhỏ nhất.Thuật toán K-Means được mô tả như sau  Thuật toán K-Means thực hiện qua các bước chính sau:1.    Chọn ngẫu nhiên K tâm (centroid) cho K cụm (cluster). Mỗi cụm được đại diện bằng các tâm của cụm.2.    Tính khoảng cách giữa các đối tượng (objects) đến K tâm (thường dùng khoảng cách Euclidean)3.    Nhóm các đối tượng vào nhóm gần nhất4.    Xác định lại tâm mới cho các nhóm5.    Thực hiện lại bước 2 cho đến khi không có sự thay đổi nhóm nào của các đối tượng Ví dụ minh họa thuật toán K-Mean:Giả sử ta có 4 loại thuốc A,B,C,D, mỗi loại thuộc được biểu diễn bởi 2 đặc trưng X và Y như sau. Mục đích của ta là nhóm các thuốc đã cho vào 2 nhóm (K=2) dựa vào các đặc trưng của chúng.Bước 1. Khởi tạo tâm (centroid) cho 2 nhóm. Giả sử ta chọn A là tâm của nhóm thứ nhất (tọa độ tâm nhóm thứ nhất c1(1,1)) và B là tâm của nhóm thứ 2 (tạo độ tâm nhóm thứ hai c2 (2,1)). Bước 2. Tính khoảng cách từ các đối tượng đến tâm của các nhóm (Khoảng cách Euclidean)Mỗi cột trong ma trận khoảng cách (D) là một đối tượng (cột thứ nhất tương ứng với đối tượng A, cột thứ 2 tương ứng với đối tượng B,…). Hàng thứ nhất trong ma trận khoảng cách biểu diễn khoảng cách giữa các đối tượng đến tâm của nhóm thứ nhất (c1) và hàng thứ 2 trong ma trận khoảng cách biểu diễn khoảng cách của các đối tượng đến tâm của nhóm thứ 2 (c2).Ví dụ, khoảng cách từ loại thuốc C=(4,3) đến tâm c1(1,1) là 3.61  và đến tâm c2(2,1) là 2.83 được tính như sau: Bước 3. Nhóm các đối tượng vào nhóm gần nhấtTa thấy rằng  nhóm 1 sau vòng lặp thứ nhất gồm có 1 đối tượng A và nhóm 2 gồm các đối tượng còn lại B,C,D.Bước 5. Tính lại tọa độ các tâm cho các nhóm mới dựa vào tọa độ của các đối tượng trong nhóm. Nhóm 1 chỉ có 1 đối tượng A nên tâm nhóm 1 vẫn không đổi, c1(1,1). Tâm nhóm 2 được tính như sau:Bước 6. Tính lại khoảng cách từ các đối tượng đến tâm mới Bước 7. Nhóm các đối tượng vào nhómBước 8. Tính lại tâm cho nhóm mới Bước 8. Tính lại khoảng cách từ các đối tượng đến tâm mớiBước 9. Nhóm các đối tượng vào nhómTa thấy G2 = G1 (Không có sự thay đổi nhóm nào của các đối tượng) nên thuật toán dừng và kết quả phân nhóm như sau: Thuật toán K-Means có ưu điểm là đơn giản, dễ hiểu và cài đặt. Tuy nhiên, một số hạn chế của K-Means là hiệu quả của thuật toán phụ thuộc vào việc chọn số nhóm K (phải xác định trước) và chi phí cho thực hiện vòng lặp tính toán khoảng cách lớn khi số cụm K và dữ liệu phân cụm lớn.3. Triển khai ứng dụng phân cụm với phần mềm WeKaTrong ví dụ này, tôi sẽ giới thiệu cách xây dựng một KnowledgeFlow để triển khai kỹ thuật phân cụm dựa trên thuật toán K-Means trên Data Mining Software WeKa.Dữ liệu dùng để phân cụm trong ví dụ này là dữ liệu dùng để phân loại khách hàng của ngân hàng (file dữ liệu bank.arff). bank.arff gồm có 11 thuộc tính và 600 khách hàng (instances). Dưới đây là cấu trúc và phân bố dữ liệu của bank.arffCác bạn có thể Down file bank.arff tại đây:Nhiệm vụ của chúng ta là dùng thuật toán K-Means để phân nhóm các khách hàng vào K nhóm (trong ví dụ này K=5) dựa vào sự tương tự (similar) trên11 thuộc tính của họ. Ta xây dựng một KnowledgeFlow trong WeKa như sau: Thiết lập các tham số cho thuật toán K-Means như số cụm (trong ví dụ này K=5), Cách tính khoảng cách (trong ví dụ này dùng khoảng cách Euclidean),…  Kết quả phân cụm chi tiết như sau: PS. The next topic is SOM (Self Organizing Maps) in Clustering Techniques. All comments please send to chucnv@ud.edu.vn.Trong kỹ thuật sinh số ngẫu nhiên (random), thông thường cho phép lặp lại các giá trị do bộ ngẫu nhiên sinh ra. Chẳn hạn như hàm rand(value1, value2) sinh ra các số ngẫu nhiên trong khoảng value1 và Value2 và các số này có thể xuất hiện lại nhiều lần.Để bộ sinh số ngẫu nhiên không sinh ra các số trùng nhau ta sử dụng tham số seed, tham số seed chính là điểm bắt đầu để sinh số ngẫu nhiên và nó là số tự nhiên.Nếu p và q là 2 điểm trong không gian n chiều (đối tượng được mô tả bởi n thuộc tính) p = (p1, p2,..., pn) và q = (q1, q2,..., qn). Công thức tính khoảng cách Euclidean giữa p và q như sau:Có rất nhiều cách tính Linkage để chọn tiêu chí phân cụm, trong đó các cách sau đây thường được sử dụng • Single Linkage: Khoảng cách giữa 2 clusters được tính là khoảng cách giữa 2 đối tượng gần nhau nhất trong 2 clusters đó (minimum distance). Xem hình dưới•  Complete Linkage: Khoảng cách giữa 2 clusters được tính là khoảng cách giữa 2 đối tượng xa nhau nhất trong 2 clusters đó (maximum distance). •  Average Group: Khoảng cách giữa 2 clusters được tính là khoảng cách trung bình giữa các đối tượng trong 2 clusters đó (average distance).• Centroid distance : Khoảng cách giữa 2 clusters được tính là khoảng cách của 2 tâm của 2 clusters đó (Centroid distance).Vấn đề chọn giá trị K (số cluster) cho giải thuật K-MeansTrong giải thuật K-Means, tham số K (số Cluster) phải được xác định trước khi triển khai thuật toán. Việc này này hưởng rất lớn đến kết quả phân cụm của thuật toán. Một khó khăn là hiện nay chưa có giải pháp nào được xem là tốt (về tính khoa học) để chọn tham số này. Việc chọn tham số K phù hợp với mô hình có thể sử dụng một số phương pháp sau:",
          "relevence": "yes"
        },
        {
          "url": "https://techmaster.vn/posts/33893/thuat-toan-phan-cum",
          "title": "Bức tranh tổng quan về thuật toán phân cụm",
          "content": "Nếu bạn đang có ý định trở thành một Data Scientist (nhà khoa học dữ liệu) thì hiện tại đang là 1 thời điểm không hề tồi chút nào. Những con người kể cả khó tính nhất cũng sẽ đổ dồn sự chú ý khi bạn đề cập tới Big Data trong cuộc hội thoại, đám đông sẽ cảm thấy hào hứng khi được nghe bạn chém gió về Trí tuệ nhân tạo cũng như Học máy. Thậm chí những con số do Google cung cấp tại đây còn cho thấy: tất cả vẫn chưa có dấu hiệu dừng lại, chúng vẫn tiếp tục phát triển với tốc độ rất nhanh.Càng ngày càng có rất nhiều các giải thuật 'thông minh' đã được phát minh ra để giúp đỡ các nhà khoa học dữ liệu. Tất cả chúng nhìn chung đều có vẻ rất phức tạp, nhưng nếu chúng ta hiểu được và biết cách phối hợp một cách nhuần nhuyễn thì mọi việc sẽ trở nên dễ dàng hơn rất nhiều.Các khóa học về khai phá dữ liệu (Data Mining) hoặc học máy (Machine Learning) vẫn thường mở đầu bằng những ví dụ về phân cụm, lí do đơn giản bởi vì chúng rất thực tế và không quá khó hiểu. Bài toán phân cụm là 1 nhánh ứng dụng chính của lĩnh vực Unsupervised Learning (Học không giám sát), trong đó dữ liệu được mô tả trong bài toán không được dán nhãn (tức là không có đầu ra). Trong trường hợp này, thuật toán sẽ tìm cách phân cụm - chia dữ liệu thành từng nhóm có đặc điểm tương tự nhau, nhưng đồng thời đặc tính giữa các nhóm đó lại phải càng khác biệt càng tốt.Dữ liệu của chúng ta có thể là bất cứ thứ gì, chẳng hạn như dữ liệu về khách hàng: Thuật toán phân cụm sẽ rất hữu ích trong việc đánh giá và chia thành các nhóm người dùng khác nhau, rồi từ đó ta có thể đưa ra những chiến lược marketing phù hợp trên từng nhóm người dùng đó.Sau khi dạo qua những màn giới thiệu chung, đa số các khóa học Data Mining sẽ bắt đầu luôn với K-Means: 1 thuật toán tuy đơn giản nhưng lại khá hiệu quả và được sử dụng rộng khắp. Trước khi bắt tay vào làm, chúng ta cần phải xác định sẵn 2 thứ: đó là hàm khoảng cách được sử dụng (ví dụ như khoảng cách Euclid) và số lượng nhóm mong muốn (ta sẽ kí hiệu trong bài viết này là k)Thuật toán bắt đầu với việc chọn ra tâm của từng cụm. Chúng ta có thể đơn giản chọn k điểm ngẫu nhiên trong bộ, hoặc sử dụng một số hướng tiếp cận nào khác, nhưng nhìn chung ngẫu nhiên vẫn là cách tốt nhất. Rồi kế tiếp, luân phiên lặp lại 2 giai đoạn sau:Sau càng nhiều vòng lặp, các tâm càng di chuyển chậm dần, và tổng khoảng cách từ mỗi điểm trong cụm tới tâm cụm lại càng nhỏ đi. Quá trình sẽ kết thúc cho tới khi hàm tổng khoảng cách hội tụ (tức là không có sự thay đổi nào xảy ra ở giai đoạn gán nữa). Lúc này tọa độ tâm vẫn sẽ bằng trung bình cộng các điểm hiện tại trong cụm, hay nói cách khác tâm sẽ không còn di chuyển tiếp nữa. Chú ý thuật toán K-Means chỉ đảm bảo được quá trình này sẽ đưa hàm tổng khoảng cách hội tụ tới điểm cực tiểu địa phương, chứ không chắc chắn đó là giá trị nhỏ nhất của toàn bộ hàm số. Tuy nhiên, điều này là có thể chấp nhận được vì KHÔNG phải mô hình nào càng sát với bộ dữ liệu huấn luyện thì cũng sẽ càng tốt.Ta có thể nhận thấy rằng việc lựa chọn tâm lúc khởi điểm cũng có ảnh hưởng tới kết quả cuối cùng thu được, do đó đã nảy sinh rất nhiều ý kiến trái chiều về vấn đề này. Một ý tưởng đơn giản là cho chạy K-Means nhiều lần với mỗi bộ tâm ngẫu nhiên khác nhau, rồi sau đó chọn ra mô hình tốt nhất thông qua việc xét giá trị nhỏ nhất của các hàm tổng khoảng cách ứng với chúng.Một hướng tiếp cận khác trong việc chọn tâm ban đầu đó là chọn những điểm \"xa nhất\". Việc này có thể cho kết quả tốt hơn, tuy nhiên ta sẽ mắc phải vấn đề với những phần tử \"nhiễu\", đó là những phần tử nằm riêng lẻ một mình tách biệt với phần còn lại trong bộ dữ liệu. Do đó chúng sẽ tự lập ra 1 cụm riêng của chính mình.Có một cách giải quyết đã được phát minh để cân bằng đồng thời được cả 2 điều trên, nó có tên gọi là K-Means++: trong đó, tâm khởi đầu vẫn được chọn ngẫu nhiên, nhưng là chọn lần lượt (thay vì đồng loạt) và kèm theo xác suất ngẫu nhiên tỉ lệ thuận với khoảng cách tới điểm tâm vừa chọn trước đó. Tức là, các điểm càng nằm phía xa sẽ có khả năng được chọn làm tâm kế tiếp càng lớn. Do đó, nếu có 1 nhóm các điểm, khả năng chỉ 1 điểm từ nhóm đó được chọn làm tâm cũng sẽ cao hơn.K-Means++ cũng đang được chọn sử dụng cho bước khởi tạo của thuật toán K-Mean trong thư viện Scikit-learn của Python. Nếu bạn đang lập trình Python, bạn có thể dùng ngay thư viện này. Đối với Java, thư viện Weka sẽ là 1 sự lựa chọn đáng để cân nhắc.Ở trong ví dụ Python phía trên, ta sử dụng bộ dữ liệu Iris chứa kích thước đài hoa và cánh hoa cho 3 giống hoa Iris khác nhau, chia những dữ liệu này thành 3 cụm, rồi sau đó so sánh với giá trị thực tế của chúng, để kiểm tra độ chính xác của thuật toán.Trong trường hợp này, chúng ta thấy rằng dữ liệu được tách thành 3 cụm (ứng với 3 giống hoa) khác nhau và K-Means đã nhận ra chính xác những phần tử nào cùng nằm chung 1 cụm (Chú ý rằng Unsupervised Learning là bài toán không có nhãn nên chỉ số k bằng (0, 1, 2) ở trên chỉ là ngẫu nhiên, có tác dụng phân biệt chứ không phải là nhãn đầu ra).Tuy nhiên, làm cách nào mà ta chọn ra được số cụm (k) thích hợp? Câu hỏi tương tự như vậy thường rất phổ biến trong Học máy. Nếu chúng ta yêu cầu nhiều cụm hơn, dữ liệu sẽ được chia nhỏ ra, và giá trị error (tổng khoảng cách) cũng sẽ nhỏ hơn. Vậy, như thế có phải sẽ là tốt hơn nếu như ta chọn k lớn nhất có thể? Chúng ta có thể chọn k = m (số điểm), như thế mỗi điểm sẽ trở thành tâm của chính nó và mỗi cụm sẽ chỉ có 1 điểm? Điều đó không sai, error sẽ bằng 0, nhưng chúng ta sẽ không thể tìm được mô tả đơn giản cho dữ liệu, và mô hình thu được cũng không thể phủ được những điểm mới thêm vào. Vấn đề này có tên gọi là overfitting, và tất nhiên chúng ta sẽ không mong gặp phải nó.Một cách để giải quyết vấn đề này là bổ sung thêm hàm phạt (penalty) cho số lượng cụm. Từ đó, mục tiêu của ta lúc này không chỉ còn giảm thiểu error, mà phải cân bằng cả error + penalty. Giá trị error sẽ tiến dần tới 0 khi chúng ta tăng số lượng cụm, nhưng đồng thời penalty cũng tăng theo. Quá trình tăng số lượng cụm sẽ dừng lại khi mà lượng error giảm đi thấp hơn so với giá trị penalty, và kết quả thu được là kết quả tối ưu. Có một giải pháp sử dụng Bayesian Information Criterion (BIC) để tính k có tên gọi là X-Means [Pelleg and Moore, 2000].Một thứ khác chúng ta cần quan tâm đó là hàm khoảng cách. Hiển nhiên, với những điểm nằm trong không gian, khoảng cách Euclid rõ ràng là hiệu quả nhất, nhưng đôi khi ta cần thêm vài \"mánh khóe\" cho những loại dữ liệu đặc trưng khác nhau, ví dụ như các giá trị rời rạc,... Việc này yêu cầu khá nhiều kiến thức chuyên ngành liên quan tới dữ liệu đó. Hoặc, chúng ta có thể nhờ tới sự trợ giúp của Học máy để huấn luyện ra hàm khoảng cách thích hợp nhất. Nếu bạn có 1 tập các dữ liệu huấn luyện (đã biết trước chúng được phân cụm thế nào qua nhãn của chúng), kĩ thuật Supervised Learning (học có giám sát) có thể được ứng dụng để tìm ra hàm khoảng cách thích hợp, rồi áp dụng nó vào trong dữ liệu cần phân cụm.Ngoài ra, có 1 thuật toán phân cụm khác có tên là Expectation-Maximization (EM) cũng gần tương tự với 2 giai đoạn được dùng trong K-Means. Nói chính xác thì K-Means có thể coi là 1 phiên bản đơn giản hơn của EM. Tuy nhiên, đừng nhầm lẫn chúng với nhau mặc dù có rất nhiều điểm chung giữa 2 thuật toán này.Như vậy, với K-Means: mỗi điểm sẽ được gán cho 1 nhóm và mỗi nhóm được đại diện bởi 1 tâm. Điều này không quá phức tạp, vì chúng ta chưa gặp phải vấn đề cụm chồng chéo, hoặc những cụm có hình dạng khác hình tròn. Với EM, ta bây giờ có thể tiến một bước xa hơn nữa và đặc tả mỗi cụm bằng tâm của nó (kì vọng), covariance (hiệp phương sai - qua đó ta có thể biểu diễn được cụm hình elip) và weight (kích thước của cụm). Xác suất mà 1 điểm thuộc về 1 cụm bây giờ được tính bằng xác suất phân phối Gauss đa biến. Chúng ta sẽ bắt đầu EM bằng cách tính, với mỗi điểm, xác suất mà nó thuộc về từng cụm là bao nhiêu (tất nhiên, các cụm ban đầu cũng được khởi tạo ngẫu nhiên). Đây là bước E-step. Nếu 1 cụm là \"ứng viên\" tốt đối với 1 điểm, nó sẽ có xác suất gần với 1. Tuy nhiên, có xảy ra trường hợp 2 hay nhiều cụm cùng là ứng viên tốt, do đó điểm của chúng ta lúc này sẽ có phân phối xác suất giữa các cụm. Tính chất này của thuật toán được gọi là \"soft clustering\".Bước M-step bây giờ tính toán lại các tham số của mỗi cụm, bằng cách sử dụng kết quả xác suất của các điểm được tính ở bước E-step. Để tính toán tâm mới, covariance mới và weight mới của 1 cụm, mỗi dữ liệu điểm sẽ được đánh trọng số tỉ lệ thuận với xác suất biến cố \"điểm đó thuộc cụm\" (lấy từ E-step).Luân phiên 2 bước này sẽ làm tăng giá trị log-likelihood của hàm xác suất cho tới khi giá trị này hội tụ tới cực đại. Nói thêm, tương tự với K-Means, thuật toán EM chỉ cho ta giá trị cực đại địa phương, vì vậy ta có thể sẽ cần phải thực hiện thuật toán nhiều lần để tìm được mô hình tốt hơn nữa.Nếu ta muốn đưa ra quyết định 1 điểm bất kỳ thuộc cụm nào, đơn giản chỉ cần chọn cụm cho ta giá trị xác suất cao nhất ứng với điểm đó. Và ta cũng có thể hoàn toàn tái tạo lại được 1 mẫu tương tự như dữ liệu ban đầu từ mô hình dựa vào dãy các xác suất thu được.Bài viết gốc: https://www.toptal.com/machine-learning/clustering-algorithmsTác giả đang bận code dạo (PHP, Java, C#, Nodejs, React) kiếm tiền mua đất cưới vợ nên chưa viết đoạn mô tả.Quy định \n                    \n                    Ghi nhớ tài khoản\n                \n                    \n                    Hiển thị mật khẩu\n                \n                    Bằng cách nhấp vào Đăng ký, bạn đã đồng ý với các\n                     Quy định  của chúng tôi.\n                ",
          "relevence": "yes"
        },
        {
          "url": "https://www.slideshare.net/trand0anha0/thut-ton-k-mean",
          "title": "Thuật toán K mean",
          "content": "\n      Slideshare uses cookies to improve functionality and performance, and to provide you with relevant advertising. If you continue browsing the site, you agree to the use of cookies on this website. See our User Agreement and Privacy Policy.\n    \n      Slideshare uses cookies to improve functionality and performance, and to provide you with relevant advertising. If you continue browsing the site, you agree to the use of cookies on this website. See our Privacy Policy and User Agreement for details.\n    \n            \n              Published on May 14, 2013\n            \n          \n                    Thuật toán K mean\n                  LinkedIn Corporation © 2017Clipping is a handy way to collect and organize the most important slides from a presentation. You can keep your great finds in clipboards organized around topics.Looks like you’ve clipped this slide to  already.",
          "relevence": "yes"
        },
        {
          "url": "https://www.youtube.com/watch?v=jfu0RlzGZLU",
          "title": "Khai thác dữ liệu: Ví dụ minh họa thuật toán  K-Means - YouTube",
          "content": "\n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \n        Đang hoạt động...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \n        Đang hoạt động...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \n        Đang hoạt động...\n    \n  \n        \n\n    \n        Đang tải danh sách phát...\n    \n  ",
          "relevence": "yes"
        },
        {
          "url": "http://diendan.congdongcviet.com/threads/t37760::thuat-toan-k-means-ung-dung-thuat-toan-k-means-trong-gom-cum-du-lieu.cpp",
          "title": "Thuật toán K-Means | ứng dụng thuật toán K-means trong gom cụm dữ liệu",
          "content": "Tìm hiểu luật lệ tham gia diễn đàn",
          "relevence": "yes"
        },
        {
          "url": "http://ict-tdu.blogspot.com/2013/06/thuat-toan-k-mean-trong-bai-toan-phan.html",
          "title": "Thuật toán K-Mean trong bài toán Phân cụm dữ liệu",
          "content": "\nBài viết hay về CNTT, cập nhật thông tin công nghệ, tuyển dụng và chia sẻ kinh nghiệm học tập, Hướng dẫn kiếm tiền online, kiếm tiền trên mạng\n",
          "relevence": "yes"
        },
        {
          "url": "https://www.scribd.com/document/100911572/Cac-thu%E1%BA%ADt-toan-gom-c%E1%BB%A5m-va-%E1%BB%A9ng-d%E1%BB%A5ng",
          "title": "Các thuật toán gom cụm và ứng dụng",
          "content": "là sự phân lớp \"tự nhiên\" các dữ liệu. Gom cụm dữ liệu có thể được dùng để hỗ trợ con người trong việc tìm ra các quy tắc, quy luật. Mục tiêu: - Nghiên cứu các giải thuật gom cụm. - Xây dựng bộ công cụ clustering dựa trên một vài giải thuật - Nghiên cứu, phân tích kết quả của bộ công cụ trên một số tập dữ liệu mẫu Yêu cầu: - Tiếng Anh: đọc hiểu tốt - Say mê nghiên cứu, học hỏi, có khả năng tự nghiên cứu tốt - Kỹ năng lập trình tốt (C, C++, Java…) Số lượng sinh viên: 1 - 2 Tài liệu tham khảo: - A.K. JAIN, M.N. MURTY, P.J. FLYNN - Data Clustering: A review - Pavel BERKHIN – Survey of Clustering Data Mining Techniques … Giáo viên hướng dẫn: Phạm Gia Tiến e-mail: pgtien@cit.ctu.edu.vn Code: Tên đề tài: Chọn đặc điểm cho phân lớp âm thanh Giới thiệu: Phân lớp là quá trình nhóm các dữ liệu dựa trên các điểm đặc trưng (đặc điểm – feature). Việc chọn đặc điểm cho các dữ liệu phức tạp (âm thanh, hình ảnh, …) giữ một vai trò quan trọng. Đề tài hướng vào việc chọn các đặc điểm cho dữ liệu âm thanh. Mục tiêu: - Nghiên cứu tổng quan về phân lớp - Nghiên cứu tổng quan về chọn đặc điểm - Chọn các đặc điểm cho dữ liệu âm thanh, đánh giá trên dữ liệu giọng nói. Yêu cầu: - Tiếng Anh: đọc hiểu tốt - Say mê nghiên cứu, học hỏi, có khả năng tự nghiên cứu tốt - Kỹ năng lập trình tốt (C, C++, Java…) Số lượng sinh viên: 1 - 2 Tài liệu tham khảo: - Luigi Portinale, Lorenza Saita – Feature Selection ,\n\n… Giáo viên hướng dẫn: Phạm Gia Tiến e-mail: pgtien@cit.ctu.edu.vn\n\n\fMã số đề tài: LVTN________\n1. Tên đề tài: TÌM HIỂU MÔI TRƯỜNG LẬP TRÌNH SQUEAK. VẬN DỤNG: XÂY DỰNG BẢN ĐỒ 3D CHO ĐH CẦN THƠ.\n\n2. Loại đề tài: Lập trình đa phương tiện. 3. Giáo viên hướng dẫn: ThS. Nguyễn Công Huy (Email: nchuy@cit.ctu.edu.vn) Bộ môn Hệ Thống Máy Tính và Truyền Thông 4. Số lượng sinh viên tham gia: 1 sinh viên. 5. Đặt vấn đề: Squeak là một môi trường lập trình mạnh, hiện đại, mã nguồn mở và đa nền. Squeak bao gồm máy ảo và các công cụ phân tích, sửa lỗi và phát triển ứng dụng sử dụng ngôn ngữ Smalltalk. Có rất nhiều những project xây dựng trên Squeak từ ứng dụng đa phương tiện, ứng dụng cho giáo dục và các ứng dụng web thương mại. Squeak bao gồm rất nhiều những gói lập trình hỗ trợ trong lập trình multimedia (xử lý 2D, 3D, âm thanh, hình ảnh, video, …). Các nhà phát triển đã xây dựng các công cụ từ Squeak sử dụng cho dự án “Mỗi laptop cho mỗi trẻ em” và nhất là để thiết kế các game 3D. 6. Yêu cầu của đề tài: * Yêu cầu về lý thuyết: - Tìm hiểu ngôn ngữ Smalltalk và Squeak. - Tìm hiểu cách thức xây dựng 1 ứng dụng sử dụng Squeak. - Khai thác Croquet là nền mã nguồn mở hỗ trợ lập trình 3D. * Yêu cầu về chương trình: - Xây dựng 1 bản đồ dạng 3D cho khuôn viên Đại học Cần Thơ. Ứng dụng phải có chức năng hỗ trợ người dùng tìm đường đi đến 1 địa điểm nào đó trên bản đồ. - Giao diện thiết kế phải thân thiện với người sử dụng. - Môi trường và ngôn ngữ lập trình: Squeak * Yêu cầu về sinh viên: - Có kiến thức về lập trình hướng đối tượng và ngôn ngữ Smalltalk. - Có khả năng đọc hiểu tài liệu tiếng Anh. - Có khả năng làm việc độc lập, đặc biệt trong xây dựng ý tưởng và thiết kế CT. 7. Tài liệu tham khảo - [1] http://www.squeak.org. - [2] Andrew Black, Stéphane Ducasse, Oscar Nierstrasz, Squeak by example, 2007. - [3] Case Studies Summer in 2002 of CS2340 course, College of Computing, Georgia Institute of Technology, http://coweb.cc.gatech.edu/cs2340/17.\n\n\fLUẬN VĂN TỐT NGHIỆP ĐẠI HỌC K29 1. Tên đề tài: Semantic Web và Sử dụng các công nghệ ngữ nghĩa để thực hiện “pha trộn” và điều khiển các dịch vụ, thông tin và trình bày thông tin. 2. Loại đề tài: Web service, Semantic Web, làm việc theo nhóm. 3. Giáo viên hướng dẫn: Phan Thượng Cang. 4. Số lượng sinh viên tham gia: 3 sinh viên (có điểm trung bình trên 7.2). 5. Yêu cầu của đề tài. Tóm tắt bài toán, khả năng ứng dụng. HTML và Web đã tạo nên kho dữ liệu khổng lồ mà máy tính có thể “đọc được” nhưng không thể “hiểu được”. Và Semantic Web là Web mà ở đó nó có thể mô tả mọi thứ theo một kiểu cách mà máy tính “có thể hiểu được” để đáp ứng với những đòi hỏi ngày càng cao từ người dùng. Chẳng hạn, khi chúng ta muốn tìm kiếm một quyển sách của tác giả Washington, trên trang Google hay Yahoo ta tìm kiếm với từ khóa là “Washington” thì chúng sẽ hiển thị tất cả các liên kết đến có thể như: các cửa hiệu sách trực tuyến (online bookstore) Washington, thủ đô Washington, nhân vật Washington, trường đại học Washington… mà không thể đáp ứng chính xác với những gì mong muốn từ người dùng. Minh họa:\n\n\fVì vậy chúng ta cần tạo ra một ứng dụng “mashup” mà nó thực hiện việc kết hợp và “pha trộn” dữ liệu từ nhiều dịch vụ (multiple services) để tạo ra một thứ mới hơn, để rồi sau đó người dùng có thể chọn lựa trên tập dữ liệu mới đó đáp ứng với mong muốn của mình. Với những ứng dụng dịch vụ đơn lẻ (single-service applications) hiện nay là chưa đáp ứng được với những đòi hỏi trên mà đòi hỏi chúng ta cần phải sử dụng các công nghệ web ngữ nghĩa để “pha trộn” (mashup) và điều khiễn các dịch vụ, thông tin, và trình bày thông tin. Đề tài này sẽ tuần tự thực hiện những bước tiếp cận sau: o Xây dựng một ứng dụng “mashup”: sử dụng và kết hợp các dịch vụ Web. o Quản lý vùng trữ dữ liệu kết hợp (mashup data cache): lưu lại những kết quả của những đòi hỏi trước đó để dùng cho những đòi hỏi sau (sử dụng pureXML và cơ sở dữ liệu DB2 để cải tiến việc thực thi) o Tăng cường khả năng thông minh cho ứng dụng mashup (sự chọn lựa tự động giữa các dịch vụ và các thành phần của dịch vụ, sự chuyển đổi từ dịch vụ này sang dịch vụ khác mà không cần biết chính xác thông tin hiện có như thế nào) : sử dụng các công nghệ web ngữ nghĩa như RDF (Resource Description Framework), RDFs (RDF Schema Language) và OWL (Web Ontology Language). o Tạo một ontology giản đơn cho bookstore: định nghĩa các khái niệm và các quan hệ cho bookstore o Cung cấp khả năng chọn lựa dịch vụ cho người dùng: sử dụng các ontology đã định nghĩa, người dùng có thể thay đổi hoàn toàn các nguồn thông tin (information sources) o Cho phép người dùng điều khiễn các dịch vụ, thông tin và cách hiển thị thông tin Về lý thuyết cần nghiên cứu: Web service, ontology, semantic Web (RDF, RDFs, OWL) Ngôn ngữ cài đặt là Java, JSP, XML và Servlet. Phần mềm nguồn mở: Jena, DB2 database 6. Tài liệu tham khảo. [1] W3schools website. Semantic Web Tutorial. Tham khảo tại địa chỉ: http://www.w3schools.com/semweb/default.asp [2] Infomesh website.The Semantic Web. Tham khảo tại địa chỉ: http://infomesh.net/2001/swintro/ [3] W3schools website. RDF Tutorial. Tham khảo tại địa chỉ: http://www.w3schools.com/rdf/default.asp [4] Frank Manola and Eric Miller. RDF Primer. W3C Recommendation 10 February 2004. Tham khảo tại địa chỉ: http://www.w3.org/TR/rdf-primer/ [5] Ora Lassila and Ralph R. Swick. RDF Model and Syntax Specification. W3C Recommendation 10 February 2004. Tham khảo tại địa chỉ: http://www.w3.org/TR/REC-rdf-syntax [6] Michael K. Smith, Chris Welty, and Deborah L. McGuinness. OWL Web Ontology Language Guide. W3C Recommendation, 10 February 2004. Tham khảo tại địa chỉ: http://www.w3.org/TR/owl-guide/ [7] Mike Dean and Guus Schreiber. OWL Web Ontology Language Reference. W3C Recommendation, 10 February 2004. Tham khảo tại địa chỉ: http://www.w3.org/TR/owl-ref/\n\n\f[8] Peter F. Patel-Schneider, Pat Hayes, and Ian Horrocks. OWL Web Ontology Language Semantics and Abstract Syntax. W3C Recommendation, 10 February 2004. Tham khảo tại địa chỉ: http://www.w3.org/TR/owl-semantics/ [9] Sun Microsystems website. Learning the Java Language. Sun Microsystems documentation. Tham khảo tại địa chỉ: http://java.sun.com/docs/books/tutorial/ [10] Prentice Hall and Sun Microsystems website. Servlet and JSP Quick Reference. Prentice Hall and Sun Microsystems Documentation. Tham khảo tại địa chỉ: http://pdf.coreservlets.com/CSAJSP-Appendix.pdf [11] Philip McCarthy. Introduction to Jena: Use RDF models in your Java applications with the Jena Semantic Web Framework. IBM Documentation, 23 Jun 2004. Tham khảo tại địa chỉ: http://www.ibm.com/developerworks/xml/library/j-jena/ [12] Nicholas Chase. Building Web service applications with the Google API. IBM Documentation, 15 May 2002. Tham khảo tại địa chỉ: http://www.ibm.com/developerworks/edu/ws-dw-wsgoog-i.html\n\n\fLUẬN VĂN TỐT NGHIỆP ĐẠI HỌC K29 Nhóm 1: Tên đề tài: Hệ thống quản lý công văn. Loại đề tài: Trí tuệ nhân tạo. Giáo viên hướng dẫn: Nguyễn Thị Minh Luân. Số lượng sinh viên tham gia: 1 sinh viên. Yêu cầu của đề tài. Tóm tắt bài toán, khả năng ứng dụng. Mục đích của đề tài là xây dựng một ứng dụng quản lý công văn phục vụ công tác quản lý điều hành của các cơ quan, xí nghiệp. Ngoài các chức năng cơ bản như quản lý công văn đến, công văn đi, ứng dụng còn cho phép người sử dụng có thể chat hoặc gởi tin nhắn offline cũng như có thể trao đổi thông tin thoại theo kiểu PC2PC. Ngoài ra, để có thể tin học hóa hiệu quả các ứng dụng văn phòng, việc nghiên cứu và sử dụng các giải pháp đề bảo mật thông tin trong lưu trữ là hết sức quan trọng. Do đó, đề tài cũng sẽ quan tâm tìm hiểu, so sánh và lựa chọn sử dụng một giải pháp bảo mật phù hợp phục vụ công tác quản lý văn thư. Về lí thuyết cần nghiên cứu: UML, hệ thống quản lý công văn, bảo mật dữ liệu, VoIP. Ngôn ngữ cài đặt: SV tự chọn. 6. Tài liệu tham khảo. [1] Các qui định về quản lý và lưu trữ văn thư, tham khảo tại trang Web của Cục Văn thư và Lưu trữ nhà nước: www.luutruvn.gov.vn [2] Các tài liệu về VOIP trên Internet Nhóm 2: 7. Tên đề tài: Ứng dụng hệ thống đa tác tử trong công tác dự đoán. 8. Loại đề tài: Trí tuệ nhân tạo, làm việc theo nhóm. 9. Giáo viên hướng dẫn: Nguyễn Thị Minh Luân. 10. Số lượng sinh viên tham gia: 2 sinh viên. 11. Yêu cầu của đề tài. Tóm tắt bài toán, khả năng ứng dụng. Đồng bằng sông Cửu Long (ĐBSCL) đã có những bước tiến vượt bậc về sản xuất lúa gạo trong hơn mười năm qua và đã mang lại nhiều lợi ích cho người sản xuất và cho ngành lương thực phục vụ xuất khẩu nhờ vào thâm canh tăng vụ. Nhưng chính điều này cũng là một cơ hội cho sự bộc phát dịch hại, đặc biệt là dịch hại rầy nâu trong những vùng sản xuất lúa trọng điểm của cả nước. Để giải quyết vấn đề trên, đầu tiên ta phải hiểu được hành vi di trú sâu bệnh, côn trùng gây hại để từ đó đề ra các biện pháp ngăn chặn sự lây lan phù hợp. Trong đề tài này, chúng ta sẽ ứng dụng công nghệ đa tác tử nhằm mục đích nghiên cứu vấn đề một cách trực quan hơn và giúp các nhà chuyên môn có thể can thiệp đến mức thấp nhất của mô hình, cụ thể ở đây là côn trùng gây hại. Mặt khác để có thể ứng dụng trên những địa bàn cụ thể, chúng ta cần tích hợp hệ thống thông tin địa lý đặc tả điều kiện địa hình từng vùng vào hệ thống mô phỏng đa tác tử. Về lí thuyết cần nghiên cứu: hệ thống đa tác tử (multi-agent system), hệ thống thông tin địa lý (GIS), côn trùng gây hại. Ngôn ngữ cài đặt: SV tự chọn. 12. Tài liệu tham khảo. [1] J. Bank. Handbook of simulation. Weley-Interscience, 1998. 1. 2. 3. 4. 5.\n\n\f[2] S.A. DeLoach and M. Wood. An overview of the multiagent systems engineering methodology. AOSE, pages 207- 222, 2000. [3] http://www.swarm.org/wiki/Main_Page [4] http://repast.sourceforge.net/\n\n\fLUẬN VĂN TỐT NGHIỆP ĐẠI HỌC K29 7. Tên đề tài: Web 2.0 và ứng dụng trong thương mại điện tử. 8. Loại đề tài:Thương mại điện tử. 9. Giáo viên hướng dẫn: Lê Văn Lâm. 10. Số lượng sinh viên tham gia: 1 sinh viên (điểm trung bình >7.5). 11. Yêu cầu của đề tài. Web 2.0 có thể được định nghĩa như một thế hệ mới của những dịch vụ trên WWW. Nó tạo ra một hình thức sáng tạo mới trong việc tương tác người dùng, sáng tạo trong nội dung và thông tin chia sẻ trên WWW. Việc sử dụng Web 2.0 vào lĩnh vực thương mại điện tử đã tạo nên những cái tên rất nổi tiếng và quen thuộc như Amazon, eBay... Việc nghiên cứu những yếu tố thành công, cũng như những vấn đề cần giải quyết trong việc ứng dụng Web 2.0 trong thương mại điện tử là rất quan trọng. Đề tài luận văn tốt nghiệp thực hiện các công việc sau đây: Nghiên cứu những mô hình thành công trong việc áp dụng Web 2.0 Những vấn đề cần giải quyết Xây dựng một mô hình thương mại điện tử có sử dụng Web 2.0 được rút kết từ nghiên cứu trên. 12. Tài liệu tham khảo. [1] John Musser, “Web 2.0: Princeples and Best Practices”, O’Reilly Media, 2007. [2] Stern, Allen, “Future of Web Apps—Kevin Rose”, September 13, 2006, http://www.centernetworks.com/future-of-web-apps-kevin-rose [3] “Customer Satisfaction Index Finds Satisfaction with eCommerce”, http://www.the-dma.org/cgi/dispnewsstand?article=4486+++++ [4] Barnes & Noble book page Web 2.0 elements, including customer reviews, authorized sellers, people who bought this book also bought, and online reading groups. May 2006, http://search.barnesandnoble.com/booksearch/isbninquiry.asp?ISBN=0307277674&z=y &cds2Pid=9481 [5] Amazon.com Community Participation Guidelines, http://www.amazon.com/gp/help/customer/display.html?nodeId=14279631\n\n\fLUẬN VĂN TỐT NGHIỆP ĐẠI HỌC K29 Nhóm 1: 13. Tên đề tài: Xây dựng phần mềm SMSC server và phần mềm SMS client (mô phỏng mobile phone) 14. Loại đề tài: TCP/IP application, làm việc theo nhóm. 15. Giáo viên hướng dẫn: Lê Phụng Anh. 16. Số lượng sinh viên tham gia: 2 sinh viên nam. 17. Yêu cầu của đề tài. Tóm tắt SMPP protocol, khả năng ứng dụng. Để giải quyết vấn đề này cần nắm vững cấu trúc giao thức SMPP và nguyên lý hoạt động nhắn tin của hệ thống mạng thông tin di động tế bào. Nội dung gồm 2 phần chính: Phần giao thức SMPP và phần cài đặt (implement). Riêng phần cài đặt có 2 phần: SMSC và mô phỏng SMS client. Về lí thuyết cần nghiên cứu: TCPI/IP, mạng thông tin di động tế bào, hoạt động nhắn tin ngắn, và SMPP protocol. Ngôn ngữ cài đặt có thể là C++ hoặc Java (yêu cầu dùng mã nguồn mở). 18. Tài liệu tham khảo. [6] Thư viện mã nguồn mở SMPP của Asterisk và khác. Tham khảo tại địa chỉ: http://www.asterisk.com http://opensmpp.logica.com/introhtml/menu.htm [7] Tài liệu kỹ thuật cơ bản về SMSC. Tham khảo tại địa chỉ: http://www.developershome.com/sms/sms_tutorial.asp?page=smsc\n\n\fLUẬN VĂN TỐT NGHIỆP ĐẠI HỌC K29 19. Tên đề tài: Nghiên cứu các giải pháp tích hợp hệ thống thông tin. 20. Loại đề tài: Công nghệ phần mềm, làm việc theo nhóm. 21. Giáo viên hướng dẫn: Nguyễn Phú Trường 22. Số lượng sinh viên tham gia: 2 sinh viên. 23. Yêu cầu của đề tài. Tóm tắt bài toán, khả năng ứng dụng. Trong quá trình tin học hoá quản lý của cơ quan đặc biệt là cơ quan lớn, mỗi khi một bộ phận nào trong cơ quan có nhu cầu tin học hoá, họ xây dựng một phần mềm tương ứng. Thí dụ, bộ phận tài vụ cần có phần mềm kế toán, bộ phận nhân sự cần có phần mềm quản lý nhân sự và tiền lương,…Tuy nhiên, mỗi phần mềm được phát triển một cách độc lập và vào những thời điểm khác nhau nên các vấn đề phát sinh Các phần mềm không thể giao tiếp với nhau nên không thể trao đổi dữ liệu Công nghệ được áp dụng trong việc phát triển phần mềm cũng khác nhau. Thí dụ, phần mềm quản lý kế toán được viết bằng Visual FoxPro dùng hệ quản trị cơ sở dữ liệu SQL Server nhưng phần mềm quản lý nhân sự được viết bằng C# dùng hệ quản trị cơ sở dữ liệu Oracle Cùng một thông tin nhưng được biểu diễn theo nhiều cách khác nhau trong các phần mềm khác nhau. Thí dụ, với mã nhân viên được quản lý kiểu ký tự với 4 ký tự trong phần mềm kế toán nhưng lại được quản lý kiểu số với 5 con số trong phần mềm quản lý nhân sự. Vấn đề đặt ra là phải tìm giải pháp để tích hợp các phần mềm độc lập thành một hệ thống thông tin thống nhất thoả các yêu cầu sau: 1. Các phần mềm có thể trao đổi dữ liệu với nhau một cách tự động. Mỗi sự thay đổi thông tin trên phần mềm này phải được cập nhật đến phần mềm kia 2. Việc tích hợp hệ thống thông tin không làm thay đổi các phần mềm đã có (nếu có thể) 3. Dữ liệu của các phần mềm đã có thể sử dụng trong việc phát triển phần mềm mới trong hệ thống thông tin tích hợp Để giải quyết vấn đề này có nhiều giải pháp thực hiện: Tích hợp ở mức cơ sở dữ liệu: với giải pháp này người tích hợp ứng dụng có thể dựa vào đặc điểm đồng bộ hoá dữ liệu của các hệ quản trị cơ sở dữ liệu hoặc viết các middleware để thực hiện việc trao đổi dữ liệu dữ các phần mềm. Tích hợp ở mức ứng dụng: tiếp cận này đòi hỏi phải hiệu chỉnh lại các phần mềm đã có. Điều này, sẽ vi phạm với yêu cầu 2. Tuy nhiên, tiếp cận này vẫn có thể áp dụng trong trường hợp các phần mềm được phát triển bởi cùng một nhà phát triển hoặc mã nguồn được chia sẻ. Để thực hiện đề tài này sinh viên cần thực hiện các yêu cầu sau: Về lí thuyết cần nghiên cứu: các giải pháp tích hợp hệ thống thông tin Xây dựng chương trình: i. Viết các công cụ để đóng vai trò middleware để thực hiện việc trao đổi dữ liệu giữa các phần mềm. ii. Khai thác tính năng đồng bộ dữ liệu của các hệ quản trị cơ sở dữ liệu để thực hiện việc trao đổi dữ liệu giữa các phần mềm. Có thể viết các script với dạng store procedure. iii. Phương pháp hiệu chỉnh các phần mềm đã có để chúng có thể giao tiếp với nhau. 24. Tài liệu tham khảo.\n\n\f[13] Nguyễn Hoàng Việt, Nguyễn Phú Trường. Tích hợp hệ thống thông tin trường Đại học Cần Thơ. 2005. [14] System Integration Solutions, tham khảo tại địa chỉ: http://www.altera.com/technology/integration/int-index.html\n\n\fProposition de mémoire 2007-2008 Classification de Spams Responsables: The-Phi Pham1, Thanh-Nghi Do2\n1\n\nLaboratoire : Faculté des Technologies de l’Information Adresse : Université de Can Tho, 1 rue Ly Tu Trong, Can Tho Mail : ptphi@cit.ctu.edu.vn 2 Laboratoire : INRIA Futurs Adresse : L.R.I., Université Paris-Sud, Bât. 490, 91405 Orsay Cedex Mail : dtnghi@lri.fr Objectifs. Ces dernières années, les utilisateurs d’internet reçoivent nombreux de messages non sollicités, souvent publicitaires, envoyés en grand nombre, on parle de Spam. Une étude du Spam [Sung-jin, 2003] a rapporté que le coût du Spam s’est élevé à 2 milliards de dollars pour l’année 2002. Une autre étude [Mi2g, 2003] a calculé que pendant le mois d’octobre 2003 le coût du Spam a été de 10,4 milliards de dollars. D’après [Doug, 2003], si un spammeur (une personne qui crée des Spams) gagne 10 mille dollars par mois alors le coût de ses Spams est de 100 mille dollars. La lutte contre le Spam est indispensable pour réduire les gaspillages de ressources et de temps du monde informatique. L’objectif de ce mémoire est d’étudier les approches qui permettant de classifier les messages en Spam ou non Spam. Pour ce faire, l’étudiant(e) devra créer une base de spams à partir des courriers électroniques que l’on a reçus. Ensuite c’est nécessaire de transformer la base de spams, initialement en format texte, en une représentation numérique à l’aide de l’outil Bow [McCallum, 1998]. Ceci comprend l’extraction de termes et la sélection des termes les plus pertinents. Une fois ce prétraitement terminé, on peut représenter les textes sous la forme de vecteurs numériques que les algorithmes automatiques peuvent traiter. On va utiliser les algorithmes des arbres de décision, C4.5 [Quinlan, 1993] et une forêt aléatoire (Random Forest) [Breiman, 2001] pour catégoriser de spams. La mesure de qualité des résultats obtenus [Uren, 2000] prend en compte le taux de précision, le taux de rappel. Références [Breiman, 2001] L. Breiman. Random Forests. Machine Learning, 45(1), pp. 5-32, 2001. [Do et Fekete, 2007] T-N. Do et J-D. Fekete. Flot visuel de données. (à paraître) RNTI – Série Extraction et Gestion des Connaissances, Cépaduès Editions, 2007. [Do et Poulet, 2004] T-N. Do et F. Poulet. La catégorisation de textes. Rapport de contrat Fondation Vediorbis, ESIEA Recherche, Laval, 2004. [Doug, 2003] Doug. Spam’s Economic Damage. Doug’s Inner Net News 15-12-2003, 2003. [McCallum, 1998] A. McCallum. Bow: A Toolkit for Statistical Language Modeling, Text Retrieval, Classification and Clustering. 1998. http://www-2.cs.cmu.edu/~mccallum/bow. [Mi2g, 2003] Mi2g. Spam Overtakes Malware and Hacking Damage. Mi2g 06-11-2003, 2003. [Pham et al., 2007] N-K. Pham, T-N. Do et F. Poulet. Catégorisation de textes avec Boosting de PSVM. (à paraître) actes de la conf. nationale des technologies de l’information, Da Lat, Vietnam. [Quinlan, 1993] J. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, 1993. [Sung-jin, 2003] K. Sung-jin. Internet Users Sustain $2 Bil. In Damages Due to Spam Mail. Korea Times 12-01-2003, 2003.\n\n\f[Uren, 2000] V. Uren. An Evaluation of Text Categorisation Errors. Proceeding of the One-Day Workshop on Evaluation of Info. Management Systems, Queen Mary and Westfield College, London, UK, 2000, pp. 79-87.\n\n\fNghiên cứu phân loại Spams Giáo viên hướng dẫn: Ths. Đỗ Hiệp Thuận, Ts. Đỗ Thanh Nghị Bộ môn hệ thống máy tính và truyền thông Khoa công nghệ thông tin Trường Đại học cần thơ email: dhthuan@cit.ctu.edu.vn, dtnghi@cit.ctu.edu.vn Mục tiêu. Trong những năm qua, người dùng máy tính trên toàn cầu nhận nhiều thư điện tử không phục vụ cho mục đích sử dụng máy tính của họ, thường là những quảng cáo, thư phản động, chơi đánh bạc, thậm chí là những đoạn mã độc hại, đồi trụy khác, mà chúng ta gọi đó là spam. Một nghiên cứu của [Sungjin, 2003] đã cho biết tổn thất của người dùng máy tính lên đến 2 tỉ đô la trong năm 2002. Một nghiên cứu khác của [Mi2g, 2003] cưng cho biết trong tháng 10 năm 2003, tổn thất này lên đến 10,4 tỉ đô la. Theo [Doug, 2003], nếu một người phát tán spam thu được lợi 10 ngàn đô la trong 1 tháng thì tổn thất do họ gây ra là 100 ngàn đô la. Chính vì lẽ đó, việc chống spam cần được quan tâm hơn. Mục tiêu của đề tài là nghiên cứu những phương pháp phân loại thư điện tử để nhận biết spam. Để làm được điều này, sinh viên nên tạo tập dữ liệu thư điện tử gồm có spam và không phải là spam. Sau đó sẽ chuyển đổi cách biểu diễn tập dữ liệu về với dạng bảng dữ liệu véc tơ kiểu số dự trên thư viện Bow [McCallum, 1998]. Nó bao gồm các bước phân tích từ vựng và chọn tập hợp từ mà có thể dùng để phân loại thư spam. Tiếp theo là sinh viên sẽ sử dụng giải thuật máy học cây quyết định C4.5 [Quinlan, 1993] et giải thuật Bayes « thơ ngây » để phân loại thư spam. Đánh giá kết quả đạt được [Uren, 2000] dựa trên các tiêu chí precision và recall. Số lượng sinh viên tham gia: 2 sinh viên. Tài liệu tham khảo [Doug, 2003] Doug. Spam’s Economic Damage. Doug’s Inner Net News 15-12-2003, 2003. [McCallum, 1998] A. McCallum. Bow: A Toolkit for Statistical Language Modeling, Text Retrieval, Classification and Clustering. 1998. http://www-2.cs.cmu.edu/~mccallum/bow. [Mi2g, 2003] Mi2g. Spam Overtakes Malware and Hacking Damage. Mi2g 06-11-2003, 2003. [Pham et al., 2007] N-K. Pham, T-N. Do et F. Poulet. Phân loại văn bản với Boosting PSVM. Hội thảo quốc gia về công nghệ thông tin và truyền thông, Đà lạt, 2006. [Quinlan, 1993] J. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, 1993. [Sung-jin, 2003] K. Sung-jin. Internet Users Sustain $2 Bil. In Damages Due to Spam Mail. Korea Times 12-01-2003, 2003. [Uren, 2000] V. Uren. An Evaluation of Text Categorisation Errors. Proceeding of the One-Day Workshop on Evaluation of Info. Management Systems, Queen Mary and Westfield College, London, UK, 2000, pp. 79-87.\n\n\fKhai mỏ dữ liệu với ngôn ngữ R Giáo viên hướng dẫn: Th.s Đỗ Hiệp Thuận, Ts. Đỗ Thanh Nghị Bộ môn hệ thống máy tính và truyền thông Khoa công nghệ thông tin Trường Đại học cần thơ email : dhthuan@cit.ctu.edu.vn dtnghi@cit.ctu.edu.vn Mục tiêu. Trong những năm 1990, cuộc cách mạng kỹ thuật số cho phép số hóa thông tin dễ dàng và chi phí thấp, thêm vào đó là sự phát triển của công nghệ thông tin bao gồm cả phần cứng lẫn phần mềm, công nghệ truyền thông, web, internet đã góp phần đưa máy tính vào các sinh hoạt thường nhật của con người. Tất cả các họat động kinh doanh, vui chơi giải trí, nghiên cứu khoa học, giáo dục, truyền thông,… đều có sự hỗ trợ của máy tính. Hệ quả là một khối lượng lớn dữ liệu được sinh ra và lưu trữ trong các cơ sở dữ liệu, thiết bị lưu trữ như băng từ, đĩa từ. Năm 1999, Giáo sư P. Lyman, Đại học Berkeley đã tiến hành thống kê dữ liệu được sinh ra hằng năm trên toàn cầu. Trong năm 2002-2003 (tham khảo ở địa chỉ http://www.sims.berkeley.edu/research/projects/how-much-info-2003/), dữ liệu tăng 5 Exabytes (5. 1018 bytes). U. Fayyad et al. [8] ước lượng dữ liệu toàn cầu tăng gấp đôi trong vòng 9 tháng. Khám phá tri thức là một quá trình lặp phức tạp được định nghĩa bởi [7] sử dụng nhiều kỹ thuật như cơ sở dữ liệu, máy học, phương pháp thống kê trong phân tích dữ liệu, hiển thị dữ liệu, tối ưu hóa, trí tuệ nhân tạo, nhằm tìm ra những kiến thức cần thiết, tìm kiếm tri thức từ kho dữ liệu khổng lồ để phân loại, quản lý trở thành vấn đề rất được quan tâm. Mục tiêu của đề tài là nghiên cứu ngôn ngữ lập trình hàm cấp cao R dành cho phân tích dữ liệu, khám phá tri thức và khai mỏ dữ liệu. Đây là một phần mềm miễn phí mã nguồn mở, rất dễ học và có thể phát triển nhanh các ứng dụng khai mỏ dữ liệu trong thời gian ngắn. R là môi trường thích hợp cho việc giảng dạy, học tập và nghiên cứu trong trường đại học về khai mỏ dữ liệu. Không phải tốn chi phí cho bản quyền, hơn nữa R hỗ trợ rất nhiều công cụ hữu ích cho quá trình khai mỏ dữ liệu như: các giải thuật học tự động của cây quyết định, phân cụm, mạng nơron, máy học vectơ hỗ trợ, hồi quy và các giao diện truy vấn dữ liệu, hiển thị dữ liệu. Có thể lập trình một cách dễ dàng trong R. Sinh viên cần chỉ ra làm thế nào có thể khai mỏ dữ liệu với môi trường R. Nội dung thực hiện bao gồm trình bày tóm tắt cơ bản về ngôn ngữ R và khả năng lập trình trong R. Tiếp theo, trình bày các hỗ trợ của R cho quá trình khai mỏ dữ liệu như : nhập xuất dữ liệu, hiển thị dữ liệu với và thực hiện các giải thuật khai mỏ dữ liệu. Số lượng sinh viên tham gia: 1 sinh viên. Tài liệu tham khảo [3] R.A. Becker, J.M. Chambers and A.R. Wilks.: The New S Language: A Programming Environment for Data Analysis and Graphics. Chapman & Hall, 1988. [4] C. Blake and C. Merz.: UCI Repository of Machine Learning Databases. 1998. http://www.ics.uci.edu/~mlearn/MLRepository.html [5] L. Breiman, J. Friedman, R. Olshen and C. Stone.: Classification and Regression Trees. Chapman & Hall, New York, 1984. [6] L. Breiman.: Random Forests. Machine Learning, 45(1), pp. 5-32, 2001. [7] M.J. Crawley.: Statistics: An Introduction using R. Wiley, 2005. [8] T-N. Do, N-K. Pham, H-T. Do and N-C. Lam. Khai mỏ dữ liệu với ngôn ngữ R. in proc. of FAIR’07, The Third National Symposium Fundamental & Applied IT Research, Vietnam, 2007.\n\n\f[9] U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth.: From Data Mining to Knowledge Discovery in Databases. AI Magazine, 17(3), pp.37-54, 1996. [10] U. Fayyad, G. Piatetsky-Shapiro, and R. Uthurusamy.: Summary from the KDD-03 Panel – Data Mining: The Next 10 Years. in SIGKDD Explorations, 5(2), pp.191-196, 2004. [11] U. Fayyad, G. Grinstein, and A. Wierse.: Information Visualization in Data Mining and Knowledge Discovery. Morgan Kaufmann Publishers, 2001. [12] R. Ihaka and R. Gentleman.: R: A language for data analysis and graphics. Journal of Computational and Graphical Statistics, 5(3), pp. 299-314, 1996. [13] D. Keim.: Databases and Visualization. Tutorial Notes, ACM-SIGMOD’96, 1996. [14] T. Kohonen.: Self-Organizing Maps. Springer, Berlin, Heidelberg, New York, 1995. [15] J. Maindonald and J. Braun.: Data Analysis and Graphics Using R. Cambridge University Press, 2003. [16] J. MacQueen.: Some Methods for classification and Analysis of Multivariate Observations. Proceeding of 5th Berkeley Symposium on Mathematical Statistics and Probability, Berkeley, University of California Press, Vol. 1, pp. 281-297, 1967. [17] J-R. Quinlan.: C4.5: Programs for Machine Learning. Morgan-Kaufman Publishers, 1993. [18] P. Spector.: An Introduction to S and S-Plus. Duxbury Press, 1994. [19] P. Spector.: An Introduction to R. Statistical Computing Facility, University of California, Berkeley, 2004. [20] The Comprehensive R Archive Network (CRAN). http://cran.r-project.org\n[21] V. Vapnik.: The Nature of Statistical Learning Theory. Springer-Verlag, New York, 1995. [22] W. N. Venables and D. M. Smith.: An Introduction to R. Network Theory, 2002. [23] P. Burns.: An Introduction to the S Language. 2002.\n\n\fThis action might not be possible to undo. Are you sure you want to continue?Use one of your book credits to continue reading from where you left off, or restart the preview.",
          "relevence": "no"
        },
        {
          "url": "http://text.123doc.org/document/2635213-mot-so-phuong-phap-gom-cum-du-lieu-thuat-toan-k-means.htm",
          "title": "MỘT SỐ PHƯƠNG PHÁP GOM CỤM DỮ LIỆU -THUẬT TOÁN K-MEANS - Tài liệu text",
          "content": "Tài liệu liên quanTải bản đầy đủ ngay",
          "relevence": "yes"
        },
        {
          "url": "http://text.123doc.org/document/2635484-thuat-toan-k-means-va-ung-dung-gom-cum-tai-lieu-van-ban-tieng-viet.htm",
          "title": "Thuật toán K-Means và ứng dụng gom cụm tài liệu văn bản Tiếng Việt - Tài liệu text",
          "content": "Tài liệu liên quanTải bản đầy đủ ngay",
          "relevence": "yes"
        },
        {
          "url": "https://www.youtube.com/watch?v=jfu0RlzGZLU",
          "title": "Khai thác dữ liệu: Ví dụ minh họa thuật toán  K-Means - YouTube",
          "content": "\n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \n        Đang hoạt động...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \n        Đang hoạt động...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \nĐang tải...\n    \n  \n        \n\n    \n        Đang hoạt động...\n    \n  \n        \n\n    \n        Đang tải danh sách phát...\n    \n  ",
          "relevence": "yes"
        },
        {
          "url": "http://ict-tdu.blogspot.com/2013/06/thuat-toan-k-mean-trong-bai-toan-phan.html",
          "title": "Thuật toán K-Mean trong bài toán Phân cụm dữ liệu",
          "content": "\nBài viết hay về CNTT, cập nhật thông tin công nghệ, tuyển dụng và chia sẻ kinh nghiệm học tập, Hướng dẫn kiếm tiền online, kiếm tiền trên mạng\n",
          "relevence": "yes"
        },
        {
          "url": "http://diendan.congdongcviet.com/threads/t37760::thuat-toan-k-means-ung-dung-thuat-toan-k-means-trong-gom-cum-du-lieu.cpp",
          "title": "Thuật toán K-Means | ứng dụng thuật toán K-means trong gom cụm dữ liệu",
          "content": "Tìm hiểu luật lệ tham gia diễn đàn",
          "relevence": "yes"
        },
        {
          "url": "https://www.scribd.com/document/100911572/Cac-thu%E1%BA%ADt-toan-gom-c%E1%BB%A5m-va-%E1%BB%A9ng-d%E1%BB%A5ng",
          "title": "Các thuật toán gom cụm và ứng dụng",
          "content": "là sự phân lớp \"tự nhiên\" các dữ liệu. Gom cụm dữ liệu có thể được dùng để hỗ trợ con người trong việc tìm ra các quy tắc, quy luật. Mục tiêu: - Nghiên cứu các giải thuật gom cụm. - Xây dựng bộ công cụ clustering dựa trên một vài giải thuật - Nghiên cứu, phân tích kết quả của bộ công cụ trên một số tập dữ liệu mẫu Yêu cầu: - Tiếng Anh: đọc hiểu tốt - Say mê nghiên cứu, học hỏi, có khả năng tự nghiên cứu tốt - Kỹ năng lập trình tốt (C, C++, Java…) Số lượng sinh viên: 1 - 2 Tài liệu tham khảo: - A.K. JAIN, M.N. MURTY, P.J. FLYNN - Data Clustering: A review - Pavel BERKHIN – Survey of Clustering Data Mining Techniques … Giáo viên hướng dẫn: Phạm Gia Tiến e-mail: pgtien@cit.ctu.edu.vn Code: Tên đề tài: Chọn đặc điểm cho phân lớp âm thanh Giới thiệu: Phân lớp là quá trình nhóm các dữ liệu dựa trên các điểm đặc trưng (đặc điểm – feature). Việc chọn đặc điểm cho các dữ liệu phức tạp (âm thanh, hình ảnh, …) giữ một vai trò quan trọng. Đề tài hướng vào việc chọn các đặc điểm cho dữ liệu âm thanh. Mục tiêu: - Nghiên cứu tổng quan về phân lớp - Nghiên cứu tổng quan về chọn đặc điểm - Chọn các đặc điểm cho dữ liệu âm thanh, đánh giá trên dữ liệu giọng nói. Yêu cầu: - Tiếng Anh: đọc hiểu tốt - Say mê nghiên cứu, học hỏi, có khả năng tự nghiên cứu tốt - Kỹ năng lập trình tốt (C, C++, Java…) Số lượng sinh viên: 1 - 2 Tài liệu tham khảo: - Luigi Portinale, Lorenza Saita – Feature Selection ,\n\n… Giáo viên hướng dẫn: Phạm Gia Tiến e-mail: pgtien@cit.ctu.edu.vn\n\n\fMã số đề tài: LVTN________\n1. Tên đề tài: TÌM HIỂU MÔI TRƯỜNG LẬP TRÌNH SQUEAK. VẬN DỤNG: XÂY DỰNG BẢN ĐỒ 3D CHO ĐH CẦN THƠ.\n\n2. Loại đề tài: Lập trình đa phương tiện. 3. Giáo viên hướng dẫn: ThS. Nguyễn Công Huy (Email: nchuy@cit.ctu.edu.vn) Bộ môn Hệ Thống Máy Tính và Truyền Thông 4. Số lượng sinh viên tham gia: 1 sinh viên. 5. Đặt vấn đề: Squeak là một môi trường lập trình mạnh, hiện đại, mã nguồn mở và đa nền. Squeak bao gồm máy ảo và các công cụ phân tích, sửa lỗi và phát triển ứng dụng sử dụng ngôn ngữ Smalltalk. Có rất nhiều những project xây dựng trên Squeak từ ứng dụng đa phương tiện, ứng dụng cho giáo dục và các ứng dụng web thương mại. Squeak bao gồm rất nhiều những gói lập trình hỗ trợ trong lập trình multimedia (xử lý 2D, 3D, âm thanh, hình ảnh, video, …). Các nhà phát triển đã xây dựng các công cụ từ Squeak sử dụng cho dự án “Mỗi laptop cho mỗi trẻ em” và nhất là để thiết kế các game 3D. 6. Yêu cầu của đề tài: * Yêu cầu về lý thuyết: - Tìm hiểu ngôn ngữ Smalltalk và Squeak. - Tìm hiểu cách thức xây dựng 1 ứng dụng sử dụng Squeak. - Khai thác Croquet là nền mã nguồn mở hỗ trợ lập trình 3D. * Yêu cầu về chương trình: - Xây dựng 1 bản đồ dạng 3D cho khuôn viên Đại học Cần Thơ. Ứng dụng phải có chức năng hỗ trợ người dùng tìm đường đi đến 1 địa điểm nào đó trên bản đồ. - Giao diện thiết kế phải thân thiện với người sử dụng. - Môi trường và ngôn ngữ lập trình: Squeak * Yêu cầu về sinh viên: - Có kiến thức về lập trình hướng đối tượng và ngôn ngữ Smalltalk. - Có khả năng đọc hiểu tài liệu tiếng Anh. - Có khả năng làm việc độc lập, đặc biệt trong xây dựng ý tưởng và thiết kế CT. 7. Tài liệu tham khảo - [1] http://www.squeak.org. - [2] Andrew Black, Stéphane Ducasse, Oscar Nierstrasz, Squeak by example, 2007. - [3] Case Studies Summer in 2002 of CS2340 course, College of Computing, Georgia Institute of Technology, http://coweb.cc.gatech.edu/cs2340/17.\n\n\fLUẬN VĂN TỐT NGHIỆP ĐẠI HỌC K29 1. Tên đề tài: Semantic Web và Sử dụng các công nghệ ngữ nghĩa để thực hiện “pha trộn” và điều khiển các dịch vụ, thông tin và trình bày thông tin. 2. Loại đề tài: Web service, Semantic Web, làm việc theo nhóm. 3. Giáo viên hướng dẫn: Phan Thượng Cang. 4. Số lượng sinh viên tham gia: 3 sinh viên (có điểm trung bình trên 7.2). 5. Yêu cầu của đề tài. Tóm tắt bài toán, khả năng ứng dụng. HTML và Web đã tạo nên kho dữ liệu khổng lồ mà máy tính có thể “đọc được” nhưng không thể “hiểu được”. Và Semantic Web là Web mà ở đó nó có thể mô tả mọi thứ theo một kiểu cách mà máy tính “có thể hiểu được” để đáp ứng với những đòi hỏi ngày càng cao từ người dùng. Chẳng hạn, khi chúng ta muốn tìm kiếm một quyển sách của tác giả Washington, trên trang Google hay Yahoo ta tìm kiếm với từ khóa là “Washington” thì chúng sẽ hiển thị tất cả các liên kết đến có thể như: các cửa hiệu sách trực tuyến (online bookstore) Washington, thủ đô Washington, nhân vật Washington, trường đại học Washington… mà không thể đáp ứng chính xác với những gì mong muốn từ người dùng. Minh họa:\n\n\fVì vậy chúng ta cần tạo ra một ứng dụng “mashup” mà nó thực hiện việc kết hợp và “pha trộn” dữ liệu từ nhiều dịch vụ (multiple services) để tạo ra một thứ mới hơn, để rồi sau đó người dùng có thể chọn lựa trên tập dữ liệu mới đó đáp ứng với mong muốn của mình. Với những ứng dụng dịch vụ đơn lẻ (single-service applications) hiện nay là chưa đáp ứng được với những đòi hỏi trên mà đòi hỏi chúng ta cần phải sử dụng các công nghệ web ngữ nghĩa để “pha trộn” (mashup) và điều khiễn các dịch vụ, thông tin, và trình bày thông tin. Đề tài này sẽ tuần tự thực hiện những bước tiếp cận sau: o Xây dựng một ứng dụng “mashup”: sử dụng và kết hợp các dịch vụ Web. o Quản lý vùng trữ dữ liệu kết hợp (mashup data cache): lưu lại những kết quả của những đòi hỏi trước đó để dùng cho những đòi hỏi sau (sử dụng pureXML và cơ sở dữ liệu DB2 để cải tiến việc thực thi) o Tăng cường khả năng thông minh cho ứng dụng mashup (sự chọn lựa tự động giữa các dịch vụ và các thành phần của dịch vụ, sự chuyển đổi từ dịch vụ này sang dịch vụ khác mà không cần biết chính xác thông tin hiện có như thế nào) : sử dụng các công nghệ web ngữ nghĩa như RDF (Resource Description Framework), RDFs (RDF Schema Language) và OWL (Web Ontology Language). o Tạo một ontology giản đơn cho bookstore: định nghĩa các khái niệm và các quan hệ cho bookstore o Cung cấp khả năng chọn lựa dịch vụ cho người dùng: sử dụng các ontology đã định nghĩa, người dùng có thể thay đổi hoàn toàn các nguồn thông tin (information sources) o Cho phép người dùng điều khiễn các dịch vụ, thông tin và cách hiển thị thông tin Về lý thuyết cần nghiên cứu: Web service, ontology, semantic Web (RDF, RDFs, OWL) Ngôn ngữ cài đặt là Java, JSP, XML và Servlet. Phần mềm nguồn mở: Jena, DB2 database 6. Tài liệu tham khảo. [1] W3schools website. Semantic Web Tutorial. Tham khảo tại địa chỉ: http://www.w3schools.com/semweb/default.asp [2] Infomesh website.The Semantic Web. Tham khảo tại địa chỉ: http://infomesh.net/2001/swintro/ [3] W3schools website. RDF Tutorial. Tham khảo tại địa chỉ: http://www.w3schools.com/rdf/default.asp [4] Frank Manola and Eric Miller. RDF Primer. W3C Recommendation 10 February 2004. Tham khảo tại địa chỉ: http://www.w3.org/TR/rdf-primer/ [5] Ora Lassila and Ralph R. Swick. RDF Model and Syntax Specification. W3C Recommendation 10 February 2004. Tham khảo tại địa chỉ: http://www.w3.org/TR/REC-rdf-syntax [6] Michael K. Smith, Chris Welty, and Deborah L. McGuinness. OWL Web Ontology Language Guide. W3C Recommendation, 10 February 2004. Tham khảo tại địa chỉ: http://www.w3.org/TR/owl-guide/ [7] Mike Dean and Guus Schreiber. OWL Web Ontology Language Reference. W3C Recommendation, 10 February 2004. Tham khảo tại địa chỉ: http://www.w3.org/TR/owl-ref/\n\n\f[8] Peter F. Patel-Schneider, Pat Hayes, and Ian Horrocks. OWL Web Ontology Language Semantics and Abstract Syntax. W3C Recommendation, 10 February 2004. Tham khảo tại địa chỉ: http://www.w3.org/TR/owl-semantics/ [9] Sun Microsystems website. Learning the Java Language. Sun Microsystems documentation. Tham khảo tại địa chỉ: http://java.sun.com/docs/books/tutorial/ [10] Prentice Hall and Sun Microsystems website. Servlet and JSP Quick Reference. Prentice Hall and Sun Microsystems Documentation. Tham khảo tại địa chỉ: http://pdf.coreservlets.com/CSAJSP-Appendix.pdf [11] Philip McCarthy. Introduction to Jena: Use RDF models in your Java applications with the Jena Semantic Web Framework. IBM Documentation, 23 Jun 2004. Tham khảo tại địa chỉ: http://www.ibm.com/developerworks/xml/library/j-jena/ [12] Nicholas Chase. Building Web service applications with the Google API. IBM Documentation, 15 May 2002. Tham khảo tại địa chỉ: http://www.ibm.com/developerworks/edu/ws-dw-wsgoog-i.html\n\n\fLUẬN VĂN TỐT NGHIỆP ĐẠI HỌC K29 Nhóm 1: Tên đề tài: Hệ thống quản lý công văn. Loại đề tài: Trí tuệ nhân tạo. Giáo viên hướng dẫn: Nguyễn Thị Minh Luân. Số lượng sinh viên tham gia: 1 sinh viên. Yêu cầu của đề tài. Tóm tắt bài toán, khả năng ứng dụng. Mục đích của đề tài là xây dựng một ứng dụng quản lý công văn phục vụ công tác quản lý điều hành của các cơ quan, xí nghiệp. Ngoài các chức năng cơ bản như quản lý công văn đến, công văn đi, ứng dụng còn cho phép người sử dụng có thể chat hoặc gởi tin nhắn offline cũng như có thể trao đổi thông tin thoại theo kiểu PC2PC. Ngoài ra, để có thể tin học hóa hiệu quả các ứng dụng văn phòng, việc nghiên cứu và sử dụng các giải pháp đề bảo mật thông tin trong lưu trữ là hết sức quan trọng. Do đó, đề tài cũng sẽ quan tâm tìm hiểu, so sánh và lựa chọn sử dụng một giải pháp bảo mật phù hợp phục vụ công tác quản lý văn thư. Về lí thuyết cần nghiên cứu: UML, hệ thống quản lý công văn, bảo mật dữ liệu, VoIP. Ngôn ngữ cài đặt: SV tự chọn. 6. Tài liệu tham khảo. [1] Các qui định về quản lý và lưu trữ văn thư, tham khảo tại trang Web của Cục Văn thư và Lưu trữ nhà nước: www.luutruvn.gov.vn [2] Các tài liệu về VOIP trên Internet Nhóm 2: 7. Tên đề tài: Ứng dụng hệ thống đa tác tử trong công tác dự đoán. 8. Loại đề tài: Trí tuệ nhân tạo, làm việc theo nhóm. 9. Giáo viên hướng dẫn: Nguyễn Thị Minh Luân. 10. Số lượng sinh viên tham gia: 2 sinh viên. 11. Yêu cầu của đề tài. Tóm tắt bài toán, khả năng ứng dụng. Đồng bằng sông Cửu Long (ĐBSCL) đã có những bước tiến vượt bậc về sản xuất lúa gạo trong hơn mười năm qua và đã mang lại nhiều lợi ích cho người sản xuất và cho ngành lương thực phục vụ xuất khẩu nhờ vào thâm canh tăng vụ. Nhưng chính điều này cũng là một cơ hội cho sự bộc phát dịch hại, đặc biệt là dịch hại rầy nâu trong những vùng sản xuất lúa trọng điểm của cả nước. Để giải quyết vấn đề trên, đầu tiên ta phải hiểu được hành vi di trú sâu bệnh, côn trùng gây hại để từ đó đề ra các biện pháp ngăn chặn sự lây lan phù hợp. Trong đề tài này, chúng ta sẽ ứng dụng công nghệ đa tác tử nhằm mục đích nghiên cứu vấn đề một cách trực quan hơn và giúp các nhà chuyên môn có thể can thiệp đến mức thấp nhất của mô hình, cụ thể ở đây là côn trùng gây hại. Mặt khác để có thể ứng dụng trên những địa bàn cụ thể, chúng ta cần tích hợp hệ thống thông tin địa lý đặc tả điều kiện địa hình từng vùng vào hệ thống mô phỏng đa tác tử. Về lí thuyết cần nghiên cứu: hệ thống đa tác tử (multi-agent system), hệ thống thông tin địa lý (GIS), côn trùng gây hại. Ngôn ngữ cài đặt: SV tự chọn. 12. Tài liệu tham khảo. [1] J. Bank. Handbook of simulation. Weley-Interscience, 1998. 1. 2. 3. 4. 5.\n\n\f[2] S.A. DeLoach and M. Wood. An overview of the multiagent systems engineering methodology. AOSE, pages 207- 222, 2000. [3] http://www.swarm.org/wiki/Main_Page [4] http://repast.sourceforge.net/\n\n\fLUẬN VĂN TỐT NGHIỆP ĐẠI HỌC K29 7. Tên đề tài: Web 2.0 và ứng dụng trong thương mại điện tử. 8. Loại đề tài:Thương mại điện tử. 9. Giáo viên hướng dẫn: Lê Văn Lâm. 10. Số lượng sinh viên tham gia: 1 sinh viên (điểm trung bình >7.5). 11. Yêu cầu của đề tài. Web 2.0 có thể được định nghĩa như một thế hệ mới của những dịch vụ trên WWW. Nó tạo ra một hình thức sáng tạo mới trong việc tương tác người dùng, sáng tạo trong nội dung và thông tin chia sẻ trên WWW. Việc sử dụng Web 2.0 vào lĩnh vực thương mại điện tử đã tạo nên những cái tên rất nổi tiếng và quen thuộc như Amazon, eBay... Việc nghiên cứu những yếu tố thành công, cũng như những vấn đề cần giải quyết trong việc ứng dụng Web 2.0 trong thương mại điện tử là rất quan trọng. Đề tài luận văn tốt nghiệp thực hiện các công việc sau đây: Nghiên cứu những mô hình thành công trong việc áp dụng Web 2.0 Những vấn đề cần giải quyết Xây dựng một mô hình thương mại điện tử có sử dụng Web 2.0 được rút kết từ nghiên cứu trên. 12. Tài liệu tham khảo. [1] John Musser, “Web 2.0: Princeples and Best Practices”, O’Reilly Media, 2007. [2] Stern, Allen, “Future of Web Apps—Kevin Rose”, September 13, 2006, http://www.centernetworks.com/future-of-web-apps-kevin-rose [3] “Customer Satisfaction Index Finds Satisfaction with eCommerce”, http://www.the-dma.org/cgi/dispnewsstand?article=4486+++++ [4] Barnes & Noble book page Web 2.0 elements, including customer reviews, authorized sellers, people who bought this book also bought, and online reading groups. May 2006, http://search.barnesandnoble.com/booksearch/isbninquiry.asp?ISBN=0307277674&z=y &cds2Pid=9481 [5] Amazon.com Community Participation Guidelines, http://www.amazon.com/gp/help/customer/display.html?nodeId=14279631\n\n\fLUẬN VĂN TỐT NGHIỆP ĐẠI HỌC K29 Nhóm 1: 13. Tên đề tài: Xây dựng phần mềm SMSC server và phần mềm SMS client (mô phỏng mobile phone) 14. Loại đề tài: TCP/IP application, làm việc theo nhóm. 15. Giáo viên hướng dẫn: Lê Phụng Anh. 16. Số lượng sinh viên tham gia: 2 sinh viên nam. 17. Yêu cầu của đề tài. Tóm tắt SMPP protocol, khả năng ứng dụng. Để giải quyết vấn đề này cần nắm vững cấu trúc giao thức SMPP và nguyên lý hoạt động nhắn tin của hệ thống mạng thông tin di động tế bào. Nội dung gồm 2 phần chính: Phần giao thức SMPP và phần cài đặt (implement). Riêng phần cài đặt có 2 phần: SMSC và mô phỏng SMS client. Về lí thuyết cần nghiên cứu: TCPI/IP, mạng thông tin di động tế bào, hoạt động nhắn tin ngắn, và SMPP protocol. Ngôn ngữ cài đặt có thể là C++ hoặc Java (yêu cầu dùng mã nguồn mở). 18. Tài liệu tham khảo. [6] Thư viện mã nguồn mở SMPP của Asterisk và khác. Tham khảo tại địa chỉ: http://www.asterisk.com http://opensmpp.logica.com/introhtml/menu.htm [7] Tài liệu kỹ thuật cơ bản về SMSC. Tham khảo tại địa chỉ: http://www.developershome.com/sms/sms_tutorial.asp?page=smsc\n\n\fLUẬN VĂN TỐT NGHIỆP ĐẠI HỌC K29 19. Tên đề tài: Nghiên cứu các giải pháp tích hợp hệ thống thông tin. 20. Loại đề tài: Công nghệ phần mềm, làm việc theo nhóm. 21. Giáo viên hướng dẫn: Nguyễn Phú Trường 22. Số lượng sinh viên tham gia: 2 sinh viên. 23. Yêu cầu của đề tài. Tóm tắt bài toán, khả năng ứng dụng. Trong quá trình tin học hoá quản lý của cơ quan đặc biệt là cơ quan lớn, mỗi khi một bộ phận nào trong cơ quan có nhu cầu tin học hoá, họ xây dựng một phần mềm tương ứng. Thí dụ, bộ phận tài vụ cần có phần mềm kế toán, bộ phận nhân sự cần có phần mềm quản lý nhân sự và tiền lương,…Tuy nhiên, mỗi phần mềm được phát triển một cách độc lập và vào những thời điểm khác nhau nên các vấn đề phát sinh Các phần mềm không thể giao tiếp với nhau nên không thể trao đổi dữ liệu Công nghệ được áp dụng trong việc phát triển phần mềm cũng khác nhau. Thí dụ, phần mềm quản lý kế toán được viết bằng Visual FoxPro dùng hệ quản trị cơ sở dữ liệu SQL Server nhưng phần mềm quản lý nhân sự được viết bằng C# dùng hệ quản trị cơ sở dữ liệu Oracle Cùng một thông tin nhưng được biểu diễn theo nhiều cách khác nhau trong các phần mềm khác nhau. Thí dụ, với mã nhân viên được quản lý kiểu ký tự với 4 ký tự trong phần mềm kế toán nhưng lại được quản lý kiểu số với 5 con số trong phần mềm quản lý nhân sự. Vấn đề đặt ra là phải tìm giải pháp để tích hợp các phần mềm độc lập thành một hệ thống thông tin thống nhất thoả các yêu cầu sau: 1. Các phần mềm có thể trao đổi dữ liệu với nhau một cách tự động. Mỗi sự thay đổi thông tin trên phần mềm này phải được cập nhật đến phần mềm kia 2. Việc tích hợp hệ thống thông tin không làm thay đổi các phần mềm đã có (nếu có thể) 3. Dữ liệu của các phần mềm đã có thể sử dụng trong việc phát triển phần mềm mới trong hệ thống thông tin tích hợp Để giải quyết vấn đề này có nhiều giải pháp thực hiện: Tích hợp ở mức cơ sở dữ liệu: với giải pháp này người tích hợp ứng dụng có thể dựa vào đặc điểm đồng bộ hoá dữ liệu của các hệ quản trị cơ sở dữ liệu hoặc viết các middleware để thực hiện việc trao đổi dữ liệu dữ các phần mềm. Tích hợp ở mức ứng dụng: tiếp cận này đòi hỏi phải hiệu chỉnh lại các phần mềm đã có. Điều này, sẽ vi phạm với yêu cầu 2. Tuy nhiên, tiếp cận này vẫn có thể áp dụng trong trường hợp các phần mềm được phát triển bởi cùng một nhà phát triển hoặc mã nguồn được chia sẻ. Để thực hiện đề tài này sinh viên cần thực hiện các yêu cầu sau: Về lí thuyết cần nghiên cứu: các giải pháp tích hợp hệ thống thông tin Xây dựng chương trình: i. Viết các công cụ để đóng vai trò middleware để thực hiện việc trao đổi dữ liệu giữa các phần mềm. ii. Khai thác tính năng đồng bộ dữ liệu của các hệ quản trị cơ sở dữ liệu để thực hiện việc trao đổi dữ liệu giữa các phần mềm. Có thể viết các script với dạng store procedure. iii. Phương pháp hiệu chỉnh các phần mềm đã có để chúng có thể giao tiếp với nhau. 24. Tài liệu tham khảo.\n\n\f[13] Nguyễn Hoàng Việt, Nguyễn Phú Trường. Tích hợp hệ thống thông tin trường Đại học Cần Thơ. 2005. [14] System Integration Solutions, tham khảo tại địa chỉ: http://www.altera.com/technology/integration/int-index.html\n\n\fProposition de mémoire 2007-2008 Classification de Spams Responsables: The-Phi Pham1, Thanh-Nghi Do2\n1\n\nLaboratoire : Faculté des Technologies de l’Information Adresse : Université de Can Tho, 1 rue Ly Tu Trong, Can Tho Mail : ptphi@cit.ctu.edu.vn 2 Laboratoire : INRIA Futurs Adresse : L.R.I., Université Paris-Sud, Bât. 490, 91405 Orsay Cedex Mail : dtnghi@lri.fr Objectifs. Ces dernières années, les utilisateurs d’internet reçoivent nombreux de messages non sollicités, souvent publicitaires, envoyés en grand nombre, on parle de Spam. Une étude du Spam [Sung-jin, 2003] a rapporté que le coût du Spam s’est élevé à 2 milliards de dollars pour l’année 2002. Une autre étude [Mi2g, 2003] a calculé que pendant le mois d’octobre 2003 le coût du Spam a été de 10,4 milliards de dollars. D’après [Doug, 2003], si un spammeur (une personne qui crée des Spams) gagne 10 mille dollars par mois alors le coût de ses Spams est de 100 mille dollars. La lutte contre le Spam est indispensable pour réduire les gaspillages de ressources et de temps du monde informatique. L’objectif de ce mémoire est d’étudier les approches qui permettant de classifier les messages en Spam ou non Spam. Pour ce faire, l’étudiant(e) devra créer une base de spams à partir des courriers électroniques que l’on a reçus. Ensuite c’est nécessaire de transformer la base de spams, initialement en format texte, en une représentation numérique à l’aide de l’outil Bow [McCallum, 1998]. Ceci comprend l’extraction de termes et la sélection des termes les plus pertinents. Une fois ce prétraitement terminé, on peut représenter les textes sous la forme de vecteurs numériques que les algorithmes automatiques peuvent traiter. On va utiliser les algorithmes des arbres de décision, C4.5 [Quinlan, 1993] et une forêt aléatoire (Random Forest) [Breiman, 2001] pour catégoriser de spams. La mesure de qualité des résultats obtenus [Uren, 2000] prend en compte le taux de précision, le taux de rappel. Références [Breiman, 2001] L. Breiman. Random Forests. Machine Learning, 45(1), pp. 5-32, 2001. [Do et Fekete, 2007] T-N. Do et J-D. Fekete. Flot visuel de données. (à paraître) RNTI – Série Extraction et Gestion des Connaissances, Cépaduès Editions, 2007. [Do et Poulet, 2004] T-N. Do et F. Poulet. La catégorisation de textes. Rapport de contrat Fondation Vediorbis, ESIEA Recherche, Laval, 2004. [Doug, 2003] Doug. Spam’s Economic Damage. Doug’s Inner Net News 15-12-2003, 2003. [McCallum, 1998] A. McCallum. Bow: A Toolkit for Statistical Language Modeling, Text Retrieval, Classification and Clustering. 1998. http://www-2.cs.cmu.edu/~mccallum/bow. [Mi2g, 2003] Mi2g. Spam Overtakes Malware and Hacking Damage. Mi2g 06-11-2003, 2003. [Pham et al., 2007] N-K. Pham, T-N. Do et F. Poulet. Catégorisation de textes avec Boosting de PSVM. (à paraître) actes de la conf. nationale des technologies de l’information, Da Lat, Vietnam. [Quinlan, 1993] J. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, 1993. [Sung-jin, 2003] K. Sung-jin. Internet Users Sustain $2 Bil. In Damages Due to Spam Mail. Korea Times 12-01-2003, 2003.\n\n\f[Uren, 2000] V. Uren. An Evaluation of Text Categorisation Errors. Proceeding of the One-Day Workshop on Evaluation of Info. Management Systems, Queen Mary and Westfield College, London, UK, 2000, pp. 79-87.\n\n\fNghiên cứu phân loại Spams Giáo viên hướng dẫn: Ths. Đỗ Hiệp Thuận, Ts. Đỗ Thanh Nghị Bộ môn hệ thống máy tính và truyền thông Khoa công nghệ thông tin Trường Đại học cần thơ email: dhthuan@cit.ctu.edu.vn, dtnghi@cit.ctu.edu.vn Mục tiêu. Trong những năm qua, người dùng máy tính trên toàn cầu nhận nhiều thư điện tử không phục vụ cho mục đích sử dụng máy tính của họ, thường là những quảng cáo, thư phản động, chơi đánh bạc, thậm chí là những đoạn mã độc hại, đồi trụy khác, mà chúng ta gọi đó là spam. Một nghiên cứu của [Sungjin, 2003] đã cho biết tổn thất của người dùng máy tính lên đến 2 tỉ đô la trong năm 2002. Một nghiên cứu khác của [Mi2g, 2003] cưng cho biết trong tháng 10 năm 2003, tổn thất này lên đến 10,4 tỉ đô la. Theo [Doug, 2003], nếu một người phát tán spam thu được lợi 10 ngàn đô la trong 1 tháng thì tổn thất do họ gây ra là 100 ngàn đô la. Chính vì lẽ đó, việc chống spam cần được quan tâm hơn. Mục tiêu của đề tài là nghiên cứu những phương pháp phân loại thư điện tử để nhận biết spam. Để làm được điều này, sinh viên nên tạo tập dữ liệu thư điện tử gồm có spam và không phải là spam. Sau đó sẽ chuyển đổi cách biểu diễn tập dữ liệu về với dạng bảng dữ liệu véc tơ kiểu số dự trên thư viện Bow [McCallum, 1998]. Nó bao gồm các bước phân tích từ vựng và chọn tập hợp từ mà có thể dùng để phân loại thư spam. Tiếp theo là sinh viên sẽ sử dụng giải thuật máy học cây quyết định C4.5 [Quinlan, 1993] et giải thuật Bayes « thơ ngây » để phân loại thư spam. Đánh giá kết quả đạt được [Uren, 2000] dựa trên các tiêu chí precision và recall. Số lượng sinh viên tham gia: 2 sinh viên. Tài liệu tham khảo [Doug, 2003] Doug. Spam’s Economic Damage. Doug’s Inner Net News 15-12-2003, 2003. [McCallum, 1998] A. McCallum. Bow: A Toolkit for Statistical Language Modeling, Text Retrieval, Classification and Clustering. 1998. http://www-2.cs.cmu.edu/~mccallum/bow. [Mi2g, 2003] Mi2g. Spam Overtakes Malware and Hacking Damage. Mi2g 06-11-2003, 2003. [Pham et al., 2007] N-K. Pham, T-N. Do et F. Poulet. Phân loại văn bản với Boosting PSVM. Hội thảo quốc gia về công nghệ thông tin và truyền thông, Đà lạt, 2006. [Quinlan, 1993] J. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, 1993. [Sung-jin, 2003] K. Sung-jin. Internet Users Sustain $2 Bil. In Damages Due to Spam Mail. Korea Times 12-01-2003, 2003. [Uren, 2000] V. Uren. An Evaluation of Text Categorisation Errors. Proceeding of the One-Day Workshop on Evaluation of Info. Management Systems, Queen Mary and Westfield College, London, UK, 2000, pp. 79-87.\n\n\fKhai mỏ dữ liệu với ngôn ngữ R Giáo viên hướng dẫn: Th.s Đỗ Hiệp Thuận, Ts. Đỗ Thanh Nghị Bộ môn hệ thống máy tính và truyền thông Khoa công nghệ thông tin Trường Đại học cần thơ email : dhthuan@cit.ctu.edu.vn dtnghi@cit.ctu.edu.vn Mục tiêu. Trong những năm 1990, cuộc cách mạng kỹ thuật số cho phép số hóa thông tin dễ dàng và chi phí thấp, thêm vào đó là sự phát triển của công nghệ thông tin bao gồm cả phần cứng lẫn phần mềm, công nghệ truyền thông, web, internet đã góp phần đưa máy tính vào các sinh hoạt thường nhật của con người. Tất cả các họat động kinh doanh, vui chơi giải trí, nghiên cứu khoa học, giáo dục, truyền thông,… đều có sự hỗ trợ của máy tính. Hệ quả là một khối lượng lớn dữ liệu được sinh ra và lưu trữ trong các cơ sở dữ liệu, thiết bị lưu trữ như băng từ, đĩa từ. Năm 1999, Giáo sư P. Lyman, Đại học Berkeley đã tiến hành thống kê dữ liệu được sinh ra hằng năm trên toàn cầu. Trong năm 2002-2003 (tham khảo ở địa chỉ http://www.sims.berkeley.edu/research/projects/how-much-info-2003/), dữ liệu tăng 5 Exabytes (5. 1018 bytes). U. Fayyad et al. [8] ước lượng dữ liệu toàn cầu tăng gấp đôi trong vòng 9 tháng. Khám phá tri thức là một quá trình lặp phức tạp được định nghĩa bởi [7] sử dụng nhiều kỹ thuật như cơ sở dữ liệu, máy học, phương pháp thống kê trong phân tích dữ liệu, hiển thị dữ liệu, tối ưu hóa, trí tuệ nhân tạo, nhằm tìm ra những kiến thức cần thiết, tìm kiếm tri thức từ kho dữ liệu khổng lồ để phân loại, quản lý trở thành vấn đề rất được quan tâm. Mục tiêu của đề tài là nghiên cứu ngôn ngữ lập trình hàm cấp cao R dành cho phân tích dữ liệu, khám phá tri thức và khai mỏ dữ liệu. Đây là một phần mềm miễn phí mã nguồn mở, rất dễ học và có thể phát triển nhanh các ứng dụng khai mỏ dữ liệu trong thời gian ngắn. R là môi trường thích hợp cho việc giảng dạy, học tập và nghiên cứu trong trường đại học về khai mỏ dữ liệu. Không phải tốn chi phí cho bản quyền, hơn nữa R hỗ trợ rất nhiều công cụ hữu ích cho quá trình khai mỏ dữ liệu như: các giải thuật học tự động của cây quyết định, phân cụm, mạng nơron, máy học vectơ hỗ trợ, hồi quy và các giao diện truy vấn dữ liệu, hiển thị dữ liệu. Có thể lập trình một cách dễ dàng trong R. Sinh viên cần chỉ ra làm thế nào có thể khai mỏ dữ liệu với môi trường R. Nội dung thực hiện bao gồm trình bày tóm tắt cơ bản về ngôn ngữ R và khả năng lập trình trong R. Tiếp theo, trình bày các hỗ trợ của R cho quá trình khai mỏ dữ liệu như : nhập xuất dữ liệu, hiển thị dữ liệu với và thực hiện các giải thuật khai mỏ dữ liệu. Số lượng sinh viên tham gia: 1 sinh viên. Tài liệu tham khảo [3] R.A. Becker, J.M. Chambers and A.R. Wilks.: The New S Language: A Programming Environment for Data Analysis and Graphics. Chapman & Hall, 1988. [4] C. Blake and C. Merz.: UCI Repository of Machine Learning Databases. 1998. http://www.ics.uci.edu/~mlearn/MLRepository.html [5] L. Breiman, J. Friedman, R. Olshen and C. Stone.: Classification and Regression Trees. Chapman & Hall, New York, 1984. [6] L. Breiman.: Random Forests. Machine Learning, 45(1), pp. 5-32, 2001. [7] M.J. Crawley.: Statistics: An Introduction using R. Wiley, 2005. [8] T-N. Do, N-K. Pham, H-T. Do and N-C. Lam. Khai mỏ dữ liệu với ngôn ngữ R. in proc. of FAIR’07, The Third National Symposium Fundamental & Applied IT Research, Vietnam, 2007.\n\n\f[9] U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth.: From Data Mining to Knowledge Discovery in Databases. AI Magazine, 17(3), pp.37-54, 1996. [10] U. Fayyad, G. Piatetsky-Shapiro, and R. Uthurusamy.: Summary from the KDD-03 Panel – Data Mining: The Next 10 Years. in SIGKDD Explorations, 5(2), pp.191-196, 2004. [11] U. Fayyad, G. Grinstein, and A. Wierse.: Information Visualization in Data Mining and Knowledge Discovery. Morgan Kaufmann Publishers, 2001. [12] R. Ihaka and R. Gentleman.: R: A language for data analysis and graphics. Journal of Computational and Graphical Statistics, 5(3), pp. 299-314, 1996. [13] D. Keim.: Databases and Visualization. Tutorial Notes, ACM-SIGMOD’96, 1996. [14] T. Kohonen.: Self-Organizing Maps. Springer, Berlin, Heidelberg, New York, 1995. [15] J. Maindonald and J. Braun.: Data Analysis and Graphics Using R. Cambridge University Press, 2003. [16] J. MacQueen.: Some Methods for classification and Analysis of Multivariate Observations. Proceeding of 5th Berkeley Symposium on Mathematical Statistics and Probability, Berkeley, University of California Press, Vol. 1, pp. 281-297, 1967. [17] J-R. Quinlan.: C4.5: Programs for Machine Learning. Morgan-Kaufman Publishers, 1993. [18] P. Spector.: An Introduction to S and S-Plus. Duxbury Press, 1994. [19] P. Spector.: An Introduction to R. Statistical Computing Facility, University of California, Berkeley, 2004. [20] The Comprehensive R Archive Network (CRAN). http://cran.r-project.org\n[21] V. Vapnik.: The Nature of Statistical Learning Theory. Springer-Verlag, New York, 1995. [22] W. N. Venables and D. M. Smith.: An Introduction to R. Network Theory, 2002. [23] P. Burns.: An Introduction to the S Language. 2002.\n\n\fThis action might not be possible to undo. Are you sure you want to continue?Use one of your book credits to continue reading from where you left off, or restart the preview.",
          "relevence": "no"
        },
        {
          "url": "http://jbis.ueh.edu.vn/index.php/TSTHQL/article/view/20",
          "title": "SỬ DỤNG THUẬT TOÁN GOM CỤM MỜ KHAI PHÁ CƠ SỞ DỮ LIỆU ERP TRONG DOANH NGHIỆP DƯỢC PHẨM | Thuân | Tập San Tin Học Quản Lý - Khoa CNTTKD",
          "content": "Thuật toán gom cụm Fuzzy c-means và Fuzzy c-means kết hợp véctor  trọng số được được sử dụng  trong bài báo này nhằm gom cụm dữ  liệu  trong cơ  sở dữ  liệu ERP doanh nghiệp. Mục đích  là  thực nghiệm, so sánh hai  thuật  toán này  trong việc  tìm kiếm các nhóm dữ liệu phù hợp cụ thể ở đây là các nhóm khách hàng có cùng những đặc tính như doanh số, lợi nhuận, số tiền thanh toán, nợ quá hạn... nhằm hỗ trợ cho ban lãnh đạo doanh nghiệp có các quyết định đúng đắn khi đề ra các chính sách  thích ứng cho  từng nhóm khách hàng này, từ đó gia tăng hiệu quả hoạt động kinh doanh cho doanh nghiệp.ĐOÀN HUẤN, Các tài liệu, hồ sơ khảo sát, phân tích thiết kế phần mềm ERP, Công ty Cổ phần Giải pháp Phần mềm EnterSoft. 2001-2011.NGUYEN BICH LIEN, DO PHUC. An application of data mining to revenue cycle in ERP and E-commerce environment. Proceedings of the sixth international conference on Information Technology for Education and Research. 2010.TAOYING LI AND YAN CHEN. Fuzzy K-Means Incremental Clustering Based on K-Center and Vector Quantization. Journal of computers, Vol 5, No 11, Nov 2010.ĐỖ PHÚC. Giáo trình Khai thác dữ liệu, Nhà xuất bản Đại học Quốc gia TPHCM - 2009.",
          "relevence": "yes"
        },
        {
          "url": "https://idoc.vn/threads/243608/",
          "title": "Tìm hiểu thuật toán gom cụm sử dụng liên kết đơn và cài đặt chương trình minh họa | Website chia sẻ tài liệu, luận văn, đồ án, khóa luận tốt nghiệp.",
          "content": "\n\tThảo luận trong 'Luận văn CNTT' bắt đầu bởi Uploader02, 25/3/15.\nBạn vui lòng nhập mã thẻ cào dưới lớp tráng bạc trên thẻ.(Không giới hạn thời gian sử dụng)Nạp Tiền Qua SMS Hiện ₫ang Nâng Cấp, Vui Lòng Nạp Tiền BằngThẻ ₫iện Thoại   hoặc   Tài khoản Ngân Hàng\n\t\t\tNạp Tiền Qua Ngân Hàng Hiện ₫ang Nâng Cấp, Vui Lòng Nạp Tiền BằngThẻ ₫iện Thoại   hoặc   Nạp Tiền SMS\n\t\t\tĐăng nhập idoc.vn bằng tài khoản Facebook của bạn.Tôi không có hoặc không vào ₫ược Facebook",
          "relevence": "yes"
        },
        {
          "url": "https://www.researchgate.net/publication/305983637_Gom_cum_du_lieu_web_video_dua_tren_huong_tiep_can_early_fusion_cho_dac_trung_van_ban",
          "title": "Gom cụm dữ liệu web video dựa trên hướng tiếp cận early fusion cho đặc trưng văn bản (PDF Download Available)",
          "content": "",
          "relevence": "no"
        },
        {
          "url": "http://deantinhoc-cntt.blogspot.com/2014/01/nghien-cuu-thuat-toan-gom-cum-k-means.html",
          "title": "Đề Án Tin Học - CNTT.:   Nghiên cứu thuật toán gom cụm K-means và cài đặt chương trình Demo (C#).",
          "content": "",
          "relevence": "yes"
        },
        {
          "url": "http://ungdung.khoa-hnvd.com/Hoc_thuat/Cai_tien_KMeans.html",
          "title": "THUẬT TOÁN CẢI TIẾN KMEANS",
          "content": "Cải tiến thuật toán KMeanVề trang chủÝ tưởng cải tiến  Hình : Hình dạng cụm  không cân đối trong các cụm dạng này α thường lớn do dmax lớn.\r\n  Hình : Hình dạng cụm  khá cân đối trong các cụm dạng này α thường nhỏ.\r\n  Nếu  kết hợp thêm dmax (khoảng cách lớn nhất giữa các phần tử trong cụm)  là lớn thì rõ ràng cụm đang xét có kích thước khá lớn và các phần tử trong các  cụm dạng này càng có tính tương đồng không cao. Nên đây cũng là một dấu hiệu phụ  cho ta biết khi nào cần phải tách cụm ngoài dấu hiệu chính từ hệ số trong α ở  trên. Dĩ nhiên khái niệm độ lớn của dmax là tùy thuộc vào từng bài  toán cụ thể.\r\n  Gọi αmax  là hệ số trong lớn nhất trong số các hệ số trong α của tất cả các cụm sau khi  gom cụm.\r\n  Để  có một gom cụm tốt, rõ ràng sau khi gom cụm, mà còn tồn tại một cụm nào đó có hệ  số trong α lớn tức là αmax cũng lớn thì cần phải tách cụm đó tức là  phải tăng số cụm cần gom.\r\n  Do  đó α , αmax và dmax sẽ được tính toán để làm các giá trị  định lượng một cách có cơ sở giúp cho việc định hướng khuyến nghị điều chỉnh số  cụm.\r\n  Khi  hệ số ngoài β (là tỷ số giữa khoảng cách nhỏ nhất từ trung tâm cụm này (giả sử  cụm 1) đến các phần tử của cụm khác (giả sử cụm 2) và khoảng cách trung bình từ  trung tâm cụm này đến các phần tử của cụm khác đó) càng tiến gần đến 1 thì cụm  2 có kích thước khá nhỏ và dẹp (β =1 khi cụm 2 chỉ có một phần tử). Đây là một  dấu hiệu cho thấy xu hướng ghép cụm. Như vậy khi hệ số β càng tiến gần đến 1 sẽ  cho ta dấu hiệu càng cần phải ghép cụm. \r\n   \r\n  Hình:  Hình dạng hai cụm có xu hướng ghép vào nhau khi β tiến gần đến 1\r\nNếu  kết hợp thêm ϕmin (khoảng cách nhỏ nhất từ trung tâm cụm này đến cụm  khác) là nhỏ thì rõ ràng cụm 2 nằm sát biên của cụm 1 và kích thước cụm 2 là nhỏ,  dẹp và phân bố sát theo biên của cụm 1 nên nó có xu hướng ghép vào cụm 1 và nếu  được ghép vào thì hầu như vẫn giữ được tính tương đồng trong cụm mới. Nên đây  cũng là một dấu hiệu phụ cho ta biết khi nào cần phải ghép cụm ngoài dấu hiệu  chính từ hệ số trong β ở trên. Dĩ nhiên khái niệm độ lớn hay nhỏ của ϕmin  là tùy thuộc vào từng bài toán cụ thể.Các bước của thuật toán K-Means cải tiến như  sau: ,     j = 1…n                             Cjl  =      với  j = 1…c,    l = 1,..kdji  = \r\nVới k là số thuộc tính của xi,    j = 1,..c,      i = 1,…nNếu  dji > 0 thì   μji =  \r\nNgược  lại nếu dji = 0 thì xji trùng với trọng tâm Cj  của cụm j, μji = 1.Để xác định  là U thay đổi nhỏ chúng tôi dùng:\r\n  | - | < epsilon.   Với nghĩa  là  tại bước lặp thứ  n.dmax  =    \r\nVới  j = 1,..p,   i = 1,…p,  i # j , p là số phần tử  của cụm, k là số thuộc tính, còn q là số khoảng cách giữa các phần tử  trong cụm.davr  =       \r\nVới  j = 1,..p,  i = 1,…p,   i # j , p là số phần tử của cụm, k là số thuộc tính, còn q là  số khoảng cách giữa các phần tử trong cụm.ϕmin  =   \r\nVới Cj  là trung tâm của cụm j, i = 1,…q,  với q  là số phần tử của cụm x,  k là số thuộc  tính và q cũng chính là số khoảng cách từ trung tâm cụm j đến q phần tử của cụm  x.ϕavr  =       \r\nVới Cj  là trung tâm của cụm j, i = 1,…q,  với q  là số phần tử của cụm x,  k là số thuộc  tính và q cũng chính là số khoảng cách từ trung tâm cụm j đến q phần tử của cụm  x.Dựa trên kết quả tính toán các hệ số  khuyến nghị điều chỉnh số cụm α, αmax, β, βmax nếu xét thấy  việc gom cụm chưa phù hợp thì quay lại bước 4 điều chỉnh số cụm theo chỉ dấu khuyến nghị, ngược lại thuật  toán kết thúc.Video Demo Cải tiến KMean  Download Demo\r\n    Hệ điều hành: Windows XP, 7,8 \r\nMicrosoft Frameworks: 4.0\r\nFile chạy : Gom_Cum_Hoc_Sinh.exeLiên hệ Email : vinhvinhit@gmail.com.Trao đổi thông tin cải tiến KMeans. (Nhận coding thuật toán KMeans cải tiến theo yêu cầu, đề tài, ngôn ngữ C#)\r\nLàm luận văn cải tiến thuật toán KMeans.",
          "relevence": "yes"
        },
        {
          "url": "https://books.google.com.vn/books/about/Ph%C3%A1t_tri%E1%BB%83n_thu%E1%BA%ADt_to%C3%A1n_khai_ph%C3%A1_t.html?id=BPKjnQAACAAJ&redir_esc=y",
          "title": "Ph�t triển thuật to�n khai ph� tập thường xuy�n dựa v�o sự gom cụm dữ liệu - Thị Hi�n Mai - Google S�ch",
          "content": "Duyệt eBookstore lớn nhất của thế giới v� bắt đầu đọc ngay h�m nay tr�n web, m�y t�nh bảng, điện thoại hoặc thiết bị đọc s�ch điện tử.Chuyển đến Google Play ngay b�y giờ »",
          "relevence": "yes"
        },
        {
          "url": "http://jbis.ueh.edu.vn/index.php/TSTHQL/article/view/20",
          "title": "SỬ DỤNG THUẬT TOÁN GOM CỤM MỜ KHAI PHÁ CƠ SỞ DỮ LIỆU ERP TRONG DOANH NGHIỆP DƯỢC PHẨM | Thuân | Tập San Tin Học Quản Lý - Khoa CNTTKD",
          "content": "Thuật toán gom cụm Fuzzy c-means và Fuzzy c-means kết hợp véctor  trọng số được được sử dụng  trong bài báo này nhằm gom cụm dữ  liệu  trong cơ  sở dữ  liệu ERP doanh nghiệp. Mục đích  là  thực nghiệm, so sánh hai  thuật  toán này  trong việc  tìm kiếm các nhóm dữ liệu phù hợp cụ thể ở đây là các nhóm khách hàng có cùng những đặc tính như doanh số, lợi nhuận, số tiền thanh toán, nợ quá hạn... nhằm hỗ trợ cho ban lãnh đạo doanh nghiệp có các quyết định đúng đắn khi đề ra các chính sách  thích ứng cho  từng nhóm khách hàng này, từ đó gia tăng hiệu quả hoạt động kinh doanh cho doanh nghiệp.ĐOÀN HUẤN, Các tài liệu, hồ sơ khảo sát, phân tích thiết kế phần mềm ERP, Công ty Cổ phần Giải pháp Phần mềm EnterSoft. 2001-2011.NGUYEN BICH LIEN, DO PHUC. An application of data mining to revenue cycle in ERP and E-commerce environment. Proceedings of the sixth international conference on Information Technology for Education and Research. 2010.TAOYING LI AND YAN CHEN. Fuzzy K-Means Incremental Clustering Based on K-Center and Vector Quantization. Journal of computers, Vol 5, No 11, Nov 2010.ĐỖ PHÚC. Giáo trình Khai thác dữ liệu, Nhà xuất bản Đại học Quốc gia TPHCM - 2009.",
          "relevence": "yes"
        },
        {
          "url": "https://machinelearningcoban.com/2017/01/01/kmeans/",
          "title": "Machine Learning cơ bản",
          "content": "Trong trang này:\nTrong bài trước, chúng ta đã làm quen với thuật toán Linear Regression - là thuật toán đơn giản nhất trong Supervised learning. Bài này tôi sẽ giới thiệu một trong những thuật toán cơ bản nhất trong Unsupervised learning - thuật toán K-means clustering (phân cụm K-means).Trong thuật toán K-means clustering, chúng ta không biết nhãn (label) của từng điểm dữ liệu. Mục đích là làm thể nào để phân dữ liệu thành các cụm (cluster) khác nhau sao cho dữ liệu trong cùng một cụm có tính chất giống nhau.Ví dụ: Một công ty muốn tạo ra những chính sách ưu đãi cho những nhóm khách hàng khác nhau dựa trên sự tương tác giữa mỗi khách hàng với công ty đó (số năm là khách hàng; số tiền khách hàng đã chi trả cho công ty; độ tuổi; giới tính; thành phố; nghề nghiệp; …). Giả sử công ty đó có rất nhiều dữ liệu của rất nhiều khách hàng nhưng chưa có cách nào chia toàn bộ khách hàng đó thành một số nhóm/cụm khác nhau. Nếu một người biết Machine Learning được đặt câu hỏi này, phương pháp đầu tiên anh (chị) ta nghĩ đến sẽ là K-means Clustering. Vì nó là một trong những thuật toán đầu tiên mà anh ấy tìm được trong các cuốn sách, khóa học về Machine Learning. Và tôi cũng chắc rằng anh ấy đã đọc blog Machine Learning cơ bản. Sau khi đã phân ra được từng nhóm, nhân viên công ty đó có thể lựa chọn ra một vài khách hàng trong mỗi nhóm để quyết định xem mỗi nhóm tương ứng với nhóm khách hàng nào. Phần việc cuối cùng này cần sự can thiệp của con người, nhưng lượng công việc đã được rút gọn đi rất nhiều.Ý tưởng đơn giản nhất về cluster (cụm) là tập hợp các điểm ở gần nhau trong một không gian nào đó (không gian này có thể có rất nhiều chiều trong trường hợp thông tin về một điểm dữ liệu là rất lớn). Hình bên dưới là một ví dụ về 3 cụm dữ liệu (từ giờ rôi sẽ viết gọn là cluster).Giả sử mỗi cluster có một điểm đại diện (center) màu vàng. Và những điểm xung quanh mỗi center thuộc vào cùng nhóm với center đó. Một cách đơn giản nhất, xét một điểm bất kỳ, ta xét xem điểm đó gần với center nào nhất thì nó thuộc về cùng nhóm với center đó. Tới đây, chúng ta có một bài toán thú vị: Trên một vùng biển hình vuông lớn có ba đảo hình vuông, tam giác, và tròn màu vàng như hình trên. Một điểm trên biển được gọi là thuộc lãnh hải của một đảo nếu nó nằm gần đảo này hơn so với hai đảo kia . Hãy xác định ranh giới lãnh hải của các đảo.Hình dưới đây là một hình minh họa cho việc phân chia lãnh hải nếu có 5 đảo khác nhau được biểu diễn bằng các hình tròn màu đen:Chúng ta thấy rằng đường phân định giữa các lãnh hải là các đường thẳng (chính xác hơn thì chúng là các đường trung trực của các cặp điểm gần nhau). Vì vậy, lãnh hải của một đảo sẽ là một hình đa giác.Cách phân chia này trong toán học được gọi là Voronoi Diagram.Trong không gian ba chiều, lấy ví dụ là các hành tinh, thì (tạm gọi là) lãnh không của mỗi hành tinh sẽ là một đa diện. Trong không gian nhiều chiều hơn, chúng ta sẽ có những thứ (mà tôi gọi là) siêu đa diện (hyperpolygon).Quay lại với bài toán phân nhóm và cụ thể là thuật toán K-means clustering, chúng ta cần một chút phân tích toán học trước khi đi tới phần tóm tắt thuật toán ở phần dưới. Nếu bạn không muốn đọc quá nhiều về toán, bạn có thể bỏ qua phần này. (Tốt nhất là đừng bỏ qua, bạn sẽ tiếc đấy).\n\nMục đích cuối cùng của thuật toán phân nhóm này là: từ dữ liệu đầu vào và số lượng nhóm chúng ta muốn tìm, hãy chỉ ra center của mỗi nhóm và phân các điểm dữ liệu vào các nhóm tương ứng. Giả sử thêm rằng mỗi điểm dữ liệu chỉ thuộc vào đúng một nhóm.Giả sử có \\(N\\) điểm dữ liệu là \\( \\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_N] \\in \\mathbb{R}^{d \\times N}\\) và \\(K < N\\) là số cluster chúng ta muốn phân chia. Chúng ta cần tìm các center \\( \\mathbf{m}_1, \\mathbf{m}_2, \\dots, \\mathbf{m}_K \\in \\mathbb{R}^{d \\times 1} \\) và label của mỗi điểm dữ liệu.Lưu ý về ký hiệu toán học: trong các bài viết của tôi, các số vô hướng được biểu diễn bởi các chữ cái viết ở dạng không in đậm, có thể viết hoa, ví dụ \\(x_1, N, y, k\\). Các vector được biểu diễn bằng các chữ cái thường in đậm, ví dụ \\(\\mathbf{m}, \\mathbf{x}_1 \\). Các ma trận được biểu diễn bởi các chữ viết hoa in đậm, ví dụ \\(\\mathbf{X, M, Y} \\). Lưu ý này đã được nêu ở bài Linear Regression. Tôi xin được không nhắc lại trong các bài tiếp theo.\nVới mỗi điểm dữ liệu \\( \\mathbf{x}_i \\) đặt \\(\\mathbf{y}_i = [y_{i1}, y_{i1}, \\dots, y_{iK}]\\) là label vector của nó, trong đó nếu \\( \\mathbf{x}_i \\) được phân vào cluster \\(k\\) thì  \\(y_{ik} = 1\\) và \\(y_{ij} = 0, \\forall j \\neq k \\). Điều này có nghĩa là có đúng một phần tử của vector \\(\\mathbf{y}_i\\) là bằng 1 (tương ứng với cluster của \\(\\mathbf{x}_i \\)), các phần tử còn lại bằng 0. Ví dụ: nếu một điểm dữ liệu có label vector là \\([1,0,0,\\dots,0]\\) thì nó thuộc vào cluster 1, là \\([0,1,0,\\dots,0]\\) thì nó thuộc vào cluster 2, \\(\\dots\\). Cách mã hóa label của dữ liệu như thế này được goi là biểu diễn one-hot. Chúng ta sẽ thấy cách biểu diễn one-hot này rất phổ biến trong Machine Learning ở các bài tiếp theo.Ràng buộc của \\(\\mathbf{y}_i \\) có thể viết dưới dạng toán học như sau:\n\\[\n y_{ik} \\in \\{0, 1\\},~~~ \\sum_{k = 1}^K y_{ik} = 1 ~~~ (1)\n\\]\n\n\n\n\nNếu ta coi center \\(\\mathbf{m}_k \\)  là center (hoặc representative) của mỗi cluster và ước lượng tất cả các điểm được phân vào cluster này bởi \\(\\mathbf{m}_k \\), thì một điểm dữ liệu \\(\\mathbf{x}_i \\) được phân vào cluster \\(k\\) sẽ bị sai số là \\( (\\mathbf{x}_i - \\mathbf{m}_k) \\). Chúng ta mong muốn sai số này có trị tuyệt đối nhỏ nhất nên (giống như trong bài Linear Regression) ta sẽ tìm cách để đại lượng sau đây đạt giá trị nhỏ nhất: \n\\[\n\\|\\mathbf{x}_i - \\mathbf{m}_k\\|_2^2\n\\]Hơn nữa, vì \\(\\mathbf{x}_i \\) được phân vào cluster \\(k\\) nên \\(y_{ik} = 1, y_{ij} = 0, ~\\forall j \\neq k \\). Khi đó, biểu thức bên trên sẽ được viết lại là:\n\\[\ny_{ik}\\|\\mathbf{x}_i - \\mathbf{m}_k\\|_2^2 =  \\sum_{j=1}^K y_{ij}\\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2\n\\](Hy vọng chỗ này không quá khó hiểu)Sai số cho toàn bộ dữ liệu sẽ là: \n\\[\n\\mathcal{L}(\\mathbf{Y}, \\mathbf{M}) = \\sum_{i=1}^N\\sum_{j=1}^K y_{ij} \\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2\n\\]Trong đó \\( \\mathbf{Y} = [\\mathbf{y}_1; \\mathbf{y}_2; \\dots; \\mathbf{y}_N]\\), \\( \\mathbf{M} = [\\mathbf{m}_1, \\mathbf{m}_2, \\dots \\mathbf{m}_K] \\) lần lượt là các ma trận được tạo bởi label vector của mỗi điểm dữ liệu và center của mỗi cluster. Hàm số mất mát trong bài toán K-means clustering của chúng ta là hàm \\(\\mathcal{L}(\\mathbf{Y}, \\mathbf{M})\\) với ràng buộc như được nêu trong phương trình \\((1)\\).Tóm lại, chúng ta cần tối ưu bài toán sau: \n\\[\n\\mathbf{Y}, \\mathbf{M} = \\arg\\min_{\\mathbf{Y}, \\mathbf{M}} \\sum_{i=1}^N\\sum_{j=1}^K y_{ij} \\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2~~~~~(2)\n\\]\\[\n\\text{subject to:} ~~ y_{ij} \\in \\{0, 1\\}~~ \\forall i, j;~~~ \\sum_{j = 1}^K y_{ij} = 1~~\\forall i\n\\](subject to nghĩa là thỏa mãn điều kiện).Nhắc lại khái niệm \\(\\arg\\min\\): Chúng ta biết ký hiệu \\(\\min\\) là giá trị nhỏ nhất của hàm số, \\(\\arg\\min\\) chính là giá trị của biến số để hàm số đó đạt giá trị nhỏ nhất đó. Nếu \\(f(x) = x^2 -2x + 1 = (x-1)^2 \\) thì giá trị nhỏ nhất của hàm số này bằng 0, đạt được khi \\(x = 1\\). Trong ví dụ này \\(\\min_{x} f(x) = 0\\) và \\(\\arg\\min_{x} f(x) = 1\\). Thêm ví dụ khác, nếu \\(x_1 = 0, x_2 = 10, x_3 = 5\\) thì ta nói \\(\\arg\\min_{i} x_i = 1\\) vì \\(1\\) là chỉ số để \\(x_i\\) đạt giá trị nhỏ nhất (bằng \\(0\\)). Biến số viết bên dưới \\(\\min\\) là biến số cúng ta cần tối ưu. Trong các bài toán tối ưu, ta thường quan tâm tới \\(\\arg\\min\\) hơn là \\(\\min\\).\n\n\nBài toán \\((2)\\) là một bài toán khó tìm điểm tối ưu vì nó có thêm các điều kiện ràng buộc. Bài toán này thuộc loại mix-integer programming (điều kiện biến là số nguyên) - là loại rất khó tìm nghiệm tối ưu toàn cục (global optimal point, tức nghiệm làm cho hàm mất mát đạt giá trị nhỏ nhất có thể). Tuy nhiên, trong một số trường hợp chúng ta vẫn có thể tìm được phương pháp để tìm được nghiệm gần đúng hoặc điểm cực tiểu. (Nếu chúng ta vẫn nhớ chương trình toán ôn thi đại học thì điểm cực tiểu chưa chắc đã phải là điểm làm cho hàm số đạt giá trị nhỏ nhất).Một cách đơn giản để giải bài toán \\((2)\\) là xen kẽ giải \\(\\mathbf{Y}\\) và \\( \\mathbf{M}\\) khi biến còn lại được cố định. Đây là một thuật toán lặp, cũng là kỹ thuật phổ biến khi giải bài toán tối ưu. Chúng ta sẽ lần lượt giải quyết hai bài toán sau đây:Giả sử đã tìm được các centers, hãy tìm các label vector để hàm mất mát đạt giá trị nhỏ nhất. Điều này tương đương với việc tìm cluster cho mỗi điểm dữ liệu.Khi các centers là cố định, bài toán tìm label vector cho toàn bộ dữ liệu có thể được chia nhỏ thành bài toán tìm label vector cho từng điểm dữ liệu \\(\\mathbf{x}_i\\) như sau:\\[\n\\mathbf{y}_i = \\arg\\min_{\\mathbf{y}_i} \\sum_{j=1}^K y_{ij}\\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2 ~~~ (3)\n\\]\n\\[\n\\text{subject to:} ~~ y_{ij} \\in \\{0, 1\\}~~ \\forall j;~~~ \\sum_{j = 1}^K y_{ij} = 1\n\\]Vì chỉ có một phần tử của label vector \\(\\mathbf{y}_i\\) bằng \\(1\\) nên bài toán \\((3)\\) có thể tiếp tục được viết dưới dạng đơn giản hơn: \n\\[\nj = \\arg\\min_{j} \\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2\n\\]Vì \\(\\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2\\) chính là bình phương khoảng cách tính từ điểm \\(\\mathbf{x}_i \\) tới center \\(\\mathbf{m}_j \\), ta có thể kết luận rằng mỗi điểm \\(\\mathbf{x}_i \\) thuộc vào cluster có center gần nó nhất! Từ đó ta có thể dễ dàng suy ra label vector của từng điểm dữ liệu.Giả sử đã tìm được cluster cho từng điểm, hãy tìm center mới cho mỗi cluster để hàm mất mát đạt giá trị nhỏ nhất.Một khi chúng ta đã xác định được label vector cho từng điểm dữ liệu, bài toán tìm center cho mỗi cluster được rút gọn thành:\\[\n\\mathbf{m}_j = \\arg\\min_{\\mathbf{m}_j} \\sum_{i = 1}^{N} y_{ij}\\|\\mathbf{x}_i - \\mathbf{m}_j \\|_2^2.\n\\]\n Tới đây, ta có thể tìm nghiệm bằng phương pháp giải đạo hàm bằng 0, vì hàm cần tối ưu là một hàm liên tục và có đạo hàm xác định tại mọi điểm. Và quan trọng hơn, hàm này là hàm convex (lồi) theo \\(\\mathbf{m}_j \\) nên chúng ta sẽ tìm được giá trị nhỏ nhất và điểm tối ưu tương ứng. Sau này nếu có dịp, tôi sẽ nói thêm về tối ưu lồi (convex optimization) - một mảng cực kỳ quan trọng trong toán tối ưu.Đặt \\(l(\\mathbf{m}_j)\\) là hàm bên trong dấu \\(\\arg\\min\\), ta có đạo hàm:\n\\[\n\\frac{\\partial l(\\mathbf{m}_j)}{\\partial \\mathbf{m}_j} = 2\\sum_{i=1}^N y_{ij}(\\mathbf{m}_j - \\mathbf{x}_i) \n\\]Giải phương trình đạo hàm bằng 0 ta có: \n\\[\n\\mathbf{m}_j \\sum_{i=1}^N y_{ij} = \\sum_{i=1}^N y_{ij} \\mathbf{x}_i \n\\]\n\\[\n\\Rightarrow \\mathbf{m}_j = \\frac{ \\sum_{i=1}^N y_{ij} \\mathbf{x}_i}{\\sum_{i=1}^N y_{ij}}\n\\]Nếu để ý một chút, chúng ta sẽ thấy rằng mẫu số chính là phép đếm số lượng các điểm dữ liệu trong cluster \\(j\\) (Bạn có nhận ra không?). Còn tử số chính là tổng các điểm dữ liệu trong cluster \\(j\\). (Nếu bạn đọc vẫn nhớ điều kiện ràng buộc của các \\(y_{ij} \\) thì sẽ có thể nhanh chóng nhìn ra điều này).Hay nói một cách đơn giản hơn nhiều: \\(\\mathbf{m}_j\\) là trung bình cộng của các điểm trong cluster \\(j\\).Tên gọi K-means clustering cũng xuất phát từ đây.\n\nTới đây tôi xin được tóm tắt lại thuật toán (đặc biệt quan trọng với các bạn bỏ qua phần toán học bên trên) như sau:Đầu vào: Dữ liệu \\(\\mathbf{X}\\) và số lượng cluster cần tìm \\(K\\).Đầu ra: Các center \\(\\mathbf{M}\\) và label vector cho từng điểm dữ liệu \\(\\mathbf{Y}\\).Chúng ta có thể đảm bảo rằng thuật toán sẽ dừng lại sau một số hữu hạn vòng lặp. Thật vậy, vì hàm mất mát là một số dương và sau mỗi bước 2 hoặc 3, giá trị của hàm mất mát bị giảm đi. Theo kiến thức về dãy số trong chương trình cấp 3: nếu một dãy số giảm và bị chặn dưới thì nó hội tụ! Hơn nữa, số lượng cách phân nhóm cho toàn bộ dữ liệu là hữu hạn nên đến một lúc nào đó, hàm mất mát sẽ không thể thay đổi, và chúng ta có thể dừng thuật toán tại đây.Chúng ta sẽ có một vài thảo luận về thuật toán này, về những hạn chế và một số phương pháp khắc phục. Nhưng trước hết, hãy xem nó thể hiện như thế nào trong một ví dụ cụ thể dưới đây.Để kiểm tra mức độ hiểu quả của một thuật toán, chúng ta sẽ làm một ví dụ đơn giản (thường được gọi là toy example). Trước hết, chúng ta chọn center cho từng cluster và tạo dữ liệu cho từng cluster bằng cách lấy mẫu theo phân phối chuẩn có kỳ vọng là center của cluster đó và ma trận hiệp phương sai (covariance matrix) là ma trận đơn vị.Trước tiên, chúng ta cần khai báo các thư viện cần dùng. Chúng ta cần numpy và matplotlib như trong bài Linear Regression cho việc tính toán ma trận và hiển thị dữ liệu. Chúng ta cũng cần thêm thư viện scipy.spatial.distance để tính khoảng cách giữa các cặp điểm trong hai tập hợp một cách hiệu quả.Tiếp theo, ta tạo dữ liệu bằng cách lấy các điểm theo phân phối chuẩn có kỳ vọng tại các điểm có tọa độ (2, 2), (8, 3) và (3, 6), ma trận hiệp phương sai giống nhau và là ma trận đơn vị. Mỗi cluster có 500 điểm. (Chú ý rằng mỗi điểm dữ liệu là một hàng của ma trận dữ liệu.Chúng ta cần một hàm kmeans_display để hiển thị dữ liệu. Sau đó hiển thị dữ liệu theo nhãn ban đầu.Trong đồ thị trên, mỗi cluster tương ứng với một màu. Có thể nhận thấy rằng có một vài điểm màu đỏ bị lẫn sang phần cluster màu xanh.\n\nViết các hàm:Phần chính của K-means clustering:Áp dụng thuật toán vừa viết vào dữ liệu ban đầu, hiển thị kết quả cuối cùng.Từ kết quả này chúng ta thấy rằng thuật toán K-means clustering làm việc khá thành công, các centers tìm được khá gần với kỳ vọng ban đầu. Các điểm thuộc cùng một cluster hầu như được phân vào cùng một cluster (trừ một số diểm màu đỏ ban đầu đã bị phân nhầm vào cluster màu xanh da trời, nhưng tỉ lệ là nhỏ và có thể chấp nhận được).Dưới đây là hình ảnh động minh họa thuật toán qua từng vòng lặp, chúng ta thấy rằng thuật toán trên hội tụ rất nhanh, chỉ cần 6 vòng lặp để có được kết quả cuối cùng:Các bạn có thể xem thêm các trang web minh họa thuật toán K-means cluster tại:Để kiểm tra thêm, chúng ta hãy so sánh kết quả trên với kết quả thu được bằng cách sử dụng thư viện scikit-learn.Thật may mắn (cho tôi), hai thuật toán cho cùng một đáp số! Với cách thứ nhất, tôi mong muốn các bạn hiểu rõ được thuật toán K-means clustering làm việc như thế nào. Với cách thứ hai, tôi hy vọng các bạn biết áp dụng thư viện sẵn có như thế nào.Có một vài hạn chế của thuật toán K-means clustering:Để ý thấy rằng trong thuật toán nêu trên, chúng ta cần biết đại lượng \\(K\\) là số lượng clusters. Trong thực tế, nhiều trường hợp chúng ta không xác định được giá trị này. Có một số phương pháp giúp xác định số lượng clusters, tôi sẽ dành thời gian nói về chúng sau nếu có dịp. Bạn đọc có thể tham khảo Elbow method - Determining the number of clusters in a data set.Tùy vào các center ban đầu mà thuật toán có thể có tốc độ hội tụ rất chậm, ví dụ:\nhoặc thậm chí cho chúng ta nghiệm không chính xác (chỉ là local minimum - điểm cực tiểu - mà không phải giá trị nhỏ nhất):Có một vài cách khắc phục đó là:Chạy K-means clustering nhiều lần với các center ban đầu khác nhau rồi chọn cách có hàm mất mát cuối cùng đạt giá trị nhỏ nhất.K-means++ -Improve initialization algorithm - wiki.Bạn nào muốn tìm hiểu sâu hơn có thể xem bài báo khoa học Cluster center initialization algorithm for K-means clustering.Dưới đây là một ví dụ với 3 cluster với 20, 50, và 1000 điểm. Kết quả cuối cùng không chính xác.Tức các cluster tuân theo phân phối chuẩn và ma trận hiệp phương sai là ma trận đường chéo có các điểm trên đường chéo giống nhau.Dưới đây là 1 ví dụ khi 1 cluster có dạng hình dẹt.Đây là ví dụ kinh điển về việc K-means clustering không thể phân cụm dữ liệu. Một cách tự nhiên, chúng ta sẽ phân ra thành 4 cụm: mắt trái, mắt phải, miệng, xunh quanh mặt. Nhưng vì mắt và miệng nằm trong khuôn mặt nên K-means clustering không thực hiện được:Mặc dù có những hạn chế, K-means clustering vẫn cực kỳ quan trọng trong Machine Learning và là nền tảng cho nhiều thuật toán phức tạp khác sau này. Chúng ta cần bắt đầu từ những thứ đơn giản. Simple is best!Clustering documents using k-meansVoronoi Diagram - WikipediaCluster center initialization algorithm for K-means clusteringVisualizing K-Means ClusteringVisualizing K-Means Clustering - Standford",
          "relevence": "yes"
        },
        {
          "url": "http://text.xemtailieu.com/tai-lieu/nghien-cuu-bai-toan-gom-cum-trong-khai-pha-du-lieu-167356.html",
          "title": "Nghiên cứu bài toán gom cụm trong khai phá dữ liệu - Tài liệu, Luận văn, Giáo trình, Truyện đọc",
          "content": "pdf - 12 trang",
          "relevence": "yes"
        },
        {
          "url": "http://tailieu.vn/doc/bao-cao-nghien-cuu-bai-toan-gom-cum-trong-khai-pha-du-lieu-1281755.html",
          "title": "Báo cáo: NGHIÊN CỨU BÀI TOÁN GOM CỤM  TRONG KHAI PHÁ DỮ LIỆU - TaiLieu.VN",
          "content": "Chia sẻ: Trương Ngọc Khánh Khánh\n\t\t\t\t\t| Ngày: \n\t\t\t\t\t| Loại File: PDF\n\t\t\t\t\t| Số trang:12\n\t\t\t\t\t\n\t\t\n\t\t\tKhái niệm gom cụm:\r\nGom cụm (hay phân cụm) dữ liệu là quá trình phân chia một tập dữ liệu ban đầu thành các cụm dữ liệu thỏa mãn các điều kiện:\r\n- Các đối tượng trong cùng một cụm “tương tự” nhau về một số tiêu chí nào đó.\r\n- Các đối tượng khác cụm thì “không tương tự” nhau.\r\nGiải quyết các vấn đề tìm kiếm, phát hiện các cụm, các mẫu dữ liệu trong một tập hợp ban đầu các dữ liệu không có nhãn....\t\t\n\t\n\t\t\n\t\tBình luận(0)\n\t\t\n\t\t\t\t\t\tĐăng nhập để gửi bình luận!\n\t\t\t\t\t\n\t\n\t\t\t\tCÓ THỂ BẠN MUỐN DOWNLOAD\n\t\t\tLUẬN VĂN: NGHIÊN CỨU CÁC LUẬT KẾT HỢP SONG SONG TRONG KHAI PHÁ DỮ LIỆU\n\t\t\t\t \n\t\t\t\t73 p |  337\n\t\t\t|  175\t\t\t\t\n\t\t\t Luận văn: Nghiên cứu và áp dụng một số kỹ thuật khai phá dữ liệu với cơ sơ sở dữ liệu ngành Thuế Việt Nam.\n\t\t\t\t \n\t\t\t\t112 p |  198\n\t\t\t|  124\t\t\t\t\n\t\t\tLuận văn:Khai phá dữ liệu và thuật toán khai phá luật kết hợp song song\n\t\t\t\t \n\t\t\t\t86 p |  193\n\t\t\t|  120\t\t\t\t\n\t\t\tKhóa luận tốt nghiệp: Nghiên cứu tính toán lưới và áp dụng giải bài toán trong an toàn thông tin \n\t\t\t\t \n\t\t\t\t66 p |  191\n\t\t\t|  113\t\t\t\t\n\t\t\tLUẬN VĂN:ÁP DỤNG PHưƠNG PHÁP TRÍCH CHỌN THUỘC TÍNH ĐẶC TRƯNG ĐỂ NÂNG CAO HIỆU QUẢ PHÂN LỚP KHI KHAI PHÁ DỮ LIỆU LỚN\n\t\t\t\t \n\t\t\t\t58 p |  180\n\t\t\t|  101\t\t\t\t\n\t\t\tLUẬN VĂN: NGHIÊN CỨU CÁC THUẬT TOÁN PHÂN LỚP DỮ LIỆU DỰA TRÊN CÂY QUYẾT ĐỊNH\n\t\t\t\t \n\t\t\t\t67 p |  208\n\t\t\t|  86\t\t\t\t\n\t\t\tTiểu luận: Báo cáo nghiên cứu tổ chức hệ thống thông tin trong doanh nghiệp\n\t\t\t\t \n\t\t\t\t43 p |  126\n\t\t\t|  62\t\t\t\t\n\t\t\tBáo cáo thực tập: Bài toán quản lý bán hàng máy tính linh kiện và các thiết bị văn phòng dựa trên hệ quản trị cơ sở dữ liệu Microsoft Access và ngôn ngữ lập trình Visual Basic\n\t\t\t\t \n\t\t\t\t57 p |  266\n\t\t\t|  55\t\t\t\t\n\t\t\tBáo cáo nghiên cứu khoa học: Nhận dạng biển số xe\n\t\t\t\t \n\t\t\t\t58 p |  70\n\t\t\t|  46\t\t\t\t\n\t\t\tLUẬN VĂN: SỬ DỤNG PHƯƠNG PHÁP XẾP HẠNG TRONG BÀI TOÁN PHÂN CỤM TIẾNG VIỆT\n\t\t\t\t \n\t\t\t\t55 p |  146\n\t\t\t|  45\t\t\t\t\n\t\t\tLUẬN VĂN: BÀI TOÁN TRÍCH XUẤT THÔNG TIN CHO DỮ LIỆU BÁN CẤU TRÚC VÀ ÁP DỤNG XÂY DỰNG HỆ THỐNG TÌM KIẾM GIÁ CẢ SẢN PHẨM\n\t\t\t\t \n\t\t\t\t71 p |  102\n\t\t\t|  33\t\t\t\t\n\t\t\tLUẬN VĂN:XÂY DỰNG CƠ SỞ DỮ LIỆU BÀI BÁO ĐIỆN TỬ LIÊN QUAN TỚI GỐM SỨ VIỆT NAM PHỤC VỤ ĐÀO TẠO TẠI HỌC VIỆN BÁO CHÍ VÀ TUYÊN TRUYỀN\n\t\t\t\t \n\t\t\t\t53 p |  81\n\t\t\t|  32\t\t\t\t\n\t\t\tLUẬN VĂN:NGHIÊN CỨU BÀI TOÁN XÁC ĐỊNH COLLOCATION TRONG TIẾNG VIỆT\n\t\t\t\t \n\t\t\t\t63 p |  70\n\t\t\t|  26\t\t\t\t\n\t\t\tBáo cáo nghiên cứu khoa học: Nghiên cứu áp dụng phần mềm Moodle trong giảng dạy tiếng Anh tại trường Đại học Công nghệ GTVT\n\t\t\t\t \n\t\t\t\t66 p |  49\n\t\t\t|  25\t\t\t\t\n\t\t\tBáo cáo: \"nghiên cứu khai thác môđun CAD trong phần mềm Pro engineer\"\n\t\t\t\t \n\t\t\t\t36 p |  42\n\t\t\t|  17\t\t\t\t\n\t\t\tBáo cáo nghiên cứu khoa học: \"   VAI TRÒ CỦA SỞ HỮU TRÍ TUỆ TRONG NGHIÊN CỨU KHOA HỌC VÀ CHUYỂN GIAO CÔNG NGHỆ TRONG CÁC TRƯỜNG ĐẠI HỌC\"\n\t\t\t\t \n\t\t\t\t7 p |  40\n\t\t\t|  13\t\t\t\t\n\t\t\tTiểu luận:NGHIÊN CỨU SỰ PHÁT TRIỂN CỦA CÔNG NGHỆ CƠ SỞ DỮ LIỆU VÀ KHAI PHÁ DỮ LIỆU\n\t\t\t\t \n\t\t\t\t19 p |  33\n\t\t\t|  7\t\t\t\t\n\t\t\tGiấy phép ICP số: 670/GP-BTTTT cấp ngày 30/11/2015 Copyright © 2009-2015 TaiLieu.VN. All rights reserved. TaiLieu.VN hiển thị tốt nhất với trình duyệt Chrome, Firefox, Internet Explorer 8.",
          "relevence": "yes"
        },
        {
          "url": "http://eitguide.net/category/machine-learning/",
          "title": "Machine Learning",
          "content": "Kiến thức về các thuật toán máy học sử dụng để phân lớp dữ liệu (classfication) và gom cụm dữ liệu (clustering). Thuật toán học có giám sát và các thuật toán học không giám sát như một số các thuật toán phổ biến như mạng nơron Neraul Network, KNN K-Nearest Neighbor, SVM Support Vector Machine, K-Mean...Bài toán phân lớp Bài toán phân lớp (classification) và bài toán gom cụm (cluster) là hai bài toán lớn trong lĩnh vực Machine Learnig (ML). Bài toán phân lớp là quá trình phân lớp một đối tượng dữ liệu vào một hay nhiều lớp đã cho trước nhờ một mô hình phân lớp (model). Mô hình này được xây dựng dựa trên một tập dữ liệu được xây dựng trước đó có gán nhãn (hay còn gọi là tập…  Đây là một thư viện hổ trợ các thuật toán Machine Learning như Decison Tree, KNN, Navie Bayes, SVM(Support Vector Machine), ANN (Artificial Neural Network), Linear Regression, K-Mean… Scikit-learn là một thư viện viết bằng python và rất dễ sử dụng. Chúng ta tương tác với thư viện này hoàn toàn quay dòng lệnhGiới thiệu Thuật toán SVM(Support Vector Machine) là một thuật toán máy học dùng cho bài toán phân lớp dữ liệu. Ý tưởng của thuật toán là tìm một siêu phẳng tối ưu (optimal hyperplane) sao cho margin tới các điểm support vector có khoảng cách lớn nhất. Hiện nay thuật toán này được sử dụng khá nhiều trong lĩnh vực Machine Learning cùng với các thuật toán như cây quyết định (Tree Decision), K-NN (K-Nearest Neighbor), mạng noron (Nerual Network)….",
          "relevence": "yes"
        },
        {
          "url": "https://viblo.asia/p/spectral-clustering-va-bai-toan-tim-kiem-cong-dong-an-phan-12-BAQ3vVDXMbOr",
          "title": "Spectral clustering và bài toán tìm kiếm cộng đồng ẩn (Phần 1/2) - Viblo",
          "content": "Trong phần này, tôi xin giới hạn chỉ giới thiệu về thuật toán Spectral\nclustering và mô hình giải quyết bài toán. Phần demo và hướng dẫn chi\ntiết xin hẹn ở phần tiếp theo.Trong thống kê đa biến và phân cụm dữ liệu, kỹ thuật spectral clustering cho phép tận dụng giá trị đặc trưng của ma trận tương tự với dữ liệu lớn để thực hiện giảm chiều trước khi chia thành các kích thước nhỏ hơn. Đầu vào là ma trận tương tự bao gồm định lượng giống nhau tương đối của mỗi cặp trong tập dữ liệu.Mục tiêu của thuật toán này là:Trong toàn bài viết, mình sẽ sử dụng ví dụ với mạng sau:Để hiểu được thuật toán, trước tiên ta cần tìm hiểu về Adjacency\nMatrix (A), Degree Matrix (D), Cut và K-means.Adjacency Matrix (A)A là ma trận NxN nhị phân đối xứng. Các cột và các hàng tiêu biểu cho\ncác đỉnh, đầu vào tiêu biểu cho các cạnh của đồ thị. Ma trận có đường\nchéo là 0. A(i,j) = 0 nếu i, j không có kết nối. A(i,j) = 1 nếu i, j có\nkết nối.Ví dụ: trong mạng ví dụ đã nêu ở đầu bài ta có:Degree Matrix (D)D là ma trận NxN đối xứng mà bao gồm thông tin về bậc của các đỉnh. Bậc\nd(vi) của đỉnh vi trong đồ thị là số cạnh gắn liền của đỉnh. D(i,j) = 0\nnếu i≠j. D(i,j) = d(vi) nếu i=j → D = diag(d1,d2, … ,dn).VD: Trong mạng hình ví dụ:Cut.Có thể dễ dàng nhận thấy rằng: trong một mạng có nhiều nhóm nhỏ, phần lớn\nsự tương tác là ở bên trong nhóm, còn tương tác giữa các nhóm là rất ít.\nCut là sự phân chia của các đỉnh đồ thị thành hai bộ phận. Chính thế có\nthể nói bài toán phát hiện cộng đồng là tập hợp các bài toán tối thiểu\nvề cut.Minimum cut problem (bài toán cắt cực tiểu): tìm một sự phân chia đồ\nthị như là số cạnh giữa hai phần là nhỏ nhất. Cắt cực tiểu thường trả lại\nmột phân vùng thiếu cân bằng với một phần là duy nhất. Có thể thay đổi khách quan để cân nhắc độ lớn của cộng đồng. Có các loại cut:vol(Ci) là tổng tất cả các cạnh của mỗi nút.Ví dụ: Trong mạng ví dụ, ta sẽ thực hiện cắt 2 đường như sau:Với đường cut (1):Với đường cut (2):Cả Ratio và Normalized Cut đều đưa ra phân vùng cân bằng với (1), tuy\nnhiên chỉ Ratio đưa ra vùng cân bằng ở (2).K-meansK-Means là thuật toán rất quan trọng và được sử dụng phổ biến trong kỹ\nthuật phân cụm. Tư tưởng chính của thuật toán K-Means là tìm cách phân\nnhóm các đối tượng (objects) đã cho vào K cụm (K là số các cụm được xác\nđinh trước, K nguyên dương) sao cho tổng bình phương khoảng cách giữa\ncác đối tượng đến tâm nhóm (centroid) là nhỏ nhất.Cả ratio cut và normalized cut đều có thể được đưa vào công thức như:Trong đó:Giải pháp: S là vector riêng của L với giá trị riêng nhỏ nhất (trừ\ntrường hợp đầu tiên). Sau đó áp dụng k-means cho S, ta được cộng đồng\ncần tìm.Trong đó:V là vector riêng.Áp dụng vào trong mạng ví dụ:Áp dụng K-means ta được 2 cộng đồng: {1,2,3,4},{5,6,7,8,9}\n                    © 2017 Viblo. All rights reserved.\n                ",
          "relevence": "yes"
        },
        {
          "url": "http://luanvan.co/luan-van/mot-so-phuong-phap-phan-cum-du-lieu-34694/",
          "title": "\r\n\tMột số phương pháp phân cụm dữ liệu - Luận văn, đồ án, đề tài tốt nghiệp\r\n",
          "content": "Luận văn, đồ án, đề tài, tiểu luận, luận ánCộng đồng chia sẻ luận văn, đồ án, tiểu luận, đề tài tham khảo cho các bạn học sinh, sinh viên\r\n                    MỤC LỤC\nMỤC LỤC . . 1\nDANH MỤC HÌNH MINH HỌA . . 3\nLỜI CẢM ƠN . . 4\nCHƯƠNG 1: TỔNG QUAN VỀ KHAI PHÁ DỮ LIỆU . . 5\n1.1 Giới thiệu về khám phá tri thức . . 5\n1.2 Khai phá dữ liệu và các khái niệm liên quan . . 7\n1.2.1 Khái niệm khai phá dữ liệu . . 7\n1.2.2 Các phương pháp khai phá dữ liệu . 7\n1.2.3 Các lĩnh vực ứng dụng trong thực tiễn . 8\n1.2.4 Các hướng tiếp cận cơ bản và kỹ thuật áp dụng trong khai phá dữ liệu 8\nCHƯƠNG 2: PHÂN CỤM DỮ LIỆU VÀ CÁC TIẾP CẬN . 10\n2.1 Khái niệm chung . 10\n2.2 Các kiểu dữ liệu và độ đo tương tự . 10\n2.2.1 Các kiểu dữ liệu . 10\n2.2.2 Độ đo tương tự và phi tương tự . 12\n2.3 Các kỹ thuật tiếp cận trong phân cụm dữ liệu . 15\n2.3.1 Phương pháp phân cụm phân hoạch . 15\n2.3.2 Phương pháp phân cụm phân cấp . 15\n2.3.3 Phương pháp phân cụm dựa trên mật độ . 16\n2.3.4 Phương pháp phân cụm dựa trên lưới . 17\n2.3.5 Phương pháp phân cụm dựa trên mô hình . 18\n2.3.6 Phương pháp phân cụm có dữ liệu ràng buộc . 19\n2.4 Các ứng dụng phân cụm dữ liệu . 20\nCHƯƠNG 3: MỘT SỐ THUẬT TOÁN CƠ BẢN TRONG PHÂN CỤM DỮ LIỆU 21\n3.1 Các thuật toán phân cụm phân hoạch . 21\n3.1.1 Thuật toán K-means . 21\n3.1.2 Thuật toán K-Medoids . 23\n3.2 Thuật toán phân cụm phân cấp . 24\n3.3 Thuật toán COP-Kmeans . 26\nCHƯƠNG 4: ỨNG DỤNG THUẬT TOÁN K-MEANS CHO PHÂN ĐOẠN ẢNH . 28\n4.1 Tổng quan về phân vùng ảnh . 28\n 4.1.1 Phân vùng ảnh theo ngưỡng biên độ . 28\n 4.1.2 Phân vùng ảnh theo miền đồng nhất . 29\n 4.1.3 Phân vùng dựa theo đường biên . 31\n 4.1.4 Phân đoạn dựa theo kết cấu bề mặt . 31\n 4.2 Thuật toán K-means cho phân đoạn ảnh . 32\n4.2.1 Mô tả bài toán . 32\n 4.2.2 Các bước thực hiện chính trong thuật toán . 33\n4.2.2.1 Tìm kiếm Top X color . 34\n4.2.2.2 Tính khoảng cách và phân cụm . 36\n4.2.2.3 Tính lại trọng tâm cụm . 37\n4.2.2.4 Kiểm tra hội tụ . 38\n4.2.3 Kết quả thực nghiệm . 39\n4.2.3.1 Môi trường cài đặt. 39\n4.2.3.2 Một số giao diện. 39\nKẾT LUẬN . 41\nTÀI LIỆU THAM KHẢO . 42\n \n \n \n \nMột số phương pháp phân cụm dữ liệu \n \n \n \n \nCHƯƠNG 1: TỔNG QUAN VỀ KHAI PHÁ DỮ LIỆU\n1.1 Giới thiệu về khám phá tri thức\nNếu cho rằng các điện tử và các sóng điện tử chính là bản chất của\ncông nghệ điện tử truyền thống thì dữ liệu, thông tin và tri thức hiện đang là\ntiêu điểm của một lĩnh vực mới trong nghiên cứu và ứng dụng về phát hiện tri\nthức (Knowledge Discovery) và khai phá dữ liệu (Data Mining).\nThông thường chúng ta coi dữ liệu như một dãy các bit, hoặc các số và\ncác ký hiệu, hoặc các “đối tượng” với một ý nghĩa nào đó khi được gửi cho\nmột chương trình dưới một dạng nhất định. Chúng ta sử dụng các bit để đo\nlường các thông tin và xem nó như là các dữ liệu đã được lọc bỏ các dư thừa,\nđược rút gọn tới mức tối thiểu để đặc trưng một cách cơ bản cho dữ liệu.\nChúng ta có thể xem tri thức như là các thông tin tích hợp, bao gồm các sự\nkiện và các mối quan hệ giữa chúng. Các mối quan hệ này có thể được hiểu\nra, có thể được phát hiện, hoặc có thể được học. Nói cách khác, tri thức có thể\nđược coi là dữ liệu có độ trừu tượng và tổ chức cao.\nPhát hiện tri thức trong các cơ sở dữ liệu là một qui trình nhận biết các\nmẫu hoặc các mô hình trong dữ liệu với các tính năng: hợp thức, mới, khả ích,\nvà có thể hiểu được. Còn khai thác dữ liệu là một bước trong qui trình phát\nhiện tri thức gồm có các thuật toán khai thác dữ liệu chuyên dùng dưới một số\nqui định về hiệu quả tính toán chấp nhận được để tìm ra các mẫu hoặc các mô\nhình trong dữ liệu. Nói một cách khác, mục đích của phát hiện tri thức và khai\nphá dữ liệu chính là tìm ra các mẫu hoặc các mô hình đang tồn tại trong các\ncơ sở dữ liệu nhưng vẫn còn bị che khuất bởi hàng núi dữ liệu.\r\n                Các file đính kèm theo tài liệu này:122 trang | Lượt xem: 1029 | Lượt tải: 321 trang | Lượt xem: 1479 | Lượt tải: 057 trang | Lượt xem: 3310 | Lượt tải: 1874 trang | Lượt xem: 1479 | Lượt tải: 022 trang | Lượt xem: 1048 | Lượt tải: 157 trang | Lượt xem: 1041 | Lượt tải: 2238 trang | Lượt xem: 1256 | Lượt tải: 129 trang | Lượt xem: 3594 | Lượt tải: 2230 trang | Lượt xem: 750 | Lượt tải: 062 trang | Lượt xem: 879 | Lượt tải: 1",
          "relevence": "yes"
        },
        {
          "url": "https://ongxuanhong.wordpress.com/2015/08/27/gom-nhom-clustering-analysis-tap-du-lieu-labor/",
          "title": "Gom nhóm (Clustering analysis) tập dữ liệu Labor – Ông Xuân Hồng",
          "content": "Trong bài viết này, ta sẽ áp dụng các phương pháp gom nhóm (clustering) trên tập dữ liệu Labor. Đây là tập dữ liệu chứa các thông tin (số ngày nghỉ, số giờ làm việc, lương tăng hàng năm, …) để phân biệt nhân viên tốt (good) và nhân viên không tốt (bad). Hai thuật toán được sử dụng là K-mean và Hierarchical Clustering (AGNES). Để dễ tiếp cận, các phương pháp được thực hiện với Weka.LaborTập dữ liệu: labor\nĐịa chỉ: https://archive.ics.uci.edu/ml/machine-learning-databases/labor-negotiations/labor-negotiations.data\nMô tả: https://archive.ics.uci.edu/ml/machine-learning-databases/labor-negotiations/labor-negotiations.names\nGithub: https://github.com/ongxuanhong/Clustering-analysis-with-Labor-datasetSau khi nạp dữ liệu labor.arff vào Weka, ta khảo sát các thông tin về tập dữ liệu này.Load labor datasetThuật toán chạy 2 lần với dữ liệu chưa điền giá trị thiếu và đã điền giá trị thiếu (sử dụng bộ lọc ReplaceMissingValues). Ta thiết lập các thông số trước khi tiến hành gom nhóm dữ liệu.Clustering labor datasetĐánh giá chất lượng nhóm là nhiệm vụ khó khăn và phức tạp nhất trong phân tích nhóm. Chất lượng nhóm thể hiện qua:Phương pháp Classes To Clusters sử dụng độ đo chất lượng nhóm External index (đo mức độ các nhãn lớp tương đồng với các nhãn lớp bên ngoài đã cung cấp sẵn).External index được tính như sau: đối với mỗi nhóm j, xác định  là xác suất để một mẫu thuộc nhóm j(cluster j) thuộc về lớp i(class i).*  là số mẫu của cluster j.\n*  là số mẫu của class i thuộc cluster j.K clusteringƯu điểmNhược điểmHierachical clusteringKhông cần xác định trước số nhóm k. Xác định số nhóm cần thiết bằng việc cắt ngang sơ đồ hình cây tại mức thích hợp.Nhược điểmMời bạn điền thông tin vào ô dưới đây hoặc kích vào một biểu tượng để đăng nhập: Bạn đang bình luận bằng tài khoản WordPress.com ( Đăng xuất / Thay đổi ) Bạn đang bình luận bằng tài khoản Twitter ( Đăng xuất / Thay đổi ) Bạn đang bình luận bằng tài khoản Facebook ( Đăng xuất / Thay đổi ) Bạn đang bình luận bằng tài khoản Google+ ( Đăng xuất / Thay đổi )Connecting to %s Notify me of new comments via email. Thông báo cho tôi bằng email khi có bài đăng mới. \n\nNếu ngày mai tớ trở thành người nổi tiếng. Gặp tớ giữa đường bạn đừng ngại tớ nghe. Cứ nhìn tớ như ngày xưa bạn thấy. Đừng nên nghĩ tớ quá xa vời. Nếu một ngày tớ là kẻ ăn xin. Thì bố thí cho tớ vài kỷ niệm. Dù rất nhỏ nhưng đừng nên khinh tớ. Vì tớ vẫn là tớ của ngày xưa. Nếu ngày mai tớ chẳng biết đường về. Hãy chỉ giùm tớ con đường đi đúng đắn. Đừng để tớ sa chân nơi bùn nhớp. Lúc rút chân lên thì đã bẩn mất rồi. Nếu ngày mai tớ gặp bạn giữa đường. Thì ta sẽ gặp nhau và cười nhé. Đừng nên trách và đừng nên đánh mất.Đôi khi có một số người lướt qua cuộc đời bạn và ngay tức khắc bạn nhận ra rằng sự có mặt của họ ý nghĩa như thế nào. Họ đã dạy bạn những bài học, đã giúp bạn nhận ra giá trị của chính mình hoặc trở thành con người mà bạn từng mơ ước. Có lẽ bạn sẽ không biết được những con người này từ đâu đến ( bạn cùng phòng, người hàng xóm, vị giáo sư, người bạn mất liên lạc từ lâu hay thậm chí là một người hoàn toàn xa lạ ). Nhưng khi bạn thờ ơ với họ, hãy nhớ rằng trong từng khoảnh khắc họ sẽ ảnh hưởng rất sâu sắc đến cuộc đời bạn. …Cuộc sống không hề làm khó bạn mà chính bạn đã tự làm khó mình bằng cách đặt ra những câu hỏi đại loại như: “Tại sao lại thế này?… Tại sao lại thế kia?…”. Không ai có thể trả lời giúp bạn mà chính bạn phải tự đi tìm lời giải đáp cho mình. Hãy luôn nghĩ về người khác trước khi nghĩ đến bản thân, chắc chắn lúc đó bạn sẽ hài lòng với câu trả lời của chính mình. Và đừng quên luôn đặt câu hỏi: “Mình đã làm được gì?” trước khi tự hỏi: “Mình đã nhận được gì?” nhé! Tôi tin là bạn sẽ thành công!  Thật ra trên đời, con người luôn cần có nhau để giải quyết rất nhiều vấn đề. Nhờ những lúc qua lại giúp đỡ đó mà hoá giải bao muộn phiền, nảy sinh các tình cảm, cùng trải nghiệm biết bao điều thú vị, hiểu biết thêm nhiều tri thức mới. Đây chính là cuộc sống!Vì thế hãy trân trọng những người đang “làm phiền” bên cạnh bạn nhé! Đừng để đến khi mất đi rồi mới nghẹn ngào lại bảo “giá như” …ĐĐ. GS. Thích Phước Tiến\n__(())__Namo Bụt SakyamuniNhận Email khi có bài viết mới\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\tThis slideshow requires JavaScript.",
          "relevence": "yes"
        },
        {
          "url": "http://enews.agu.edu.vn/index.php?option=com_content&view=article&id=18545&Itemid=128",
          "title": "e-News - Kmean một kỹ thuật phân nhóm dữ liệu phổ biến",
          "content": "\r\n\tThuật toán phân nhóm K-Means do MacQueen giới thiệu trong tài liệu “J. Some Methods for Classification and Analysis of Multivariate Observations” năm 1967. K-Means là thuật toán rất quan trọng và được sử dụng phổ biến trong kỹ thuật phân cụm. Tư tưởng chính của thuật toán K-Means là tìm cách phân nhóm các đối tượng (objects) đã cho vào K cụm (K là số các cụm được xác đinh trước, K nguyên dương) sao cho tổng bình phương khoảng cách giữa các đối tượng đến tâm nhóm (centroid ) là nhỏ nhất.\r\n\tÝ tưởng giải thuật\r\n\tVề nguyên lý, có n đối tượng, mỗi đối tượng có m thuộc tính, ta phân chia được các đối tượng thành k nhóm dựa trên các thuộc tính của đối tượng bằng việc áp dụng thuật toán này. Coi mỗi thuộc tính của đối tượng (đối tượng có m thuộc tính) như một toạ độ của không gian m chiều và biểu diễn đối tượng như một điểm của không gian m chiều\r\n\t\r\n\tPhương thức phân loại/nhóm dữ liệu thực hiện dựa trên khoảng cách Euclidean nhỏ nhất giữa đối tượng đến phần tử trung tâm của các nhóm.\r\n\tPhần tử trung tâm của nhóm được xác định bằng giá trị trung bình các phần tử trong nhóm\r\n\tKhoảng cách Euclidean\r\n\t\r\n\tKhoảng cách Euclidean từ đối tượng ai đến phần tử trung tâm nhóm j cj được tính toán dựa trên công thức:\r\n\t\r\n\tPhần tử trung tâm\r\n\tPhần tử trung tâm của nhóm được xác định bằng giá trị trung bình các phần tử trong nhóm.\r\n\tk phần tử trung tâm (k nhóm) ban đầu được chọn ngẫu nhiên, sau mỗi lần nhóm các đối tượng vào các nhóm, phần tử trung tâm được tính toán lại. \r\n\t\r\n\tThuật toán\r\n\tKhởi tạo k phần tử trung tâm một cách ngẫu nhiên (mỗi phần tử trung tâm đại diện cho một nhóm)\r\n\tThực hiện các bước cơ bản sau cho đến khi tất cả các đối tượng được phân loại và không còn còn sự thay đổi của các đối tượng đến các nhóm:\r\n\t1. Chọn ngẫu nhiên K tâm (centroid) cho K cụm (cluster). Mỗi cụm được đại diện bằng các tâm của cụm.\r\n\t2. Tính khoảng cách giữa các đối tượng (objects) đến K tâm (thường dùng khoảng cách Euclidean)\r\n\t3. Nhóm các đối tượng vào nhóm gần nhất\r\n\t4. Xác định lại tâm mới cho các nhóm\r\n\t5. Thực hiện lại bước 2 cho đến khi không có sự thay đổi nhóm nào của các đối tượng\r\n\tLược đồ mô tả giải thuật\r\n\t\r\n\tĐánh giá giải thuật\r\n\tThuật toán K-means có ưu điểm là dễ dàng cài đặt cho kết quả dễ hiểu, nhưng lại có nhược điểm là phải chỉ ra số lượng cluster và yêu cầu CSDL cần phân nhóm phải xác định được tâm. Thuật toán này không phù hợp với việc khai phá các dữ liệu gồm các cluster có hình dạng không lồi và KMeans hay gặp lỗi với các dữ  liệu có phần tử ngoại lai (outliers).\r\n\tĐối với các tập dữ liệu có số chiều lớn dữ liệu có nhiều phần tử nhiễu như các tập dữ liệu biểu hiện gien thì giải thuật Kmeans thực hiện không đạt hiệu quả cao (Dipti 2015).\r\n\tBên cạnh đó trong một số tập dữ liệu không phải lúc nào mỗi đối tượng cũng chỉ thuộc về 1 cụm, chỉ phù hợp với đường biên giữa các cụm rõ trong trường hợp này cần sử dụng các giải thuật BisClustering\r\n\t CLB Tin Học\n\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\tTrở lên trên\t\t\t\t\n\t\t\t\tPhụ trách chung e-News  ThS. Nguyễn Thị Bích Châu \n\t\t\t\n\t\t\t\t© 2017 e-News \n\t\t\t\t Số 18 Ung Văn Khiêm, Đông Xuyên, Long Xuyên, An Giang \n\t\t\t\t +84 296 625 6565 nhánh 1605 \n\t\t\t\t enews@agu.edu.vn \n\t\t\t",
          "relevence": "yes"
        },
        {
          "url": "http://laptrinhx.com/topic/11218/ung-dung-thuat-toan-phan-cum-k-means-trong-nhan-dien-chu-so-viet-tay",
          "title": "Ứng dụng thuật toán phân cụm K-means trong nhận diện chữ số viết tay | Laptrinh | Dịch vụ chia sẻ kiến thức về lập trình miễn phí",
          "content": "\r\n\t\t\t\tYour browser does not seem to support JavaScript. As a result, your viewing experience will be diminished, and you have been placed in read-only mode.\r\n\t\t\t\r\n\t\t\t\tPlease download a browser that supports JavaScript, or enable it if it's disabled (i.e. NoScript).\r\n\t\t\tBài này tôi sẽ giới thiệu một trong những thuật toán cơ bản nhất trong Unsupervised learning - thuật toán K-means clustering (phân cụm K-means).Trong thuật toán K-means clustering, chúng ta không biết nhãn (label) của từng điểm dữ liệu. Mục đích là làm thể nào để phân dữ liệu thành các cụm (cluster) khác nhau sao cho dữ liệu trong cùng một cụm có tính chất giống nhau.Ý tưởng đơn giản nhất về cluster (cụm) là tập hợp các điểm ở gần nhau trong một không gian nào đó (không gian này có thể có rất nhiều chiều trong trường hợp thông tin về một điểm dữ liệu là rất lớn). Hình bên dưới là một ví dụ về 3 cụm dữ liệu (từ giờ viết gọn là cluster).Giả sử mỗi cluster có một điểm đại diện (center) màu vàng. Và những điểm xung quanh mỗi center thuộc vào cùng nhóm với center đó. Một cách đơn giản nhất, xét một điểm bất kỳ, ta xét xem điểm đó gần với center nào nhất thì nó thuộc về cùng nhóm với center đó. Tới đây, chúng ta có một bài toán thú vị: Trên một vùng biển hình vuông lớn có ba đảo hình vuông, tam giác, và tròn màu vàng như hình trên. Một điểm trên biển được gọi là thuộc lãnh hải của một đảo nếu nó nằm gần đảo này hơn so với hai đảo kia . Hãy xác định ranh giới lãnh hải của các đảo.**Đầu vào: ** Dữ liệu XX và số lượng cluster cần tìm KK.Đầu ra: Các center MM và label vector cho từng điểm dữ liệu YY.Chúng ta có thể đảm bảo rằng thuật toán sẽ dừng lại sau một số hữu hạn vòng lặp. Thật vậy, vì hàm mất mát là một số dương và sau mỗi bước 2 hoặc 3, giá trị của hàm mất mát bị giảm đi. Theo kiến thức về dãy số trong chương trình cấp 3: nếu một dãy số giảm và bị chặn dưới thì nó hội tụ! Hơn nữa, số lượng cách phân nhóm cho toàn bộ dữ liệu là hữu hạn nên đến một lúc nào đó, hàm mất mát sẽ không thể thay đổi, và chúng ta có thể dừng thuật toán tại đây.Chúng ta sẽ có một vài thảo luận về thuật toán này, về những hạn chế và một số phương pháp khắc phục. Nhưng trước hết, hãy xem nó thể hiện như thế nào trong một ví dụ cụ thể dưới đây.Bộ cơ sở dữ liệu MNISTBộ cơ sở dữ liệu MNIST là bộ cơ sở dữ liệu lớn nhất về chữ số viết tay và được sử dụng trong hầu hết các thuật toán nhận dạng hình ảnh (Image Classification).MNIST bao gồm hai tập con: tập dữ liệu huấn luyện (training set) có tổng cộng 60k ví dụ khác nhau về chữ số viết tay từ 0 đên 9, tập dữ liệu kiểm tra (test set) có 10k ví dụ khác nhau. Tất cả đều đã được gán nhãn. Hình dưới đây là ví dụ về một số hình ảnh được trích ra từ MNIST.Mỗi bức ảnh là một ảnh đen trắng (có 1 channel), có kích thước 28x28 pixel (tổng cộng 784 pixels). Mỗi pixel mang một giá trị là một số tự nhiên từ 0 đến 255. Các pixel màu đen có giá trị bằng 0, các pixel càng trắng thì có giá trị càng cao (nhưng không quá 255). Dưới đây là một ví dụ về chữ số 7 và giá trị các pixel của nó. (Vì mục đích hiển thị ma trận pixel ở bên phải, tôi đã resize bức ảnh về 14x14)Bài toán: Giả sử rằng chúng ta không biết nhãn của các chữ số này, chúng ta muốn phân nhóm các bức ảnh gần giống nhau về một nhóm.Lại thêm một giả sử nữa là chúng ta mới chỉ biết tới thuật toán phân nhóm K-means clustering gần đây, chúng ta sẽ giải quyết bài toán này thế nào?Trước khi áp dụng thuật toán K-means clustering, chúng ta cần coi mỗi bức ảnh là một điểm dữ liệu. Và vì mỗi điểm dữ liệu là 1 vector (hàng hoặc cột) chứ không phải ma trận như số 7 ở trên, chúng ta phải làm thêm một bước đơn giản trung gian gọi là vectorization (vector hóa). Nghĩa là, để có được 1 vector, ta có thể tách các hàng của ma trận pixel ra, sau đó đặt chúng cạnh nhau, và chúng ta được một vector hàng rất dài biểu diễn 1 bức ảnh chữ số.Chú ý: Cách làm này chỉ là cách đơn giản nhất để mô tả dữ liệu ảnh bằng 1 vector. Trên thực tế, người ta áp dụng rất nhiều kỹ thuật khác nhau để có thể tạo ra các vector đặc trưng (feature vector) giúp các thuật toán có được kết quả tốt hơn.Trước tiên các bạn vào trang chủ của MNIST để download bộ cơ sở dữ liệu này. Trong bài này chúng ta chỉ dùng bộ dữ liệu tét với 10k ảnh và không cần label nên các bạn chỉ cần download file t10k-images-idx3-ubyte.gzTrước tiên chúng ta cần khai báo một số thư viện:numpy cho các phép toán liên quan đến ma trận. mnist để đọc dữ liệu từ MNIST. matplotlib để hiển thị hình vẽ. sklearn chính là scikit-learn mà chúng ta đã làm quen trong các bài trước.Để hiện thị nhiều bức ảnh các chữ số cùng một lúc, tôi có dùng thêm hàm số display_network.py.Thực hiện thuật toán K-means clustering trên toàn bộ 10k chữ số.Đến đây, sau khi đã tìm được các center và phân nhóm dữ liệu vào từng cluster, tôi muốn hiển thị xem center trông như thế nào và các bức ảnh được phân vào mỗi cluster có giống nhau hay không. Dưới đây là kết quả khi tôi chọn ngẫu nhiên 20 bức ảnh từ mỗi cluster.\nÁp dụng K-means clustering vào tập test set của bộ cơ sở dữ liệu MNIST với K = 10 cluster. Cột 1: centers của các cluster. Các cột còn lại: Mỗi hàng là 20 điểm dữ liệu ngẫu nhiên được chọn ra từ mỗi cluster.Mỗi hàng tương ứng với một cluster, cột đầu tiên có nền xanh bên trái là centers tìm được của các clusters (màu đỏ hơn là các pixel có giá trị cao hơn). Chúng ta thấy rằng các center đều hoặc là giống với một chữ số nào đó, hoặc là kết hợp của hai/ba chữ số nào đó. Ví dụ: center của nhóm thứ 4 là sự kết hợp của các số 4, 7, 9; của hàng thứ 7 là kết hợp của chữ số 7, 8 và 9.Tuy nhiên, các bức ảnh lấy ra ngẫu nhiên từ mỗi nhóm trông không thực sự giống nhau. Lý do có thể là những bức ảnh này ở xa các center của mỗi nhóm (mặc dù center đó đã là gần nhất). Như vậy thuật toán K-means clustering làm việc không thực sự tốt trong trường hợp này. (Thật may là vì thế nên chúng ta vẫn còn nhiều thứ để học nữa).Chúng ta vẫn có thể khai thác một số thông tin hữu ích sau khi thực hiện thuật toán này. Bây giờ, thay vì chọn ngẫu nhiên các bức ảnh trong mỗi cluster, tôi chọn 20 bức ảnh gần center của mỗi cluster nhất, vì càng gần center thì độ tin cậy càng cao. Hãy xem hình dưới đây:\n\nÁp dụng K-means clustering vào tập test set của bộ cơ sở dữ liệu MNIST với K = 10 cluster. Cột 1: centers của các cluster. Các cột còn lại: Mỗi hàng là 20 điểm dữ liệu gần center nhất của mỗi cluster.Bạn có thể thấy dữ liệu trong mỗi hàng khá giống nhau và giống với center ở cột đầu tiên bên trái. Có một vài quan sát thú vị có thể rút ra từ đây:Có hai kiểu viết chữ số 1, một thẳng, một chéo. Và K-means clustering nghĩ rằng đó là hai chữ số khác nhau. Điều này là dễ hiểu vì K-means clustering là thuật toán Unsupervised learning. Nếu có sự can thiệp của con người, chúng ta có thể nhóm hai clusters này vào làm một.Hàng số 9, chữ số 4 và 9 được phân vào cùng 1 cluster. Sự thật là hai chữ số này cũng khá giống nhau. Điều tương tự xảy ra đối với hàng số 7 với các chữ số 7, 8, 9 được xếp vào 1 cluster. Với các cluster này, chúng ta có thể tiếp tục áp dụng K-means clustering để phân nhỏ cluster đó ra.Trong clustering có một kỹ thuật thường được sử dụng là Hierarchical clustering (clustering phân tầng ). Có hai loại Hierachical clustering:Agglomerative tức “đi từ dưới lên”. Ban đầu coi mỗi điểm dữ liệu thuộc 1 cluster khác nhau, sau đó các cặp cluster gần giống nhau được gộp lại làm một cluster lớn hơn. Lặp lại quá trình này đến khi nhận được kết quả chấp nhận được.Divisive tức “đi từ trên xuống”. Ban đầu coi tất cả các điểm dữ liệu thuộc cùng một cluster, sau đó chia nhỏ mỗi cluster bằng một thuật toán clustering nào đó.Cảm ơn các bạn đã đọc bài viết!\nNguồn: VibloCó vẻ như bạn đã mất kết nối tới LaptrinhX, vui lòng đợi một lúc để chúng tôi thử kết nối lại.",
          "relevence": "yes"
        }
      ]
    },
    {
      "query": "thuật toán svm",
      "description": "Muốn tìm hiểu về giải thuật svm, bất kỳ trang web nào cung cấp thông tin về giải thuật svm trong học máy: giải thích mô tả về giải thuật học máy svm, hướng dẫn cài đặt giải thuật svm trên một ngôn ngữ lập trình đều là kết quả phù hợp.",
      "sites": [
        {
          "url": "https://vi.wikipedia.org/wiki/M%C3%A1y_vect%C6%A1_h%E1%BB%97_tr%E1%BB%A3",
          "title": "Máy vectơ hỗ trợ",
          "content": "\n\t\t\t\t\t\t\t\t\tBách khoa toàn thư mở Wikipedia\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\tBước tới:\t\t\t\t\tmenu, \t\t\t\t\ttìm kiếm\n\t\t\t\t\n\t\t\t\tMáy vectơ hỗ trợ (SVM - viết tắt tên tiếng Anh support vector machine) là một khái niệm trong thống kê và khoa học máy tính cho một tập hợp các phương pháp học có giám sát liên quan đến nhau để phân loại và phân tích hồi quy. SVM dạng chuẩn nhận dữ liệu vào và phân loại chúng vào hai lớp khác nhau. Do đó SVM là một thuật toán phân loại nhị phân. Với một bộ các ví dụ luyện tập thuộc hai thể loại cho trước, thuật toán luyện tập SVM xây dựng một mô hình SVM để phân loại các ví dụ khác vào hai thể loại đó. Một mô hình SVM là một cách biểu diễn các điểm trong không gian và lựa chọn ranh giới giữa hai thể loại sao cho khoảng cách từ các ví dụ luyện tập tới ranh giới là xa nhất có thể. Các ví dụ mới cũng được biểu diễn trong cùng một không gian và được thuật toán dự đoán thuộc một trong hai thể loại tùy vào ví dụ đó nằm ở phía nào của ranh giới.\n\n\n\nMục lục\n\n\n1 Tổng quan về máy vectơ hỗ trợ\n2 Lịch sử\n3 Đặt vấn đề\n4 SVM tuyến tính\n\n4.1 Dạng ban đầu\n4.2 Dạng đối ngẫu\n\n\n5 Lề mềm\n\n5.1 Dạng đối ngẫu\n\n\n6 Xem thêm\n7 Ghi chú\n8 Liên kết ngoài\n9 Tham khảo\n\n\n\nTổng quan về máy vectơ hỗ trợ[sửa | sửa mã nguồn]\nMột máy vectơ hỗ trợ xây dựng một siêu phẳng hoặc một tập hợp các siêu phẳng trong một không gian nhiều chiều hoặc vô hạn chiều, có thể được sử dụng cho phân loại, hồi quy, hoặc các nhiệm vụ khác. Một cách trực giác, để phân loại tốt nhất thì các siêu phẳng nằm ở càng xa các điểm dữ liệu của tất cả các lớp (gọi là hàm lề) càng tốt, vì nói chung lề càng lớn thì sai số tổng quát hóa của thuật toán phân loại càng bé.\nTrong nhiều trường hợp, không thể phân chia các lớp dữ liệu một cách tuyến tính trong một không gian ban đầu được dùng để mô tả một vấn đề. Vì vậy, nhiều khi cần phải ánh xạ các điểm dữ liệu trong không gian ban đầu vào một không gian mới nhiều chiều hơn, để việc phân tách chúng trở nên dễ dàng hơn trong không gian mới. Để việc tính toán được hiệu quả, ánh xạ sử dụng trong thuật toán SVM chỉ đòi hỏi tích vô hướng của các vectơ dữ liệu trong không gian mới có thể được tính dễ dàng từ các tọa độ trong không gian cũ. Tích vô hướng này được xác định bằng một hàm hạt nhân K(x,y) phù hợp.[1] Một siêu phẳng trong không gian mới được định nghĩa là tập hợp các điểm có tích vô hướng với một vectơ cố định trong không gian đó là một hằng số. Vectơ xác định một siêu phẳng sử dụng trong SVM là một tổ hợp tuyến tính của các vectơ dữ liệu luyện tập trong không gian mới với các hệ số αi. Với siêu phẳng lựa chọn như trên, các điểm x trong không gian đặc trưng được ánh xạ vào một siêu mặt phẳng là các điểm thỏa mãn:\n\nΣi αi K(xi,x) = hằng số.\n\nGhi chú rằng nếu K(x,y) nhận giá trị ngày càng nhỏ khi y xa dần khỏi x thì mỗi số hạng của tổng trên được dùng để đo độ tương tự giữa x với điểm xi tương ứng trong dữ liệu luyện tập. Như vậy, tác dụng của tổng trên chính là so sánh khoảng cách giữa điểm cần dự đoán với các điểm dữ liệu đã biết. Lưu ý là tập hợp các điểm x được ánh xạ vào một siêu phẳng có thể có độ phức tạp tùy ý trong không gian ban đầu, nên có thể phân tách các tập hợp thậm chí không lồi trong không gian ban đầu.\nLịch sử[sửa | sửa mã nguồn]\nThuật toán SVM ban đầu được tìm ra bởi Vladimir N. Vapnik và dạng chuẩn hiện nay sử dụng lề mềm được tìm ra bởi Vapnik và Corinna Cortes năm 1995.[2]\nĐặt vấn đề[sửa | sửa mã nguồn]\n\n\n\n\nH3 (màu xanh lá cây) không chia tách hai lớp dữ liệu. H1 (màu xanh lơ) phân tách hai lớp với lề nhỏ và H2 (màu đỏ) phân tách với lề cực đại.\n\n\nPhân loại thống kê là một nhiệm vụ phổ biến trong học máy. Trong mô hình học có giám sát, thuật toán được cho trước một số điểm dữ liệu cùng với nhãn của chúng thuộc một trong hai lớp cho trước. Mục tiêu của thuật toán là xác định xem một điểm dữ liệu mới sẽ được thuộc về lớp nào. Mỗi điểm dữ liệu được biểu diễn dưới dạng một vector p-chiều, và ta muốn biết liệu có thể chia tách hai lớp dữ liệu bằng một siêu phẳng p − 1 chiều. Đây gọi là phân loại tuyến tính. Có nhiều siêu phẳng có thể phân loại được dữ liệu. Một lựa chọn hợp lý trong chúng là siêu phẳng có lề lớn nhất giữa hai lớp.\nSVM tuyến tính[sửa | sửa mã nguồn]\nTa có một tập huấn luyện \n  \n    \n      \n        \n          \n            D\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}}\n  \n gồm n điểm có dạng\n\n\n  \n    \n      \n        \n          \n            D\n          \n        \n        =\n        \n          \n            {\n            (\n            \n              \n                x\n              \n              \n                i\n              \n            \n            ,\n            \n              y\n              \n                i\n              \n            \n            )\n            ∣\n            \n              \n                x\n              \n              \n                i\n              \n            \n            ∈\n            \n              \n                R\n              \n              \n                p\n              \n            \n            ,\n            \n            \n              y\n              \n                i\n              \n            \n            ∈\n            {\n            −\n            1\n            ,\n            1\n            }\n            }\n          \n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}=\\left\\{(\\mathbf {x} _{i},y_{i})\\mid \\mathbf {x} _{i}\\in \\mathbb {R} ^{p},\\,y_{i}\\in \\{-1,1\\}\\right\\}_{i=1}^{n}}\n  \n\n\nvới yi mang giá trị 1 hoặc −1, xác định lớp của điểm \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n  \n. Mỗi \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n  \n là một vectơ thực p-chiều. Ta cần tìm siêu phẳng có lề lớn nhất chia tách các điểm có \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle y_{i}=1}\n  \n và các điểm có \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        −\n        1\n      \n    \n    {\\displaystyle y_{i}=-1}\n  \n. Mỗi siêu phẳng đều có thể được viết dưới dạng một tập hợp các điểm \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n  \n thỏa mãn\n\n\n\n\nSiêu phẳng với lề cực đại cho một SVM phân tách dữ liệu thuộc hai lớp. Các ví dụ nằm trên lề được gọi là các vectơ hỗ trợ.\n\n\n\n\n  \n    \n      \n        \n          w\n        \n        ⋅\n        \n          x\n        \n        −\n        b\n        =\n        0\n        ,\n        \n      \n    \n    {\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} -b=0,\\,}\n  \n\n\nvới \n  \n    \n      \n        ⋅\n      \n    \n    {\\displaystyle \\cdot }\n  \n ký hiệu cho tích vô hướng và \n  \n    \n      \n        \n          \n            w\n          \n        \n      \n    \n    {\\displaystyle {\\mathbf {w} }}\n  \n là một vectơ pháp tuyến của siêu phẳng. Tham số \n  \n    \n      \n        \n          \n            \n              b\n              \n                ∥\n                \n                  w\n                \n                ∥\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {b}{\\|\\mathbf {w} \\|}}}\n  \n xác định khoảng cách giữa gốc tọa độ và siêu phẳng theo hướng vectơ pháp tuyến \n  \n    \n      \n        \n          \n            w\n          \n        \n      \n    \n    {\\displaystyle {\\mathbf {w} }}\n  \n.\nChúng ta cần chọn \n  \n    \n      \n        \n          \n            w\n          \n        \n      \n    \n    {\\displaystyle {\\mathbf {w} }}\n  \n và \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n để cực đại hóa lề, hay khoảng cách giữa hai siêu mặt song song ở xa nhau nhất có thể trong khi vẫn phân chia được dữ liệu. Các siêu mặt ấy được xác định bằng\n\n\n  \n    \n      \n        \n          w\n        \n        ⋅\n        \n          x\n        \n        −\n        b\n        =\n        1\n        \n      \n    \n    {\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} -b=1\\,}\n  \n\n\nvà\n\n\n  \n    \n      \n        \n          w\n        \n        ⋅\n        \n          x\n        \n        −\n        b\n        =\n        −\n        1.\n        \n      \n    \n    {\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} -b=-1.\\,}\n  \n\n\nĐể ý rằng nếu dữ liệu huấn luyện có thể được chia tách một cách tuyến tính, thì ta có thể chọn hai siêu phẳng của lề sao cho không có điểm nào ở giữa chúng và sau đó tăng khoảng cách giữa chúng đến tối đa có thể. Bằng hình học, ta tìm được khoảng cách giữa hai siêu phẳng là \n  \n    \n      \n        \n          \n            \n              2\n              \n                ∥\n                \n                  w\n                \n                ∥\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {2}{\\|\\mathbf {w} \\|}}}\n  \n. Vì vậy ta muốn cực tiểu hóa giá trị \n  \n    \n      \n        ∥\n        \n          w\n        \n        ∥\n      \n    \n    {\\displaystyle \\|\\mathbf {w} \\|}\n  \n. Để đảm bảo không có điểm dữ liệu nào trong lề, ta thêm vào các điều kiện sau:\nVới mỗi \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n ta có\n\n\n  \n    \n      \n        \n          w\n        \n        ⋅\n        \n          \n            x\n          \n          \n            i\n          \n        \n        −\n        b\n        ≥\n        1\n        \n        \n           cho \n        \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} _{i}-b\\geq 1\\qquad {\\text{ cho }}\\mathbf {x} _{i}}\n  \n thuộc lớp thứ nhất\n\nhoặc\n\n\n  \n    \n      \n        \n          w\n        \n        ⋅\n        \n          \n            x\n          \n          \n            i\n          \n        \n        −\n        b\n        ≤\n        −\n        1\n        \n        \n           cho \n        \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} _{i}-b\\leq -1\\qquad {\\text{ cho }}\\mathbf {x} _{i}}\n  \n thuộc lớp thứ hai\n\nCó thể viết gọn lại như sau với mọi \n  \n    \n      \n        1\n        ≤\n        i\n        ≤\n        n\n      \n    \n    {\\displaystyle 1\\leq i\\leq n}\n  \n:\n\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        (\n        \n          w\n        \n        ⋅\n        \n          \n            x\n          \n          \n            i\n          \n        \n        −\n        b\n        )\n        ≥\n        1\n        ,\n        \n        \n        (\n        1\n        )\n      \n    \n    {\\displaystyle y_{i}(\\mathbf {w} \\cdot \\mathbf {x} _{i}-b)\\geq 1,\\qquad \\qquad (1)}\n  \n\n\nTóm lại, ta có bài toán tối ưu hóa sau:\nCực tiểu hóa (theo \n  \n    \n      \n        \n          \n            w\n          \n          ,\n          b\n        \n      \n    \n    {\\displaystyle {\\mathbf {w} ,b}}\n  \n)\n\n\n  \n    \n      \n        ∥\n        \n          w\n        \n        ∥\n      \n    \n    {\\displaystyle \\|\\mathbf {w} \\|}\n  \n\n\nvới điều kiện (với mọi \n  \n    \n      \n        i\n        =\n        1\n        ,\n        …\n        ,\n        n\n      \n    \n    {\\displaystyle i=1,\\dots ,n}\n  \n)\n\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        (\n        \n          w\n        \n        ⋅\n        \n          \n            x\n            \n              i\n            \n          \n        \n        −\n        b\n        )\n        ≥\n        1.\n        \n      \n    \n    {\\displaystyle y_{i}(\\mathbf {w} \\cdot \\mathbf {x_{i}} -b)\\geq 1.\\,}\n  \n\n\nDạng ban đầu[sửa | sửa mã nguồn]\nBài toán tối ưu ở mục trên tương đối khó giải vì hàm mục tiêu phụ thuộc vào ||w||, là một hàm có khai căn. Tuy nhiên có thể thay ||w|| bằng hàm mục tiêu \n  \n    \n      \n        \n          \n            \n              1\n              2\n            \n          \n        \n        ∥\n        \n          w\n        \n        \n          ∥\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {1}{2}}\\|\\mathbf {w} \\|^{2}}\n  \n (hệ số 1/2 để tiện cho các biến đổi toán học sau này) mà không làm thay đổi lời giải (lời giải của bài toán mới và bài toán ban đầu có cùng w và b). Đây là một bài toán quy hoạch toàn phương. Cụ thể hơn:\nCực tiểu hóa (theo \n  \n    \n      \n        \n          \n            w\n          \n          ,\n          b\n        \n      \n    \n    {\\displaystyle {\\mathbf {w} ,b}}\n  \n)\n\n\n  \n    \n      \n        \n          \n            1\n            2\n          \n        \n        ∥\n        \n          w\n        \n        \n          ∥\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{2}}\\|\\mathbf {w} \\|^{2}}\n  \n\n\nvới điều kiện (với mọi \n  \n    \n      \n        i\n        =\n        1\n        ,\n        …\n        ,\n        n\n      \n    \n    {\\displaystyle i=1,\\dots ,n}\n  \n)\n\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        (\n        \n          w\n        \n        ⋅\n        \n          \n            x\n            \n              i\n            \n          \n        \n        −\n        b\n        )\n        ≥\n        1.\n      \n    \n    {\\displaystyle y_{i}(\\mathbf {w} \\cdot \\mathbf {x_{i}} -b)\\geq 1.}\n  \n\n\nBằng cách thêm các nhân tử Lagrange \n  \n    \n      \n        \n          α\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\alpha }}}\n  \n, bài toán trên trở thành\n\n\n  \n    \n      \n        \n          min\n          \n            \n              w\n            \n            ,\n            b\n          \n        \n        \n          max\n          \n            \n              α\n            \n            ≥\n            0\n          \n        \n        \n          {\n          \n            \n              1\n              2\n            \n          \n          ∥\n          \n            w\n          \n          \n            ∥\n            \n              2\n            \n          \n          −\n          \n            ∑\n            \n              i\n              =\n              1\n            \n            \n              n\n            \n          \n          \n            \n              α\n              \n                i\n              \n            \n            [\n            \n              y\n              \n                i\n              \n            \n            (\n            \n              w\n            \n            ⋅\n            \n              \n                x\n                \n                  i\n                \n              \n            \n            −\n            b\n            )\n            −\n            1\n            ]\n          \n          }\n        \n      \n    \n    {\\displaystyle \\min _{\\mathbf {w} ,b}\\max _{{\\boldsymbol {\\alpha }}\\geq 0}\\left\\{{\\frac {1}{2}}\\|\\mathbf {w} \\|^{2}-\\sum _{i=1}^{n}{\\alpha _{i}[y_{i}(\\mathbf {w} \\cdot \\mathbf {x_{i}} -b)-1]}\\right\\}}\n  \n\n\nnghĩa là ta cần tìm một điểm yên ngựa. Khi đó, tất cả các điểm không nằm trên lề, nghĩa là \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        (\n        \n          w\n        \n        ⋅\n        \n          \n            x\n            \n              i\n            \n          \n        \n        −\n        b\n        )\n        −\n        1\n        >\n        0\n      \n    \n    {\\displaystyle y_{i}(\\mathbf {w} \\cdot \\mathbf {x_{i}} -b)-1>0}\n  \n đều không ảnh hưởng đến giá trị hàm mục tiêu vì ta có thể chọn \n  \n    \n      \n        \n          α\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{i}}\n  \n bằng không.\nCó thể giải bài toán này bằng các kĩ thuật thông thường cho quy hoạch toàn phương. Theo điều kiện Karush–Kuhn–Tucker, lời giải có thể được viết dưới dạng tổ hợp tuyến tính của các vectơ luyện tập\n\n\n  \n    \n      \n        \n          w\n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \n            α\n            \n              i\n            \n          \n          \n            y\n            \n              i\n            \n          \n          \n            \n              x\n              \n                i\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {w} =\\sum _{i=1}^{n}{\\alpha _{i}y_{i}\\mathbf {x_{i}} }.}\n  \n\n\nChỉ có một vài \n  \n    \n      \n        \n          α\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{i}}\n  \n nhận giá trị lớn hơn 0. Các điểm \n  \n    \n      \n        \n          \n            x\n            \n              i\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x_{i}} }\n  \n tương ứng là các vectơ hỗ trợ nằm trên lề và thỏa mãn \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        (\n        \n          w\n        \n        ⋅\n        \n          \n            x\n            \n              i\n            \n          \n        \n        −\n        b\n        )\n        =\n        1\n      \n    \n    {\\displaystyle y_{i}(\\mathbf {w} \\cdot \\mathbf {x_{i}} -b)=1}\n  \n. Từ điều kiện này, ta nhận thấy\n\n\n  \n    \n      \n        \n          w\n        \n        ⋅\n        \n          \n            x\n            \n              i\n            \n          \n        \n        −\n        b\n        =\n        1\n        \n          /\n        \n        \n          y\n          \n            i\n          \n        \n        =\n        \n          y\n          \n            i\n          \n        \n        \n        ⟺\n        \n        b\n        =\n        \n          w\n        \n        ⋅\n        \n          \n            x\n            \n              i\n            \n          \n        \n        −\n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {w} \\cdot \\mathbf {x_{i}} -b=1/y_{i}=y_{i}\\iff b=\\mathbf {w} \\cdot \\mathbf {x_{i}} -y_{i}}\n  \n\n\ntừ đó ta suy ra được giá trị \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n. Trên thực tế, một cách thức tốt hơn để tính \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n là tính giá trị trung bình từ tất cả \n  \n    \n      \n        \n          N\n          \n            S\n            V\n          \n        \n      \n    \n    {\\displaystyle N_{SV}}\n  \n vectơ hỗ trợ:\n\n\n  \n    \n      \n        b\n        =\n        \n          \n            1\n            \n              N\n              \n                S\n                V\n              \n            \n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            \n              N\n              \n                S\n                V\n              \n            \n          \n        \n        \n          (\n          \n            w\n          \n          ⋅\n          \n            \n              x\n              \n                i\n              \n            \n          \n          −\n          \n            y\n            \n              i\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle b={\\frac {1}{N_{SV}}}\\sum _{i=1}^{N_{SV}}{(\\mathbf {w} \\cdot \\mathbf {x_{i}} -y_{i})}}\n  \n\n\nDạng đối ngẫu[sửa | sửa mã nguồn]\nNếu viết điều kiện phân loại dưới dạng đối ngẫu không điều kiện thì sẽ dễ dàng nhận thấy siêu phẳng với lề lớn nhất, và do đó nhiệm vụ phân loại, chỉ phụ thuộc vào các điểm luyện tập nằm trên lề, còn gọi là các vectơ hỗ trợ.\nVì \n  \n    \n      \n        ∥\n        \n          w\n        \n        \n          ∥\n          \n            2\n          \n        \n        =\n        w\n        ⋅\n        w\n      \n    \n    {\\displaystyle \\|\\mathbf {w} \\|^{2}=w\\cdot w}\n  \n và \n  \n    \n      \n        \n          w\n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \n            α\n            \n              i\n            \n          \n          \n            y\n            \n              i\n            \n          \n          \n            \n              x\n              \n                i\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {w} =\\sum _{i=1}^{n}{\\alpha _{i}y_{i}\\mathbf {x_{i}} }}\n  \n, ta nhận thấy bài toán đối ngẫu của SVM là chính là bài toán tối ưu hóa sau:\nCực đại hóa (theo \n  \n    \n      \n        \n          α\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{i}}\n  \n)\n\n\n  \n    \n      \n        \n          \n            \n              L\n              ~\n            \n          \n        \n        (\n        \n          α\n        \n        )\n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          α\n          \n            i\n          \n        \n        −\n        \n          \n            1\n            2\n          \n        \n        \n          ∑\n          \n            i\n            ,\n            j\n          \n        \n        \n          α\n          \n            i\n          \n        \n        \n          α\n          \n            j\n          \n        \n        \n          y\n          \n            i\n          \n        \n        \n          y\n          \n            j\n          \n        \n        \n          \n            x\n          \n          \n            i\n          \n          \n            T\n          \n        \n        \n          \n            x\n          \n          \n            j\n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          α\n          \n            i\n          \n        \n        −\n        \n          \n            1\n            2\n          \n        \n        \n          ∑\n          \n            i\n            ,\n            j\n          \n        \n        \n          α\n          \n            i\n          \n        \n        \n          α\n          \n            j\n          \n        \n        \n          y\n          \n            i\n          \n        \n        \n          y\n          \n            j\n          \n        \n        k\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        ,\n        \n          \n            x\n          \n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\tilde {L}}(\\mathbf {\\alpha } )=\\sum _{i=1}^{n}\\alpha _{i}-{\\frac {1}{2}}\\sum _{i,j}\\alpha _{i}\\alpha _{j}y_{i}y_{j}\\mathbf {x} _{i}^{T}\\mathbf {x} _{j}=\\sum _{i=1}^{n}\\alpha _{i}-{\\frac {1}{2}}\\sum _{i,j}\\alpha _{i}\\alpha _{j}y_{i}y_{j}k(\\mathbf {x} _{i},\\mathbf {x} _{j})}\n  \n\n\nvới điều kiện (với mọi \n  \n    \n      \n        i\n        =\n        1\n        ,\n        …\n        ,\n        n\n      \n    \n    {\\displaystyle i=1,\\dots ,n}\n  \n)\n\n\n  \n    \n      \n        \n          α\n          \n            i\n          \n        \n        ≥\n        0\n        ,\n        \n      \n    \n    {\\displaystyle \\alpha _{i}\\geq 0,\\,}\n  \n\n\nvà điều kiện sau ứng với việc cực tiểu hóa theo \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n\n\n\n  \n    \n      \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          α\n          \n            i\n          \n        \n        \n          y\n          \n            i\n          \n        \n        =\n        0.\n      \n    \n    {\\displaystyle \\sum _{i=1}^{n}\\alpha _{i}y_{i}=0.}\n  \n\n\nỞ đây hàm hạt nhân được định nghĩa là \n  \n    \n      \n        k\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        ,\n        \n          \n            x\n          \n          \n            j\n          \n        \n        )\n        =\n        \n          \n            x\n          \n          \n            i\n          \n        \n        ⋅\n        \n          \n            x\n          \n          \n            j\n          \n        \n      \n    \n    {\\displaystyle k(\\mathbf {x} _{i},\\mathbf {x} _{j})=\\mathbf {x} _{i}\\cdot \\mathbf {x} _{j}}\n  \n.\nSau khi giải xong, có thể tính \n  \n    \n      \n        \n          w\n        \n      \n    \n    {\\displaystyle \\mathbf {w} }\n  \n từ các giá trị \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n tìm được như sau:\n\n\n  \n    \n      \n        \n          w\n        \n        =\n        \n          ∑\n          \n            i\n          \n        \n        \n          α\n          \n            i\n          \n        \n        \n          y\n          \n            i\n          \n        \n        \n          \n            x\n          \n          \n            i\n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {w} =\\sum _{i}\\alpha _{i}y_{i}\\mathbf {x} _{i}.}\n  \n\n\nLề mềm[sửa | sửa mã nguồn]\nNăm 1995, Corinna Cortes và Vladimir N. Vapnik đề xuất một ý tưởng mới cho phép thuật toán gán nhãn sai cho một số ví dụ luyện tập.[2] Nếu không tồn tại siêu phẳng nào phân tách được hai lớp dữ liệu, thì thuật toán lề mềm sẽ chọn một siêu phẳng phân tách các ví dụ luyện tập tốt nhất có thể, và đồng thời cực đại hóa khoảng cách giữa siêu phẳng với các ví dụ được gán đúng nhãn. Phương pháp này sử dụng các biến bù \n  \n    \n      \n        \n          ξ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\xi _{i}}\n  \n, dùng để đo độ sai lệch của ví dụ \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  \n\n\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        (\n        \n          w\n        \n        ⋅\n        \n          \n            x\n            \n              i\n            \n          \n        \n        −\n        b\n        )\n        ≥\n        1\n        −\n        \n          ξ\n          \n            i\n          \n        \n        \n        1\n        ≤\n        i\n        ≤\n        n\n        .\n        \n        \n        (\n        2\n        )\n      \n    \n    {\\displaystyle y_{i}(\\mathbf {w} \\cdot \\mathbf {x_{i}} -b)\\geq 1-\\xi _{i}\\quad 1\\leq i\\leq n.\\quad \\quad (2)}\n  \n\n\nHàm mục tiêu có thêm một số hạng mới để phạt thuật toán khi \n  \n    \n      \n        \n          ξ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\xi _{i}}\n  \n khác không, và bài toán tối ưu hóa trở thành việc trao đổi giữa lề lớn và mức phạt nhỏ. Nếu hàm phạt là tuyến tính thì bài toán trở thành:\n\n\n  \n    \n      \n        \n          min\n          \n            \n              w\n            \n            ,\n            \n              ξ\n            \n            ,\n            b\n          \n        \n        \n          {\n          \n            \n              1\n              2\n            \n          \n          ∥\n          \n            w\n          \n          \n            ∥\n            \n              2\n            \n          \n          +\n          C\n          \n            ∑\n            \n              i\n              =\n              1\n            \n            \n              n\n            \n          \n          \n            ξ\n            \n              i\n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\min _{\\mathbf {w} ,\\mathbf {\\xi } ,b}\\left\\{{\\frac {1}{2}}\\|\\mathbf {w} \\|^{2}+C\\sum _{i=1}^{n}\\xi _{i}\\right\\}}\n  \n\n\nvới điều kiện (với mọi \n  \n    \n      \n        i\n        =\n        1\n        ,\n        …\n        n\n      \n    \n    {\\displaystyle i=1,\\dots n}\n  \n)\n\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        (\n        \n          w\n        \n        ⋅\n        \n          \n            x\n            \n              i\n            \n          \n        \n        −\n        b\n        )\n        ≥\n        1\n        −\n        \n          ξ\n          \n            i\n          \n        \n        ,\n         \n         \n         \n         \n        \n          ξ\n          \n            i\n          \n        \n        ≥\n        0\n      \n    \n    {\\displaystyle y_{i}(\\mathbf {w} \\cdot \\mathbf {x_{i}} -b)\\geq 1-\\xi _{i},~~~~\\xi _{i}\\geq 0}\n  \n\n\nCó thể giải bài toán trên bằng nhân tử Lagrange tương tự như trường hợp cơ bản ở trên. Bài toán cần giải trở thành:\n\n\n  \n    \n      \n        \n          min\n          \n            \n              w\n            \n            ,\n            \n              ξ\n            \n            ,\n            b\n          \n        \n        \n          max\n          \n            \n              α\n            \n            ,\n            \n              β\n            \n          \n        \n        \n          {\n          \n            \n              1\n              2\n            \n          \n          ∥\n          \n            w\n          \n          \n            ∥\n            \n              2\n            \n          \n          +\n          C\n          \n            ∑\n            \n              i\n              =\n              1\n            \n            \n              n\n            \n          \n          \n            ξ\n            \n              i\n            \n          \n          −\n          \n            ∑\n            \n              i\n              =\n              1\n            \n            \n              n\n            \n          \n          \n            \n              α\n              \n                i\n              \n            \n            [\n            \n              y\n              \n                i\n              \n            \n            (\n            \n              w\n            \n            ⋅\n            \n              \n                x\n                \n                  i\n                \n              \n            \n            −\n            b\n            )\n            −\n            1\n            +\n            \n              ξ\n              \n                i\n              \n            \n            ]\n          \n          −\n          \n            ∑\n            \n              i\n              =\n              1\n            \n            \n              n\n            \n          \n          \n            β\n            \n              i\n            \n          \n          \n            ξ\n            \n              i\n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\min _{\\mathbf {w} ,\\mathbf {\\xi } ,b}\\max _{{\\boldsymbol {\\alpha }},{\\boldsymbol {\\beta }}}\\left\\{{\\frac {1}{2}}\\|\\mathbf {w} \\|^{2}+C\\sum _{i=1}^{n}\\xi _{i}-\\sum _{i=1}^{n}{\\alpha _{i}[y_{i}(\\mathbf {w} \\cdot \\mathbf {x_{i}} -b)-1+\\xi _{i}]}-\\sum _{i=1}^{n}\\beta _{i}\\xi _{i}\\right\\}}\n  \n\n\nvới \n  \n    \n      \n        \n          α\n          \n            i\n          \n        \n        ,\n        \n          β\n          \n            i\n          \n        \n        ≥\n        0\n      \n    \n    {\\displaystyle \\alpha _{i},\\beta _{i}\\geq 0}\n  \n.\nDạng đối ngẫu[sửa | sửa mã nguồn]\nCực đại hóa (theo \n  \n    \n      \n        \n          α\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{i}}\n  \n)\n\n\n  \n    \n      \n        \n          \n            \n              L\n              ~\n            \n          \n        \n        (\n        \n          α\n        \n        )\n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          α\n          \n            i\n          \n        \n        −\n        \n          \n            1\n            2\n          \n        \n        \n          ∑\n          \n            i\n            ,\n            j\n          \n        \n        \n          α\n          \n            i\n          \n        \n        \n          α\n          \n            j\n          \n        \n        \n          y\n          \n            i\n          \n        \n        \n          y\n          \n            j\n          \n        \n        k\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        ,\n        \n          \n            x\n          \n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\tilde {L}}(\\mathbf {\\alpha } )=\\sum _{i=1}^{n}\\alpha _{i}-{\\frac {1}{2}}\\sum _{i,j}\\alpha _{i}\\alpha _{j}y_{i}y_{j}k(\\mathbf {x} _{i},\\mathbf {x} _{j})}\n  \n\n\nvới điều kiện (với mọi \n  \n    \n      \n        i\n        =\n        1\n        ,\n        …\n        ,\n        n\n      \n    \n    {\\displaystyle i=1,\\dots ,n}\n  \n)\n\n\n  \n    \n      \n        0\n        ≤\n        \n          α\n          \n            i\n          \n        \n        ≤\n        C\n        ,\n        \n      \n    \n    {\\displaystyle 0\\leq \\alpha _{i}\\leq C,\\,}\n  \n\n\nvà\n\n\n  \n    \n      \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          α\n          \n            i\n          \n        \n        \n          y\n          \n            i\n          \n        \n        =\n        0.\n      \n    \n    {\\displaystyle \\sum _{i=1}^{n}\\alpha _{i}y_{i}=0.}\n  \n\n\nƯu điểm của việc dùng hàm phạt tuyến tính là các biến bù biến mất khỏi bài toán đối ngẫu, và hằng số C chỉ xuất hiện dưới dạng một chặn trên cho các nhân tử Lagrange. Cách đặt vấn đề trên đã mang lại nhiều thành quả trong thực tiễn, và Cortes và Vapnik đã nhận được giải Paris Kanellakis của ACM năm 2008 cho đóng góp này.[3] Các hàm phạt phi tuyến cũng được sử dụng, đặc biệt là để giảm ảnh hưởng của các trường hợp ngoại lệ, tuy nhiên nếu không lựa chọn hàm phạt cẩn thận thì bài toán trở thành không lồi, và việc tìm lời giải tối ưu toàn cục thường là rất khó.\nXem thêm[sửa | sửa mã nguồn]\n\nIn situ adaptive tabulation\nMáy hạt nhân\nPredictive analytics\nRelevance vector machine, Một mô hình máy hạt nhân thưa xác suất có dạnghàm số giống như SVM.\nTối ưu hóa nhỏ nhất tuần tự\n\nGhi chú[sửa | sửa mã nguồn]\n\n\n^ Press, William H.; Teukolsky, Saul A.; Vetterling, William T.; Flannery, B. P. (2007). “Section 16.5. Support Vector Machines”. Numerical Recipes: The Art of Scientific Computing (ấn bản 3). New York: Cambridge University Press. ISBN 978-0-521-88068-8. \n^ a ă Cortes, Corinna; and Vapnik, Vladimir N.; \"Support-Vector Networks\", Machine Learning, 20, 1995. http://www.springerlink.com/content/k238jx04hm87j80g/\n^ ACM Website, Press release of March 17th 2009. http://www.acm.org/press-room/news-releases/awards-08-groupa\n\n\nLiên kết ngoài[sửa | sửa mã nguồn]\n\nBurges, Christopher J.C. (1998), “A Tutorial on Support Vector Machines for Pattern Recognition” (PDF), Data Mining and Knowledge Discovery 2: 121–167  * www.kernel-machines.org (thông tin tổng quan và danh sách các bài báo nghiên cứu)\nwww.support-vector-machines.org (Bài báo nghiên cứu, đánh giá,, phần mềm, liên kết có liên quan đến máy vectơ hỗ trợ)\nvideolectures.net (Video bài giảng về SVM)\nPhim ngắn: Minh họa SVM sử dụng hàm hạt nhân đa thức.\nMột hướng dẫn sử dụng SVM cho người mới học bởi Tristan Fletcher [1].\nwww.shogun-toolbox.org (Shogun (hộp công cụ) gồm khoảng 20 thư viện lập trình SVM)\nlibsvm libsvm là một thư viện lập trình SVM\nliblinear liblinear là một thư viện lập trình gồm nhiều thuật toán phân loại tuyến tính, trong đó có SVM\nflssvm flssvm là một thư viện lập trình svm bình phương nhỏ nhất viết bằng fortran\nShark Shark là một thư viện học máy viết bằng C++ có chứa nhiều loại SVM\ndlib dlib là một thư viện C++ cho máy hạt nhân và SVM\nSVM light là một bộ phần mềm cho học máy và phân loại bằng SVM.\n\nTham khảo[sửa | sửa mã nguồn]\n\nSergios Theodoridis and Konstantinos Koutroumbas \"Pattern Recognition\", 4th Edition, Academic Press, 2009, ISBN 978-1-59749-272-0\nNello Cristianini and John Shawe-Taylor. An Introduction to Support Vector Machines and other kernel-based learning methods. Cambridge University Press, 2000. ISBN 0-521-78019-5 ([2] SVM Book)\nHuang T.-M., Kecman V., Kopriva I. (2006), Kernel Based Algorithms for Mining Huge Data Sets, Supervised, Semi-supervised, and Unsupervised Learning, Springer-Verlag, Berlin, Heidelberg, 260 pp. 96 illus., Hardcover, ISBN 3-540-31681-7 [3]\nVojislav Kecman: \"Learning and Soft Computing — Support Vector Machines, Neural Networks, Fuzzy Logic Systems\", The MIT Press, Cambridge, MA, 2001.[4]\nBernhard Schölkopf and A. J. Smola: Learning with Kernels. MIT Press, Cambridge, MA, 2002. (Partly available on line: [5].) ISBN 0-262-19475-9\nBernhard Schölkopf, Christopher J.C. Burges, and Alexander J. Smola (editors). \"Advances in Kernel Methods: Support Vector Learning\". MIT Press, Cambridge, MA, 1999. ISBN 0-262-19416-3. [6]\nJohn Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004. ISBN 0-521-81397-2 ([7] Kernel Methods Book)\nIngo Steinwart and Andreas Christmann. Support Vector Machines. Springer-Verlag, New York, 2008. ISBN 978-0-387-77241-7 ([8] SVM Book)\nP.J. Tan and D.L. Dowe (2004), MML Inference of Oblique Decision Trees, Lecture Notes in Artificial Intelligence (LNAI) 3339, Springer-Verlag, pp1082-1088. (This paper uses minimum message length (MML) and actually incorporates probabilistic support vector machines in the leaves of decision trees.)\nVladimir Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, 1995. ISBN 0-387-98780-0\nVladimir Vapnik, S.Kotz \"Estimation of Dependences Based on Empirical Data\" Springer, 2006. ISBN 0-387-30865-2, 510 pages [this is a reprint of Vapnik's early book describing philosophy behind SVM approach. The 2006 Appendix describes recent development].\nDmitriy Fradkin and Ilya Muchnik \"Support Vector Machines for Classification\" in J. Abello and G. Carmode (Eds) \"Discrete Methods in Epidemiology\", DIMACS Series in Discrete Mathematics and Theoretical Computer Science, volume 70, pp. 13–20, 2006. [9]. Succinctly describes theoretical ideas behind SVM.\nKristin P. Bennett and Colin Campbell, \"Support Vector Machines: Hype or Hallelujah?\", SIGKDD Explorations, 2,2, 2000, 1–13. [10]. Excellent introduction to SVMs with helpful figures.\nOvidiu Ivanciuc, \"Applications of Support Vector Machines in Chemistry\", In: Reviews in Computational Chemistry, Volume 23, 2007, pp. 291–400. Reprint available: [11]\nCatanzaro, Sundaram, Keutzer, \"Fast Support Vector Machine Training and Classification on Graphics Processors\", In: International Conference on Machine Learning, 2008 [12]\n\n\n\n\n\n\n\n\t\t\t\t\t\n\t\t\t\t\t\tLấy từ “https://vi.wikipedia.org/w/index.php?title=Máy_vectơ_hỗ_trợ&oldid=26464254”\t\t\t\t\t\n\t\t\t\tThể loại: Giải thuật phân loạiHọc máyPhân loại thống kêPhân loại bằng thống kêThể loại ẩn: Trang sử dụng liên kết tự động ISBN\t\t\t\t\n\t\t\t\t\t\t\t",
          "relevence": "yes"
        },
        {
          "url": "https://www.stdio.vn/articles/read/436/gioi-thieu-ve-mo-hinh-svm",
          "title": "Giới Thiệu Về Mô Hình SVM :: Bài viết :: STDIO",
          "content": "SVM là mô hình được sử dụng trong nhiều ngành, là một mô hình máy học giám sát được dùng để phân tích, phân lớp dữ liệu. Có thể những điều ở đây khá trừ tượng. Trong bài viết này, tôi sẽ giới thiệu một cách tổng quan về mô hình SVM và ví dụ về SVM trong OPenCV.Bài viết nằm trong loạt bài viết chương trình Tự Học OpenCV Qua Các Ví Dụ Thực Tế của STDIO. Kiến thức trong bài viết này rất quan trọng trong việc hoàn thành ứng dụng nhận diện biển số xe và nhận diện chữ viết tay,...Bài viết hướng tới các bạn đọc mới tìm hiểu về xử lý ảnh, đặc biệt là với OpenCV và chưa có kiến thức tương đương.Trong bài viết này tôi sử dụng Windows 10 (64-bit), Visual Stdio 2013 Ultimate và OpenCV 3.0.SVM (Support Vector Machine) là một khái niệm trong thống kê và khoa học máy tính cho một tập hợp các phương pháp học có giám sát liên quan đến nhau để phân loại và phân tích hồi quy.SVM là một thuật toán phân loại nhị phân, SVM nhận dữ liệu vào và phân loại chúng vào hai lớp khác nhau. Với một bộ các ví dụ luyện tập thuộc hai thể loại cho trước, thuật toán luyện tập SVM xây dựng một mô hình SVM để phân loại các ví dụ khác vào hai thể loại đó.Tôi có một khôn gian có nhiều điểm và các kí hiệu như sau:Phân tíchvàTổng quanVới các điểm tổng quan ở trên thì nhiệm vụ chính là phân loại thống kế.Dữ liệu được huấn luyện được tạo thành bởi một tập điểm có giá trị 2D. Ở đoạn code trên, tôi có:Chú ý: Số lượng phần tử của ma trận labels bằng với số cột của ma trận của trainingData.Chú ý: Hàm CvSVM::train() sử dụng kiểu dữ liệu truyền vào là kiểu Mat.Phân tíchChú ý: Như ở phần Đối tượng hướng đến trong bài viết. Tôi sử dụng OpenCV 3.0. Với các phiên bản khác của OpenCV, để thiết lập thông số cho SVM sẽ có cách hiện thực khác.Để xây dựng mô hình huấn luyện SVM, tôi sử dụng phương thức CvSVM::train().Như vậy tôi đã huấn luyện xong cho mô hình SVM của ví dụ ở trên.Phương pháp CvSVM::predict() được sử dụng để phân loại một mẫu đầu vào bằng cách sử dụng một SVM được huấn luyện. Như bạn thấy ở trên, đây chính là kết quả của đoạn code trên.Trong OpenCV 3.0, để có thể lấy được các điểm Support Vector thì sử dụng phương thức SVM::getSupportVectors(). Kiểu trả về ở đây là một Mat, với số hàng (Rows) chính là số lượng của các điểm Support VectorTrong bài viết này, tôi đã giới thiệu sơ qua về mô hình SVM cũng như ví dụ về hoạt động của SVM trong OpenCV. Ứng dụng của SVM trong các lĩnh vực nói chung là rất nhiều. Ở trong xử lý ảnh, SVM cũng tác dụng to lớn nhiều trong các ứng dụng như trong ứng dụng nhận dạng ký tự viết tay, nhận dạng biển số xe,...Mọi thắc mắc có thể bình luận tại bài viết hoặc liên hệ với  Trương Đạt.http://docs.opencv.org/https://en.wikipedia.org/wiki/Support_vector_machine",
          "relevence": "yes"
        },
        {
          "url": "https://ongxuanhong.wordpress.com/2015/09/19/support-vector-machine-svm-hoi-gi-dap-nay/",
          "title": "Support vector machine (SVM) hỏi gì đáp nấy – Ông Xuân Hồng",
          "content": "SVM classificationDùng để làm gì? Support vector machine (SVM) xây dựng (learn) một siêu phẳng (hyperplane) để phân lớp (classify) tập dữ liệu thành 2 lớp riêng biệt.Siêu phẳng là cái gì? Một siêu phẳng là một hàm tương tự như phương trình đường thẳng, y = ax + b. Trong thực tế, nếu ta cần phân lớp tập dữ liệu chỉ gồm 2 feature, siêu phẳng lúc này chính là một đường thẳng.Về ý tưởng thì SVM sử dụng thủ thuật để ánh xạ tập dữ liệu ban đầu vào không gian nhiều chiều hơn. Khi đã ánh xạ sang không gian nhiều chiều, SVM sẽ xem xét và chọn ra siêu phẳng phù hợp nhất để phân lớp tập dữ liệu đó.Có ví dụ đơn giản nào không? Ta lấy ví dụ đơn giản về phân chia tập các quả bóng xanh và đỏ đặt trên một cái bàn. Nếu các quả bóng phân bố không quá đan xen vào nhau, ta có thể dùng một cây que dài để chia các quả bóng thành hai tập xanh và đỏ mà không động đến các quả bóng.Lúc này, khi đưa một quả bóng mới đặt lên mặt bàn, bằng cách xác định nó nằm bên phía nào của cây que, ta có thể dự đoán màu sắc của quả bóng đó.Vậy các quả bóng, cái bàn và cây que tượng trưng cho cái gì? Các quả bóng tượng trưng cho các điểm dữ liệu, màu xanh và đỏ tượng trưng cho 2 lớp. Cái bàn tượng trưng cho một mặt phẳng. Cây que tượng trưng cho một siêu phẳng đơn giản đó là một đường thẳng.Điểm hấp dẫn ở đây đó là SVM có thể hình dung ra được đâu là siêu phẳng phù hợp nhất.Đối với trường hợp phức tạp hơn thì sao? Thật ra dữ liệu ngoài thực tế rất phức tạp. Nếu các quả bóng xen lẫn vào nhau thì một cây que khó có thể phân lớp được.Ta sử dụng thủ thuật: nhấc bổng cái bàn lên, nhanh chóng hất các quả bóng lên trời. Trong khi các quả bóng đang lơ lửng và nằm ở các vị trí thích hợp, ta dùng một mảnh giấy lớn để phân lớp các quả bóng đang lơ lửng này.Nghe có vẻ gian dối? Không đâu, việc hất các quả bóng lên trời tương đương với việc ánh xạ tập dữ liệu ban đầu vào không gian nhiều chiều hơn. Trong trường hợp này, ta đi từ không gian 2 chiều đó là cái bàn vào không gian 3 chiều đó các quả bóng đang lơ lửng trên trời.Làm sao SVM thực hiện được điều này? Bằng cách sử dụng một kernel, ta có thể đơn giản nâng dữ liệu ban đầu vào không gian nhiều chiều hơn. Mảnh giấy lớn lúc này cũng được gọi là một siêu phẳng, chỉ có điều đây là phương trình mặt phẳng chứ không phải phương trình đường thẳng.Clip bên dưới sẽ minh họa cho thao tác trên của SVMViệc ánh xạ này trong thực tế được thực hiện như thế nào? Quả bóng nằm trên mặt bàn có một vị trí cụ thể, ta dùng trục tọa độ để xác định vị trí này. Ví dụ, quả bóng nằm cách mép trái 20cm và cách mép dưới 50cm được thể hiện trên trục tọa độ (x, y) tương ứng là (20, 50). x và y chính là không gian hai chiều của quả bóng. Khi đưa lên chiều thứ 3 là z(x, y), ta có thể tính được tọa độ của z trong không gian 3 chiều dựa vào tọa độ x,y ban đầu.Margin SVMThuật ngữ margin trong SVM có nghĩa là gì? Margin là khoảng cách giữa siêu phẳng đến 2 điểm dữ liệu gần nhất tương ứng với các phân lớp. Trong ví dụ quả bóng và cái bàn, đó là khoảng cách giữa cây que và hai quả bóng xanh và đỏ gần nó nhất.Điểm mấu chốt ở đây đó là SVM cố gắng maximize margin này, từ đó thu được một siêu phẳng tạo khoảng cách xa nhất so với 2 quả bóng xanh và đỏ. Nhờ vậy, SVM có thể giảm thiểu việc phân lớp sai (misclassification) đối với điểm dữ liệu mới đưa vào.Tên gọi SVM xuất phát từ đâu? Trong ví dụ cái bàn và quả bóng, siêu phẳng cách đều với bóng xanh và bóng đỏ. Các quả bóng này chính là các điểm dữ liệu gọi là support vectors, vì chúng hỗ trợ (support) để dựng lên siêu phẳng.Tại sao sử dụng SVM? SVM cho độ chính xác cao đối với tập dữ liệu có kiểu dữ liệu liên tục (continuous value), cùng với thuật toán cây quyết định là hai phương pháp thường được dùng để phân lớp dữ liệu. Tuy nhiên, không có mô hình phân lớp (classifier) nào là tốt nhất theo No Free Lunch Theorem. Thêm vào đó, việc lựa chọn kernel và diễn giải cho người dùng hiểu là một điểm trừ của SVM.Có thư viện nào cho SVM? Hiện tại có rất nhiều thư viện cài đặt SVM. Một vài thư viện phổ biến như scikit-learn, MATLAB và dĩ nhiên là libsvm.Nguồn tham khảo:việc tính toán để tìm được khoảng cách margin lớn nhất trong thuật toán như thế nào addcos thể trình bày cụ thể được không ạ?\ncó ví dụ cụ thể nào không?Số lượt thíchSố lượt thíchHi bạn, svm được chuyển đổi sang bài toán tối ưu Lagrange, mình thường sử dụng Matlab để thực nghiệm và ko đi sâu vào thuật toán, bạn có thể tham khảo note liên quan\nSupport Vector Machines  http://cs229.stanford.edu/notes/cs229-notes3.pdfSố lượt thíchSố lượt thíchthầy cho em ít tài liệu về phần phân lớp dữ liệu trong SVM được ko ạ?Số lượt thíchSố lượt thíchHi, em có thể tham khảo bài giảng này http://cs229.stanford.edu/notes/cs229-notes3.pdfSố lượt thíchSố lượt thíchThầy ơi, thầy có tài liệu về phân loại ảnh dựa trên cây quyết định (decision tree) ko ạ? Thầy cho em xin đc ko thầy? Em cảm ơn thầy.\n.Số lượt thíchSố lượt thíchem chuyển ảnh mxn thành 1xk (lấy mỗi dòng của ảnh ghép lại với nhau thành vector duy nhất, không còn dạng ma trận nữa) rồi áp dụng decision tree bình thường.Số lượt thíchSố lượt thíchEm chào thầy ẹ! em đang làm khóa luận có liên quan đến sử dụng SVM cho dự đoán chứng khoán. thầy cho e tài liệu về phần dự đoán của SVM được không ạ?   em tìm tài liệu nhưng phần này rất ít. e cảm ơn thầy ạSố lượt thíchSố lượt thíchem nên liên hệ với thầy hướng dẫn nhé, anh cũng không đi sâu vào mô hình này, chỉ áp dụng thư viện để thực nghiệm thôi. Nếu muốn tìm các bài báo hay tài liệu em có thể vào https://scholar.google.com.vn tìm với keyword là “svm stock prediction”Số lượt thíchSố lượt thíchcho em hỏi bài toán này áp dụng với các dữ liệu liên tục ? thầy giải thích cụ thể hơn được không ạ.  Em có một bài toán phân lớp mà dữ liệu đầu vào các feature có thể có mối liên hệ với nhau thì dùng giải thuật này được không ạ, em muốn dùng một giải thuật  ANN, giải thuật nào của ANN thì phù hợp bài toán của em ạ.Số lượt thíchSố lượt thíchDữ liệu đầu vào có nhiều loại\nhttp://blog.minitab.com/blog/understanding-statistics/understanding-qualitative-quantitative-attribute-discrete-and-continuous-data-types\nCác thuật toán như SVM hay ANN đều cần thuộc tính là số liên tục để phân lớp (không phải danh mục, text)Số lượt thíchSố lượt thíchCho em hỏi thêm chút ạ. Ví dụ em có bài toán mà đầu vào vector [12, 10000, 10 , 30,  …] mà các số là các feature thể hiện một mặt của dữ liệu, mà các số đó em nghĩ là kết hợp với nhau (có mối quan hệ) để từ đó đưa ra phán đoán tốt nhất cho label ấy ạ.  thì giải thuật  ANN phù hợp với bài toán này ạ, em hỏi thêm nữa là các số có độc lệch về giá trị kiểu như 10,  20, 100000 thì có cần xử lý sao cho các giá trị gần khoảng lớn với nhau như chia cho bao nhiêu những giá trị feature thứ 2 chẳng hạn không anh.\nEm cảm ơn anh nhiều :))Số lượt thíchSố lượt thíchCâu hỏi 1: em có thể giảm số chiều dữ liệu bằng PCA https://ongxuanhong.wordpress.com/2015/07/18/exploratory-data-analysis-giam-so-chieu-du-lieu/\nCâu hỏi 2: em có thể chuẩn hóa feature https://en.wikipedia.org/wiki/Feature_scalingSố lượt thíchSố lượt thíchEm cảm ơn anh ạ, còn câu hỏi về giải thuật ANN nào phù hợp với bài toán mà các dữ liệu liên quan như em trình bày không ạ.Số lượt thíchSố lượt thíchCơ bản là thuật toán Gradient Descent từ đó ta sẽ có các phương pháp tối ưu về tốc độ và độ chính xác http://cs231n.github.io/neural-networks-3/Số lượt thíchSố lượt thíchEm cảm ơn anh :))Số lượt thíchSố lượt thíchAnh ơi, cho em hỏi chút, em có đọc về sự khác biệt giữa deep learning và neural network thì deep learning là neural network with multi layer, có đúng không ạ. Một câu nữa là neural network có thể giải quyết các bài toán mà những thuật toán ML cổ điển có thể giải quyết nhưng vấn đề là tốn thời gian train, có phải không ạ.\nEm cảm ơnSố lượt thíchSố lượt thích1) deep learning là neural network with multi layer -> đúng một phần, phần còn lại phụ thuộc các phép biến đổi giữa các layer (CNN, RNN, LSTM, …)\n2) neural network có thể giải quyết các bài toán mà những thuật toán ML cổ điển có thể giải quyết nhưng vấn đề là tốn thời gian train -> không đúng, lý do tuỳ bài toán và thuật toán mà thời gian train sẽ khác nhau (k-NN là một ví dụ), hơn nữa nếu NN có độ chính xác kém hơn các mô hình khác thì dĩ nhiên ta không xem NN giải quyết được vấn đề.Số lượt thíchSố lượt thíchcho em hỏi một chút, là những thuật toán ANN như CNN, RNN, LSTM, thì có phải là tạo ra để giải quyết những bài toán đặc trưng về xử lý ảnh hay ngôn ngữ tự nhiên không ạ, hay là nó sẽ giải quyết những bài toán tùy thuộc vào feature đầu vào.Số lượt thíchSố lượt thíchDo ANN là representation model, ta không cần làm feature engineering như các model khác. Và các thuật toán trên chỉ mang tính chất tham khảo để ta dựa trên đó áp dụng vào bào toán của chúng ta. Ta không bó hẹp phạm vi ứng dụng mà có thể áp dụng vào nhiều lĩnh vực khác tuỳ thuộc vào khả năng sáng tạo của nghiên cứu sinh.Số lượt thíchSố lượt thíchEm chào anh. Hiện tại em đang thực hiện đồ án tốt nghiệp với đề tài “Nhận dạng đối tượng dựa trên bộ tham số”. Cụ thể bài toán của em như sau:\n– Một đối tượng có thể có nhiều chế độ hoạt động khác nhau.\n– Các tham số của mỗi đối tượng là cố định (khoảng 7-8 tham số)\n– Bộ dữ liệu được cung cấp có 164 đối tượng khác nhau và 302 chế độ hoạt động của các đối tượng naỳ.\n– Khi hệ thống thu nhận được một bộ tham số, ta cần nhận dạng xem bộ tham số này là thuộc loại nào trong số các đối tượng đã biết.\nEm đọc về bài viết SVM của anh, em thấy bài toán của em có thể áp dụng được multi-class, tuy nhiên, trong trường hợp số lớp cần được phân lớp là khá lớn. Anh có bổ sung hay hướng đi nào khác để em có thể tham khảo không ạ.\nEm cám ơn anh!Số lượt thíchSố lượt thíchChào em, với bài toán multi-classification, em có thể áp dụng kỹ thuật one-vs-all (quy về bài toán binary-classification) hoặc mô hình Neural Network, Naive Bayes với đầu ra là số lớp cần phân lớp (có thể gán nhãn từ 0-k) hay áp dụng các thuật toán clustering với số k bằng số đối tượng của tập dữ liệu.Số lượt thíchSố lượt thíchAnh ơi, cho em hỏi chút ạ:\nEm đang đọc về SVM nhưng có vài điểm thắc mắc ạ, với những bài toán mà linear kernel của SVM không thể giải quyết thì SVM có Gaussian kernel giải quyết ( em đọc thì gaussian kernel không phải chỉ dùng mình trong SVM mà dùng chung cho ML) giúp giảm chiều của feature nhưng em không hiểu chính xác ý nghĩa của Gaussian kernel lắm, có một khái niệm nữa là kernel trick, giúp mở rộng chiều của feature giúp dễ dàng xác định mặt siêu phẳng trong SVM, em hỏi là 2 khái niệm kernel và kernel trick có khác hay liên quan không ạ, Em cảm ơn.Số lượt thíchSố lượt thíchKernel dùng để tăng số chiều, ví dụ hàm x được nâng lên x^2 sau khi áp dụng kernel.\nKernel trick – trick ở đây là thủ thuật, không có nghĩa gì khác.\nGaussian là một trong số các kernel được sử dụng trong SVM http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/Số lượt thíchLiked by 1 personvâng ạ, em cảm ơn anhSố lượt thíchSố lượt thíchAnh có thể giúp em lấy ví dụ về sử dụng kernel như thế nào trong SVM không ạ, ví dụ em có feature 2 chiều [[1,2],[3,4]] thì khi đó nếu áp dụng kernel dạng tuyến tính thì K=u.v (em không hiểu lắm là khi này thì u là x1[1,2] thì v sẽ là gì ạ) mà khi đó K sẽ là một giá trịi hay sẽ là một tập feature mới có chiều nhiều hơn tập dữ liệu cũ ạ. ( em có tìm hiểu nhưng vẫn chưa hiểu rõ lắm). Thêm một khái niệm inner product và dot product anh có thể giải thích cho em hiểu không ja.\nem cảm ơnSố lượt thíchSố lượt thíchAnh cho em ví dụ tổng quát để em dễ hình dung.\nCho \nPolynomial Kernels\n\n\n\nv sẽ là [3,4]. Ở đây, em chỉ có 2 feature vector nên K sẽ trả về 1 giá trị. Nếu em có nhiều hơn 2 feature, ta sẽ tính kernel K cho từng cặp vector để trả về một ma trận vuông với số chiều bằng số lượng vector hay còn gọi là Gram matrix.Mục tiêu của chúng ta khi cho trước vector , ta đi tính feature mới . Trong đó,  là số thành phần trong vector  và  là số lượng vector có trong tập dữ liệu. Lúc này, thay vì giải bài toán ban đầu  ta đi giải . Gram matrix được sử dụng tại đây, do . Tức là Gram matrix sẽ là đầu vào cho bài toán mới của chúng ta.Dot product (tích vô hướng): là tổng của tích các thành phần giữa 2 vector. Ý nghĩa dùng làm phép biến đổi nâng chiều trong Kernel.Inner product (tích có hướng): cho biết mối tương quan giữa 2 vector. Tích 2 vector càng gần 1 thì 2 vector càng giống nhau ngược lại càng gần 0 thì 2 vector khác nhau. Ý nghĩa giảm bớt số chiều của Gram matrix khi ta quan sát thấy các vector trùng hoặc gần giống nhau.Số lượt thíchSố lượt thíchem hiểu rồi ạ, nhưng trong một số baì viết về kernel sử dụng gaussian kernel thì họ sử dụng hàm kernel là K(x,y) thì y người ta lấy là ma trận chuyển vị của x luôn thì có khác với việc sử dụng y như a nói ở trên không ạ.Số lượt thíchSố lượt thíchKhông có gì khác, đây chính là  trong Gram matrix.Số lượt thíchLiked by 1 personThầy cho em hỏi chút. SVM phân 2 lớp thì input đầu vào dạng:\n-1 1:1 6:1 14:1 22:1 36:1 42:1 49:1 64:1 67:1 72:1 74:1 77:1 80:1 83:1\n-1 1:1 6:1 17:1 19:1 39:1 42:1 53:1 64:1 67:1 73:1 74:1 76:1 80:1 83:1\n-1 2:1 6:1 18:1 20:1 37:1 42:1 48:1 64:1 71:1 73:1 74:1 76:1 81:1 83:1\n+1 5:1 11:1 15:1 32:1 39:1 40:1 52:1 63:1 67:1 73:1 74:1 76:1 78:1 83:1\n-1 5:1 16:1 30:1 35:1 41:1 64:1 67:1 73:1 74:1 76:1 80:1 83:1\n-1 5:1 6:1 15:1 20:1 37:1 40:1 50:1 63:1 67:1 73:1 75:1 76:1 80:1 83:1\n-1 5:1 7:1 16:1 29:1 39:1 40:1 48:1 63:1 67:1 73:1 74:1 76:1 78:1 83:1\n-1 1:1 11:1 18:1 20:1 37:1 42:1 59:1 62:1 71:1 72:1 74:1 76:1 80:1 83:1\n+1 5:1 18:1 19:1 39:1 40:1 63:1 67:1 73:1 74:1 76:1 80:1 83:1\nTrong đố: +1, -1 là nhãn.\nNếu phân đa lớp cụ thể là 4 lớp thì cách input đầu vào như thế nào Thầy? Khi chạy vẫn chạy như 2 lớp hả Thầy. Em cảm ơn Thầy. Mong Thầy giải đápSố lượt thíchSố lượt thíchE áp dụng one-vs-all sau đó lấy nhãn có xác xuất cao nhất làm kết quả cuối cùngSố lượt thíchSố lượt thíchEm cảm ơn Thầy. Thầy có thể giải thích qua cho em một chút được không?Số lượt thíchSố lượt thíchem đọc tài liệụ có một số chiến lược phân đa lớp với SVM: như OAR, OAO, Fuzzy OAO. Nhưng em không hiểu cách input dữ liệu để đưa vào chạy SVM. Phân đa lớp chạy giống phân 2 lớp đúng không Thầy.Số lượt thíchSố lượt thíchĐúng rồi e, e tạo ma trận Y(mxk) với m là số mẫu, k là số lớp. Mẫu nào thuộc lớp k_i thì thay nhãn bằng 1 ngược lại bằng 0. Tiếp tục thay nhãn cho các lớp còn lại trong ma trận Y.\nOutput sẽ là k model của svm. Khi phân lớp e chọn k model nào có xác xuất cao nhất so vs model còn lại https://www.youtube.com/watch?v=ZvaELFv5IpMSố lượt thíchSố lượt thíchThầy! Em có mẫu dữ liệu trên web: http://www.cs.colorado.edu/~jbg/teaching/CSCI_5622/rank.train\nThầy có thể giải thích các thông số cho em chút được không?\n1 qid:1 1:0.057239 2:0.000439 3:0.320218 4:1.000000 5:0.000000 6:0.000000 7:0.000000 # Terminator 3: Rise of the Machines\n2 qid:1 1:-0.059483 2:0.051360 3:1.159401 4:0.000000 5:0.000000 6:0.000000 7:0.000000 # Sleuth\n3 qid:1 1:-0.074544 2:-0.017120 3:0.669877 4:0.000000 5:1.000000 6:0.000000 7:0.000000 # Party, The\n4 qid:1 1:0.034648 2:0.032045 3:0.809741 4:0.000000 5:0.000000 6:0.000000 7:1.000000 # Donnie Brasco\n5 qid:1 1:-0.093370 2:0.098768 3:0.879673 4:0.000000 5:0.000000 6:0.000000 7:1.000000 # Gattopardo, Il\n1 qid:2 1:0.012057 2:0.003951 3:-0.309169 4:1.000000 5:1.000000 6:0.000000 7:0.000000 # Hard Way, The\n2 qid:2 1:-0.149849 2:-0.013608 3:0.949605 4:0.000000 5:0.000000 6:0.000000 7:1.000000 # Key Largo\n3 qid:2 1:0.023352 2:0.025022 3:0.809741 4:0.000000 5:1.000000 6:0.000000 7:1.000000 # Yin shi nan nu\n4 qid:2 1:-0.018065 2:0.039069 3:0.460082 4:0.000000 5:0.000000 6:0.000000 7:1.000000 # Silkwood\n5 qid:2 1:0.023352 2:-0.001317 3:-0.588897 4:0.000000 5:1.000000 6:0.000000 7:1.000000 # Love Affair\n1 qid:3 1:-0.006770 2:0.000439 3:0.040490 4:1.000000 5:0.000000 6:0.000000 7:0.000000 # F/X\n2 qid:3 1:0.057239 2:0.002195 3:-0.728761 4:1.000000 5:0.000000 6:0.000000 7:0.000000 # League of Extraordinary Gentlemen, The\n3 qid:3 1:0.019587 2:0.005707 3:-0.379101 4:1.000000 5:0.000000 6:0.000000 7:1.000000 # Program, The\n4 qid:3 1:0.015822 2:-0.024143 3:-0.588897 4:1.000000 5:0.000000 6:0.000000 7:1.000000 # Rapid Fire\n5 qid:3 1:0.049709 2:0.026777 3:0.250286 4:0.000000 5:0.000000 6:0.000000 7:1.000000 # Score, The\nEm cảm ơn Thầy.Số lượt thíchSố lượt thíchĐây là movie ranking. Có lẽ rank cho một bộ phim nào đó từ 1-7 có trọng số tương ứng đi kèm. Có thể bài toán đi tìm kết quả tìm kiếm bộ phim phù hợp nhất đối với người dùng nào đó.\nhttp://www.cs.colorado.edu/~jbg/teaching/CSCI_5622/13a.pdfSố lượt thíchSố lượt thíchThầy! Theo em đọc thì em hiểu: Nếu phân 3 lớp: a, b, c ta sẽ chia theo công thức n*(n-1)/2 lớp con: (a,b), (b,c)(c,a). Nếu phân làm 4 lớp a, b, c, d ta chia làm 6 cặp theo công thức trên (a,b), (a,c), (a,d), (b,c), (b,d), (c,d). Nếu em có tập véc tơ đại diện cho 4 lớp (0 – 3) ví dụ như:\n0 1:1 6:1 14:1 22:1 36:1 42:1 49:1 64:1 67:1 72:1 74:1 77:1 80:1 83:1\n1 1:1 6:1 17:1 19:1 39:1 42:1 53:1 64:1 67:1 73:1 74:1 76:1 80:1 83:1\n2 2:1 6:1 18:1 20:1 37:1 42:1 48:1 64:1 71:1 73:1 74:1 76:1 81:1 83:1\n3 5:1 11:1 15:1 32:1 39:1 40:1 52:1 63:1 67:1 73:1 74:1 76:1 78:1 83:1\n1 5:1 16:1 30:1 35:1 41:1 64:1 67:1 73:1 74:1 76:1 80:1 83:1\n3 5:1 6:1 15:1 20:1 37:1 40:1 50:1 63:1 67:1 73:1 75:1 76:1 80:1 83:1\n0 5:1 7:1 16:1 29:1 39:1 40:1 48:1 63:1 67:1 73:1 74:1 76:1 78:1 83:1\n1 1:1 11:1 18:1 20:1 37:1 42:1 59:1 62:1 71:1 72:1 74:1 76:1 80:1 83:1\nthì em sẽ chuyển như thế nào để đưa vào SVM chạy được Thầy. Em Cảm ơn Thầy. Rất mong Thầy giải thích giúp em.Số lượt thíchSố lượt thíchEm cần transform label sang dạng true/false hoặc 1/0 như bên dưới0 vs all\n1 1:1 6:1 14:1 22:1 36:1 42:1 49:1 64:1 67:1 72:1 74:1 77:1 80:1 83:1\n0 1:1 6:1 17:1 19:1 39:1 42:1 53:1 64:1 67:1 73:1 74:1 76:1 80:1 83:1\n0 2:1 6:1 18:1 20:1 37:1 42:1 48:1 64:1 71:1 73:1 74:1 76:1 81:1 83:1\n0 5:1 11:1 15:1 32:1 39:1 40:1 52:1 63:1 67:1 73:1 74:1 76:1 78:1 83:1\n0 5:1 16:1 30:1 35:1 41:1 64:1 67:1 73:1 74:1 76:1 80:1 83:1\n0 5:1 6:1 15:1 20:1 37:1 40:1 50:1 63:1 67:1 73:1 75:1 76:1 80:1 83:1\n1 5:1 7:1 16:1 29:1 39:1 40:1 48:1 63:1 67:1 73:1 74:1 76:1 78:1 83:1\n0 1:1 11:1 18:1 20:1 37:1 42:1 59:1 62:1 71:1 72:1 74:1 76:1 80:1 83:11 vs all\n0 1:1 6:1 14:1 22:1 36:1 42:1 49:1 64:1 67:1 72:1 74:1 77:1 80:1 83:1\n1 1:1 6:1 17:1 19:1 39:1 42:1 53:1 64:1 67:1 73:1 74:1 76:1 80:1 83:1\n0 2:1 6:1 18:1 20:1 37:1 42:1 48:1 64:1 71:1 73:1 74:1 76:1 81:1 83:1\n0 5:1 11:1 15:1 32:1 39:1 40:1 52:1 63:1 67:1 73:1 74:1 76:1 78:1 83:1\n1 5:1 16:1 30:1 35:1 41:1 64:1 67:1 73:1 74:1 76:1 80:1 83:1\n0 5:1 6:1 15:1 20:1 37:1 40:1 50:1 63:1 67:1 73:1 75:1 76:1 80:1 83:1\n0 5:1 7:1 16:1 29:1 39:1 40:1 48:1 63:1 67:1 73:1 74:1 76:1 78:1 83:1\n1 1:1 11:1 18:1 20:1 37:1 42:1 59:1 62:1 71:1 72:1 74:1 76:1 80:1 83:12 vs all\n0 1:1 6:1 14:1 22:1 36:1 42:1 49:1 64:1 67:1 72:1 74:1 77:1 80:1 83:1\n0 1:1 6:1 17:1 19:1 39:1 42:1 53:1 64:1 67:1 73:1 74:1 76:1 80:1 83:1\n1 2:1 6:1 18:1 20:1 37:1 42:1 48:1 64:1 71:1 73:1 74:1 76:1 81:1 83:1\n0 5:1 11:1 15:1 32:1 39:1 40:1 52:1 63:1 67:1 73:1 74:1 76:1 78:1 83:1\n0 5:1 16:1 30:1 35:1 41:1 64:1 67:1 73:1 74:1 76:1 80:1 83:1\n0 5:1 6:1 15:1 20:1 37:1 40:1 50:1 63:1 67:1 73:1 75:1 76:1 80:1 83:1\n0 5:1 7:1 16:1 29:1 39:1 40:1 48:1 63:1 67:1 73:1 74:1 76:1 78:1 83:1\n0 1:1 11:1 18:1 20:1 37:1 42:1 59:1 62:1 71:1 72:1 74:1 76:1 80:1 83:13 vs all\n0 1:1 6:1 14:1 22:1 36:1 42:1 49:1 64:1 67:1 72:1 74:1 77:1 80:1 83:1\n0 1:1 6:1 17:1 19:1 39:1 42:1 53:1 64:1 67:1 73:1 74:1 76:1 80:1 83:1\n0 2:1 6:1 18:1 20:1 37:1 42:1 48:1 64:1 71:1 73:1 74:1 76:1 81:1 83:1\n1 5:1 11:1 15:1 32:1 39:1 40:1 52:1 63:1 67:1 73:1 74:1 76:1 78:1 83:1\n0 5:1 16:1 30:1 35:1 41:1 64:1 67:1 73:1 74:1 76:1 80:1 83:1\n1 5:1 6:1 15:1 20:1 37:1 40:1 50:1 63:1 67:1 73:1 75:1 76:1 80:1 83:1\n0 5:1 7:1 16:1 29:1 39:1 40:1 48:1 63:1 67:1 73:1 74:1 76:1 78:1 83:1\n0 1:1 11:1 18:1 20:1 37:1 42:1 59:1 62:1 71:1 72:1 74:1 76:1 80:1 83:1Số lượt thíchSố lượt thíchEm Cảm ơn Thầy. không cần tính xác suất cho các cặp hả Thầy.Số lượt thíchSố lượt thíchSau khi train ra được k svm model. Em đưa dữ liệu test vào từng model để có kết quả phân lớp, model nào cho xác suất cao nhất thì mẫu dữ liệu này thuộc về lớp đó.Số lượt thíchSố lượt thíchThầy có thể giải thích qua cho em cách chuyển được không Thầy.Số lượt thíchSố lượt thíchFor i in range(0, 4): if line_str[0] == i: line_str[0] = 1 else: line_str[0] = 0Số lượt thíchSố lượt thíchThầy có Tool nào về phân đa lớp trong SVM, dữ liệu mẫu nào về phần này không cho em tham khảo với ạ. Em cảm ơn Thầy.Số lượt thíchSố lượt thíchhttps://www.csie.ntu.edu.tw/~cjlin/libsvmtools/multilabel/Số lượt thíchSố lượt thíchThầy có dữ liệu mẫu nào về phân đa lớp không Thầy (4 lớp) để em chạy thử nghiệm SVM. Em tải dữ liệu mẫu trên một số trang khi đọc trong libsvm-3.22 trong windows toán báo sai đầu vào Thầy.Số lượt thíchSố lượt thíchEm chào thầy em đang làm bài tập lớn về SVM , cho em hỏi là các ưu điểm và nhược điểm của SVM trong phân lớp là gì ak. em cảm ơn nhiềuSố lượt thíchSố lượt thíchEm có thể tham khảo bài viết này\nhttps://www.quora.com/What-are-some-pros-and-cons-of-Support-Vector-Machines\nhttp://www.kdnuggets.com/2016/07/support-vector-machines-simple-explanation.htmlSố lượt thíchSố lượt thíchEm đang làm đồ án nhận dạng số dựa vào thuật toán SVM. Em dùng đặc trưng haar để nhận dạng, nhưng em vẫn chưa hiểu làm sao để từ một cái hình có số đưa vào mà nó dựa vào SVM để nhận dạng. Xin thầy hãy giáp đáp tường tận giúp em với ạ. Em cảm ơn thầy!Số lượt thíchSố lượt thíchEm có thể tham khảo bài viết này https://gurus.pyimagesearch.com/lesson-sample-segmenting-characters-from-license-plates/Số lượt thíchSố lượt thíchEm chào thầy! Em đang làm phân lớp văn bản dùng: gSpan + SVM. Em đã có tập đồ thị phổ biến. Nhưng em vẫn chưa hiểu làm sao để chuyển dữ liệu sang dạng vector để đưa vào SVM để huấn luyện và phân lớp. Xin thầy hãy giải đáp giúp em với ạ. Em cảm ơn thầy!Số lượt thíchSố lượt thíchChào thầy . Em đang làm đồ án về nhận diện cảm xúc . Ban đầu em chỉ trích những đặc trưng từ mắt và miệng rồi gắn nhãn cho nó như mặt buồn, vui , ngạc nhiên rồi đưa vào train trong libsvm . Nhưng em không biết tạo dữ liệu như thế nào để đưa những đặc trưng (mắt + miệng) từ 1 hình mới vào để lấy nghiệm là 1 label (buồn or vui or …) . Em cảm ơn , mong thầy giúp đỡ.Số lượt thíchSố lượt thíchEm có thể sử dụng mô hình Bag of Words\nhttp://www.springer.com/cda/content/document/cda_downloaddocument/9783319056951-c2.pdf?SGWID=0-0-45-1450724-p176654764\nhttp://www.robots.ox.ac.uk/~az/icvss08_az_bow.pdf\nhttp://www.norbertwiener.umd.edu/FFT/FFT11/slides/Woodard.pdfSố lượt thíchSố lượt thíchEm đang làm về phân lớp văn bản, nếu em dùng SVM có được kết quả tối ưu nhất ko a. Em xin cảm ơnSố lượt thíchSố lượt thíchEm cần thực nghiệm để chọn ra mô hình cho kết quả tốt nhất. “No free launch theory”Số lượt thíchSố lượt thíchcho em hỏi trên bàn thầy ví dụ chỉ có bóng xanh, đỏ, giả sử có nhiều hơn vàng, tím… thì siêu phẳng xác định thế  nào, hay chỉ tìm siêu phẳng  cho từng cập màu quả bóng hả thầy.Số lượt thíchSố lượt thíchđúng rồi em, ta xét từng cặp bằng phương pháp One-Vs-All, nếu là Neural Network ta có thể output là số lượng classes phân lớp tương ứngSố lượt thíchSố lượt thíchEm Kính Chào Anh, Hiện tại em đang làm luận văn với đề tài ” Tối ưu lịch trình di chuyển của xe đến các trạm đón khách ” trong đó thì có nhiều yếu tố cần tối ưu ( tối ưu đa mục tiêu ) như là thời gian di chuyển, thời gian chờ đón khách, đón khách ở nhiều địa điểm khác nhau. Em có đọc một luận văn tương tự thì tác giả sử dụng PSO ( particle swarm optimization ) . Em xin hỏi liệu có thể sử dụng SVM để giải lại bài toán như trên để có kết quả tối ưu hơn được không?, bởi em thấy SVM dường như chỉ đang phân tách dữ liệu? thành công hay là thất bại. Em xin cám ơn .Số lượt thíchSố lượt thíchVề lý thuyết thì có thể, em cần tìm một hàm lỗi tương ứng và tiến hành optimize hàm lỗi này. Em có thể xem SVM ứng dụng vào regression https://en.wikipedia.org/wiki/Support_vector_machine#RegressionSố lượt thíchSố lượt thíchEm chào anh,\nỞ trên anh có nói: “Do ANN là representation model, ta không cần làm feature engineering như các model khác. Và các thuật toán trên chỉ mang tính chất tham khảo để ta dựa trên đó áp dụng vào bào toán của chúng ta. Ta không bó hẹp phạm vi ứng dụng mà có thể áp dụng vào nhiều lĩnh vực khác tuỳ thuộc vào khả năng sáng tạo của nghiên cứu sinh.”\nAnh có thểcho em hỏi rõ hơn: representation model nghĩa là sao ạ?\nCòn feature engineering ở các model khác là giai đoạn nào?\nEm cảm ơn.Số lượt thíchSố lượt thíchrepresentation model là đại diện cho tập dữ liệu đang xét. Ví dụ, mô hình linear regression dự đoán giá nhà khi cho đầu vào là diện tích nhà cũng là representation model, đó là đường thẳng hay đường cong cố gắng xấp xỉ với tập dữ liệu đang xét. feature engineering thật chất cũng có tồn tại trong ANN, đó là khi em design số lượng node ở mỗi layer. Các model khác như ví dụ dự đoán giá nhà, nếu em có thêm thuộc tính chiều rộng, chiều dài, bên cạnh diện tích nhà thì rõ ràng các thuộc tính chiều rộng, chiều dài không độc lập tuyến tính đối với diện tích nhà. Lúc này em cần làm feature engineering để loại bỏ các thuộc tính phụ thuộc tuyến tính đó. Ngược lại, đối với ANN thì do cơ chế cập nhật qua mỗi bước backpropagation, thì giá trị node (feature) tại mỗi layer sẽ tự động hội tụ lại thành các thành phần vector độc lập tuyến tính mà không cần làm feature engineering.Số lượt thíchSố lượt thíchThầy cho em hỏi trong tập huấn luyện có 1 cột toàn giá trị 0, nhưng tập test cột đó có 1 vài giá trị   1. Khi chạy trên R thì báo lỗi. can’t not scale data. Em phải sửa thế nào ah. Cảm ơn thầySố lượt thíchSố lượt thíchem có thể thêm tham số scale = FALSE\nhttp://r.789695.n4.nabble.com/Cannot-scale-data-td4663526.htmlSố lượt thíchSố lượt thíchEm để scale = FALSE thì độ chính xác giảm đi rất nhiều thầy ah.Số lượt thíchSố lượt thíchThầy ơi, cho em hỏi độ phức tạp của thuật toán SVM này là như thế nào được không ạ?Số lượt thíchSố lượt thíchĐối với nonlinear SVM có độ phức tạp O(max(n,d)  min (n,d)^2) với n là số lượng mẫu dữ liệu training, d là số chiều của tập dữ liệu đó.\nhttp://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf\nhttp://colt2009.cs.mcgill.ca/papers/021.pdf\nhttps://pdfs.semanticscholar.org/2a74/3601cc310d6cb6c8e22c144bbf29dce79150.pdfSố lượt thíchSố lượt thíchdạ, em cảm ơn thầy nhiều ạ. Cho em hỏi một ý nữa đó là: ví dụ bài toán của em tính toán đặc trưng hình ảnh và xuất ra mỗi ảnh là một giá trị vector gồm 32 phần tử trong đó. Vậy cái ảnh đó trong thuật toán đó chính là x trong công thức đúng k ạ?\nEm vẫn chưa hiểu rõ giá trị đó thế như thế nào vào công thức :3, thầy giải đáp giúp em với ạSố lượt thíchSố lượt thích32 chính là d:dimension đó eSố lượt thíchSố lượt thíchTại em mới nghiên cứu nó mà đọc tài liệu thấy có công thức w.x +b =0, em phân vân k biết thế cái 32 chiều đó ở chỗ nào :3. Thầy có tài liệu nào nói rõ về cái này không ạ?Số lượt thíchSố lượt thíchx là mẫu vector 32×1 đó e, e xem linear regression có cthuc nhu e đề cậpSố lượt thíchSố lượt thíchDạ, em cảm ơn thầy nhiều ạSố lượt thíchSố lượt thíchThầy ơi, cho em hỏi SVM tuyến tính và phi tuyến tính giống và khác nhau như thế nào ạ?Số lượt thíchSố lượt thíchCả hai đều nói đến cách sử dụng kernel trong SVM.\nLinear kernel: Polynomial\nNon-linear kernel: Gaussian (RBF), SigmoidSố lượt thíchSố lượt thíchEm chào thầy ạ. Em đang chuẩn bị làm đồ án tốt nghiệp. Thầy có thể gợi ý cho em một vài đề tài sử dụng phương pháp SVM để làm  được không ạSố lượt thíchSố lượt thíchbạn nên nói chi tiết hơn bạn đang học ngành gì, thạc sĩ hay tiến sĩ. trường nào. thì thầy sẽ dễ trả lời hơn .Số lượt thíchSố lượt thíchHi em, về chủ đề thì em nên liên hệ giáo viên hướng dẫn đang nghiên cứu về lĩnh vực của họ nhé. Nhưng đầu tiên, em cần xác định mình thích làm với tập dữ liệu và bài toán nào: xử lý ngôn ngữ tự nhiên, thị giác máy tính, xử lý tín hiệu số, khai thác dữ liệu, …Số lượt thíchSố lượt thíchMời bạn điền thông tin vào ô dưới đây hoặc kích vào một biểu tượng để đăng nhập: Bạn đang bình luận bằng tài khoản WordPress.com ( Đăng xuất / Thay đổi ) Bạn đang bình luận bằng tài khoản Twitter ( Đăng xuất / Thay đổi ) Bạn đang bình luận bằng tài khoản Facebook ( Đăng xuất / Thay đổi ) Bạn đang bình luận bằng tài khoản Google+ ( Đăng xuất / Thay đổi )Connecting to %s Notify me of new comments via email. Thông báo cho tôi bằng email khi có bài đăng mới. \n\nNếu ngày mai tớ trở thành người nổi tiếng. Gặp tớ giữa đường bạn đừng ngại tớ nghe. Cứ nhìn tớ như ngày xưa bạn thấy. Đừng nên nghĩ tớ quá xa vời. Nếu một ngày tớ là kẻ ăn xin. Thì bố thí cho tớ vài kỷ niệm. Dù rất nhỏ nhưng đừng nên khinh tớ. Vì tớ vẫn là tớ của ngày xưa. Nếu ngày mai tớ chẳng biết đường về. Hãy chỉ giùm tớ con đường đi đúng đắn. Đừng để tớ sa chân nơi bùn nhớp. Lúc rút chân lên thì đã bẩn mất rồi. Nếu ngày mai tớ gặp bạn giữa đường. Thì ta sẽ gặp nhau và cười nhé. Đừng nên trách và đừng nên đánh mất.Đôi khi có một số người lướt qua cuộc đời bạn và ngay tức khắc bạn nhận ra rằng sự có mặt của họ ý nghĩa như thế nào. Họ đã dạy bạn những bài học, đã giúp bạn nhận ra giá trị của chính mình hoặc trở thành con người mà bạn từng mơ ước. Có lẽ bạn sẽ không biết được những con người này từ đâu đến ( bạn cùng phòng, người hàng xóm, vị giáo sư, người bạn mất liên lạc từ lâu hay thậm chí là một người hoàn toàn xa lạ ). Nhưng khi bạn thờ ơ với họ, hãy nhớ rằng trong từng khoảnh khắc họ sẽ ảnh hưởng rất sâu sắc đến cuộc đời bạn. …Cuộc sống không hề làm khó bạn mà chính bạn đã tự làm khó mình bằng cách đặt ra những câu hỏi đại loại như: “Tại sao lại thế này?… Tại sao lại thế kia?…”. Không ai có thể trả lời giúp bạn mà chính bạn phải tự đi tìm lời giải đáp cho mình. Hãy luôn nghĩ về người khác trước khi nghĩ đến bản thân, chắc chắn lúc đó bạn sẽ hài lòng với câu trả lời của chính mình. Và đừng quên luôn đặt câu hỏi: “Mình đã làm được gì?” trước khi tự hỏi: “Mình đã nhận được gì?” nhé! Tôi tin là bạn sẽ thành công!  Thật ra trên đời, con người luôn cần có nhau để giải quyết rất nhiều vấn đề. Nhờ những lúc qua lại giúp đỡ đó mà hoá giải bao muộn phiền, nảy sinh các tình cảm, cùng trải nghiệm biết bao điều thú vị, hiểu biết thêm nhiều tri thức mới. Đây chính là cuộc sống!Vì thế hãy trân trọng những người đang “làm phiền” bên cạnh bạn nhé! Đừng để đến khi mất đi rồi mới nghẹn ngào lại bảo “giá như” …ĐĐ. GS. Thích Phước Tiến\n__(())__Namo Bụt SakyamuniNhận Email khi có bài viết mới\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\n\t\t\n\t\t\n\t\t\n\tThis slideshow requires JavaScript.",
          "relevence": "yes"
        },
        {
          "url": "https://viblo.asia/p/mot-chut-ve-thuat-toan-svm-support-vector-machine-algorithm-OeVKBgGAZkW",
          "title": "Một chút về thuật toán SVM (Support Vector Machine algorithm) - Viblo",
          "content": "Việc nắm vững về các thuật toán máy tính không phải là khủng khiếp với tất cả mọi người. Đa số những người mới bắt đâu sẽ học về đệ quy. Nó đơn giản để học và sử dụng, nhưng điều đó có giải quyết được  mục tiêu của bạn. Tất nhiên là không, bởi vì bạn có thể làm được nhiều hơn chứ không chỉ là hồi quy dữ liệu nào đấy.\nCác thuật toán máy tính (machine learning algorithms), xem chúng như một kho vũ khí với rất nhiều chủng loại rìu, cưa, cuốc, xẻng, súng, lựu đạn..v.v, bạn có nhiều công cụ khác nhau, nhưng cần phải học cách sử dụng chúng vào đúng thời điểm. Như là một phép loại suy, suy nghĩ về việc \"đệ quy\" như là một thanh kiếm có thể cắt và chia dữ liệu một cách hiệu quả, nhưng nó không có khả năng xử lí với các dữ liệu rất phức tạp. Ngược lại Support Vector Machines (SVM) như là một con dao nhỏ sắc nhọn- nó hoạt động trên các tập dữ liệu nhỏ. nhưng trên đó nó có khả năng xử lí mạnh mẽ hơn trong việc xây dựng các mô hình.SVM là một thuật toán giám sát, nó có thể sử dụng cho cả việc phân loại hoặc đệ quy. Tuy nhiên nó được sử dụng chủ yếu cho việc phân loại. Trong thuật toán này, chúng ta vẽ đồi thị dữ liệu là các điểm trong n  chiều ( ở đây n là số lượng các tính năng bạn có) với giá trị của mỗi tính năng sẽ là một phần liên kết. Sau đó chúng ta thực hiện tìm \"đường bay\" phân chia các lớp. Đường bay - nó chỉ hiểu đơn giản là 1 đường thằng có thể phân chia các lớp ra thành hai phần riêng biệt.\n\nSupport Vectors hiểu một cách đơn giản là các đối tượng trên đồ thị tọa độ quan sát,  Support Vector Machine là một biên giới để chia hai lớp tốt nhất.Ở trên, chúng ta đã thấy được việc chia hyper-plane. Bấy giờ  làm thế nào chúng ta có thể xác định \"Làm sao để vẽ-xác định đúng hyper-plane\".\nChúng ta sẽ theo các tiêu chí sau:Bạn cần nhớ quy tắc số một để chọn 1 hyper-lane, chọn một hyper-plane để phân chia hai lớp tốt nhất. Trong ví dụ này chính là đường BỞ đây chúng ta cũng có 3 đường hyper-plane (A,B và C), theo quy tắc số 1, chúng đều thỏa mãn\n\nQuy tắc thứ hai chính là xác định khoảng cách lớn nhất từ điểu gần nhất của một lớp nào đó đến đường hyper-plane. Khoảng cách này được gọi là \"Margin\", Hãy nhìn hình bên dưới, trong đấy bạn có thể nhìn thấy khoảng cách margin lớn nhất đấy là đường C. Ở đây bạn nhớ nếu chọn lầm hyper-lane có margin thấp hơn thì sau này khi dữ liệu tăng lên thì sẽ sinh ra nguy cơ cao về việc xác định nhầm lớp cho dữ liệuBạn hãy sử dụng các nguyên tắc đã nêu trên để chọn ra hyper-plane cho trường hợp sau\n\nCó thể có một vài bạn sẽ chọn đường B bởi vì nó có margin cao hơn đường A, nhưng đấy sẽ không đúng bởi vì nguyên tắt đầu tiên sẽ là nguyên tắc số 1., chúng ta cần chọn hyper-plane để phân chia các lớp thành riêng biệt. Vì vậy đường A mới là lựa chọn chính xác.Tiếp the hãy xem hình bên dưới, mình không thể chia  thành hai lớp riêng biệt với 1 đường thẳng, để tạo 1 phần chỉ có các ngôi sao và một vùng chỉ chứa các điểm tròn.\n\nỞ đây chúng ta sẽ chấp nhận, một ngôi sao ở bên ngoài cuối được em như một ngôi sao phía ngoài hơn, SVM có tính năng cho phép bỏ qua các ngoại lệ và tìm ra hyper-plane có biên giới tối đa . Do đó chúng chúng ta có thể nói, SVM có khả năng mạnh trong việc chấp nhận ngoại lệ.\nTrong trường hợp dưới đây, chúng ta khong thể tìm ra 1 đường hyper-plane tương đối để chia các lớp, vậy làm thế nào để SVM phân tách dữ liệu thành hai lớp riêng biệt? Cho đến bây giờ chúng ta chỉ nhìn vào các đường tuyến tính hyper-plane\n\nSVM có thể giải quyết vấn đề này, Khá đơn giản, nó sẽ được giải quyết bằng việc thêm một tính năng, Ở đây chúng ta sẽ thêm tính năng z = x^2+ y^2. Bây giờ  dữ liện sẽ được biến đổi theo trục x và z như sau\n\nTrong sơ đồ trên, các điểm cần xem xét là:Trong SVM, rất dễ dàng để có một siêu phẳng tuyến tính (linear hyper-plane) để chia thành hai lớp, Nhưng một câu hỏi sẽ nảy sinh đấy là,  chúng ta có cần phải thêm một tính năng phân chia này bằng tay hay không. Không, bởi vì SVM có một kỹ thuật được gọi là kernel trick ( kỹ thuật hạt nhân), đây là tính năng có không gian đầu vào có chiều sâu thấm và biến đổi nó thành không gian có chiều cao hơn, tức là nó không phân chia các vấn đề thành các vấn đề riêng biệt, các tính năng này được gọi là kernel. Nói một cách đơn giản nó thực hiện một số biết đổi dữ liệu phức tạp, sau đó tìm ra quá trình tách dữ liệu dựa trên các nhãn hoặc đầu ra mà chúng ra đã xác định trước.Mình sẽ tìm hiểu và demo về SVM trong mục tiếp theo với python. Thân mọi người đã đọc\n                    © 2017 Viblo. All rights reserved.\n                ",
          "relevence": "yes"
        },
        {
          "url": "https://viblo.asia/p/ung-dung-support-vector-machine-trong-bai-toan-phan-loai-hoa-PdbGnLXBkyA",
          "title": "Ứng dụng Support Vector Machine trong bài toán phân loại hoa - Viblo",
          "content": "Xin chào các bạn, mình lại trở lại rồi đây. Tiếp tục với loạt bài viết về Machine Learning trong bài trước mình đã giới thiệu với các bạn một cách tổng quan nhất về Support Vector Machine - một phương pháp vô cùng hiệu quả trong bài toán phân lớp dữ liệu. Tuy nhiên nếu đọc lý thuyết nhiều quá hẳn sẽ rất khó khăn cho những bạn mới bắt đầu (mình cũng mới bắt đầu tìm hiểu về Machine Learning mà). Chính vì thế, ngày hôm nay chúng ta sẽ đến với một bài toán khá cụ thể đó là sử dụng SVM trong bài toán phân loại hoa. Hiểu nôm na đó là chúng ta sẽ xây dựng một mô hình (model) sao cho khi input vào là một bông hoa và ouput trả ra kết quả nó là loài hoa gì. Thú vị phải không các bạn . OK chúng ta bắt đầu thôi nào.Cái gì cũng phải có cơ sở một chút phải không nào. Ví dụ trong bài toán của ta đang quan tâm đến đối tượng là hoa chẳng hạn, vậy có một câu hỏi đặt ra là \"Làm thế nào để phân biệt các loại hoa với nhau ???\".Con người chúng ta qua quá trình lớn lên đã tự tích lũy cho mình những đặc điểm rất riêng của mỗi loài hoa. Ví dụ hoa loa kèn có hình giống cái kèn, hoa huệ màu trắng có mùi thơm đặc trưng, hoa hồng màu đỏ và có gai... Vậy tức là trong đầu chúng ta đã có một tập dữ liệu về các loài hoa và nó chính là căn cứ để chúng ta nhận dạng một loài hoa khi chúng ta gặp phải. Nguyên tắc học của máy tính cũng như vậy thôi. Chúng ta cần phải cung cấp cho nó một tập dữ liệu và huấn luyện cho nó băng một cách nào đó để nó có thể căn cứ vào đó mà đoán được một bông hoa mới thuộc vào loài hoa nào. OK, quan trọng nhất vẫn là phải có một tập dữ liệu huấn luyện phải không nào? Trong bài viết này chúng ta sẽ sử dụng một tập dữ liệu về hoa vô cùng nổi tiếng đó là tập dữ liệu Iris. Chúng ta cùng nhau tìm hiểu sâu hơn về tập dữ liệu này nhé.Tập dữ liệu này còn có tên gọi khác là **Fisher's Iris ** vì nó do Ronald Fisher thu thập và tổng hợp. Tập dữ liệu này gồm 50 mẫu về 3 loài hoa khác nhau của họ Iris là (Iris setosa, Iris virginica và Iris versicolor). Cho cái ảnh cho các bạn dễ hình dungVới mỗi một mẫu hoa này hắn thu thập bốn thuộc tính là chiều dài và chiều rộng của đài hoa và cánh hoa với đơn vị centimet. Để có thể sử dụng tập dữ liệu này chúng ta sẽ  sử dụng thư viện datasets trong sklearn.Sau khi đã có dữ liệu rồi, chúng ta hoàn toàn có thể chạy thuật toán SVM ngay để tiến hành phân loại. Tuy nhiên, mình nghĩ nên giúp các bạn có một cái nhìn trực quan hơn về tập dữ liệu này. Mà để hiển thị trực quan nhất thì không gì bằng biểu đồ với thư viện matplotlib thần thánh. Chúng ta cùng thử biểu diễn trên đồ thị xem tập dữ liệu mà chúng ta nhét vào máy tính nó là cái gì nhé.Chúng ta tưởng tượng tập dữ liệu của ta là một tập hợp của 150 điểm dữ liệu tương ứng với 150 bông hoa.. Chúng ta sẽ lấy chúng ta từ datasets bằng hàm sau:Lúc này đã có một tập hợp các điểm dữ liệu. Mỗi điểm dữ liệu bao gồm 4 thuộc tính như đã nói ở trên. Tuy nhiên để biểu diễn trong đồ thị hai chiều chúng ta cần phải giảm bớt số thuộc tính biểu diễn. Ở đây giả sử chọn hai thuộc tính đầu tiên là độ rộng và chiều cao của đài hoa. Chúng ta có hàm xử lý vẽ đồ thị 2D như sau:Và đây là thành quảChúng ta có thể thấy được các điểm dữ liệu với hai thuộc tính trên đồ thị hai chiều. Các điểm này được phân biệt bằng mắt thường với 3 màu khác nhau. Tuy nhiên muốn máy tính phân biệt được như chúng ta lại là một câu chuyện hoàn toàn khác và bạn sẵn sàng cùng tôi đi đến cuối cùng của câu chuyện này chứ. OK chúng ta tiếp tục thôiVới tập dữ liệu Iris chúng ta cần phân loại các bông hoa thành 3 lớp dữ liệu. Sử dụng SVM với các phương pháp khác nhau sẽ cho hiệu quả phân lớp khác nhau. Cũng tương tự như trên, chúng ta chỉ xem xét đến 2 thuộc tính đầu tiên của tập dữ liệu, tức là phân lớp trong không gian 2 chiều. Chúng ta sử dụng các Kernel khác nhau bao gồm:Đầu tiên chúng ta cần huấn luyện dữ liệuTiếp theo là việc đẩy các mô hình thu được vào đồ thịBây giờ sau khi vẽ các đồ thị trên là chúng ta có thể so sánh hiệu quả phân lớp của các Kernel khác nhau rồi đấy. Kết quả như sau:Bằng việc phân lớp chúng ta có thể chia mặt phẳng tọa độ thành các phần khác nhau. Khi có một điểm dữ liệu mới chúng ta có thể dựa vào tọa độ của chúng để phán đoán xem nó thuộc lớp nào.Chúng ta hoàn toàn có thể  tự custom một Kernel riêng của mình. Điều đó làm cho SVM trở nên linh hoạt hơn.Mình rất hi vọng được trình bày với các bạn sâu hơn về Custom Kernel của mình để việc phân lớp cụ thể là phân lớp ảnh được tốt hơn. Nhưng trong phạm vi bài viết này mình sẽ dừng lại ở các Kernel tuyến tính và phi tuyến thôi đã. Kẻo nói nhiều quá chúng ta khó năm bắt được hết vấn đề.Các bạn có thể tham khảo mã nguồn mình viết trong bài này tại đâySVM Kernel Scikit LearnSVM Scikit LearnIris Flower Dataset\n                    © 2017 Viblo. All rights reserved.\n                ",
          "relevence": "yes"
        },
        {
          "url": "https://viblo.asia/p/support-vector-machine-trong-hoc-may-mot-cai-nhin-don-gian-hon-XQZkxoQmewA",
          "title": "Support Vector Machine trong học máy - Một cái nhìn đơn giản hơn - Viblo",
          "content": "Xin chào các bạn, nếu như các bạn có theo dõi các bài viết trước của mình về các mô hình hồi quy thì chúng ta có thể dễ dàng nhận thấy được sự đơn giản và dễ áp dụng của phương pháp hồi quy, nhất là trong các bài toán dự đoán (prediction). Tuy nhiên chính sự đơn giản đó của mô hình làm cho hiệu quả của thuật toán chưa thật sự được như mong muốn. Có rất nhiều phương pháp cho hiệu quả tốt hơn các phương pháp hồi quy, và một trong số đó là Support Vector Machine (SVM) mà mình sẽ giới thiệu thật kĩ trong bài viết này. Tuy nhiên, để tránh nhàm chán với những yếu tố học thuật trong bài này chúng ta sẽ tìm hiểu SVM theo cách mà người ta vẫn hay kiểm tra học sinh tiểu học theo dạng cô hỏi - trò đáp. OK chúng ta bắt đầu thôiNhìn hình ảnh chúng ta cũng có thể đoán được mục đích của nó đúng không. SVM sử dụng để tìm ra một siêu phẳng (hyperplane) - chính là cái đường cong cong như hình trên đó. Nhưng thử tưởng tượng trong không gian nhiều chiều hơn chẳng hạn, nó có thể là một mặt cầu, mặt bầu dục... Tóm lại mục đích của cái siêu phẳng đó là phân tách tập dữ liệu thành hai phần riêng biệt - tư tưởng của bài toán phân lớp. Ví dụ như ảnh trên, chúng ta có một mặt bàn đựng hai loại quả lê và táo. Siêu phẳng phân tách đống quả này thành hai lớp, bản chất là đi tìm một hàm toán học phụ thuộc tọa độ của một quả trên mặt bàn. Có nghĩa là khi nhét một quả mới vào trên mặt bàn, dựa vào tọa độ của nó ta có thể biết được nó là quả táo hay quả lê nhờ vào việc nó nằm bên phải hay bên trái của siêu phẳng. Có thể hiểu đơn giản như thế. Tuy nhiên các phần sau bắt đầu phức tạp rồi đấy, các bạn chuẩn bị tinh thần nha.Trở lại với ví dụ trên của chúng ta, nếu như các quả táo và lê không năm quá đan xen nhạu thì chúng ta hoàn toàn có thể dùng một cái que (siêu phẳng) phân tách chúng. Tuy nhiên, thực tế không phải đơn giản như thế, có nghĩa là các quả táo và quả lê nằm tại các vị trí rất lung tung trên mặt bàn và rất khó có thể tìm được một cái que như thế để phân tách giữa chúng. Vậy thì làm thế nào bây giờ??? Một cách giải quyết đó là vận dụng tư tưởng của trò chơi tung hứng. Giả sử chúng ta trong một cơn tức giận hất tung chiếc bạn đựng táo và lê lên trời, các quả táo và lê bay lơ lửng trên không trung. Lúc này chúng đã ở các vị trí khác nhau và chúng ta hoàn toàn có thể dùng một mặt cong tưởng tượng để phân tách giữa chúng. Ví dụ như mặt phẳng xanh bên đưới đây.Các bạn sẽ nghĩ là có cái gì đó chém gió ở đây phải không? Không hề đâu, trò chơi tung hứng trong thực tế tương đương với việc chuyển đổi từ không gian hai chiều (mặt bàn) sang không gian nhiều chiều hơn (không trung). SVM thực hiện điều này một cách rất tự nhiên thông qua Kernel. Mình không hề chém gió chút nào đâu. Chúng ta cũng có thể hình dung dễ hơn việc tung bóng này của SVM thực hiện ra sao trong video dưới đây:Như chúng ta đã thảo luận ở các phần trên, bản chất của phương pháp SVM là chuyển không gian dữ liệu ban đầu thành một không gian mới hữu hạn chiều mà ở đó cho khả năng phân lớp dễ dàng hơn. Một quả bất kì nằm trên mặt bàn sẽ được gắn với một tọa độ cụ thể. Ví dụ, quả táo nằm cách mép trái 2cm và cách mép dưới 5cm được thể hiện trên trục tọa độ (x, y) tương ứng là (2, 5). x và y chính là tọa độ trong không gian hai chiều của quả táo. Khi đưa lên chiều thứ 3 là z(x, y), ta có thể tính được tọa độ của z trong không gian 3 chiều dựa vào tọa độ x,y ban đầu. Điểm làm SVM hiệu quả hơn các phương pháp khác chính là việc sử dụng Kernel Method giúp cho SVM không còn bị giới hạn bởi việc phân lớp một cách tuyến tính, hay nói cách khác các siêu phẳng có thể được hình thành từ các hàm phi tuyến.Margin là khoảng cách giữa siêu phẳng đến 2 điểm dữ liệu gần nhất tương ứng với các phân lớp. Trong ví dụ quả táo quả lê đặt trên mặt bán, margin chính là khoảng cách giữa cây que và hai quả táo và lê gần nó nhất.Điều quan trọng ở đây đó là phương pháp SVM luôn cô gắng cực đại hóa margin này, từ đó thu được một siêu phẳng tạo khoảng cách xa nhất so với 2 quả táo và lê. Nhờ vậy, SVM có thể giảm thiểu việc phân lớp sai (misclassification) đối với điểm dữ liệu mới đưa vào.Là một kĩ thuật phân lớp khá phổ biến, SVM thể hiện được nhiều ưu điểm trong số đó có việc tính toán hiệu quả trên các tập dữ liệu lớn. Có thể kể thêm một số ưu điểm của phương pháp này như:SVM là một phương pháp hiệu quả cho bài toán phân lớp dữ liệu. Nó là một công cụ đắc lực cho các bài toán về xử lý ảnh, phân loại văn bản, phân tích quan điểm. Một yếu tố làm nên hiệu quả của SVM đó là việc sử dụng Kernel function khiến cho các phương pháp chuyển không gian trở nên linh hoạt hơn.Support Vector Machine TutorialSVM Exercise\n                    © 2017 Viblo. All rights reserved.\n                ",
          "relevence": "yes"
        },
        {
          "url": "https://machinelearningcoban.com/2017/04/22/kernelsmv/",
          "title": "Machine Learning cơ bản",
          "content": "Bạn đọc được khuyến khích đọc Bài 19 và Bài 20 trước khi đọc bài này.\nCó một sự tương ứng thú vị giữa hai nhóm thuật toán phân lớp phổ biến nhất: Neural Network và Support Vector Machine. Chúng đều bắt đầu từ bài toán phân lớp với 2 linearly separable classes, tiếp theo đến 2 almost linear separable classes, đến bài toán có nhiều classes rồi các bài toán với biên không tuyến tính. Sự tương ứng được cho trong bảng dưới đây:Trong Bài 21 này, tôi sẽ viết về Kernel SVM, tức việc áp dụng SVM lên bài toán mà dữ liệu giữa hai classes là hoàn toàn không linear separable (tôi tạm dịch là không phân biệt tuyến tính). Bài toán phân biệt nhiều classes sẽ được tôi trình bày trong Bài 22: Multiclass SVM.Ý tưởng cơ bản của Kernel SVM và các phương pháp kernel nói chung là tìm một phép biến đổi sao cho dữ liệu ban đầu là không phân biệt tuyến tính được biến sang không gian mới. Ở không gian mới này, dữ liệu trở nên phân biệt tuyến tính.Xét ví dụ dưới đây với việc biến dữ liệu không phân biệt tuyến tính trong không gian hai chiều thành phân biệt tuyến tính trong không gian ba chiều bằng cách giới thiệu thêm một chiều mới:Để xem ví dụ này một cách sinh động hơn, bạn có thể xem clip nhỏ dưới đây:Nói một cách ngắn gọn, Kernel SVM là việc đi tìm một hàm số biến đổi dữ liệu \\(\\mathbf{x}\\) từ không gian feature ban đầu thành dữ liệu trong một không gian mới bằng hàm số \\(\\Phi(\\mathbf{x})\\). Trong ví dụ này, hàm \\(\\Phi()\\) đơn giản là giới thiệu thêm một chiều dữ liệu mới (một feature mới) là một hàm số của các features đã biết. Hàm số này cần thỏa mãn mục đích của chúng ta: trong không gian mới, dữ liệu giữa hai classes là phân biệt tuyến tính hoặc gần như phần biệt tuyến tính. Khi đó, ta có thể dùng các bộ phân lớp tuyến tính thông thường như PLA, Logistic Regression, hay Hard/Soft Margin SVM.Nếu phải so sánh, ta có thể thấy rằng hàm biến đổi \\(\\Phi()\\) tương tự như activation functions trong Neural Networks. Tuy nhiên, có một điểm khác biệt ở đây là: trong khi nhiệm vụ của activation function là phá vỡ tính tuyến tính của mô hình, hàm biến đổi \\(\\Phi()\\) đi biến dữ liệu không phân biệt tuyến tính thành phân biệt tuyến tính. Như vậy là để đạt được mục đích chung, ta có hai cách nhìn khác nhau về cách giải quyết.Các hàm \\(\\Phi()\\) thường tạo ra dữ liệu mới có số chiều cao hơn số chiều của dữ liệu ban đầu, thậm chí là vô hạn chiều. Nếu tính toán các hàm này trực tiếp, chắc chắn chúng ta sẽ gặp các vấn đề về bộ nhớ và hiệu năng tính toán. Có một cách tiếp cận là sử dụng các kernel functions mô tả quan hệ giữa hai điểm dữ liệu bất kỳ trong không gian mới, thay vì đi tính toán trực tiếp từng điểm dữ liệu trong không gian mới. Kỹ thuật này được xây dựng dựa trên quan sát về bài toán đối ngẫu của SVM.Trong Mục 2 dưới đây, chúng ta cùng tìm hiểu cơ sở toán học của Kernel SVM và Mục 3 sẽ giới thiệu một số hàm Kernel thường được sử dụng.Tôi xin nhắc lại bài toán đối ngẫu trong Soft Margin SVM cho dữ liệu gần phân biệt tuyến tính:\\begin{eqnarray}\n     \\lambda &=& \\arg \\max_{\\lambda} \\sum_{n=1}^N \\lambda_n - \\frac{1}{2} \\sum_{n=1}^N\\sum_{m=1}^N \\lambda_n \\lambda_m y_n y_m \\mathbf{x}_n^T \\mathbf{x}_m &&\\\\\n     \\text{subject to:}~ && \\sum_{n=1}^N \\lambda_ny_n = 0 &&\\quad\\quad\\quad\\quad(1)\\\\\n     && 0 \\leq \\lambda_n \\leq C, ~\\forall n= 1, 2, \\dots, N \n \\end{eqnarray}Trong đó:\\(N\\): số cặp điểm dữ liệu trong tập training.\\(\\mathbf{x}_n\\): feature vector của dữ liệu thứ \\(n\\) trong tập training.\\(y_n\\): nhãn của dữ liệu thứ \\(n\\), bằng 1 hoặc -1.\\(\\lambda_n\\): nhân tử Lagrange ứng với điểm dữ liệu thứ \\(n\\).\\(C\\): hằng số dương giúp cân đối độ lớn của margin và sự hy sinh của các điểm nằm trong vùng không an toàn. Khi \\(C = \\infty\\) hoặc rất lớn, Soft Margin SVM trở thành Hard Margin SVM.Sau khi giải được \\(\\lambda\\) cho bài toán \\((1)\\), nhãn của một điểm dữ liệu mới sẽ được xác định bởi dấu của biểu thức: \n\\[\n\\sum_{m \\in \\mathcal{S}} \\lambda_m y_m \\mathbf{x}_m^T \\mathbf{x} + \\frac{1}{N_{\\mathcal{M}}} \\sum_{n \\in \\mathcal{M}} \\left(y_n - \\sum_{m \\in \\mathcal{S}} \\lambda_m y_m \\mathbf{x}_m^T\\mathbf{x}_n\\right)~~~~~~~~~ (2)\n\\]Trong đó:\\(\\mathcal{M} = \\{n: 0 < \\lambda_n < C\\}\\) là tập hợp những điểm nằm trên margin.\\(\\mathcal{S} = \\{n: 0 < \\lambda_n\\}\\) là tập hợp các điểm support.\\(N_{\\mathcal{M}}\\) là số phần tử của \\(\\mathcal{M}\\).Với dữ liệu thực tế, rất khó để có dữ liệu gần phân biệt tuyến tính, vì vậy nghiệm của bài toán \\((1)\\) có thể không thực sự tạo ra một bộ phân lớp tốt. Giả sử rằng ta có thể tìm được hàm số \\(\\Phi()\\) sao cho sau khi được biến đổi sang không gian mới, mỗi điểm dữ liệu \\(\\mathbf{x}\\) trở thành \\(\\Phi(\\mathbf{x})\\), và trong không gian mới này, dữ liệu trở nên gần phân biệt tuyến tính. Lúc này, hy vọng rằng nghiệm của bài toán Soft Margin SVM sẽ cho chúng ta một bộ phân lớp tốt hơn.Trong không gian mới, bài toán \\((1)\\) trở thành: \n \\begin{eqnarray}\n     \\lambda &=& \\arg \\max_{\\lambda} \\sum_{n=1}^N \\lambda_n - \\frac{1}{2} \\sum_{n=1}^N\\sum_{m=1}^N \\lambda_n \\lambda_m y_n y_m \\Phi(\\mathbf{x}_n)^T \\Phi(\\mathbf{x}_m) &&\\\\\n     \\text{subject to:}~ && \\sum_{n=1}^N \\lambda_ny_n = 0 &&\\quad\\quad\\quad\\quad(3)\\\\\n     && 0 \\leq \\lambda_n \\leq C, ~\\forall n= 1, 2, \\dots, N \n \\end{eqnarray}và nhãn của một điểm dữ liệu mới được xác định bởi dấu của biểu thức:\\[\n\\mathbf{w}^T\\Phi(\\mathbf{x}) + b = \\sum_{m \\in \\mathcal{S}} \\lambda_m y_m \\Phi(\\mathbf{x}_m)^T \\Phi(\\mathbf{x}) + \\frac{1}{N_{\\mathcal{M}}} \\sum_{n \\in \\mathcal{M}} \\left(y_n - \\sum_{m \\in \\mathcal{S}} \\lambda_m y_m \\Phi(\\mathbf{x}_m)^T\\Phi(\\mathbf{x}_n)\\right)~~~~~~~~~ (4)\n\\]Như đã nói ở trên, việc tính toán trực tiếp \\(\\Phi(\\mathbf{x})\\) cho mỗi điểm dữ liệu có thể sẽ tốn rất nhiều bộ nhớ và thời gian vì số chiều của \\(\\Phi(\\mathbf{x})\\) thường là rất lớn, có thể là vô hạn! Thêm nữa, để tìm nhãn của một điểm dữ liệu mới \\(\\mathbf{x}\\), ta lại phải tìm biến đổi của nó \\(\\Phi(\\mathbf{x})\\) trong không gian mới rồi lấy tích vô hướng của nó với tất cả các \\(\\Phi(\\mathbf{x}_m)\\) với \\(m\\) trong tập hợp support. Để tránh việc này, ta quan sát thấy một điều thú vị sau đây.Trong bài toán \\((3)\\) và biểu thức \\((4)\\), chúng ta không cần tính trực tiếp \\(\\Phi(\\mathbf{x})\\) cho mọi điểm dữ liệu. Chúng ta chỉ cần tính được \\(\\Phi(\\mathbf{x})^T\\Phi(\\mathbf{z})\\) dựa trên hai điểm dữ liệu \\(\\mathbf{x}, \\mathbf{z}\\) bất kỳ! Kỹ thuật này còn được gọi là kernel trick. Những phương pháp dựa trên kỹ thuật này, tức thay vì trực tiếp tính tọa độ của một điểm trong không gian mới, ta đi tính tích vô hướng giữa hai điểm trong không gian mới, được gọi chung là kerrnel method.Lúc này, bằng cách định nghĩa hàm kernel \\(k(\\mathbf{x}, \\mathbf{z}) = \\Phi(\\mathbf{x})^T\\Phi(\\mathbf{z}) \\), ta có thể viết lại bài toán \\((3)\\) và biểu thức \\((4)\\) như sau:\\begin{eqnarray}\n    \\lambda &=& \\arg \\max_{\\lambda} \\sum_{n=1}^N \\lambda_n - \\frac{1}{2} \\sum_{n=1}^N\\sum_{m=1}^N \\lambda_n \\lambda_m y_n y_m k(\\mathbf{x}_n,\\mathbf{x}_m) &&\\\\\n    \\text{subject to:}~ && \\sum_{n=1}^N \\lambda_ny_n = 0 &&\\quad\\quad\\quad\\quad(5)\\\\\n    && 0 \\leq \\lambda_n \\leq C, ~\\forall n= 1, 2, \\dots, N &&\n\\end{eqnarray}\nvà:\\[\n\\sum_{m \\in \\mathcal{S}} \\lambda_m y_m k(\\mathbf{x}_m, \\mathbf{x}) + \\frac{1}{N_{\\mathcal{M}}} \\sum_{n \\in \\mathcal{M}} \\left(y_n - \\sum_{m \\in \\mathcal{S}} \\lambda_m y_m k(\\mathbf{x}_m, \\mathbf{x}_n)\\right)~~~~~~~~~ (6)\n\\]Ví dụ: Xét phép biến đổi 1 điểm dữ liệu trong không gian hai chiều \\(\\mathbf{x} = [x_1, x_2]^T\\) thành một điểm trong không gian 5 chiều \\(\\Phi(\\mathbf{x}) = [1, \\sqrt{2} x_1, \\sqrt{2} x_2, x_1^2, \\sqrt{2} x_1x_2, x_2^2]^T\\). Ta có:\\begin{eqnarray}\n\\Phi(\\mathbf{x})^T\\Phi(\\mathbf{z}) &=& [1, \\sqrt{2} x_1, \\sqrt{2} x_2, x_1^2, \\sqrt{2} x_1x_2, x_2^2] [1, \\sqrt{2} z_1, \\sqrt{2} z_2, z_1^2, \\sqrt{2} z_1z_2, z_2^2]^T \\\\\n&=& 1 + 2x_1z_1 + 2x_2z_2 + x_1^2x_2^2 + 2x_1z_1x_2z_2 + x_2^2z_2^2 \\\\\n&=& (1 + x_1z_1 + x_2z_2)^2 = (1 + \\mathbf{x}^T\\mathbf{z})^2 = k(\\mathbf{x}, \\mathbf{z})\n\\end{eqnarray}Trong ví dụ này, rõ ràng rằng việc tính toán hàm kernel \\(k()\\) cho hai điểm dữ liệu dễ dàng hơn việc tính từng \\(\\Phi()\\) rồi nhân chúng với nhau.Vậy những hàm số kernel cần có những tính chất gì, và những hàm như thế nào được sử dụng trong thực tế?\nKhông phải hàm \\(k()\\) bất kỳ nào cũng được sử dụng. Các hàm kerrnel cần có các tính chất:Đối xứng: \\(k(\\mathbf{x}, \\mathbf{z}) = k(\\mathbf{z}, \\mathbf{x}) \\). Điều này dễ nhận ra vì tích vô hướng của hai vector có tính đối xứng.Về lý thuyết, hàm kerrnel cần thỏa mãn điều kiện Mercer: \n\\[\n\\sum_{n=1}^N \\sum_{m=1}^N k(\\mathbf{x}_m, \\mathbf{x}_n) c_nc_m \\geq 0, ~~ \\forall c_i \\in \\mathbb{R}, i = 1, 2, \\dots, N \\quad \\quad (7)\n\\]\nTính chất này để đảm bảo cho việc hàm mục tiêu của bài toán đối ngẫu \\((5)\\) là lồi.Trong thực hành, có một vài hàm số \\(k()\\) không thỏa mãn điều kiện Merrcer nhưng vẫn cho kết quả chấp nhận được. Những hàm số này vẫn được gọi là kernel. Trong bài viết này, tôi chỉ tập trung vào các hàm kernel thông dụng và có sẵn trong các thư viện.Nếu một hàm kerrnel thỏa mãn điều kiện \\((7)\\), xét \\(c_n = y_n \\lambda_n\\), ta sẽ có: \n\\[\n\\lambda^T \\mathbf{K} \\lambda = \\sum_{n=1}^N \\sum_{m=1}^N k(\\mathbf{x}_m, \\mathbf{x}_n) y_ny_m \\lambda_n \\lambda_m \\geq 0, ~\\forall \\lambda_n \\quad\\quad (8)\n\\]\nvới \\(\\mathbf{K}\\) là một ma trận đối xứng mà phần tử ở hàng thứ \\(n\\) cột thứ \\(m\\) của nó được định nghĩa bởi: \n\\(\nk_{nm} = y_ny_m k(\\mathbf{x}_n, \\mathbf{x}_m)\n\\)Từ \\((8)\\) ta suy ra \\(\\mathbf{K}\\) là một ma trận nửa xác định dương. Vì vậy, bài toán tối ưu \\((5)\\) có ràng buộc là lồi và hàm mục tiêu là một hàm lồi (một quadratic form). Vì vậy chúng ta có thể giải quyết bài toán này một cách hiệu quả.Trong bài viết này, tôi sẽ không đi sâu vào việc giải quyết bài toán \\((5)\\) vì nó hoàn toàn tương tự như bài toán đối ngẫu của Soft Margin SVM. Thay vào đó, tôi sẽ trình bày các hàm kernel thông dụng và hiệu năng của chúng trong các bài toán thực tế. Việc này sẽ được thực hiện thông qua các ví dụ và cách sử dụng thư viện sklearn.Đây là trường hợp đơn giản với kernel chính tích vô hướng của hai vector: \n\\[\nk(\\mathbf{x}, \\mathbf{z}) = \\mathbf{x}^T\\mathbf{z}\n\\]Hàm số này, như đã chứng minh trong Bài 19, thỏa mãn điều kiện \\((7)\\).Khi sử dụng hàm sklearn.svm.SVC, kernel này được chọn bằng cách đặt kernel = 'linear'\\[\nk(\\mathbf{x}, \\mathbf{z}) = (r + \\gamma \\mathbf{x}^T\\mathbf{z})^d\n\\]Với \\(d\\) là một số dương để chỉ bậc của đa thức. \\(d\\) có thể không là số tự nhiên vì mục đích chính của ta không phải là bậc của đa thức mà là cách tính kernel. Polynomial kernel có thể dùng để mô tả hầu hết các đa thức có bậc không vượt quá \\(d\\) nếu \\(d\\) là một số tự nhiên.Phần kiểm tra liệu hàm này có thỏa mãn điều kiện \\((7)\\) hay không xin được bỏ qua.Khi sử dụng thư viện sklearn, kerrnel này được chọn bằng cách đặt kernel = 'poly'. Thông tin cụ thể về cách sử dụng có thể xem tại đây.Radial Basic Function (RBF) kernel hay Gaussian kernel được sử dụng nhiều nhất trong thực tế, và là lựa chọn mặc định trong sklearn. Nó được định nghĩa bởi:\n\\[\nk(\\mathbf{x}, \\mathbf{z}) = \\exp(-\\gamma ||\\mathbf{x} - \\mathbf{z}||_2^2), ~~ \\gamma > 0\n\\]Trong sklearn, kernel = 'rbf'.Sigmoid function cũng được sử dụng làm kernel:\n\\[\nk(\\mathbf{x}, \\mathbf{z}) = \\text{tanh}(\\gamma \\mathbf{x}^T\\mathbf{z} + r)\n\\]kernel = 'sigmoid'Dưới đây là bảng tóm tắt các kernel thông dụng và cách sử dụng trong sklearn.Nếu bạn muốn sử dụng các thư viện cho C/C++, các bạn có thể tham khảo LIBSVM và LIBLINEARNgoài các hàm kernel thông dụng như trên, chúng ta cũng có thể tự định nghĩa các kernel của mình như trong hướng dẫn này. \nChúng ta cùng quay lại với bài toán XOR. Chúng ta biết rằng bài toán XOR không thể giải quyết nếu chỉ dùng một bộ phân lớp tuyến tính. Neurrel Network cần 2 layers để giải quyết bài toán này. Với SVM, chúng ta có cách để chỉ cần sử dụng một bộ phân lớp. Dưới đây là ví dụ:Kết quả được cho trong Hình 2 dưới đây:Ta có các nhận xét đối với mỗi kernel như sau:sigmoid: nghiệm tìm được không thật tốt vì có 3 trong 4 điểm nằm chính xác trên đường phân chia. Nói cách khác, nghiệm này rất nhạy cảm với nhiễu.poly: Nghiệm này có tốt hơn nghiệm của sigmoid nhưng kết quả có phần giống với overfitting.rbf: Dữ liệu được tạo ra một cách đối xứng, đường phân lớp tìm được cũng tạo ra các vùng đối xứng với mỗi class. Nghiệm này được cho là hợp lý hơn. Trên thực tế, các rbf kernel được sử dụng nhiều nhất và cũng là lựa chọn mặc định trong hàm sklearn.svm.SVC.Xét một ví dụ khác với dữ liệu giữa hai classes là gần phân biệt tuyến tính như HÌnh 3 dưới đây:Trong ví dụ này, kernel = 'poly' cho kết quả tốt hơn kernel = 'rbf' vì trực quan cho ta thấy rằng nửa bên phải của mặt phẳng nên hoàn thoàn thuộc vào class xanh. sigmoid kernel cho kết quả không thực sự tốt và ít được sử dụng.Bài toán này đã được đề cập ở Bài 12 với dữ liệu đầu vào là các ảnh khuôn mặt. Vì tôi không được phép phân phối cơ sở dữ liệu gốc này, tôi sẽ chia sẻ cho các bạn về dữ liệu đã qua xử lý, được lưu trong file myARgender.mat, có thể được download tại đây. Dưới đây là ví dụ về cách sử dụng thư viện sklearn.svm.SVC để giải quyết bài toán:Kết quả không tệ! Các bạn thử thay các kernel và thiết lập các tham số khác xem kết quả thay đổi như thế nào. Vì dữ liệu giữa hai classes là gần phân biệt tuyến tính nên không có sự khác nhau nhiều giữa các kernel.Nếu dữ liệu của hai lớp là không phân biệt tuyến tính, chúng ta có thể tìm cách biến đổi dữ liệu sang một không gian mới sao cho trong không gian mới ấy, dữ liệu của hai lớp là phân biệt tuyến tính hoặc gần phân biệt tuyến tính.Việc tính toán trực tiếp hàm \\(\\Phi()\\) đôi khi phức tạp và tốn nhiều bộ nhớ. Thay vào đó, ta có thể sử dụng kernel trick. Trong cách tiếp cận này, ta chỉ cần tính tích vô hướng của hai vector bất kỳ trong không gian mới: \\(k(\\mathbf{x}, \\mathbf{z}) = \\Phi(\\mathbf{x})^T\\Phi(\\mathbf{z})\\).Thông thường, các hàm \\(k()\\) thỏa mãn điều kiện Merrcer, và được gọi là kernel. Cách giải bài toán SVM với kernel hoàn toàn giống với cách giải bài toán Soft Margin SVM.Có 4 loại kernel thông dụng: linear, poly, rbf, sigmoid. Trong đó, rbf được sử dụng nhiều nhất và là lựa chọn mặc định trong các thư viện SVM.Với dữ liệu gần phân biệt tuyến tính, linear và poly kernels cho kết quả tốt hơn.Source code.[1] Bishop, Christopher M. “Pattern recognition and Machine Learning.”, Springer  (2006). (book)[2] Duda, Richard O., Peter E. Hart, and David G. Stork. Pattern classification. John Wiley & Sons, 2012.[3] sklearn.svm.SVC[4] LIBSVM – A Library for Support Vector Machines[5] Bennett, K. P. (1992). “Robust linear programming discrimination of two linearly separable sets”. Optimization Methods and Software 1, 23–34.[6] Sch¨olkopf, B., A. Smola, R. C.Williamson, and P. L. Bartlett (2000). “New support vector algorithms”. Neural Computation 12(5), 1207–1245[7]  Rosasco, L.; De Vito, E. D.; Caponnetto, A.; Piana, M.; Verri, A. (2004). “Are Loss Functions All the Same?”. Neural Computation. 16 (5): 1063–1076[8] slearn Kernel functions[9] Kernel method[10] http://www.support-vector-machines.org/",
          "relevence": "yes"
        },
        {
          "url": "https://www.scribd.com/document/208063738/Machine-Learning-K%E1%BB%B9-thu%E1%BA%ADt-SVM",
          "title": "[Machine Learning] Kỹ thuật SVM",
          "content": "HỒ CHÍ MINH\n\nTRƯỜNG ĐẠI HỌC CÔNG NGHỆ THÔNG TIN\nKHOA KHOA HỌC MÁY TÍNH\nBÁO CÁO ĐỒ ÁN\nNHẬP MÔN CÔNG NGHỆ TRI THỨC VÀ MÁY HỌC\n\nPHƯƠNG PHÁP SUPPORT VECTOR MACHINES\nLÝ THUYẾT VÀ ỨNG DỤNG\n\nGiảng viên hướng dẫn: THS. NGUYỄN ĐÌNH HIỂN\nSinh viên thực hiện:\nNGUYỄN TRÍ HẢI\n\n11520094\n\nNGUYỄN HOÀNG NGHĨA\n\n11520603\n\nLớp:\nLớp môn học:\nKhoá:\n\nKHTN2011\nC110.E11\n2011\n\nTP. Hồ Chí Minh, Ngày 04 tháng 12 năm 2013\n\n\fĐẠI HỌC QUỐC GIA TP. HỒ CHÍ MINH\n\nTRƯỜNG ĐẠI HỌC CÔNG NGHỆ THÔNG TIN\nKHOA KHOA HỌC MÁY TÍNH\nBÁO CÁO ĐỒ ÁN\nNHẬP MÔN CÔNG NGHỆ TRI THỨC VÀ MÁY HỌC\n\nPHƯƠNG PHÁP SUPPORT VECTOR MACHINES\nLÝ THUYẾT VÀ ỨNG DỤNG\n\nGiảng viên hướng dẫn: THS. NGUYỄN ĐÌNH HIỂN\nSinh viên thực hiện:\nNGUYỄN TRÍ HẢI\n\n11520094\n\nNGUYỄN HOÀNG NGHĨA\n\n11520603\n\nLớp:\nLớp môn học:\nKhoá:\n\nKHTN2011\nCS110.E11\n2011\n\nTP. Hồ Chí Minh, Ngày 04 tháng 12 năm 2013\n\n\fMỞ ĐẦU\nTầm quan trọng của việc học trong tri thức của con người là luôn\nlà vấn đề đặt lên hàng đầu. Trong tin học, khi mà các hệ chuyên gia\nchưa đáp ứng đủ các vấn đề cần giải quyết. Đồng thời việc cập nhật sự\nthay đổi tự nhiên là việc rất tốn kém. Giải pháp đặt ra là cho các máy\ntính tự động học và giải quyết các vấn đề dựa trên những dữ liệu thực\ntế. Học máy (Machine learning) là một nhánh quan trọng của trí tuệ\nnhân tạo nghiên cứu cac phương pháp, kỹ thuật cho phép máy tính có\nthể tự động học dữ liệu để giải quyết một vấn đề cụ thể nào đó.\nTrong quá trình tiếp nhận tri thức của con người. Phân loại\n(Classification) là một quá trình tự nhiên giúp cho việc tiếp nhận và tri\nthức có thể được hệ thống lưu trữ cụ thể. Có nhiều phương pháp phân\nloại đã được nghiên cứu và được áp dụng. Hiện nay, phương pháp phân\nloại Support Vector Machines là một trong những phương pháp mạnh và\nhiệu quả để giải quyết các bài toán phân lớp phi tuyến được Vapnik và\nChervonenkis giới thiệu vào năm 1995.\n\n\fLỜI CẢM ƠN\nLời đầu tiên chúng em xin được bày tỏ lòng biết ơn sâu sắc nhất\ntới ThS. Nguyễn Đình Hiển, Khoa Khoa học máy tính, Đại học Công nghệ\nthông tin ĐHQG-HCM, người đã tận tình hướng dẫn nhóm em kiến thức\ncăn bản và thiết yếu để hoàn thành đồ án.\nTiếp đến, xin cám ơn các bạn lớp CS110.E11 đã cùng nhóm trao\nđổi và bổ sung kiến thức hỗ trợ cho việc hoàn thiện đồ án.\n\nTP. Hồ Chí Minh, Ngày 04 tháng 12 năm 2013\n\nNguyễn Trí Hải – Nguyễn Hoàng Nghĩa\n\n\fNHẬN XÉT CỦA GIẢNG VIÊN\n···································································································\n···································································································\n···································································································\n···································································································\n···································································································\n···································································································\n···································································································\n···································································································\n···································································································\n···································································································\n···································································································\n···································································································\n···································································································\n···································································································\n···································································································\n···································································································\n···································································································\n···································································································\n···································································································\n···································································································\n···································································································\n···································································································\n···································································································\n\n\fMỤC LỤC\nDANH MỤC HÌNH VẼ ............................................................ B\nCHƯƠNG I. TỔNG QUAN ĐỒ ÁN ............................................. 1\n1. Nội dung chung ...................................................... 1\n2. Phân công thực hiện ............................................... 1\nCHƯƠNG II. CÁC KHÁI NIỆM CƠ BẢN ..................................... 2\n1. Bài toán phân lớp\nvà phương pháp Support Vector Machines. ................ 2\n2. Phân lớp tuyến tính ................................................ 2\n3. Ma trận GRAM ........................................................ 3\n4. Không gian đặc trưng.............................................. 3\n5. Hàm hạt nhân ........................................................ 4\n6. Kết chương ............................................................ 6\nCHƯƠNG III. PHƯƠNG PHÁP SUPPORT VECTOR MACHINES ....... 7\n1. Ý tưởng ................................................................. 7\n2. Cơ sở lý thuyết ....................................................... 8\n3. Bài toán phân 2 lớp với SVM .................................... 8\n4. Bài toán phân đa lớp với SVM ................................... 13\n5. Các bước chính của phương pháp SVM ...................... 13\n6. So sánh và một số cải tiến ....................................... 13\nCHƯƠNG IV. ỨNG DỤNG ....................................................... 15\n1. Một số ứng dụng của SVM ....................................... 15\n2. Mô phỏng phương pháp SVM\nqua biểu diễn hình học bằng Matlab .......................... 16\n3. Xây dựng chương trình lọc thư rác ............................ 16\nCHƯƠNG V. TỔNG KẾT .......................................................... 21\nPHỤ LỤC\nChạy thử mô phỏng phương pháp SVM\nqua biểu diễn hình học bằng Matlab .......................... 22\nTHAM KHẢO ........................................................................ 25\nA\n\n\fDANH MỤC HÌNH ẢNH\nHình 2.1 Ánh xạ Φ từ không gian dữ liệu X\nsang không gian đặc trưng F ....................................... 4\nHinh 2.2 Ví dụ hàm hạt nhân ................................................... 4\nHình 3.1 Mô tả phương pháp SVM ............................................ 8\nHình 3.2 Tập dữ liệu được phân chia tuyến tính ......................... 9\nHình 3.3 Tập dữ liệu phân chia tuyến tính nhưng có nhiễu .......... 10\nHình 3.4 Tập dữ liệu không phân chia tuyến tính ....................... 11\nHình 3.5 Ví dụ biểu diễn tập dữ liệu trên không gian 2 chiều ....... 12\nHình 4.1 Phân loại biểu cảm .................................................... 15\nHình 4.2 Phân loại ảnh ........................................................... 15\nHình 4.3 Tiền xử lý email ........................................................ 17\nHình 4.4 Trích xuất thông tin email .......................................... 18\nHình 4.5 Quá trình học dữ liệu ................................................. 19\nHình 4.6 Các từ mẫu có khả năng là spam ................................ 19\nHình 4.7 Chạy thử chương trình ............................................... 20\n\nB\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nCHƯƠNG I\nTỔNG QUAN ĐỒ ÁN\n1. Nội dung chung.\n* Tên đề tài:\nPhương pháp Support Vector Machines: Lý thuyết và ứng dụng.\n* Giảng viên hướng dẫn:\nThS. Nguyễn Đình Hiển.\n* Sinh viên thực hiện:\nNguyễn Trí Hải, Nguyễn Hoàng Nghĩa.\n* Khoá học:\nKhoá 2011 (Ngày nhập học: Tháng 09/2011).\n* Thông tin liên lạc của sinh viên:\nSTT Tên\nMSSV\nEmail\n1\nNguyễn Trí Hải\n11520094 11520094@gm.uit.edu.vn\n2\nNguyễn Hoàng Nghĩa 11520603 11520603@gm.uit.edu.vn\n* Chương trình, ứng dụng sử dụng:\n- Chương trình lập trình: Mathworks Matlab R2012b.\n- Môi trường lập trình: Matlab.\n2. Phân công thực hiện.\nPhân công\n\nThực hiện\nTìm kiếm, tổng hợp tài liệu\nCả nhóm\nLên nội dung cần làm cho đề tài, tìm kiếm tài liệu:\n- Thời gian thực hiện.\n- Nội dung lý thuyết, chương trình, ứng dụng hỗ trợ.\nThực hiện đồ án\nNguyễn Trí Hải\n- Tìm hiểu, xây dụng nội dung ứng dụng\nNguyễn Hoàng Nghĩa - Tổng hợp, xây dựng nội dung lý thuyết.\nCả nhóm\n- Viết báo cáo, trình bày slide\n- Sửa lỗi\n\n1\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nCHƯƠNG II.\nCÁC KHÁI NIỆM CƠ BẢN\n1. Bài toán phân lớp và phương pháp Support Vector Machines.\nPhân lớp là quá trình “nhóm” các đối tượng “giống” nhau vào “một lớp” dựa trên\ncác đặc trưng dữ liệu của chúng. Khi nghiên cứu một đối tượng, hiện tượng, chúng ta\nchỉ có thể dựa vào một số hữu hạn các đặc trưng của chúng. Nói cách khác, ta chỉ xem\nxét biểu diễn của đối tượng, hiện tượng trong một không gian hữu hạn chiều, mỗi chiều\nứng với một đặc trưng được lựa chọn.\nBài toán phân lớp có thể được mô tả như sau:\n̅̅̅̅̅\nCho tập mẫu Ω = {(𝑋𝑖 , 𝐶𝑖 ) | 𝑋𝑖 𝜖 ℝ𝑛 ; 𝐶𝑖 𝜖 ℝ𝑚 ; 𝑖 = 1,\n𝑁}\nTìm ánh xạ 𝑓: ℝ𝑛 → ℝ𝑚 , sao cho ‖𝑓(𝑋𝑖) − 𝐶𝑖 ‖ ≅ 0, ∀(𝑋𝑖 , 𝐶𝑖 ) ∈ Ω, ∀𝑖 = ̅̅̅̅̅\n1, 𝑁\nTrong đó:\nN: Số mẫu\nXi: Mẫu dữ liệu thứ i;\nCi: lớp của mẫu dữ liệu thứ i.\nÁnh xạ 𝑓 ở đây có thể hiểu là một mô hình các quy tắc, các luật để xác định từng\nđối tượng đang xét thuộc về lớp nào dựa trên các đặc trưng của chúng. Trong thực tế,\nviệc xác định các đặc trưng để phân lớp bị ảnh hưởng bởi nhiều yếu tố gây khó khăn\ncho quá trình phân lớp như: đối tượng có nhiều thuộc tích, xác định các thuộc tính nào\nlà cần thiết, các thuộc tính nào là không cần thiết.\nBài toán phân lớp sử dụng phương pháp Support vector machines (SVM) nhằm mục\nđích tìm một siêu phẳng có biên cực đại giữa lớp mẫu âm và lớp mẫu dương. Đồng thời\ncực tiểu hoá các mẫu không phân chia được trong tập huấn luyện. SVM đựa trên cơ sở\ntoán học vững chắc. Tuy nhiên, việc huấn luyện mẫu sử dụng SVM đòi hỏi phải giải\nbài toán tối ưu nhiều biến. Ban đầu, SVM được phát triển để giải các bài toán phân lớp,\nvề sau do tính ưu việt, nó còn được sử dụng rộng rãi để giải các bài toán hồi quy.\n2. Phân lớp tuyến tính.\nViệc phân lớp nhị phân được thực hiện bằng cách sử dụng hàm giá trị thực\n\n2\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\n𝑓: 𝑋 ⊆ ℝ𝑛 → ℝ là hàm tuyến tính, tương ứng với đầu ra 𝑦 ∈ {-1, 1}, được phát biểu\nnhư sau:\nĐầu vào 𝑥 = (𝑥1 , 𝑥2 , … , 𝑥𝑛 ) sẽ được gán vào lớp có nhãn 1 nếu 𝑓 (𝑥) ≥ 0, trường\nhợp ngược lại sẽ được gán vào lớp có nhãn -1. Trường hợp 𝑓(𝑥) là hàm tuyến tính của\n𝑥 ∈ 𝑋 ta có thể viết như sau:\n𝑓 (𝑥) = 〈𝑤. 𝑥〉 + 𝑏\n𝑛\n\n= ∑(𝑤𝑖 . 𝑥1 ) + 𝑏\n𝑖=1\n\nTrong đó 〈. 〉 biểu thị tích vô hướng\nVề mặt hình học, các phần tử của không gian đầu vào X sẽ được rơi vào một trong\nhai phần được phân tách bởi siêu phẳng xác định bằng biểu thức:\n〈𝑤. 𝑥〉 + 𝑏 = 0\nTrong đó: w là vector pháp tuyến cùa siêu phẳng, giá trị ngưỡng b thay đổi có thể\ntạo ra các siêu phẳng song song với nhau.\nVới mỗi mẫu dữ liệu x chưa xác định sẽ đươc phân chia thành:\n𝑥∈ {\n\n𝐿ớ𝑝 𝐼, 𝑛ế𝑢 𝑓 > 0\n𝐿ớ𝑝 𝐼𝐼, 𝑡𝑟𝑜𝑛𝑔 𝑐á𝑐 𝑡𝑟ườ𝑛𝑔 ℎợ𝑝 𝑐ò𝑛 𝑙ạ𝑖\n\n3. Ma trận GRAM.\nCho tập {𝑥1 , 𝑥1 , … , 𝑥𝑙 } các vector trong không gian tích vô hướng X, ma trận G kích\nthước 𝑙 × 𝑙 với 𝐺𝑖,𝑗 = 〈𝑥𝑖 , 𝑥𝑗 〉 được gọi là ma trận GRAM.\nĐặc điểm quan trọng của ma trận GRAM là: các dữ liệu đầu vào cho các chương\ntrình tổng hợp hoặc khái quát hoàn toàn có thể biểu diễn thông qua ma trận GRAM.\n4. Không gian đặc trưng.\nSự phức tạp của hàm mục tiêu dẫn đến quá trình học phụ thuộc vào cách nó được\ndiễn tả. Khi diễn tả dữ liệu một cách phù hợp, vấn đề học sẽ trở nên dễ dàng. Vì vậy,\nmột việc làm rất phổ biến trong học máy là chuyển đổi dữ liệu từ không gian đầu vào\nX sang không gian đặc trưng:\n𝑥 = (𝑥1 , 𝑥2 , … , 𝑥𝑛 ) ↦ Φ(𝑥) = (Φ1 (𝑥), … , Φ𝑁 (𝑥))\nTrong đó n là số chiều của đầu vào (số thuộc tính) và N là số chiều của không gian\nđặc trưng. Dữ liệu sẽ được chuyển vào không gian đặc trưng với N > n.\n3\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nKhông gian đặc trưng ký hiệu là F:\n𝐹 = {Φ(𝑥)|𝑥 𝜖 𝑋}\n\nHình 2.1 Ánh xạ 𝛷 từ không gian dữ liệu X sang không gian đặc trưng F\n5. Hàm hạt nhân.\na) Khái niệm:\nMột hàm hạt nhân là một hàm K sao cho với mọi 𝑥, 𝑧 𝜖 𝑋, ta có:\n𝐾 (𝑥, 𝑧) = 〈Φ(𝑥). Φ(𝑧)〉\nỞ đây 〈 . 〉 là tích vô hướng trong không gian đặc trưng.\nVí dụ:\nXét phép biến đổi dữ liệu từ không gian đầu vào 𝑋 = ℝ2 vào không gian đặc\ntrưng 𝐹 = ℝ3 được cho bởi:\n\nHinh 2.2 Ví dụ hàm hạt nhân\n4\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nKhi ta chọn x = (x1, x2) và z = (z1, z2) ta có:\n2\n\n2\n\n〈x. z〉2 = (∑ 𝑥𝑖 , 𝑧𝑖 ) = (𝑥1 𝑧1 + 𝑥2 𝑧2 )2\n𝑖=1\n\n= 𝑥1 2 𝑧1 2 + 2𝑥1 𝑧1 𝑥2 𝑧2 + 𝑥2 2 𝑧2 2\n= 〈(𝑥12 , √2𝑥1 𝑥2 , 𝑥22 ). (𝑧12 , √2𝑧1 𝑧2 , 𝑧22 )〉\n= 〈Φ(𝑥). Φ(𝑧)〉\nTa thấy khi chọn 𝐾 (𝑥, 𝑧) = 〈Φ(𝑥). Φ(𝑧)〉 = 〈x. z〉2 . Vì vậy ta không cần phải tính\nánh xạ Φ.\nb) Hàm hạt nhân trong máy học tuyến tính:\nMáy học không tuyến tính trên không gian đầu vào được xây dựng qua hai bước:\ntrước tiên sử dụng một ánh xạ không tuyến tính để chuyển đổi dữ liệu vào không gian\nđặc trưng và sau đó sử dụng máy học phân lớp tuyến tính trong không gian đặc trưng.\nMáy học tuyến tính trong không gian đặc trưng tương ứng với hàm:\n𝑁\n\n𝑓(𝑥) = ∑ 𝑤𝑖 . Φ𝑖 (𝑥) + 𝑏\n𝑖=1\n\nChúng ta không cần xác định tường minh trọng số w, khi triển khai tiếp bằng cách\nđưa vào vector 𝑤 = ∑𝑙𝑖=1 𝛼𝑖 . 𝑥𝑖 , ta có:\n𝑙\n\n𝑓(𝑥) = ∑ 𝛼𝑖 . 𝑦𝑖 . 〈Φ(𝑥𝑖 ). Φ(x)〉 + 𝑏\n𝑖=1\n\nHơn nữa, cũng không cần xây dựng tường minh ánh xạ Φ, nhờ sử dụng hàm hạt\nnhân nên:\n𝑙\n\n𝑓(𝑥) = ∑ 𝛼𝑖 𝑦𝑖 𝐾(𝑥𝑖 , 𝑥) + 𝑏\n𝑖=1\n\nĐặt SV là tập các chỉ số i thoả mãn 𝛼𝑖 ≠ 0, ta được:\n\n5\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\n𝑓(𝑥) = ∑ 𝛼𝑖 𝑦𝑖 𝐾(𝑥𝑖 , 𝑥) + 𝑏\n𝑖𝜖𝑆𝑉\n\nCác 𝑥𝑖 ứng với 𝛼𝑖 ≠ 0 được gọi là các vectơ trợ giúp (Support vector), hàm phân\nlớp sẽ được xác định duy nhất qua các vector này.\nc) Một số hàm hạt nhân:\n 𝐾 (𝑥, 𝑧) = 〈𝑥. 𝑧〉\n 𝐾 (𝑥, 𝑧) = (1 + 〈𝑥. 𝑧〉)𝑑 , với d là tham số do người dùng định nghĩa.\n 𝐾 (𝑥, 𝑧) =\n\n1−〈𝑥.𝑧〉𝑑\n1−〈𝑥.𝑧〉\n\n, với 𝑑 là tham số do người dùng định nghĩa\n\n, 𝑣ớ𝑖 − 1 < 〈𝑥. 𝑧〉 < 1\n 𝐾 (𝑥, 𝑧) =\n\n1\n1−〈𝑥.𝑧〉\n\n, 𝑡𝑟𝑜𝑛𝑔 đó − 1 < 〈𝑥. 𝑧〉 < 1\n\n 𝐾 (𝑥, 𝑧) = exp(−𝛾 |𝑥 − 𝑧|2 ), với 𝛾 do người dùng định nghĩa.\n6. Kết chương.\nChương 1 đã giới thiệu về bài toán phân lớp, các khái niệm về phân lớp tuyến tính,\nma trận GRAM. Và quan trọng về không gian đặc trưng và hàm hạt nhân, khả năng\nbiểu thị của hàm hạt nhân trong không gian đặc trưng.\nĐây là các khái niệm cơ bản nhất có liên quan đến bài toán phân lớp và phương\npháp SVM áp dụng cho việc giải quyết bài toán phân lớp.\n\n6\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nCHƯƠNG III.\nPHƯƠNG PHÁP SUPPORT VECTOR MACHINES\nTrong thời đại công nghệ thông tin hiện nay, sự phát triển của công nghệ kéo theo\nsự gia tăng rất lớn của lưu lượng thông tin lưu trữ và trao đổi. Do đó, yêu cầu về tổ chức\nlưu trữ và truy cập thông tin sao cho hiệu quả được đặt lên hàng đầu. Hướng giải quyết\nđược đưa ra là tổ chức, tìm kiếm và phân loại thông tin một cách hiệu quả. Bản thân con\nngười trong đời sống cũng tiếp nhận thế giới xung quanh thông qua sự phân loại và tổ chức\nghi nhớ tri thức một cách hiệu quả. Phân loại thông qua các lớp và mô tả các lớp giúp cho\ntri thức được định dạng và lưu trữ trong đó.\nSupport Vector Machines (SVM) là một phương pháp phân loại xuất phát từ lý\nthuyết học thống kê, dựa trên nguyên tắc tối thiểu rủi ro cấu trúc (Structural Risk\nMinimisation). SVM sẽ cố gắng tìm cách phân loại dữ liệu sao cho có lỗi xảy ra trên tập\nkiểm tra là nhỏ nhất (Test Error Minimisation). Đây là một phương pháp mới trong lĩnh\nvực trí tuệ nhân tạo. Vào thời kỳ đầu khi SVM xuất hiện, khả năng tính toán của máy tính\ncòn rất hạn chế, nên phương pháp SVM không được lưu tâm. Tuy nhiên, từ năm 1995 trở\nlại đây, các thuật toán sử dụng cho SVM phát triển rất nhanh, cùng với khả năng tính toán\nmạnh mẽ của máy tính, đã có được những ứng dụng rất to lớn.\n1. Ý tưởng.\nCho trước một tập huấn luyện, được biểu diễn trong không gian vector, trong đó\nmỗi tài liệu là một điểm, phương pháp này tìm ra một siêu phẳng f quyết định tốt nhất\ncó thể chia các điểm trên không gian này thành hai lớp riêng biệt tương ứng là lớp “+”\nvà lớp “−”. Chất lượng của siêu phẳng này được quyết định bởi khoảng cách (gọi là\nbiên) của điểm dữ liệu gần nhất của mỗi lớp đến mặt phẳng này. Khi đó, khoảng cách\nbiên càng lớn thì mặt phẳng quyết định càng tốt, đồng thời việc phân loại càng chính\nxác.\nÝ tưởng của nó là ánh xạ (tuyến tính hoặc phi tuyến) dữ liệu vào không gian các\nvector đặc trưng (space of feature vectors) mà ở đó một siêu phẳng tối ưu được tìm ra\nđể tách dữ liệu thuộc hai lớp khác nhau.\nMục đích của phương pháp SVM là tìm được khoảng cách biên lớn nhất:\n\n7\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nHình 3.1 Mô tả phương pháp SVM\nĐường tô đậm là siêu phẳng tốt nhất và các điểm được bao bởi hình chữ nhật là\nnhững điểm gần siêu phẳng nhất, chúng được gọi là các vector hỗ trợ (support vector).\nCác đường nét đứt mà các support vector nằm trên đó được gọi là lề (margin).\n2. Cơ sở lý thuyết.\nSVM thực chất là một bài toán tối ưu, mục tiêu của thuật toán này là tìm được một\nkhông gian F và siêu phẳng quyết định f trên F sao cho sai số phân loại là thấp nhất.\nCho tập mẫu 𝐷 = {(𝑥1 , 𝑦1 ), (𝑥1 , 𝑦1 ), … , (𝑥𝑙 , 𝑦𝑙 )} với 𝑥𝑖 ∈ ℝ𝑛 , thuộc vào hai lớp\nnhãn 𝑦𝑖 ∈ {−1, 1} là nhãn lớp tương ứng của các 𝑥𝑖 (-1 biểu thị lớp I, 1 biểu thị lớp II).\nTa có, phương trình siêu phẳng chứa vector ⃗⃗⃗\n𝑥𝑖 trong không gian:\n𝑥𝑖 . 𝑤\n⃗⃗⃗\n⃗⃗ + 𝑏 = 0\nĐặt 𝑓(𝑥\n⃗⃗⃗𝑖 ) = 𝑠𝑖𝑔𝑛(𝑥\n⃗⃗⃗𝑖 . 𝑤\n⃗⃗ + 𝑏) = {\n\n+1, 𝑥\n⃗⃗⃗𝑖 . 𝑤\n⃗⃗ + 𝑏 > 0\n−1, 𝑥\n⃗⃗⃗𝑖 . 𝑤\n⃗⃗ + 𝑏 < 0\n\nNhư vậy, 𝑓 (⃗⃗⃗\n𝑥𝑖 ) biểu diễn sự phân lớp của ⃗⃗⃗\n𝑥𝑖 vào hai lớp như đã nêu.\nTa nói 𝑦𝑖 = +1 nếu 𝑥\n⃗⃗⃗𝑖 thuộc lớp I và 𝑦𝑖 = −1 nếu ⃗⃗⃗\n𝑥𝑖 thuộc lớp II.\n3. Bài toán phân 2 lớp với SVM.\nBài toán đặt ra là: Xác định hàm phân lớp để phân lớp các mẫut trong tương lai,\nnghĩa là với một mẫu dữ liệu mới 𝑥𝑖 thì cần phải xác định 𝑥𝑖 được phân vào lớp +1 hay\nlớp −1.\nTa xét 3 trường hợp, mỗi trường hợp sẽ có 1 bài toán tối ưu, giải được bài toán tối\nưu đó ta sẽ tìm được siêu phẳng cần tìm.\n\n8\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nTrường hợp 1:\nTập D có thể phân chia tuyến tính được mà không có nhiễu (tất cả các điểm được\ngán nhãn +1 thuộc về phía dương của siêu phảng, tất cả các điểm được gán nhãn -1\nthuộc về phía âm của siêu phẳng).\n\nHình 3.2 Tập dữ liệu được phân chia tuyến tính\nTa sẽ tìm siêu phẳng tách với 𝑤 ∈ ℝ𝑛 là vector trọng số, 𝑏 ∈ ℝ𝑛 là hệ số tự do, sao\ncho:\n+1, 𝑦𝑖 = +1\nĐặt 𝑓(𝑥𝑖 ) = 𝑠𝑖𝑔𝑛(𝑥𝑖 . 𝑤 𝑇 + 𝑏) = {\n∀(𝑥𝑖 , 𝑦𝑖 ) ∈ 𝐷\n−1, 𝑦𝑖 = −1\nLúc này ta cần giải bài toán tối ưu:\n1\n‖𝑤‖2\n{\n2\n𝑦𝑖 (𝑥𝑖 . 𝑤 𝑇 + 𝑏) ≥ 1, 𝑖 = 1, … , 𝑙\n𝑀𝑖𝑛(𝐿(𝑤)) =\n\nTrường hợp 2:\nTập dữ liệu D có thể phân chia tuyến tính được nhưng có nhiễu. Trong trường hợp\nnày, hầu hết các điểm đều được phân chia đúng bởi siêu phẳng. Tuy nhiên có 1 số điểm\nbị nhiễu, nghĩa là: Điểm có nhãn dương nhưng lại thuộc phía âm của siêu phẳng, điểm\ncó nhãn âm nhưng lại thuộc phía dương của siêu phẳng.\n\n9\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nHình 3.3 Tập dữ liệu phân chia tuyến tính nhưng có nhiễu\nTrong trường hợp này, ta sử dụng 1 biến mềm 𝜀𝑖 ≥ 0 sao cho: 𝑦𝑖 (𝑥𝑖 . 𝑤 𝑇 + 𝑏) ≥\n1 − 𝜀𝑖 , 𝑖 = 1, … , 𝑙\nBài toán tối ưu trở thành:\n𝑙\n\n1\n𝑀𝑖𝑛(𝐿(𝑤, 𝜀)) = ‖𝑤‖2 + 𝐶 ∑ 𝜀𝑖\n2\n𝑖=1\n\n𝑦𝑖(𝑥𝑖 . 𝑤 𝑇 + 𝑏) ≥ 1 − 𝜀𝑖 , 𝑖 = 1, … , 𝑙\n{\n𝜀𝑖 ≥ 0, 𝑖 = 1, … , 𝑙\nTrong đó C là tham số xác định trước, định nghĩa giá trị ràng buộc, C càng lớn thì\nmức độ vi phạm đối với những lỗi thực nghiệm (là lỗi xảy ra lúc huấn luyện, tính bằng\nthương số của số phần tử lỗi và tổng số phần tử huấn luyện) càng cao.\nTrường hợp 3:\nTập dữ liệu D không thể phân chia tuyến tính được, ta sẽ ánh xạ các vector dữ liệu\nx từ không gian n chiều vào một không gian m chiều (m>n), sao cho trong không gian\nm chiều, D có thể phân chia tuyến tính được.\n\n10\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nHình 3.4 Tập dữ liệu không phân chia tuyến tính.\nGọi 𝜙 là một ánh xạ phi tuyến từ không gian ℝ𝑛 vào không gian ℝ𝑚 .\n𝜙: ℝ𝑛 → ℝ𝑚\nBài toán tối ưu trở thành:\n𝑙\n\n1\n𝑀𝑖𝑛(𝐿(𝑤, 𝜀)) = ‖𝑤‖2 + 𝐶 ∑ 𝜀𝑖\n2\n𝑖=1\n\n𝑇\n\n𝑦𝑖(𝜙(𝑥𝑖 ). 𝑤 + 𝑏) ≥ 1 − 𝜀𝑖 , 𝑖 = 1, … , 𝑙\n{\n𝜀𝑖 ≥ 0, 𝑖 = 1, … , 𝑙\nVí dụ:\nĐể dễ hiểu hơn chúng ta xét ví dụ mô tả hình học sau: Xét trong không gian\n2 chiều (n=2), tập dữ liệu được cho bởi tập các điểm trên mặt phẳng.\n\n11\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nHình 3.5 Ví dụ biểu diễn tập dữ liệu trên không gian 2 chiều\nBây giờ ta tiến hành tìm siêu phẳng phân lớp dựa trên phương pháp SVM (1). Ta sẽ\ntìm 2 siêu phẳng song song (nét đứt trong hình …) sao cho khoảng cách giữa chúng là\nlớn nhất để có thể phân tách lớp này thành 2 phía (Ta gọi là 2 siêu phẳng phân tách).\nSiêu phẳng (1) nằm giữa 2 siêu phẳng trên (nét đậm trong hình).\nHình trên cho ta tập dữ liệu có thể phân tách tuyến tính. Bây giờ ta xét trường hợp\ntập dữ liệu không thể phân tách tuyến tính. Bây giờ ta sẽ xử lý bằng cách ánh xạ tập dữ\nliệu đã cho vào một không gian mới có số chiều lớn hơn không gian cũ (Gọi là không\ngian đặc trưng) mà trong không gian này tập dữ liệu có thể phân tách tuyến tính. Trong\nkhông gian đặc trưng ta sẽ tiếp tục tìm 2 siêu phẳng phân tách như trường hợp ban đầu.\nCác điểm nằm trên 2 siêu phẳng phân tách gọi là các vector hỗ trợ (Support vector).\nCác điểm này quyết định hàm phân tách dữ liệu. Từ đây, chúng ta có thể thấy phương\npháp SVM không phụ thuộc vào các mẫu dữ liệu ban đầu, mà chỉ phụ thuộc vào các\nsuport vector (quyết định 2 siêu phẳng phân tách). Cho dù các điểm khác bị xoá thì\nthuật toán vẫn cho ra các kết quả tương tự. Đây chính là điểm nổi bật của phương pháp\nSVM so với các phương pháp khác do các điểm trong tập dữ liệu đều được dùng để tối\nưu kết quả.\nTrên thực tế người ta sẽ không xác định cụ thể ánh xạ mà dùng hàm hạt nhân như\nđã trình bày ở mục 5 chương 1, nhằm đẩy nhanh tốc độ tính toán đồng thời đảm bảo dữ\nliệu sẽ gần như phân tách tuyến tính. Thông thường người ta thường sử dụng các hàm\nhạt nhân có sẵn.\n12\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\n4. Bài toán phân đa lớp với SVM.\nĐể phân đa lớp thì kỹ thuật SVM sẽ chia không gian dữ liệu thành 2 phần và tiếp\ntục với không gian đã được phân chia. Khi đó hàm quyết định phân dữ liệu vào lớp thứ\ni sẽ là:\n𝑓𝑖 (𝑥) = 𝑤𝑖𝑇 𝑥 + 𝑏𝑖\nNhững phần tử x là support vector nếu thoả điều kiện:\n1, ∈ 𝑖\n𝑓𝑖 (𝑥) = {\n−1, ∉ 𝑖\nGiả sử bài toán phân loại k lớp (𝑘 ≥ 2), ta sẽ tiến hành 𝑘(𝑘 − 1)/2 lần phân lớp\nnhị phân sử dụng phương pháp SVM. Mỗi lớp sẽ tiến hành phân tách với k-1 lớp còn\nlại để xác định k-1 hàm phân tách (chiến lược “một-đối-một” (one-against-one).\nKỹ thuật phân đa lớp bằng phương pháp SVM hiện vẫn đang được tiếp tục nghiên\ncứu và phát triển.\n5. Các bước chính của phương pháp SVM.\n-\n\n-\n\nTiền xử lý dữ liệu: Phương pháp SVM yêu cầu dữ liệu được diễn tả như các\nvector của các số thực. Như vậy nếu đầu vào chưa phải là số thực thì ta cần phải\ntìm cách chuyển chúng về dạng số của SVM. Tránh các số quá lớn, thường nên\nco giãn dữ liệu để chuyển về đoạn [-1,1] hoặc [0,1].\nChọn hàm hạt nhân: Cần chọn hàm hạt nhân phù hợp tương ứng cho từng bài\ntoán toán cụ thể để đạt được độ chính xác cao trong quá trình phân lớp.\nThực hiện việc kiểm tra chéo để xác định các tham số cho ứng dụng.\nSử dụng các tham số cho việc huấn luyện tập mẫu.\nKiểm thử tập dữ liệu Test.\n\n6. So sánh và một số cải tiến.\nMột số phương pháp như mạng neuron, fuzzy logic, mạng fuzzy-neuron, …, cũng\nđược sử dụng thành công để giải quyết bài toán phân lớp. Ưu điểm của các phương\npháp này là không cần xác định mô hình toán đối của đối tượng (Giải quyết tốt với các\nhệ thống lớn và phức tạp).\nSVM có 2 đặc trưng cơ bản:\n- Nó luôn kết hợp với các dữ liệu có ý nghĩa về mặt vật lý, do vậy dễ dàng giải\nthích được một cách tường minh.\n- Cần một tập các mẫu huấn luyện rất nhỏ.\n\n13\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nPhương pháp SVM hiện nay được xem là một công cụ mạnh và tinh vi nhất hiện\nnay cho những bài toán phân lớp phi tuyến. Nó có mộ số biến thể như C – SVC, v –\nSVC. Cải tiến mới nhất hiện nay của phương pháp SVM đã được công bố là thuật toán\nNNSRM (Nearest Neighbor and Structural Risk Minimization) là sự kết hợp giữa 2 kỹ\nthuật SVM và Nearest Neighbor.\n\n14\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nCHƯƠNG IV.\nỨNG DỤNG\n1. Một số ứng dụng của SVM.\nSVM và các biến thể, cái tiến của nó hiện nay có rất nhiều ứng dụng thiết thực vào\ngiài quyết các vấn đề thực tế. Một số ứng dụng nổi bật của nó là:\n-\n\n-\n\n-\n\nChuẩn đoán virus máy tính: Giả sử có một tập các đối tượng cần chuẩn đoán\n{𝑥1 , 𝑥2 , … , 𝑥𝑛 } trong không gian 𝑋. Dựa vào các đặc trưng của các 𝑥𝑖 , ta sẽ áp\ndụng phương pháp SVM để phân 𝑥𝑖 vào lớp I: Có thể nhiễm virus V hoặc lớp II:\nKhông nhiễm virus V.\nLọc thư rác (Mail Classification): Với mỗi văn bản, ta sẽ trích, chọn đặc trưng\nvà vector hoá nó thành một vector n chiều 𝑥 = {𝑡1 , 𝑡2 , … , 𝑡𝑛 } có thể biểu diễn\ndạng dữ liệu SVM được. Sau đó ta tiến hành cho chương trình học thông qua\nmột tập dữ liệu vào dạng (𝑥𝑖 , 𝑦𝑖 ) với 𝑦𝑖 ∈ {1, −1}. Ta sẽ xây dựng một siêu\nphẳng bằng phương pháp SVM trên tập dữ liệu mẫu. Từ đó với mỗi mẫu văn\nbản mới, ta sẽ vector hoá nó và so sánh dấu của nó so với siêu phẳng tìm được\nđể phân loại.\nNgoài ra, phương pháp SVM cho bài toán phân đa lớp còn được ứng dụng cho\nbài toán phân loại ảnh (Image classification) hay phân loại biểu cảm trên khuôn\nmặt (Facial expression classification).\n\nHình 4.1 Phân loại biểu cảm\n\nHình 4.2 Phân loại ảnh\n15\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\n2. Mô phỏng phương pháp SVM qua biểu diễn hình học bằng Matlab.\nCác tập tin trong project Matlab:\nsetup.m : Mô phỏng quá trình phân lớp bằng SVM\ndata1.mat : Tập dữ liệu mẫu 1\ndata2.mat : Tập dữ liệu mẫu 2\ndata3.mat : Tập dữ liệu mẫu 3\nsvmTrain.m : Hàm đào tạo (training) bằng SVM\nsvmPredict.m : Hàm dự đoán (prediction) bằng SVM\nplotData.m : Biểu đồ dữ liệu 2 chiều\nvisualizeBoundaryLinear.m : Biểu đồ đường tuyến tính\nvisualizeBoundary.m : Biểu đồ đường phi tuyến\nlinearKernel.m : Hàm tuyến tính cho SVM\ngaussianKernel.m: Hàm Gaussian cho SVM\ndataset3Params.m: Tham số sử dụng cho tập dữ liệu mẫu 3\nChương mô phỏng các tập dữ liệu mẫu lên đồ thị 2 chiều và tìm siêu phẳng với\ncác dữ liệu cho trước và hàm hạt nhân Gaussian (Gaussian Kernel):\n2\n\n𝐾𝑔𝑎𝑢𝑠𝑠𝑖𝑎𝑛(𝑥 (𝑖),\n\n𝑥(𝑗) )\n\n(𝑖)\n\n(𝑗) 2\n\n∑𝑛𝑘=1(𝑥𝑘 − 𝑥𝑘 )\n‖𝑥 (𝑖) − 𝑥 (𝑗) ‖\n) = 𝑒𝑥𝑝 (−\n)\n= exp (−\n2𝜎 2\n2𝜎 2\n\n3. Xây dựng chương trình lọc thư rác.\nCác tập tin sử dụng trong project Matlab:\nsetup_spam: Quá trình phân loại email spam\nspamTrain.mat – Bộ training spam\nspamTest.mat – Bộ training test\nemailSample.txt - Mẫu email 1\nemailSample2.txt – Mẫu email 2\nspamSample.txt – Mẫu email spam 1\nspamSample2.txt – Mẫu email spam 2\n16\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nvocab.txt – Danh sách từ vựng\ngetVocabList.m – Mở danh sách từ vựng\nporterStemmer.m – Hàm Stemming\nreadFile.m – Đọc file\nprocessEmail.m – Tiền xử lý email\nemailFeatures.m – Rút trích đặt trưng từ email\nCác bước thực hiện trong bài toán phân loại Email:\nBước 1. Tiền xử lý email\nBước 2. Trích xuất đặc trưng email\nBước 3. Huấn luyện tuyến tính bằng SVM\nBước 4. Kiểm tra phân lớp Spam Email\nBước 5. Chọn ra những từ có trong mail có khả năng cao là spam\nBước 6. Đưa email và và kiểm tra\nChi tiết các bước như sau:\nBước 1. Tiền xử lý email :\nDựa theo thư viện từ đã có (vocab.txt), xác định các từ trong email có trong thư viện\ntừ hay không.\n\nHình 4.3 Tiền xử lý email\n\n17\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nBước 2. Trích xuất thông tin email:\nChương trình xuất ra chiều dài của các vectơ đặc trưng và số các đầu vào khác\nkhông.\n0\n⋮\n1\n0\nMột đặc trưng của email có dạng: 𝑥 = ⋮ ∈ ℝ𝑛 .\n1\n0\n⋮\n[0 ]\n\nHình 4.4 Trích xuất thông tin email\nKết quả:\n+ Chiều dài của các vectơ đặc trưng : 1899\n+ Số các đầu vào khác không: 45\nBước 3, 4. Quá trình training dựa theo tập dữ liệu spamTrain.mat và kiểm tra quá\ntrình training bằng dữ liệu kiểm tra spamTest.mat\nQuá trình huấn luyện với SVM và chuyển dữ liệu thành các vectơ thông qua hàm\nsvmTrain.m. Sau đó tính độ chính xác của quá trình huấn luyện thông qua hàm\nsvmPredict.m, cũng như tính độ chính xác của quá trình kiểm tra (Dùng dữ liệu\nspamTest.mat để kiểm tra)\n\n18\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nHình 4.5 Quá trình học dữ liệu\nKết quả:\n+ Độ chính xác của quá trình huấn luyện : 99.85%\n+ Độ chính xác của quá trình kiểm tra: 98.5%\nBước 5. Chọn ra những từ có trong mail có khả năng cao là spam\nChương trình đưa ra các từ (word) có thể là spam trong mail.\n\nHình 4.6 Các từ mẫu có khả năng là spam\n\n19\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nBước 6. Đưa email và và kiểm tra\n\nHình 4.7 Chạy thử chương trình\nChương trình đưa vào 2 email spamSample2.txt và emailSample2.txt và cho kết quả\nđúng như đặc tính của mail đó. (Kết quả: 1 là spam, 0 là không spam).\n\n20\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nCHƯƠNG V.\nTỔNG KẾT\nPhương pháp Support Vector Machines hiện là một trong các phương pháp tinh vi\nnhất cho bài toán phân lớp phi tuyến. Mở rộng và các ứng dụng của nó đang được tiếp\ntục nghiên cứu thêm và phát triển làm tăng tính hiệu quả.\nĐồ án nhằm cung cấp những khái niệm cơ bản, nội dung cũng như các bước thực\nhiện của phương pháp Support Vector Machines đơn giản. Đồng thời cũng cung cấp\nthêm một số thông tin về các mở rộng của phương pháp này. Phần ứng dụng giới thiệu\nmột các ứng dụng của phương pháp có thể áp dụng và xây dựng lại chương trình lọc\nthư rác trên Matlab.\n\n21\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nPHỤ LỤC\n* Chạy thử mô phỏng phương pháp SVM qua biểu diễn hình học bằng Matlab.\nTập dữ liệu mẫu 1:\nChạy chương trình bằng cách gõ dòng lệnh : setup\n\nChương trình sẽ mở dữ liệu mẫu 1 và hiển thị lên sơ đồ\n\nTiếp tục nhấn Enter, quá trình training sẽ diễn ra và cho ra kết quả:\n\n22\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nKết quả đánh giá với hàm hạt nhân Gaussian cho tập dữ liệu mẫu 1:\nsigma = 0.3247\nHàm hạt nhân Gaussian ở giữa x1 = [1; 2; 1], x2 = [0; 4; -1], sigma = 0.5 :\n0.324652\nTập dữ liệu mẫu 2:\n\nQuá trình training diễn ra với hàm hạt nhân Gaussian, kết quả:\n\n23\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nTập dữ liệu mẫu 3:\n\nQuá trình training diễn ra với hàm hạt nhân Gaussian, kết quả:\n\n24\n\n\fGVHD: Th.s Nguyễn Đình Hiển\n\nBáo cáo nhập môn công nghệ tri thức và máy học\n\nTÀI LIỆU THAM KHẢO\n[1] Thái Sơn: Luận văn thạc sỹ khoa học: Kỹ thuật Support Vector Machines và ứng dụng.\nNghành toán tin ứng dụng: Đại học Bách khoa Hà Nội, 2006.\n[2] PGS.TS Vũ Thành Nguyên, Thi Minh Nguyễn: Một số cải tiến của bài toán phân lớp\nvăn bản sử dụng thuật toán SVM và áp dụng trong phân tích tiếng Việt. Đại học Công nghệ\nthông tin – ĐHQG HCM, 2011.\n[3] SVM spam classification: https:\\\\githug.com\\.\n\n25\n\n\fThis action might not be possible to undo. Are you sure you want to continue?Use one of your book credits to continue reading from where you left off, or restart the preview.",
          "relevence": "yes"
        },
        {
          "url": "https://techtalk.vn/ung-dung-support-vector-machine-trong-bai-toan-phan-loai-hoa.html",
          "title": "Ứng dụng Support Vector Machine trong bài toán phân loại hoa | Tech Talk",
          "content": "Xin chào các bạn, mình lại trở lại rồi đây. Tiếp tục với loạt bài viết về Machine Learning trong bài trước mình đã giới thiệu với các bạn một cách tổng quan nhất về Support Vector Machine – một phương pháp vô cùng hiệu quả trong bài toán phân lớp dữ liệu. Tuy nhiên nếu đọc lý thuyết nhiều quá hẳn sẽ rất khó khăn cho những bạn mới bắt đầu (mình cũng mới bắt đầu tìm hiểu về Machine Learning mà). Chính vì thế, ngày hôm nay chúng ta sẽ đến với một bài toán khá cụ thể đó là sử dụng SVM trong bài toán phân loại hoa. Hiểu nôm na đó là chúng ta sẽ xây dựng một mô hình (model) sao cho khi input vào là một bông hoa và ouput trả ra kết quả nó là loài hoa gì. Thú vị phải không các bạn . OK chúng ta bắt đầu thôi nào.Cái gì cũng phải có cơ sở một chút phải không nào. Ví dụ trong bài toán của ta đang quan tâm đến đối tượng là hoa chẳng hạn, vậy có một câu hỏi đặt ra là “Làm thế nào để phân biệt các loại hoa với nhau ???”Con người chúng ta qua quá trình lớn lên đã tự tích lũy cho mình những đặc điểm rất riêng của mỗi loài hoa. Ví dụ hoa loa kèn có hình giống cái kèn, hoa huệ màu trắng có mùi thơm đặc trưng, hoa hồng màu đỏ và có gai… Vậy tức là trong đầu chúng ta đã có một tập dữ liệu về các loài hoa và nó chính là căn cứ để chúng ta nhận dạng một loài hoa khi chúng ta gặp phải. Nguyên tắc học của máy tính cũng như vậy thôi. Chúng ta cần phải cung cấp cho nó một tập dữ liệu và huấn luyện cho nó băng một cách nào đó để nó có thể căn cứ vào đó mà đoán được một bông hoa mới thuộc vào loài hoa nào. OK, quan trọng nhất vẫn là phải có một tập dữ liệu huấn luyện phải không nào? Trong bài viết này chúng ta sẽ sử dụng một tập dữ liệu về hoa vô cùng nổi tiếng đó là tập dữ liệu Iris. Chúng ta cùng nhau tìm hiểu sâu hơn về tập dữ liệu này nhé.Tập dữ liệu này còn có tên gọi khác là Fisher’s Iris vì nó do Ronald Fisher thu thập và tổng hợp. Tập dữ liệu này gồm 50 mẫu về 3 loài hoa khác nhau của họ Iris là (Iris setosa, Iris virginica và Iris versicolor). Cho cái ảnh cho các bạn dễ hình dung Với mỗi một mẫu hoa này hắn thu thập bốn thuộc tính là chiều dài và chiều rộng của đài hoa và cánh hoa với đơn vị centimet. Để có thể sử dụng tập dữ liệu này chúng ta sẽ sử dụng thư viện datasets trong sklearn.Sau khi đã có dữ liệu rồi, chúng ta hoàn toàn có thể chạy thuật toán SVM ngay để tiến hành phân loại. Tuy nhiên, mình nghĩ nên giúp các bạn có một cái nhìn trực quan hơn về tập dữ liệu này. Mà để hiển thị trực quan nhất thì không gì bằng biểu đồ với thư viện matplotlib thần thánh. Chúng ta cùng thử biểu diễn trên đồ thị xem tập dữ liệu mà chúng ta nhét vào máy tính nó là cái gì nhé.Chúng ta tưởng tượng tập dữ liệu của ta là một tập hợp của 150 điểm dữ liệu tương ứng với 150 bông hoa.. Chúng ta sẽ lấy chúng ta từ datasets bằng hàm sau:Lúc này đã có một tập hợp các điểm dữ liệu. Mỗi điểm dữ liệu bao gồm 4 thuộc tính như đã nói ở trên. Tuy nhiên để biểu diễn trong đồ thị hai chiều chúng ta cần phải giảm bớt số thuộc tính biểu diễn. Ở đây giả sử chọn hai thuộc tính đầu tiên là độ rộng và chiều cao của đài hoa. Chúng ta có hàm xử lý vẽ đồ thị 2D như sau:Và đây là thành quảChúng ta có thể thấy được các điểm dữ liệu với hai thuộc tính trên đồ thị hai chiều. Các điểm này được phân biệt bằng mắt thường với 3 màu khác nhau. Tuy nhiên muốn máy tính phân biệt được như chúng ta lại là một câu chuyện hoàn toàn khác và bạn sẵn sàng cùng tôi đi đến cuối cùng của câu chuyện này chứ. OK chúng ta tiếp tục thôiPhân lớp sử dụng SVM với các Kernel khác nhauVới tập dữ liệu Iris chúng ta cần phân loại các bông hoa thành 3 lớp dữ liệu. Sử dụng SVM với các phương pháp khác nhau sẽ cho hiệu quả phân lớp khác nhau. Cũng tương tự như trên, chúng ta chỉ xem xét đến 2 thuộc tính đầu tiên của tập dữ liệu, tức là phân lớp trong không gian 2 chiều. Chúng ta sử dụng các Kernel khác nhau bao gồm:Đầu tiên chúng ta cần huấn luyện dữ liệuTiếp theo là việc đẩy các mô hình thu được vào đồ thịBây giờ sau khi vẽ các đồ thị trên là chúng ta có thể so sánh hiệu quả phân lớp của các Kernel khác nhau rồi đấy. Kết quả như sau:Bằng việc phân lớp chúng ta có thể chia mặt phẳng tọa độ thành các phần khác nhau. Khi có một điểm dữ liệu mới chúng ta có thể dựa vào tọa độ của chúng để phán đoán xem nó thuộc lớp nào.Chúng ta hoàn toàn có thể tự custom một Kernel riêng của mình. Điều đó làm cho SVM trở nên linh hoạt hơn.Mình rất hi vọng được trình bày với các bạn sâu hơn về Custom Kernel của mình để việc phân lớp cụ thể là phân lớp ảnh được tốt hơn. Nhưng trong phạm vi bài viết này mình sẽ dừng lại ở các Kernel tuyến tính và phi tuyến thôi đã. Kẻo nói nhiều quá chúng ta khó năm bắt được hết vấn đề.Source code trong bài viếtCác bạn có thể tham khảo mã nguồn mình viết trong bài này tại đâyTham khảoSVM Kernel Scikit LearnSVM Scikit LearnIris Flower DatasetNgười viết Pham Van ToanTechtalk via Viblo",
          "relevence": "yes"
        },
        {
          "url": "http://viet.jnlp.org/kien-thuc-co-ban-ve-xu-ly-ngon-ngu-tu-nhien/machine-learning-trong-nlp/thuat-toan-may-ho-tro-vector",
          "title": "Thuật toán máy hỗ trợ vector - Xử lý ngôn ngữ tự nhiên (Trường đại học khoa học kỹ thuật Nagaoka)",
          "content": "Sign in|Recent Site Activity|Report Abuse|Print Page|Powered By Google Sites",
          "relevence": "yes"
        },
        {
          "url": "http://eitguide.net/category/machine-learning/",
          "title": "Machine Learning",
          "content": "Kiến thức về các thuật toán máy học sử dụng để phân lớp dữ liệu (classfication) và gom cụm dữ liệu (clustering). Thuật toán học có giám sát và các thuật toán học không giám sát như một số các thuật toán phổ biến như mạng nơron Neraul Network, KNN K-Nearest Neighbor, SVM Support Vector Machine, K-Mean...Bài toán phân lớp Bài toán phân lớp (classification) và bài toán gom cụm (cluster) là hai bài toán lớn trong lĩnh vực Machine Learnig (ML). Bài toán phân lớp là quá trình phân lớp một đối tượng dữ liệu vào một hay nhiều lớp đã cho trước nhờ một mô hình phân lớp (model). Mô hình này được xây dựng dựa trên một tập dữ liệu được xây dựng trước đó có gán nhãn (hay còn gọi là tập…  Đây là một thư viện hổ trợ các thuật toán Machine Learning như Decison Tree, KNN, Navie Bayes, SVM(Support Vector Machine), ANN (Artificial Neural Network), Linear Regression, K-Mean… Scikit-learn là một thư viện viết bằng python và rất dễ sử dụng. Chúng ta tương tác với thư viện này hoàn toàn quay dòng lệnhGiới thiệu Thuật toán SVM(Support Vector Machine) là một thuật toán máy học dùng cho bài toán phân lớp dữ liệu. Ý tưởng của thuật toán là tìm một siêu phẳng tối ưu (optimal hyperplane) sao cho margin tới các điểm support vector có khoảng cách lớn nhất. Hiện nay thuật toán này được sử dụng khá nhiều trong lĩnh vực Machine Learning cùng với các thuật toán như cây quyết định (Tree Decision), K-NN (K-Nearest Neighbor), mạng noron (Nerual Network)….",
          "relevence": "no"
        },
        {
          "url": "http://text.123doc.org/document/2343427-giai-thuat-svm-support-vector-machines.htm",
          "title": "GIẢI THUẬT SVM (Support vector machines) - Tài liệu text",
          "content": "Tài liệu liên quanTải bản đầy đủ ngay",
          "relevence": "yes"
        },
        {
          "url": "http://scv.udn.vn/nguyenduchien/BBao/8630",
          "title": "\r\n\tScience curiculum vitae personally - University of Da Nang\r\n",
          "content": "",
          "relevence": "no"
        },
        {
          "url": "https://daynhauhoc.com/t/thuat-toan-phan-loai-van-ban-svm-su-dung-tf-idf/45964",
          "title": "Thuật toán phân loại văn bản SVM sử dụng TF IDF - programming - Dạy Nhau Học",
          "content": "Chào mọi người. Em đang sử dụng thuật toán SVM để phân loại văn bản. Em sử dụng TF IDF để tính trọng số của một từ trong một câu. Khi chuyển tập traning sang dạng vector thì không có vấn đề gì. Nhưng khi chuyển tập test sang thì em đang thắc mắc chỗ này. IDF của một từ trong tập test sẽ được tính như thế nào:     - Sử dụng luôn giá trị IDF của từ đấy trong tập training (IDF này chỉ tính dựa trên tập training).    - Hay tính lại giá trị IDF của từ đấy (kết hợp cả tập training, testing để tính).Và 1 ý nữa là có những từ chưa xuất hiện trong tập training thì phải làm sao ạ.Em cảm ơnPowered by Discourse, best viewed with JavaScript enabled",
          "relevence": "no"
        },
        {
          "url": "http://openlab.forumvi.com/t16-topic",
          "title": "Support Vector Machine",
          "content": "",
          "relevence": "yes"
        },
        {
          "url": "http://www.kienthuc2plus.com/2017/03/data-mining-thuat-toan-support-vector.html",
          "title": "[Data Mining] Thuật toán Support Vector Machine (SVM) - Kiến Thức ++ | Phát triển bản thân",
          "content": "\nXin chào các bạn !!! Mình tên là Thành hiện tại mình đang học CNTT. Mình lập ra blog này để chia sẻ, tổng hợp lại những bài học, kinh nghiệm, trải nghiệm mình có trong quá trình nâng cao trình đô bản thân để giúp các bạn một phần nào đó để có thể đi nhanh hơn mình trong quá trình mày mò, nghiên cứu.      \nhttps://drive.google.com/drive/folders/0B4I0iSeUWMotY09lWXZMODVWWkk\n\n\n\n\n\n\n\n\n\n      BLOG_CMT_createIframe('https://www.blogger.com/rpc_relay.html', '03622159840755489255');\n    \n\n\n Click Liên Kết Admin Hiếu\n Đăng Khải BlogChính Trực Blog Blog Thủ Thuật Hưng Star - IT",
          "relevence": "yes"
        },
        {
          "url": "http://blog.duyet.net/2017/08/phan-lop-van-ban.html#.Wb-_l7IjGM8",
          "title": "Phân lớp văn bản",
          "content": "\nĐăng ký nhận bài viết mới qua email",
          "relevence": "yes"
        },
        {
          "url": "https://phamhuudanh.com/2017/02/18/mot-so-thuat-toan-trong-machine-learning/",
          "title": "Một số thuật toán trong Machine Learning – Phạm Hữu Danh",
          "content": "Phạm Hữu DanhLet's share the passion for technology!Đây là bài post mình dùng đe ôn tập và chia sẻ kiến thức đã học. Mình rất vui khi nhận được chia sẻ, góp ý nội dung từ các bạn. – Hữu DanhTheo mình, điểm khác nhau giữa người và máy tính chính là người có thể học tập kiến thức mới từ những thứ trong cuộc sống.\nTrước khi biết đến Machine Learning, mình chỉ nghĩ lập trình là xây dựng thuật toán để giải quyết một vấn đề cụ thể.\nCòn đối với Machine Learning, thì chúng ta lập trình để máy tính học và và giải quyết các yêu cầu từ “kiến thức đã có”. Vốn kiến thức này chính là dữ liệu (dữ liệu dùng để dạy cho máy tính học trước khi suy ra kết quả gọi là training data).Trước khi chúng ta học giải Phương trình, Vi phân, Toán cao cấp, … thì chúng ta phải học những phép tính cơ bản trước như: cộng, trừ, nhân, chia, lấy căn, …\nĐối với Machine Learning cũng vậy, khi mình tìm hiểu về nó qua các khóa học, mình được ho những thuật toán cơ bản nhất, để từ đó kết hợp và giải quyết các vấn đề phức tạp hơn.Tổng hợp lại thì ta có:\n– Decision Trees\n– Naive Bayes\n– Gradient Descent\n– Linear Regression\n– Logistic Regression\n– Neural Networks\n– ClusteringTất nhiên còn rất nhiều thuật toán khác, nếu bạn thấy thuật toán nào hay muốn chia sẻ, hãy comment phía dưới nhé!Decision Trees là một thuật toán mà trong đó chúng ta xét từng thuộc tính của đối tượng so với điều kiện mà chúng ta đặt ra trước để phân loại đối tượng vào các nhóm. Xem hình phía dưới, bạn sẽ hiểu tại sao nó được gọi là Decision Trees (cây quyết định).ví dụ kinh điển: tính khả năng sống sót của một người trên tàu TitanicTheo mình học được thì thuật toán này có các điểm mạnh sau:Hạn chế thì có:Naive Bayes là thuật toán dùng để phân nhóm dựa vào tính xác suất có điều kiện. Nếu ai đã học xác xuất thống kế thì có lẽ đã nghe đến công thức này.Ví dụ kinh điển: detecting spam emailsDựa vào training data đã phân loại, ta sẽ xác định các thuộc tính của chúng, ví dụ ở đây là từ “cheap”, ta sẽ tính tỉ lệ email là spam khi có tư “cheap”. Sau đó ta xác định với các thuộc tính khác.\nKhi có email mới sẽ xác định tỉ lệ email mới có phải spam không dựa theo việc xác định theo thuộc tính, thường là lấy tích các thuộc tính. Xem thêm về công thức Naive Bayes để hiểu cách tính.Ưu điểm mình nhận thấy là: tạo ra các bộ lọc khác nhau cho các trường hợp khác nhau, ví dụ email bị cho là spam khác nhau với mỗi người.Nhươc điểm là:Đây là một dạng thuật toán tìm thành phần ưu tiên. Thuật toán này tương tự cách chúng ta đi xuống đồi, chúng sẽ tìm nơi có độ cao thấp nhất và đi về hướng đó. Thuật toán này sẽ tìm biến để cho biểu thức xác định đạt giá trị nhỏ nhất.Thuật toán này chủ yếu được ứng dụng để xây dựng các thuật toán khác.Với mình thì các thuật toán dạng Regression giố với các bài phương trình hàm: với biểu thức y = a x + b mà ta có rất nhiều cặp số (x, y) và nhiệm vụ là tìm ngược lại a và bLinear Regression đúng như tên gọi của nó, ta cố gắng vẽ một đường thẳng sao cho tổng khoảng cách từ các điểm đến nó là ngắn nhất.\nKhi có một dữ liệu mới, ta sẽ dựa vào đường thẳng này để dự đoán.\nVí dụ, giá nhà sẽ tăng khi diện tích căn nhà tăng và ta sẽ ước tính sự liên quan này qua một đường thẳng trên biểu đồ.Tất nhiên, trong thực tế thì sẽ ít khi xuất hiện đường thẳng. Lúc đó ta cần đến Logistic Regression.Lúc này chúng ta sẽ cố gắng dự đoán bằng cách phân chia hai miền giá trị bằng một hàm đặc biệt.Regression đều ứng dụng Gradient Descent để tìm được hàm chính xác.Vấn đề đặt ra là khi mà dữ liệu mẫu phân nhóm, không bao phủ thì độ chính xác của Regression không cao nữa. Ví dụ như hình sau:Bạn thấy đấy, các đường kẻ chia cắt 2 nhóm đều đúng, vậy thì đường nào là đúng nhất?\nLúc này thì ta áp dụng Support Vector Machines (SVM). Với SVM thì ta sẽ cố gắng mở rộng mỗi miền giá trị nhất có thể.\nCó thể hiểu thuật toán này qua hình trên, chúng ta cần tìm đường thẳng chia ở giữa làm sao cho khoảng cách ngắn nhất của các điểm đến đường thẳng là dài nhất.I will update later\n\t\t\tA young and passionate student who loves learning new technologies.\t\t\t\n\t\t\t\tView all posts by Phạm Hữu Danh\t\t\t\n\t\tMachine LearningFill in your details below or click an icon to log in: You are commenting using your WordPress.com account. ( Log Out / Change ) You are commenting using your Twitter account. ( Log Out / Change ) You are commenting using your Facebook account. ( Log Out / Change ) You are commenting using your Google+ account. ( Log Out / Change )Connecting to %s Notify me of new comments via email. \n\n\n\t\t\t\t\n\t\t\t\tCreate a website or blog at WordPress.com\n\t\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\tUp ↑\n\t\t\t",
          "relevence": "no"
        },
        {
          "url": "http://vnlp.net/2010/04/",
          "title": "VNLP » 2010 » April",
          "content": "Luu Cong To – Thuật toán cây quyết địnhNguyen Tien Tung – Mở rộng thực thểDinh Thi Huong – Thuật toán SVMPham Huyen Trang – Thuật toán K-MeanDanh sách sinh viên K52 seminar.STTHọ và tênLớpPhụ tráchNội dung tìm hiểuVà phản biệnCác tài liệu chuẩn bị7Phan Nguyên Cương52CAVuTMQAPhản biện chính:Trần Nam   KhánhQuestion Answering System8Lê Đức Trọng52CAQuỳnhLHCực đại kỳ vọng EMPhản biện chính:Trần Nam   Khánh9Lưu Công Tố52CA*ThànhVTCây quyết   định C4.5Phản biện chính:Nguyễn   Thanh Sơn10Phạm Huyền   Trang52CA*KhanhTNK-meansPhản biện chính:Nguyễn Thị   HoànThuật   toán K-means11Đinh Thị   Hương52CA*ThanhVTSVMPhản biện chính:Lê Hoàng   Quỳnh12Lê Hà Tú52CA*ThanhVTBayes   classifierPhản biện chính:Nguyễn Tiến   ThanhThuật toán Bayes Naive13Trần Trung   Hiếu52CAQuỳnhLHANNPhản biện chính:Vũ Tiến   ThànhThuật toán mang neuron nhân tạo (ANN)14Bùi Nhật   Nam52CAVũTMMatchingPhản biện chính:Uông Huy   LongThuật toán Aho-corasick15Nguyễn Thị   Kim Chi52CC*TrangNTPageRankPhản biện chính:Lê T. Kim   DungThuật   toán PageRank16Nguyễn Thị   Thanh Na52CC*TrangNTApriori-luật   kết hợpPhản biện chính:Nguyễn Đạo   TháiThuật   toán Apriori tìm luật kết hợp 17Bùi Văn   Thanh52CCThuật toán phân lớp k-láng giềng gần nhất (kNN)Danh sach sinh vien K52",
          "relevence": "no"
        },
        {
          "url": "http://bis.net.vn/forums/t/619.aspx",
          "title": "\r\n\tCác phương pháp học máy (Machine Learning) - BIS\r\n",
          "content": "Trong lĩnh vực Học máy có các phương\r\npháp học sau:1) Học có giám sát (supervised\r\nlearning)2) Học không có giám sát\r\n(unsupervised learning)3) Học bán giám sát\r\n(semi-supervised learning)4) Học tăng cường (reinforcement\r\nlearning)Không phải ngẫu nhiên người ta lại\r\nphân chia và đặt tên cho các phương pháp học như vậy.Sau một thời gian tìm đọc tài liệu\r\nvề học máy, cố gắng hiểu được bản chất và phân biệt được sự giống và khác nhau\r\ncủa các phương pháp học này, tôi đưa ra một bài toán ví dụ sau:Cho một công-ten-nơ chứa đầy hoa\r\nquả. Nhiệm vụ phải chia số quả này thành các nhóm đúng với loại quả đó. (xoài\r\nra xoài, cam ra cam, táo ra táo,…)1) Với Học có giám sát\r\n(supervised learning)Là kỹ thuật học sử dụng cho các bài\r\ntoán phân lớp (Classification)Để thực hiện được bài toán trên,\r\ntrước tiên cần phải có 2 điều kiện:Điều kiện 1: phải biết trước số\r\nnhãn lớp cần phân loại, tức là phải biết trong công-ten-nơ đó có nhưng loại quả\r\ngì. Giả sử trong công-ten-nơ đó có 5 loại quả là xoài, cam, táo, ổi, đào (đây\r\nchính là 5 loại nhãn lớp).Điều kiện 2: phải có tập đặc trưng\r\ncủa mỗi loại quả, ví dụ các đặc trưng là: hình dáng, màu sắc, trọng lượng, độ cứng\r\nmềm, v.v… Tập đặc trưng này có được thông qua học một tập dữ liệu huấn luyện\r\n(chính là các công-ten-nơ của các chuyến hàng trước đó)Khi thực hiện phân loại các loại\r\nquả trong công-ten-nơ đang xét, dựa vào đặc trưng của các loại quả (điều kiện\r\n2), quả sẽ được đưa vào 1 trong 5 nhóm đã biết (điều kiện 1).2) Học không có giám sát\r\n(unsupervised learning)Là kỹ thuật học sử dụng cho các bài\r\ntoán phân cụm, gom cụm (Clustering) Để thực hiện được bài toán trên,\r\ncần phải có tập đặc trưng của mỗi loại quả. Tập đặc trưng này có được cũng thông\r\nqua học một tập dữ liệu huấn luyện (như điều kiện 2 của Học có giám sát). Điểm khác của Học không giám sát\r\nso với Học có giám sát là: trước khi phân cụm, không biết trong công-ten-nơ đang\r\nxét có bao nhiêu loại quả và đó là những loại quả gì.Khi thực hiện phân cụm, dựa vào đặc\r\ntrưng của mỗi loại quả, sẽ đưa quả đang xét vào nhóm (cụm) có đặc trưng tương đồng\r\nvới nó nhất. Khi đó, 2 quả bất kỳ ở cùng cụm sẽ tương đồng nhau, 2 quả khác cụm\r\nsẽ khác biệt nhau.* Nhận xétGiống nhau: Cả\r\nhai phương pháp học 1) và 2) đều cần phải có một tập huấn luyện (training data\r\nset) để hệ thống có thể “học” và rút ra được các đặc trưng dùng cho việc gán nhãn.Khác nhau: Phương\r\npháp 1) cần biết trước đầu ra chính là số nhãn lớp. Phương pháp 2) không cần biết\r\ntrước đầu ra  (là số cụm và nhãn) để phân\r\ncụm. Trên đây là cách hiểu của tôi về\r\nphương pháp Học có giám sát và Học không giám sát. Nhân đây tôi xin đưa một thắc\r\nmắc để các thành viên và những ai quan tâm đưa ra ý kiến để cùng thảo luận.Thắc mắc: Học bán giám sát là gì?\r\nHọc tăng cường là gì? Phân biệt sự khác nhau và giống nhau giữa các phương pháp\r\nhọc bán giám sát và học tăng cường. Cho ví dụ minh hoạ?Đúng k means là thuật toán học không giám sát vì nó được dùng để phân cụm cho các dữ liệu chưa biết nhãn. Để dễ phân biệt bạn chú ý là việc biết (cho) trước k cụm không liên quan gì đến dữ liệu đầu vào (hay dữ liệu huấn luyện - training data set). Với học có giám sát dữ liệu đầu vào đều đã được gán nhãn trước, phương pháp học này dùng giải quyết bài toán phân lớp (classification). Với học không giám sát, không biết trước nhãn của dữ liệu (hay dữ liệu đầu vào đều chưa được gán nhãn), phương pháp này dùng để giải quyết bài toán phân cụm (clustering). Còn với học nửa giám sát, sử dụng tập dữ liệu đầu vào gồm cả dữ liệu đã gán nhãn và dữ liệu chưa gán nhãn, trong đó dữ liệu gán nhãn thường (rất) ít và dữ liệu chưa gán nhãn thường (rất) nhiều. Hai kỹ thuật tiêu biểu cho học nửa giám sát là Self-training và Co-training, bạn có thể tìm kiếm trên internet.Chúc vui vẻ!Có nhiều bạn gửi mail hỏi mình về Machine Learning, các bạn thông cảm vì đi làm bận nên không trả lời riêng các bạn, và cũng để mọi người có thể cùng trao đổi mình sẽ post lên đây để mọi người chia sẻ những kiến thức về ML. Trước tiên mình nêu một số định nghĩa rất quan trọng trong ML. Học máy (hay máy học – Machine learning) là một thành phần quan trọng của trí tuệ nhân tạo nhằm nghiên cứu và phát triển các phương pháp, kỹ thuật giúp cho các hệ thống hay máy tính có khả năng học (Tiến Phong).Thuật ngữ over-fitting ra đời dùng để chỉ một hiện tượng xuất hiện trong quá trình khai phá dữ liệu sử dụng phương pháp học máy. Hiện tượng này gây khó khăn đáng kể cho việc thiết kế, xây dựng hệ thống ngay từ bước chuẩn bị dữ liệu cho đến bước kiểm thử hệ thống. Khi hiện tượng over-fitting xảy ra sẽ làm cho hiệu quả của hệ thống giảm xuống và kết quả thu được từ hệ thống không còn độ tin cậy cao. Có thể định nghĩa hiện tượng over-fitting như sau:Một hàm mục tiêu hay một giả thiết học được h, sẽ được gọi là over-fitting (quá vừa dữ liệu) với một tập dữ liệu huấn luyện nếu tồn tại một hàm mục tiêu khác là h’ sao cho:h’ kém phù hợp hơn, đạt độ chính xác kém hơn so với h trên tập dữ liệu huấn luyện, nhưng h’ lại đạt độ chính xác cao hơn h đối với toàn bộ tập dữ liệu (bao gồm cả tập dữ liệu liệu huấn luyện và tập dữ liệu kiểm tra)Giả sử gọi D là tập toàn bộ các dữ liệu có thể có, Training_D là tập các dữ liệu huấn luyệnGiả sử Err_D(h) là mức lỗi mà giả thiết h sinh ra đối với tập D, và Err_Training_D(h) là mức lỗi mà giả thiết h sinh ra đối với tập Training_D.Nếu tồn tại một giả thiết khác là h’ sao cho:Err_Training_D(h) < Err_Training_D(h’) vàErr_D(h) > Err_D(h’)Vấn đề over-fitting thường do các nguyên nhân:- Lỗi (nhiễu) trong tập huấn luyện phát sinh trong quá trình thu thập, xây dựng tập dữ liệu.- Số lượng dữ liệu của tập huấn luyện quá nhỏ, không đại diện cho toàn bộ tập dữ liệu có thể có hay toàn bộ phân bố dữ liệu của bài toán.Có thể mô hình hoá một vấn đề máy học như sau:Cho một dãy l quan sát: (x1, y1), (x2, y2), … , (xl, yl). Trong đó:- x1, x2, …, xl là các mẫu, xi Rn. Các mẫu xi được phát sinh ngẫu nhiên theo một hàm phân bố xác suất F(x) nào đó mà ta không biết.- yi là các kết quả học tương ứng với mẫu xi, yi R. Thường thì y là kết quả của một hàm f(x) nào đó – đơn trị. Tuy nhiên trong trường hợp tổng quát thì y không đơn trị. Do đó y được xác định theo một hàm phân bố điều kiện F(y|x) mà ta cũng không biết.- Bây giờ cho một mẫu x*, vấn đề của máy học là xác định một hàm f0(x) mà có thể ước lượng tốt nhất giá trị y* tương ứng. Như vậy theo lý thuyết tương quan trong thống kê thì f0(x) tốt nhất theo lý thuyết phải là kỳ vọng của y theo x theo phân bố F(y|x).f0(x) còn được gọi là phương trình hồi quy.Với x tuân theo phân bố F(x), y tuân theo phân bố có điều kiện F(y|x) thì hàm phân bố của cặp (x, y) là F(x, y) = F(x)F(y|x). Có thể thấy xác suất để có dãy (x1, y1), (x2, y2), … , (xl, yl) là tích F(x1, y1)F(x2, y2)…F(xl, yl).Tuy nhiên, ở đây ta không biết F(x) lẫn F(y|x) nên không thể xác định chính xác kỳ vọng này. Tất cả dữ liệu mà ta biết chỉ là dãy hữu hạn các mẫu quan sát (x1, y1), (x2, y2), … , (xl, yl). Nhiệm vụ của máy học là xác định chính xác nhất có thể được hàm f0(x) dựa trên các dữ liệu hữu hạn này.Trong trường hợp khi y  R, tức đây là vấn đề hồi quy (regression). Trong trường hợp bài toán phân lớp (classification) thì y {-1, 1} là trường hợp nhận dạng hai lớp, nếu yi = -1 thì xi thuộc lớp thứ nhất (không được quan tâm), còn yi = 1 thì xi thuộc lớp thứ 2 (lớp được quan tâm)Trong lĩnh vực học máy có nhiều phương pháp học khác nhau, trong phần này đề cập đến 3 phương pháp học được sử dụng phổ biến nhất, gồm có: học không giám sát, học bán/ nửa giám sát và học có giám sát.* Khái niệm học không giám sátHọc không giám sát là học với tập dữ liệu huấn luyện ban đầu hoàn toàn chưa được gán nhãn.Học không giám sát là phương pháp học sử dụng cho lớp bài toán gom cụm, phân cụm (Clustering).- Để thực hiện phân cụm, trước tiên cần một tập dữ liệu huấn luyện (training dataset) – là một tập các ví dụ học (training examples/instances). Trong đó, mỗi ví dụ học chỉ chứa thông tin biểu diễn (ví dụ: một vector các giá trị thuộc tính), mà không có bất kỳ thông tin gì về nhãn lớp hoặc giá trị đầu ra mong muốn (expected output).- Áp dụng một thuật toán học không có giám sát (ví dụ k-Means) để học hàm/mô hình mục tiêu (trong trường hợp này là hàm phân cụm ứng với thuật toán được chọn).- Sử dụng một phương pháp thử nghiệm (có thể kết hợp với một tập dữ liệu có gán nhãn) để đánh giá hiệu năng/chất lượng của hàm mục tiêu học được.Có rất nhiều thuật toán học không giám sát được ra đời và phát triển nhằm giải quyết bài toán phân cụm phục vụ khai thác hiệu quả nguồn dữ liệu chưa gán nhãn nhiều và rất đa dạng. Việc lựa chọn sử dụng thuật toán nào tuỳ thuộc vào dữ liệu và mục đích của từng bài toán. Trong đó các thuật toán thường được sử dụng như: k-means, HAC (Hierarchical Agglomerative Clustering), SOM (Self-Organizing Map), DBSCAN, FCM,... (chi tiết các thuật toán này có thể tìm kiếm trên Internet)Trong thực tế, để có được một tập dữ liệu có chất lượng và đã được gán nhãn của một lĩnh vực, thường được thực hiện thủ công bằng tay bởi người có nhiều kinh nghiệm về lĩnh vực đó. Vì vậy, dữ liệu đã được gán nhãn thường ít và đắt. Trong khi đó, dữ liệu chưa được gán nhãn lại rất nhiều và phong phú. Phương pháp học bán giám sát (hay học nửa giám sát) được đặt ra để tận dụng cả hai nguồn dữ liệu này.Học bán giám sát là học với tập dữ liệu huấn luyện gồm cả dữ liệu đã được gán nhãn và dữ liệu chưa được gán nhãn.Tuỳ vào từng mục đích cụ thể, học bán giám sát có thể được áp dụng cho bài toán phân lớp hoặc phân cụm.- Nội dung chính của học bán giám sát là hệ thống sử dụng một tập học (training set) gồm 2 phần: các ví dụ học có nhãn, thường với số lượng (rất) ít, và các ví dụ học không có nhãn, thường với số lượng (rất) nhiều. Thực tế cho thấy khi sử dụng kết hợp dữ liệu không có nhãn với một lượng nhất định dữ liệu có nhãn có thể tăng độ chính xác đáng kể.- Một thuật toán học bán giám sát được sử dụng (ví dụ Self-training) sẽ học các ví dụ có nhãn, sau đó tiến hành gán nhãn cho một số (có lựa chọn) các ví dụ không có nhãn - một cách hợp lý, có đánh giá chất lượng công việc hay độ chính xác. Tiếp theo, chọn các ví dụ vừa được gán nhãn có độ tin cậy cao (vượt trên một ngưỡng chọn trước) đưa vào kết hợp với tập dữ liệu có nhãn, tạo thành một tập dữ liệu huấn luyện mới.- Áp dụng một phương pháp kiểm thử (có thể kết hợp với một tập dữ liệu đã biết trước nhãn) để đánh giá hiệu năng/độ chính xác của mô hình.Một số thuật toán thường được sử dụng gồm có: thuật toán Cực đại kỳ vọng (EM - Expectation Maximization), SVM truyền dẫn (TSVM - Transductive Support Vector Machine), Self-training, Co-training và các phương pháp dựa trên đồ thị (graph-based).Việc lựa chọn thuật toán nào dựa trên một số định hướng: nếu các lớp dữ liệu có tính phân cụm cao thì nên dùng EM với mô hình hỗn hợp sinh; nếu đã sử dụng SVM thì mở rộng thành TSVM; khi khó nâng cấp mô hình học có giám sát đã có, thì nên dùng self-training; nếu các đặc trưng của dữ liệu phân chia tự nhiên thành hai phần riêng rẽ thì nên dùng Co-training; còn nếu hai mẫu dữ liệu có đặc trưng tương tự nhau hướng tới một lớp thì sử dụng phương pháp dựa trên đồ thị. Trong số các thuật toán học bán giám sát thông dụng, có 2 thuật toán tiêu biểu là Self-training và Co-training.Self-training là kỹ thuật học bán giám sát được sử dụng khá phổ biến do tận dụng được nguồn dữ liệu chưa gán nhãn lớn và ban đầu chỉ cần lượng nhỏ dữ liệu đã gán nhãn. Nội dung chính của Self-training là lặp nhiều lần phương pháp học có giám sát.Gọi    D: là tập các dữ liệu đã được gán nhãn.          C : là tập các dữ liệu chưa gán nhãn.Thuật toán Self-training thực hiện như sau:Lặp (cho đến khi C = Æ):i. Huấn luyện bộ phân lớp có giám sát h trên tập Dii. Sử dụng h để phân lớp dữ liệu trong tập Ciii. Tìm tập con C’ Í C có độ tin cậy cao nhất:D + C’ Þ D ; C – C’ Þ C.Ban đầu huấn luyện bộ phân lớp bằng cách cho bộ phân lớp học một tập dữ liệu huấn luyện đã được gán nhãn (tập này thường nhỏ so với tập dữ liệu chưa gán nhãn). Dùng bộ phân lớp đã được huấn luyện, phân lớp cho các dữ liệu chưa được gán nhãn. Trong số dữ liệu mới được gán nhãn, chọn các dữ liệu có độ tin cậy cao (lớn hơn một ngưỡng nào đó) kèm với nhãn vừa gán, đem bổ sung vào tập dữ liệu huấn luyện ban đầu. Sau đó, bộ phân lớp được học lại trên tập huấn luyện mới (gồm dữ liệu đã gán nhãn ban đầu và dữ liệu do bộ phân lớp mới gán nhãn) và thuật toán được lặp lại. Sau mỗi vòng lặp, bộ phân lớp sẽ bổ sung một số mẫu dữ liệu có độ tin cậy cao nhất cùng với dự đoán phân lớp của chúng vào tập dữ liệu huấn luyện. Tên gọi Self-training xuất phát từ việc sử dụng dự đoán của nó để huấn luyện chính nó.Thuật toán Co-training dựa trên giả thuyết rằng các đặc trưng của tập dữ liệu huấn luyện có thể được phân chia thành 2 tập con (trường hợp lý tưởng là hai tập con này thoả mãn điều kiện độc lập nhau - conditional independent). Nội dung chính của thuật toán như sau:+ Dùng 2 bộ phân lớp phù hợp để học 2 tập con tương ứng (mỗi tập con huấn luyện một bộ phân lớp).+ Mỗi bộ phân lớp thực hiện phân lớp cho các dữ liệu chưa gán nhãn, thu được kết quả là tập dữ liệu chưa gán nhãn kèm theo nhãn dự đoán của chúng. Trong tập kết quả của bộ phân lớp 1, chọn ra những mẫu dữ liệu (kèm nhãn đã dự đoán) có độ tin cậy cao nhất bổ sung vào tập huấn luyện của bộ phân lớp 2 và ngược lại.+ Mỗi bộ phân lớp được học lại tập dữ liệu huấn luyện (gồm dữ liệu gán nhãn ban đầu và dữ liệu gán nhãn mới bổ sung từ kết quả của bộ phân lớp kia). Quá trình được lặp lại cho đến khi tập dữ liệu chưa gán nhãn rỗng hoặc số vòng lặp đạt tới một ngưỡng được xác định trước. Thuật toán Co-training:(1). Huấn luyện hai bộ phân lớp:   f (1) từ (Xl (1), Yl), f (2) từ (Xl (2), Yl). (2). Phân lớp các mẫu dữ liệu chưa gán nhãn Xu với f (1) và f (2) tách biệt nhau. (U là tập các mẫu dữ liệu chưa gán nhãn)(3).  Chèn thêm vào f (1) k-most-confident (x, f (1)(x)) tới các dữ liệu đã gán nhãn của f (2).(4). Chèn thêm vào f (2) k-most-confident (x, f (2) (x)) tới các dữ liệu đã gán nhãn của f (1).(5). Lặp lại các quá trình trên. Thuật toán Co-training trên có thể viết như sau:L: là tập các mẫu dữ liệu đã gán nhãnU: là tập các mẫu dữ liệu chưa gán nhãn(1). L có thể phân chia thành hai tập con L1 và L2 (trường hợp lý tưởng thì L1 và L2 độc lập nhau).(2). Cho bộ phân lớp h1 học L1 (hay L1 huấn luyện bộ phân lớp h1)Cho bộ phân lớp h2 học L2 (hay dùng L2 huấn luyện bộ phân lớp h2)(3). Dùng h1 phân lớp cho U thu được tập U1’ kèm nhãn dự đoán của chúng. Dùng h2 phân lớp cho U thu được tập U2’ kèm nhãn dự đoán của chúng.(4). Từ U1’ chọn ra u1 mẫu dữ liệu kèm theo nhãn của nó, có độ tin cậy cao nhất. Bổ sung u1 vào L2. Khi đó, L2 + u1 => L2.Từ U2’ chọn ra u2 mẫu dữ liệu kèm theo nhãn của nó, có độ tin cậy cao nhất. Bổ sung u2 vào L1. Khi đó, L1 + u2 => L1.(5). Dùng L1 mới huấn luyện bộ phân lớp h1 (hay h1 học L1)Dùng L2 mới huấn luyện bộ phân lớp h2 (hay h2 học L2)(6). Lặp lại từ bước (3). cho đến khi tập U rỗng hoặc số vòng lặp đạt đến ngưỡng xác định trước. Có thể viết rút gọn bằng cách bỏ bước (5). ở trên. Bước (6). đổi thành bước (5): Lặp lại từ bước (2). cho đến khi tập U rỗng hoặc số vòng lặp đạt đến ngưỡng xác định trước. Học có giám sát là học với tập dữ liệu huấn luyện ban đầu hoàn toàn được gán nhãn từ trước.Học có giám sát là phương pháp học sử dụng cho lớp bài toán phân lớp, phân loại (Classification).- Để thực hiện phân lớp, trước tiên phải chuẩn bị một tập dữ liệu huấn luyện (trainning data set), để có tập dữ liệu huấn luyện phải thực hiện gán nhãn cho dữ liệu ban đầu, đây được gọi là quá trình thu thập tập huấn luyện. - Lựa chọn một thuật toán phân lớp (ví dụ SVM) xây dựng bộ phân lớp để học tập dữ liệu huấn luyện. Hay nói cách khác, dùng tập dữ liệu huấn luyện để huấn luyện bộ phân lớp. Thuật ngữ học có giám sát được hiểu là học tập dữ liệu đã được gán nhãn trước (các dữ liệu kèm theo nhãn tương ứng này coi như đã được giám sát bởi người thực hiện gán nhãn).- Sử dụng một tập dữ liệu kiểm tra (test data set) đã được gán nhãn trước, để kiểm tra tính đúng đắn của bộ phân lớp. Sau đó, có thể dùng bộ phân lớp để phân lớp cho các dữ liệu mới.Một số thuật toán thường được lựa chọn khi xây dựng bộ phân lớp gồm có: máy vector hỗ trợ (Support Vector Machine – SVM); k láng giềng gần nhất (K Nearest Neighbours – KNN); tiếp cận xác suất thống kê (Naïve Bayes – NB); Cây quyết định (Decision Tree – DT); sử dụng mạng nơron (Neural Network – Nnet); dựa trên vector trọng tâm (Centroid–base vector); hay tuyến tính bình phương nhỏ nhất (Linear Least Square Fit – LLSF). (Chi tiết các thuật toán này có thể tham khảo trên Internet).Trong phần “Bản chất của học máy dưới góc nhìn của xác suất thống kê” - x1, x2, …, xl là các mẫu, xi thuộc Rn. Các mẫu xi được phát sinh ngẫu nhiên theo một hàm phân bố xác suất F(x) nào đó mà ta không biết.- yi là các kết quả học tương ứng với mẫu xi, yi thuộc R. Thường thì y là kết quả của một hàm f(x) nào đó – đơn trị. Tuy nhiên trong trường hợp tổng quát thì y không đơn trị. Do đó y được xác định theo một hàm phân bố điều kiện F(y|x) mà ta cũng không biết.Trong trường hợp khi y thuộc R, tức đây là vấn đề hồi quy (regression). Trong trường hợp bài toán phân lớp (classification) thì y thuộc{-1, 1} là trường hợp nhận dạng hai lớp, nếu yi = -1 thì xi thuộc lớp thứ nhất (không được quan tâm), còn yi = 1 thì xi thuộc lớp thứ 2 (lớp được quan tâm).",
          "relevence": "no"
        },
        {
          "url": "https://sites.google.com/site/diepnn80/datamininginfo/tainguyenhocvemachinelearning",
          "title": "Tài nguyên học về Machine learning - diepnn80",
          "content": "————-&&————Sign in|Recent Site Activity|Report Abuse|Print Page|Powered By Google Sites",
          "relevence": "no"
        },
        {
          "url": "https://sites.google.com/site/diepnn80/datamininginfo/tainguyenhocvemachinelearning",
          "title": "Tài nguyên học về Machine learning - diepnn80",
          "content": "————-&&————Sign in|Recent Site Activity|Report Abuse|Print Page|Powered By Google Sites",
          "relevence": "no"
        },
        {
          "url": "https://sites.google.com/site/diepnn80/datamininginfo/tainguyenhocvemachinelearning",
          "title": "Tài nguyên học về Machine learning - diepnn80",
          "content": "————-&&————Sign in|Recent Site Activity|Report Abuse|Print Page|Powered By Google Sites",
          "relevence": "yes"
        },
        {
          "url": "https://toaiquangton.blogspot.com/2014/04/r-support-vector-machine.html",
          "title": "toai's blog: R - Support Vector Machine",
          "content": "\n\n\n\nĐăng nhận xét\n\n\n\n\n\n\n      BLOG_CMT_createIframe('https://www.blogger.com/rpc_relay.html');\n    \n\n\n",
          "relevence": "yes"
        },
        {
          "url": "https://apusvietnam.com/toi-da-den-voi-machine-learning-nhu-the-nao/",
          "title": "Tôi đã đến với Machine Learning như thế nào? | Apus Vietnam",
          "content": "Tôi đã đến với Machine Learning như thế nào, và một phần nguyên do tôi viết blog này.Tôi vốn có xuất thân từ Toán với 5 năm liền thi toán Quốc gia (cả cho học sinh cấp 3 và sinh viên đại học). Đầu năm thứ ba đại học (2009), thấy các bạn khác đã bắt đầu xin vào các lab trong khoa để làm nghiên cứu, tôi và mấy cậu bạn thân đi hỏi các thầy cô trong khoa và đều nhận được câu trả lời là: Nếu học tốt toán thì nên theo Xử lý ảnh. Chúng tôi tìm và được một thầy giao cho đề tài Nhận dạng biển số xe bằng SVM. Không biết một chút gì về tối ưu và Machine Learning, chúng tôi đã bỏ cuộc sau gần một năm vật lộn để hiểu thuật toán SVM (mà cuối cùng vẫn không hiểu, và sử dụng K-nearest neighbors với độ chính xác rất thấp).Sau đó, chúng tôi mỗi người một hướng khác nhau, tôi được một thầy làm về Hệ thống nhúng và Tính toán khả cấu hình (Embedded System and Reconfigurable Computing) nhận vào lab cuối năm 2010. Thầy rất tốt, hướng dẫn sinh viên rất tận tình (tôi khó có thể được đi học tiếp nếu không có sự giúp đỡ của thầy). Tôi chuyển hướng sang làm FPGA và cố gắng hướng đến các ứng dụng có Xử lý ảnh, video – mặc dù vẫn không biết tại sao mình thích xử lý ảnh sau bao nhiêu lần thất bại ở project kia. Đề tài tốt nghiệp đại học năm 2012 của tôi là thiết kế một IP Camera trên FPGA. Tôi và một cậu bạn vất vả mấy tháng trời mà mới có kết quả tạm chấp nhận được lúc tốt nghiệp. Chúng tôi thường tới thư viện Tạ Quang Bửu (lab tôi ở đó) lúc 8h sáng và về nhà lúc 10h tối. Mùa hè năm ấy rất nóng và KTX thường xuyên mất nước. Hai ba ngày không tắm rửa là chuyện thường. Nhưng đó là chuyện nhỏ, chuyện lớn và tôi làm cháy vài cái mạch, khi sắp đến ngày bảo vệ mà vẫn chưa có kết quả gì, trong khi bạn cùng lớp đã viết xong luận văn cách đó mấy tuần.Cuối cùng, tất nhiên chúng tôi thành công (thì tôi mới tốt nghiệp được). Tôi có làm thêm một vài đề tài khác với thầy về FPGA, cũng thành công nhưng tôi gần như không rút ra được nhiều kinh nghiệm trong những đề tài đó. Mỗi khi bắt đầu một đề tài mới, tôi lại gặp khó khăn y như những lần trước.Tôi nhận ra rằng tôi không khéo léo và không có duyên với những thứ tỉ mỉ như thiết kế mạch điện.Cả 5 năm đại học tôi cố gắng tìm lại chính mình như hồi cấp ba. Tôi không làm được việc đó.Tôi không sợ khó, không sợ khổ, chỉ sợ nhất phải làm những thứ không đúng đam mê. Tôi đã làm nhiều thứ, và cũng thành công ở nhiều thứ. Nhưng tôi chỉ hứng thú với những thứ khi làm nhiều thì kinh nghiệm của mình tăng lên, mình có thể tự tin hơn khi nói chuyện với những người trong nghề.Tôi cũng nhận ra rằng tôi chỉ đam mê với những thứ tôi biết rõ nguyên do!Lúc đó thứ duy nhất tôi có thể nghĩ ra là Xử lý ảnh, ít nhất là vì nó có nhiều toán. Tôi cố gắng apply đi học về ngành Xử lý ảnh, lúc đấy vẫn chưa thật rõ về Machine Learning. Nếu ở lại Việt Nam và đi làm ngay, tôi biết chắc chắn rằng tôi sẽ khó thành công! Thật may mắn, tôi được thầy hiện giờ nhận (thầy duy nhất trả lời tích cực trong rất nhiều thầy tôi gửi email, và Penn State cũng là trường duy nhất nhận tôi).Trong kỳ đầu ở đây, cuối năm 2013, tôi phải học ba lớp: Linear Algebra, Probabilities and Random Processes, và 1 lớp tiếng Anh. Tôi cũng phải tập trung ôn tập cho kỳ thi quan trọng đầu tiên của PhD, Candidacy exam, vào đầu kỳ thứ hai. Hai môn học chuyên ngành kia là hai môn quan trọng nhất trong cả PhD của tôi. Trong thời gian này, tôi cũng thường xuyên được nghe đến Convex Optimization và Machine Learning từ các anh trong lab. Ai ai cũng nói về những thứ đó mà tôi không biết gì. Vậy là sau khi qua kỳ Candidacy và trở thành PhD candidate, tôi tự học hai khoá miễn phí trên mạng: Machine Learning của Andrew Ng và Convex Optimization của Stephen Boyd. Tôi học được rất nhiều thứ từ hai khoá này. Cùng kỳ đó, tôi cũng được học môn Pattern Recognition từ giáo sư yêu thích nhất của tôi trong khoa. Chỗ này hơi khoe một tí, tôi đạt điểm cao nhất trong kỳ thi Candidacy của khoa và là người duy nhất đạt điểm A trong môn Pattern Recognition kỳ đó. (Và bây giờ khoa tôi đã biết rằng ở châu Á còn có Việt Nam nữa 😀)Lúc này thì tôi đã tự tin hơn rất nhiều. Quan trong hơn, cái đam mê học hỏi và làm việc như hồi cấp ba của tôi dần quay trở lại.Trong vài năm học PhD, tôi nghe quá nhiều về Deep Learning và những thành tựu không tưởng của nó, nhưng tôi vẫn cố tránh vì sợ. Rồi đến cuối 2016, tôi nhận ra rằng mình không thể trốn trành nó được nếu muốn ở lại lâu dài với Machine Learning. Tôi muốn rằng đề tài thứ ba, cũng là đề tài cuối cùng trong PhD của tôi, phải là Deep Learning. Tôi đọc khá nhiều bài báo về Deep Learning cho bài toán mà tôi thích và chợt nhận ra rằng Deep Learning cũng được xây dựng từ những thứ rất cơ bản, và có rất nhiều những thứ cơ bản về Machine Learning mà trước đây tôi tưởng rằng mình rõ, thực ra tôi lại rất mơ hồ.Tôi quyết định hệ thống lại kiến thức về Machine Learning của mình vào sáng sớm ngày 25/12/2016, khi trằn trọc không ngủ được vì đêm hôm trước uống hơi nhiều. Và để cho hấp dẫn hơn, tôi sẽ viết nó dưới dạng một blog, phải đến khi viết ra và chia sẻ với người khác thì mình mới hiểu sâu được nó.Cách học tập tốt nhất là chia sẻ kiến thức. Kiến thức là thứ mà khi cho đi, chúng ta sẽ thu được nhiều hơn.Thời gian đó là kỳ nghỉ đông nên tôi có nhiều thời gian cho việc viết và tạo dựng blog. Trong 4 ngày đầu, tôi xây dựng được bộ khung cho blog và viết liền được 3 bài để ra mắt.Những comments tích cực và số lượng like tăng vọt trong những ngày đầu như một liều thuốc mê kích thích tôi viết cực nhiều trong tháng đầu tiên (7 bài) và thậm chí bỏ bê việc nghiên cứu chính. Dần dần tôi bố trí thời gian hợp lý hơn và ra 1 bài/tuần như bây giờ. (Trước đó tôi lập trình Python rất ít, chủ yếu là Matlab, nhưng rồi vừa làm vừa học, giờ cũng khá hơn rồi.)Qua những thảo luận và những post trên page facebook, tôi tự mình học được ra rất nhiều điều. Có những niềm vui mới, có những mối quan hệ mới, có những cơ hội mới mở ra, hầu hết nằm ngoài kỳ vọng của tôi.Khi tìm được đam mê và niềm vui, tôi thường làm việc rất năng suất. Tôi không dám hứa trước, những sẽ cố gắng hết mình để hết năm nay có thể viết được những gì cần thiết cho các bạn bước chân vào Machine Learning.Niềm vui của các bạn cũng là niềm hạnh phúc của tôi. (Hơi chém tí, nhưng cũng phần nào đúng đấy)Cảm ơn các bạn đã đồng hành cùng tôi ôn tập lại kiến thức về Machine Learning.Nguồn: machinelearningbasicvn",
          "relevence": "no"
        },
        {
          "url": "http://www.bmthicong.com.vn/research.html",
          "title": "Bộ Môn Công Nghệ Và Quản Lý Xây Dựng - Nghiên cứu",
          "content": "\n\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\t\n\n\t\t\t\t08 Tháng 12 2016\n\t\t\t\n\n\t\t\t\t\t\n\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\t\tPosted in \n\t\t\t\n\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\t\n\t\t\t\t\tNghiên cứu\n\t\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\n\t\n\n\t\t\t\n\n\n\t\tBộ môn Công nghệ & Quản lý xây dựng tham gia “Hội thảo Toàn quốc lần thứ 29 về Kết cấu và Công nghệ Xây dựng”                Sáng 30/11 tại trường Đại học Xây dựng (ĐHXD) đã diễn ra “Hội thảo Toàn quốc lần thứ 29 về Kết cấu và Công nghệ Xây dựng” do Hội Kết cấu và Công nghệ Xây dựng Việt Nam (VASECT) phối hợp cùng Bộ Xây dựng, Bộ Giao thông vận tải và Bộ Nông nghiệp & Phát triển Nông thôn tổ chức.             Thứ trưởng Bộ Xây dựng Đỗ Đức Duy; Chủ tịch VASECT, GS.TSKH Nguyễn Văn Liên; Hiệu trưởng trường ĐHXD PGS.TS Phạm Duy Hòa cùng đại diện Bộ Giao thông Vận tải, Tổng Hội Xây dựng Việt Nam tham dự hội thảo. PGS.TS Hồ Ngọc Khoa – Trưởng Bộ môn Công nghệ & Quản lý xây dựng cũng đã đại diện Bộ môn tham gia Hội thảo. Chủ tịch VASECT, GS.TSKH Nguyễn Văn Liên phát biểu khai mạc hội thảo.                Phát biểu khai mạc hội thảo, GS.TSKH Nguyễn Văn Liên cho biết trong 32 năm hoạt động và phát triển, Hội Kết cấu và Công nghệ Xây dựng Việt Nam tập trung vào sinh hoạt học thuật với trọng tâm phát triển công nghệ xây dựng. Hội thảo lần này tập hợp các kết quả nghiên cứu mới nhất mang tính thời sự trong 3 lĩnh vực xây dựng gồm xây dựng dân dụng công nghiệp, xây dựng công trình giao thông và xây dựng công trình thủy lợi. Các báo cáo gắn chặt thực tế của các vấn đề chính như rủi ro trong xây dựng và thị trường xây dựng Việt Nam hiện nay.Thứ trưởng Bộ Xây dựng Đỗ Đức Duy phát biểu tại hội thảo.                Thứ trưởng Bộ Xây dựng Đỗ Đức Duy đánh giá cao hoạt động của Hội Kết cấu và Công nghệ Xây dựng Việt Nam trong những năm qua. Thứ trưởng mong muốn hội thảo sẽ là nơi mà các chuyên gia, nhà khoa học, báo cáo viên trao đổi, chia sẻ kinh nghiệm chuyên ngành.PGS.TS Phạm Duy Hòa – Hiệu trưởng trường Đại học Xây dựng phát biểu tại hội thảo.                    Phát biểu tại hội thảo, PGS.TS Phạm Duy Hòa cho biết nhiều năm qua trường ĐHXD luôn có mối quan hệ mật thiết với các hội chuyên ngành, đặc biệt là Hội Kết cấu và Công nghệ Xây dựng Việt Nam. VASECT và nhà trường đã có nhiều chương trình hợp tác hiệu quả đóng góp vào sự phát triển của trường và của Hội.Quang cảnh hội thảo.              Tại hội thảo, các chuyên gia đầu ngành trong lĩnh vực xây dựng đã báo cáo 16 tham luận về kết cấu và công nghệ xây dựng trên thế giới có thể ứng dụng tại Việt Nam. Đại diện giảng viên Bộ môn – PGS.TS Hồ Ngọc Khoa đã tham gia Hội thảo với tham luận: “Công nghệ xây dựng nhà siêu cao tầng: Xu hướng phát triển và ứng dụng ở Việt Nam”. Bài tham luận trình bày kết quả phân tích, tổng hợp các công nghệ xây dựng mới, tiên tiến được áp dụng trên thế giới và trong quá trình thi công các công trình siêu cao tầng ở Việt Nam. Qua đó đưa ra một số nhận định, đánh giá về xu hướng phát triển và tiềm năng ứng dụng phù hợp với điều kiện nước ta. PGS.TS Hồ Ngọc Khoa – Trưởng Bộ môn Công nghệ & Quản lý xây dựng báo cáo tại Hội thảo.  \n\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\t\n\n\t\t\t\t17 Tháng 11 2016\n\t\t\t\n\n\t\t\t\t\t\n\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\t\tPosted in \n\t\t\t\n\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\t\n\t\t\t\t\tNghiên cứu\n\t\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\n\t\n\n\t\t\t\n\n\n\t\tBỘ MÔN CHỦ TRÌ THÀNH CÔNG PHIÊN THẢO LUẬN TIỂU BAN CÔNG NGHỆ VÀ QUẢN LÝ XÂY DỰNG - HỘI THẢO QUỐC TẾ VỀ PHÁT TRIỂN BỀN VỮNG TRONG XÂY DỰNG.               Ngày 15 tháng 11 năm 2016, tại trường Đại học Xây dựng đã diễn ra Hội thảo Quốc tế về Phát triển Bền vững trong Xây dựng do Khoa Xây dựng Dân dụng và Công nghiệp chủ trì. Tham dự hội thảo có các học giả, nhà nghiên cứu đến từ các trường đại học, các viện nghiên cứu, các công ty xây dựng trong và ngoài nước như: Đại học Melbourne (Australia), Công ty JPE (Nhật Bản), Công ty Xây dựng Turner, Công ty Xây dựng Delta, Công ty Xây dựng Conteccon, Công ty NUCETECH… Tại phiên toàn thể diễn ra vào buổi sáng, các học giả trong nước và quốc tế đã trình bày các báo cáo khoa học về các kết quả nghiên cứu, ứng dụng trong lĩnh vực phát triển bền vững mà đơn vị mình đã nghiên cứu, thực hiện. Buổi chiều cùng ngày diễn ra các phiên báo cáo tại tiểu ban, Bộ môn Công nghệ và Quản lý xây dựng chủ trì phiên họp tại tiểu ban Công nghệ và Quản lý Xây dựng.             Trong phiên thảo luận tại tiểu ban Công nghệ và Quản lý Xây dựng, các học giả đến từ các trường Đại học Bách khoa thành phố Hồ Chí Minh, Đại học Kiên trúc Hà Nội, Đại học Xây dựng đã trình bày 13 báo cáo khoa học về các đề tài về ứng dụng công nghệ mới trong xây dựng cũng như trong quản lý. Tại hội thảo, các chuyên gia và khách mời cũng đã có những thảo luận, trao đổi về mặt học thuật cho các báo cáo viên. Ảnh: PGS. TS. Hồ Ngọc Khoa phát biểu chủ trì phiên họp tiểu ban Công nghệ và Quản lý Xây dựng.Ảnh: ThS. Vương Đỗ Tuấn Cường trình bày nghiên cứu “A sustainable approach in soft soil treatment in Vietnam” tại hội thảo.Ảnh: PGS. TS. Lương Đức Long – ĐH Bách khoa TP HCM đang trình bày báo cáo “Fuzzy programming model in optimal usage of multi-skilled labors under fuzziness in construction projects”.Ảnh: NCS Chu Thị Hải Ninh đang trình bày báo cáo “Manufacture of lightweight fireproof-insulating concrete using Hoang Thach PCB 30 portland cement and Pha Lai fly ash”.Ảnh: PGS. TS. Hồ Ngọc Khoa phát biểu góp ý cho một báo cáo tại hội thảo.Ảnh: Các chuyên gia thảo luận cùng các báo cáo viên.Ảnh: Các báo cáo viên chụp ảnh lưu niệm tại hội thảo. KẾT QUẢ BẢO VỆ CƠ SỞ CÁC ĐỀ TÀI NGHIÊN CỨU KHOA HỌC CỦA BỘ MÔNBộ môn CN & QLXD chủ trì 02 đề tài KHCN trong Chương trình KHCN cấp Bộ GD&ĐT “Thiết kế và thi công nhà siêu cao tầng” của trường ĐHXD.  Đó là các đề tài: “Nghiên cứu các giải pháp bảo trì và quản lý nhà siêu cao tầng ở Việt Nam” do PGS.TS. Hồ Ngọc Khoa chủ trì và đề tài “Nghiên cứu công nghệ xây dựng nhà siêu cao tầng ở Việt Nam” do TS. Trần Hồng Hải chủ trì.Chiều ngày 23/12/2015 đề tài “Nghiên cứu các giải pháp bảo trì và quản lý nhà siêu cao tầng ở Việt Nam” và chiều 28/12/2015 đề tài “Nghiên cứu công nghệ xây dựng nhà siêu cao tầng ở Việt Nam” đã bảo vệ thành công cấp cơ sở.Hai Hội đồng cơ sở do GS.TS. Phan Quang Minh làm Chủ tịch cùng với các thành viên: PGS.TS. Trần Văn Liên, PGS.TS. Ngô Văn Quỳ, TS. Đinh Tuấn Hải, PGS.TS. Trịnh Quốc Thắng, TS. Mỵ Duy Thành, TS. Đoàn Dương Hải, PGS.TS. Trần Chủng, TS. Nguyễn Ngọc Linh, TS. Nguyễn Đại Minh, PGS.TS. Nguyễn Ngọc Phương đánh giá cao kết quả nghiên cứu đạt được; nhận xét, góp ý để các nhóm thực hiện đề tài chỉnh sửa, hoàn thiện, chuẩn bị bảo vệ chính thức tại Hội đồng nghiệm thu cấp Bộ.Chủ trì và các thành viên hiện đề tài bao gồm PGS.TS. Hồ Ngọc Khoa, TS. Trần Hồng Hải, TS. Nguyễn Mạnh Tuấn, ThS. Vương Đỗ Tuấn Cường, ThS. Phạm Tiến Tới, ThS. Nguyễn Hùng Cường, ThS. Cao Tuấn Anh, ThS. Phạm Nguyễn Vân Phương, ThS. Lê Đình Tiến, ThS. Lê Thái Hòa cảm ơn Hội đồng, tiếp thu ý kiến đóng góp để hoàn thiện đế tài, để bảo vệ chính thức thành công, đúng tiến độ.Một vài hình ảnh của các buổi bảo vệ đề tài:PGS. TS. Hồ Ngọc Khoa trình bày các kết quả nghiên cứu đề tài bảo trìCác thành viên hội đồng tham gia đóng góp ý kiến cho đề tài bảo trìTS. Nguyễn Mạnh Tuấn trình bày kết quả nghiên cứu đề tài công nghệ thi côngPGS. TS. Trần Chủng phát biểu ý kiến phản biện đề tài công nghệ thi công \n\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\t\n\n\t\t\t\t29 Tháng 12 2014\n\t\t\t\n\n\t\t\t\t\t\n\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\t\tPosted in \n\t\t\t\n\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\t\n\t\t\t\t\tNghiên cứu\n\t\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\n\t\n\n\t\t\t\n\n\n\t\tDỰ BÁO KẾT QUẢ CỦA QUÁ TRÌNH PHUN VỮA XI MĂNG BẰNG TRÍ THÔNG MINH NHÂN TẠOAN ARTIFICIAL INTELLIGENCE APPROACH FOR GROUTABILITY ESTIMATION BASED ON AUTOTUNING SUPPORT VECTOR MACHINETS. Trần Hồng Hải, TS. Hoàng Nhật Đức        Phương pháp bơm vữa gia cố nền kiểu xâm nhập (permeation grouting) thương được sử dụng trong kỹ thuật xây dựng. Như vậy, việc dự đoán kết quả của quá trình phun vữa là một công tác rất quan trọng cần được chú trọng từ giai đoạn lập kế hoạch. Trong bài nghiên cứu này, một phương pháp mới sử dụng trí thông minh nhân tạo – máy véc tơ hỗ trợ tự động điều chỉnh (autotuning support vector machine - SVM) được đưa ra nhằm dự báo kết quả quá trình phun vữa xi măng sử dụng vữa xi măng hạt mịn. Ở phương pháp mới này, thuật toán của máy SVM được sử dụng để phân loại quá trình phun vữa bao gồm: thành công và thất bại. Trong đó, thuật toán vi phân (DE) tối ưu hóa được dùng để nhận dạng các thông số điều chỉnh tối ưu của thuật toán từ máy SVM, gồm có các thông số bù trừ và thông số chức năng cốt lõi. Sự kết hợp các thuật toán SVM và DE cho phép phương pháp dự đoán mới này có thể vận hành tự động mà không cần đến các tác động của con người, bỏ qua quá trình điều chỉnh các thông số lặp đi lặp lại. Một thí nghiệm có sử dụng các mẫu thử tại chỗ cũng chỉ ra rằng phương pháp dự báo mới này có thể đưa ra các kết quả dự đoán chính xác.Bạn đọc và các bạn sinh viên quan tâm có thể theo dõi ở đường link sau:  DỰ BÁO KẾT QUẢ CỦA QUÁ TRÌNH PHUN VỮA XI MĂNG BẰNG TRÍ THÔNG MINH NHÂN TẠO\n\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\t\n\n\t\t\t\t29 Tháng 12 2014\n\t\t\t\n\n\t\t\t\t\t\n\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\t\tPosted in \n\t\t\t\n\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\t\n\t\t\t\t\tNghiên cứu\n\t\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\n\t\n\n\t\t\t\n\n\n\t\tĐIỀU HÒA TÀI NGUYÊN CHO DỰ ÁN XÂY DỰNG BẰNG THUẬT TOÁN TIẾN HÓA VI PHÂNA NOVEL RESOURCE-LEVELLING APPROACH FOR CONSTRUCTION PROJECT BASED ON DIFFERENTIAL EVOLUTIONTS. Trần Hồng Hải, TS. Hoàng Nhật Đức        Trong kỹ thuật xây dựng, thông thường phương pháp đường Găng (CPM - Critical Path Method) được sử dụng để lập tiến độ của các dự án. Tuy nhiên, cách lập tiến độ này thường xuất hiện các biến động đáng kể trong danh mục tài nguyên, điều này không chỉ phi thực tế mà còn gây tốn kém cho nhà thầu khi thực hiện theo. Vì vậy, để điều hòa các danh mục tài nguyên đó, nhà quản lý xây dựng cần thực hiện quá trình cân bằng các hạng mục tài nguyên. Bài nghiên cứu này đề xuất ra phương pháp điều hòa tài nguyên dựa trên các thuật toán tiến hóa vi phân (viết tắt tiếng Anh: RLDE – resource levelling based on differential evolution). Cách thể hiện của RLDE so với phần mềm Microsoft Project khác biệt ở việc sử dụng các thuật toán di truyền (the genetic algorithm) và các thuật toán tối ưu hóa tổ hợp phần tử (the particle swarm optimization algorithm). Nhiều thực nghiệm đã chỉ ra rằng phương pháp tính toán mới này có thể mang lại kết quả tối ưu trong việc thực hiện điều hòa tài nguyên. Như vậy, có thể kết luận phương pháp RLDE là một phương pháp hiệu quả, có thể được sử dụng như một công cụ hữu ích cho người làm công tác quản lý, lên kế hoạch trong lĩnh vực quản lý dự án.Bạn đọc và các bạn sinh viên quan tâm có thể theo dõi ở đường link sau:  ĐIỀU HÒA TÀI NGUYÊN CHO DỰ ÁN XÂY DỰNG BẰNG THUẬT TOÁN TIẾN HÓA VI PHÂN\r\n\t\t\t\tTrang 1 / 4\t\t\t1. Sinh viên hỏi:câu hỏi từ bạn: congthien.nuce54@gmail.com Thưa thầy cô, hiện tại em đang làm đồ án thi công 1, và ở phần chọn máy thi công em đang băn khoăn chưa chọn được máy cẩu tháp nào phù hợp bởi vì khối lượng bê tông quá lớn mà các máy hiện có trong các sổ tay máy cũ không đạt yêu cầu.Cho nên hiện e đang cần catalog của một số loại cẩu tháp chạy trên ray và máy trộn bê tông những loại mới nhất hiện nay. Em xin cảm ơn ạ!Bộ môn Trả lời:Em hãy lập biện pháp kỹ thuật và tổ chức thi công theo các loại cần trục tháp và máy trộn bê tông mà em có thể biết hiện có ở Việt Nam. 2. Sinh viên hỏi:Câu hỏi từ bạn: hùng <hanhieuvi0112@gmail.com\">hanhieuvi0112@gmail.com>kính thưa bộ môn,nhóm làm đồ án tốt nghiệp 53xd3 được nhà trường phân thầy Lê Thế Thái hướng dãn phần thi công.hiện tại bọn em đã xong phần kết cấu.liên hệ với thầy thì thầy bảo tất cả các nhóm tự liên hệ với bạn Tuấn Anh.thực sự bọn em đã lên hỏi bộ môn,phòng đào tạo,tìm ai tên là tuấn anh nhưng k có thông tin gì cả.tất cả đều là tự liên hệ với thầy để nhận đề tài.kính mong bộ môn hồi âm để bọn em được nhận sự hướng dẫn của thầy ạBộ môn Trả lời: Em hãy liên hệ với thầy Lê Thế Thái và đề nghị với thầy bố trí làm việc với các em. 3. Sinh viên hỏi: Câu hỏi từ bạn: Đặng Thành Luân <thanhluanxd7@gmail.com\">thanhluanxd7@gmail.com>Em xin hỏi thày cô, khi hạ mực nước ngầm có sử dụng được bằng phương pháp bấc thấm được ko ạ? Và nếu sử dụng được thì nguyên lý tính toán và trình tự tính thế nào ạ? Mong thày cô có thể cho tiêu đề các tài liệu, hoặc cho em xin tài liệu về phương pháp bấc thấm ( Nếu có công trình đã sử dụng thì rất tốt ạ)? Chúc thày cô mạnh khỏe - Hạnh phúc - Thành đạt!Bộ môn Trả lời: Bấc thấm là biện pháp để gia cố nền chứ không phải để hạ mực nước ngầm, em nên tìm hiểu kỹ lại nguyên lý của phương pháp này.  4. Sinh viên hỏi:Câu hỏi từ bạn:lưu bá vũ <luubavudexauxa@gmail.com\">luubavudexauxa@gmail.com>các thầy cô cho em hỏi.định mức giờ công trong thống kê lắp đặt ván khuôn lấy như thế nào? Bộ môn Trả lời:Em có thể tìm hiểu tất cả các định mức lao động mà Nhà nước ViệtNam đã ban hành.Nếu muốn tìm hiểu chi tiết hơn thì em xem định mức 726Chúc các em sức khỏe và học tập tốt.Ban chủ nhiệm Bộ môn CN&QLXD Sinh viên hỏi:nguyenquyenxd113@gmail.comem chào thầy! thầy có thể cho em hỏi là: thực trạng về cơ sở pháp lý của công tác giám sát thi công công trình xây dựng không ạ?em cám ơn thầy nhiều ạ!!Bộ môn trả lời:Giám sát thi công là do các cơ quan tư vấn giám sát thực hiện. Muốn làm công việc đó ( giám sát tư vấn ) yêu cầu phải có chứng chỉ tư vấn giám sát và giấy phép hành nghề tư vấn giám sátChúc bạn sức khỏe - học tập tốt!Ban chủ nhiệm Bộ mônCâu hỏi:Em chào thầy!Em muốn nhờ thầy giúp đở em về học tập.Vậy em có thể gặp thầy Nguyễn Đình Thám vào thời gian nào trong tuần ạ!Em xin cảm ơn thầy!Bộ môn trả lời:Trả lời câu hỏi của bạn Phan Văn Hoàng - lớp 52KSCT.Bộ môn rất hoan nghênh tinh thần học tập của em.Em có thể liên hệ trực tiếp với thầy Nguyễn Đình Thám , cũng như các thầy khác trong Bộ môn qua số điện thoại của các thầy đã đăng trên websiteChúc em học tập tốt!Ban chủ nhiệm Bộ môn Công nghệ và Quản lý Xây dựng.Bản quyền thuộc Bộ môn Công nghệ và Quản lý Xây dựng - Khoa Xây dựng Dân dụng & Công nghiệp - Trường Đại học Xây Dựng Địa chỉ liên hệ: Phòng 307 - Nhà A1 - Trường Đại học Xây dựng Số 55 - Đường Giải Phóng - Quận Hai Bà Trưng - Hà NộiĐiện thoại: (+84)4.3869.7349  Email: bm.thicong@gmail.com Chủ biên: TS. Trần Hồng Hải; Th.S Cao Thế Trực",
          "relevence": "no"
        },
        {
          "url": "https://phvu.net/2011/10/05/pca-principal-component-analysis/",
          "title": "PCA – Principal Component Analysis | mlcvGru",
          "content": "Phân tích thành phần chính (Principal Component Analysis – PCA) là một trong những phương pháp phân tích dữ liệu nhiều biến đơn giản nhất. Phần 1 của bài này sẽ nói về động lực và ý tưởng chính của  PCA, sau đó trình bày từng bước trong thuật  toán PCA. Như thường lệ, để hiểu thuật toán thì cài đặt là cách tốt nhất, do đó một đoạn mã MATLAB minh họa từng bước trong PCA cũng được trình bày. Phần cuối cùng là chi tiết các khai triển Toán học trong  PCA. Do mục tiêu ứng dụng nên các phương trình cụ thể được để dành đến phần này, người đọc nếu thật sự quan tâm nguyên lí của PCA, tại sao PCA lại sử dụng những công thức như thế v.v… thì có thể tham khảo phần này để biết thêm chi tiết.Như đã nói trong bài trước, trong thống kê, thông thường cần phải “nghiên cứu” dữ liệu trước khi xây dựng các mô hình suy diễn dựa trên dữ liệu đó. Tuy nhiên đôi khi dữ liệu có số chiều lớn, không thể visualize (không biết dịch là gì) trong không gian 2 hay 3 chiều, do đó cần phải tìm cách đưa dữ liệu về không gian có số chiều nhỏ hơn.PCA là một trong những phương pháp như thế, nhưng hơn thế, PCA còn giống như một Swiss knife với nhiều đặc tính tốt:Nói một cách ngắn gọn, mục tiêu của  PCA là tìm một không gian mới (với số chiều nhỏ hơn không gian cũ). Các trục tọa độ trong không gian mới được xây dựng sao cho trên mỗi trục, độ biến thiên của dữ liệu trên đó là lớn nhất có thể. Tiếng  Việt thì dài dòng, nhưng tiếng Anh thì mục tiêu này gọi là maximize the variability. Ba chữ này gói gọn ý tưởng chính của  PCA.Minh họa PCA: phép chiếu lên các trục tọa độ khác nhau có thể cho cách nhìn rất khác nhau về cùng một dữ liệu.Một ví dụ kinh điển là hình ảnh về con lạc đà. Cùng là một con lạc đà nhưng nếu nhìn từ bên hông thì ta có được đầy đủ thông tin nhất, trong khi nhìn từ phía trước thì thật khó để nói nó là lạc đà.Một ví dụ thuyết phục hơn được minh họa trong hình sauMinh họa PCA: tìm các trục tọa độ mới sao cho dữ liệu có độ biến thiên cao nhấtGiả sử tập dữ liệu ban đầu (tập điểm màu xanh) được quan sát trong không gian 3 chiều (trục màu đen) như hình bên trái. Rõ ràng 3 trục này không biểu diễn được tốt nhất mức độ biến thiên của dữ liệu. PCA do đó sẽ tìm hệ trục tọa độ mới (là hệ trục màu đỏ trong hình bên trái). Sau khi tìm được không gian mới, dữ liệu sẽ được chuyển sang không gian này để được biểu diễn như trong hình bên phải. Rõ ràng hình bên phải chỉ cần 2 trục tọa độ nhưng biểu diễn tốt hơn độ biến thiên của dữ liệu so với hệ trục 3 chiều ban đầu.Một điểm rất đẹp nữa của  PCA là các trục tọa độ trong không gian mới luôn đảm bảo trực giao đôi một với nhau, mặc dù trong không gian ban đầu, các trục có thể không trực giao.Dài dòng như vậy là đủ, ta sẽ trình bày từng bước thuật toán PCA trong phần tới. Chi tiết về ý tưởng và khai triển toán học được dành lại để trình bày ở cuối bài.Cho ma trận . Các bước của PCA lần lượt như sau:1. Tiền xử líDữ liệu ban đầu có thể có giá trị thay đổi bất thường. Ví dụ trên feature 1 (cột 1 của  ) giá trị thay đổi trong khoảng (0, 1), trên feature 2 lại biến thiên trong đoạn (-100, 100). Rõ ràng cần phải có một bước tiền xử lí để chuẩn hóa giá trị trên các cột của ma trận X. Có 2 cách tiền xử lí thường được dùng cho PCA là Centered PCA và  Normed PCA.Centered PCA mang tất cả các feature (các cột của X) về cùng một gốc tọa độ:,,             (1a).Trong đó n là số dòng của  X,  là mean của cột thứ j của  X, được tính như trên.Normed PCA mang tất cả các feature về cùng một gốc tọa độ, đồng thời chuẩn hóa về cùng một quãng standard-deviation bằng 1:,.             (1b)Trong đó  là độ lệch chuẩn  (standard deviation) của cột thứ j trong X.Thông thường Normed PCA hay được dùng. Sau bước tiền xử lí, ma trận  sẽ là đầu vào cho bước tiếp theo.2. Xây dựng không gian mớiTính ma trận hiệp phương sai (covariance) của các feature trong :               (2)Do là tích của ma trận  với chuyển vị của nó nên  là ma trận positive semidefinite kích thước . Hơn nữa  có p trị riêng .Tiếp theo, PCA tìm trị riêng và vector riêng tương ứng của , sắp xếp theo thứ tự giảm dần của trị riêng. Giả sử p trị riêng của V là,              (3)và p vector riêng tương ứng là.               (4)Khi đó các trục của không gian mới chính là các vector riêng  ở trên, đương nhiên các vector riêng hoàn toàn độc lập tuyến tính (nghĩa là trực giao đôi một).Có thể nói trong PCA, trị riêng và vector riêng có vị trí rất đẹp, thỏa mãn tất cả các yêu cầu của PCA. Bản thân tôi khi đọc đến phần này cũng thấy bất ngờ vì lời giải cho PCA không gì khác lại hoàn toàn trọn vẹn trong trị riêng và vector riêng. Tuy nhiên tại thời điềm này, ta sẽ chấp nhận như vậy.  Phần cơ sở Toán học ở cuối bài sẽ giải thích tại sao trị riêng và vector riêng lại xuất hiện (có phần bất ngờ) trong  PCA như vậy.3. Chuyển dữ liệu từ không gian ban đầu vào không gian mớiThông thường không gian mới không được xây dựng bằng tất cả p vector riêng trong (4), mà thông thường chỉ từ k vector riêng đầu tiên, với k < p. Tại sao là các vector đầu tiên, và chọn k bao nhiêu thì tốt, ta sẽ bàn trong phần cuối.Như vậy gọi.Khi đó tọa độ các điểm trong hệ tọa độ mới là             (5)Xong. Ta đã kết thúc giải thuật PCA, không thể đơn giản hơn.Trong phần này ta chỉ thực hiện phân tích các sample trong không gian tạo bởi các feature. Ta còn có thể thực hiện khảo sát các feature trong không gian tạo bởi các sample. Đương nhiên có thể chuyển vị ma trận  X rồi thực hiện tương tự, nhưng đó là cách võ biền, cày bừa. Thực sự không cần phải như vậy, mà nhờ vào những đặc tínhđẹp của trị riêng và vector riêng, ta có thể tính ngay không gian mới của các sample (để biểu diễn các feature) từ các vector riêng trong (4). Trị riêng còn đẹp đến nỗi được dùng trong các tiêu chuẩn để chọn k. Ta sẽ trở lại với 2 vấn đề này trong phần cuối của bài.Ngoài ra để đánh giá chất lượng của không gian mới tạo bởi PCA, ta dùng 2 độ đo là contribution và squared cosine. Ta cũng dành 2 phần này cho phần cuối bài.Sau đây là đoạn mã MATLAB minh họa các bước của thuật toán PCA, thực hiện phân tích các observation trong không gian các feature, sau cùng tính các độ đo contribution và squared cosine. Download here.Xem tại đây.Một số hạn chế của PCA:Ngoai PCA thi con co ICA cung la 1 pp thong dung de xu ly data.Rất mong tác giả có thể trình bày tiếp phần “Cơ sở Toán học của  PCA”.Chào anh V, cho em hỏi là PCA và Karhunen Loeve khác nhau như thế nàoHi Khoa,\nTrong trường hợp rời rạc và áp dụng lên sample của biến ngẫu nhiên, thì PCA và Kerhunen-Loeve Transforms là một.Thanks for your replyTheo em hiểu thì Karhunen là ma trận A dùng để biến Rx (covariance của X) thành Ry (là ma trận chéo, có các phần tử trên đường chéo là các giá trị riêng của Rx)\nNhưng khi áp dụng PCA, em thấy từ Rx là mình có thể suy ra giá trị riêng, vector riêng luôn rồi ?\nTại sao lại cần ma trận chéo ?Hi em,\nEm tham khảo pp KLT ở đâu thế? Theo anh đọc thì trong trường hợp rời rạc, KLT cũng chỉ dùng ma trận tạo bởi các vector riêng để thực hiện biến đổi các vector ban đầu, giống hệt như PCA.Em đọc trong bài giảng của thầy L.Q.Ngọc\nVậy mình có thể hiểu PCA là ý tưởng cơ sở, còn Karhunen Loeve là cài đặt cụ thể của biến đổi vector không anh ?\nKhi dùng PCA để chuyển X thành Y, thì các tài liệu thường chứng minh covariance matrix của Y là ma trận chéo, có các giá trị trên đường chéo là các giá trị riêng của covariance matrix của X. Anh có thể nói rõ hơn chỗ này được ko ?Em cảm ơn.1. Anh không nghĩ thế. Anh thấy Karhunen Loeve tổng quát hơn PCA.\n2. Anh không thấy lí do tại sao covariance của Y lại phải là ma trận đường chéo. Em có thể nói rõ nội dung chứng minh thế nào ko?Thank vì bài viết 😉ps: “visualize” có thể dịch là “quan sát (một cách) trực quan” (?), tức là quan sát bằng mắt thường.đúng nghĩa tiếng Anh-Việt visualize = “hình dung” các bác ạDoc xong da hieu duoc rat nhieu thu thac mac, cam on anh nhieu!chao anhHien nay e dang lam de tai ve xu ly anh nhan dang mat nguoi su dung thuat toan pcanhung e co mot van de ko hieu ve khai niem truc giao – va truc giao de lam gie cam on anh nhieuRat hy vong nhan duoc su hoi am cua anhChao anh Em hien dang lam de tai lien quan den xu ly anh nhan dang mat nguoi ( face recognition) Qua tim hieu em co gap mot van de ma khong giai thich duocDo la truc giao – Vay truc giao la gi- y nghia cua no de lam giEm cam on anh nhieuChuc anh suc khoe va that nhieu niem vuiPS: neu co tai lieu lien quan den PCA anh gui qua mail giup e nhevie.hanguyen89@gmail.comHi Hà,\n“Trực giao” hiểu nôm na nghĩa là “vuông góc”. Trong đại số tuyến tính, ta nói 2 vector n chiều là trực giao khi và chỉ khi tích vô hướng của chúng bằng 0.ThânAnh Vu ah em co mot thac mac mong anh giup do nhe\nTheo mot tai lieu em doc duoc ve thuat toan PCA gom co 5 buoc co ban:\n1 – lay du lieu\n2 – Tru di tri trung binh ( mean) cua moi chieu ( dimension)\n3- Tinh toan ma tran hiep psai\n4 – Thiet lap vecto dac trung ( feature vector)\n5 – chuyen du lieu ban dau ve khong gian moiBuoc thu 2 em ko hieu lam\n Em nghi no la buoc tien xu ly du lieu phai khong ah\nmong anh giup em giai thich nheem cam on anh nhieu!Hi Ha,\nĐúng vậy, trong bước đó người ta muốn chuẩn hóa dữ liệu sao cho giá trị trung bình (mean) của nó bằng 0.\nVí dụ X là ma trận sau:\ntức là ta có 3 mẫu dữ liệu trong không gian 2 chiều. Nếu thực hiện centered PCA (là cách em mô tả ở trên), thì với mỗi cột, ta trừ đi giá trị trung bình của cột đó. Cụ thể trong ma trận trên thì giá trị trung bình của cột 1 là , và của cột 2 là . Như vậy sau khi biến đổi thì ma trận mới là:\n\nNhư vậy sau khi chuyển đổi, ta thấy giá trị trung bình của mỗi cột trong ma trận mới đều bằng 0, đây chính là ý nghĩa của centered PCA.Nếu ta chia thêm cho phương sai của mỗi cột, thì các cột trong ma trận cuối cùng sẽ có phương sai bằng 1, và cách làm đó gọi là Normed PCA.Thực ra  đây không phải là 1 bước bắt buộc của PCA, tuy nhiên vì PCA hoạt động dựa trên trị riêng của ma trận covariance, mà ma trận covariance khá nhạy với “variance” của các biến, nên thực tế cho thấy normed và centered PCA cho kết quả tốt hơn PCA trên ma  trận gốc ban đầu.PS: Em thuc su muon giai quyet bai toan nhan dang mat nguoi theo huong xay dung di tu: Hinh dung van de cot loi (hieu duoc muc tieu cuoi cung) – Xay dung tren co so toan hoc ( tai sao lai su dung nhung cong thuc va ham toan hoc nhu vay) va cai dat thuat toan tren matlabNeu A co tai lieu lien quan thi gui giup em nhe anh ( neu duoc thi em mong nhan duoc tai lieu thong qua gmail) hoac neu anh ban ^^ thi co the giup e ten cac tai lieu day du ve no cung duoc )\nEm cam on anh!\nPS: Chuc anh ngay le vui ve !\nAnd: Em van phai lam viec de hoan thanh final project theo dun tien do( Em rat an tuong voi cach giai quyet bai toan cua anh – Dieu ma it giao vien hien nay tiep can va huong dan students xay dung phuong huong giai quyet van de)\nAnd: Dac biet la hinh con lac daNhận dạng mặt người cũng có khá nhiều phương pháp, không biết em đang làm theo phương pháp nào?Em cam on anh nhieu! (^-^) ( Em ko nghi la duoc reply nhanh nhu vay)\nHien em dang lam final Project su dung phuong phap PCA\nThuc su la em chua qua hieu mo hinh toan hoc cua PP nen Em muon tim hieu no that ky truoc khi xay dung thuat toan cung nhu can xay dung nhung chu y (uu va nhuoc diem cua no) de khac phucneu con thoi gian em se ket hop voi mot so pp khac de cai thien do chinh xac cua chuong trinh anh ahHien gio e su dung matlab de viet chuong trinh ( GUI cho Thiet lap Camera va GUi cho giao tiep phan cung em da hoan thanh)\nmot so code mau tu tren matlab nguoi ta thuc hien cung cuc ky tuyet voi nhung em van muon tu minh viet code duoi goc do am hieu thuat toan anh ah^^ PS: Nhan duoc hoi am cua anh Em rat vui ( quen het met moi lun) ^0^Chào anh Vũ,\nTrong học máy, ví dụ dùng SVM để luyện, cho tập training set.\nMục tiêu của mình là dùng PCA để giảm số chiều của các quan sát (giảm số các feature) từ không gian trạng thái ban đầu thành không gian mới. Lấy ví dụ, tập các samples, mỗi sample có 10.000 feature (hiểu nôm na là 10.000 chiều).\n1. Trong trường hợp này cần tìm trị riêng của X^TX, XX^T.\n2. Ở bước phân loại, làm sao biến đổi tập dữ liệu cần phân loại trong không gian cũ sang không gian đã được xây dựng bởi CPA ở trên.\nXin cảm ơn anh!Hi em,\n1. Anh không hiểu câu này. Thông thường em có thể chạy PCA bằng các cài đặt có sẵn (OpenCV hay MATLAB) để tìm ma trận U. Sau đó thì chiếu toàn bộ dữ liệu ban đầu lên không gian của U. Việc này hình như cũng được làm sẵn luôn rồi.\n2. Trong trường hợp PCA là input cho SVM (tổng quát là các thuật toán supervised), thì em nên làm như sau:\n  – Chạy PCA trên training set để tìm ma trận U của các vector riêng. Lưu lại ma trận U này.\n  – Chiếu training set vào không gian mới: X’ = X*U. Dùng X’ để huấn luyện SVM.\n  – Trong quá trình test, vì các vector trong tập test ở trong không gian ban đầu nên cần phải chiếu vào không gian của U, tức là tính Y’ = Y*U, trong đó Y là tập test ban đầu, Y’ là tập test trong không gian PCA.\n  – Dùng Y’ để test mô hình đã huấn luyện.Thân.Cảm ơn anh Vũ.\nAnh giải thích đúng ý muốn hỏi rồi đó.\nChúc anh khỏe!Anh Vu oi! A co tai lieu (tieng viet) về thuật toán 2D-PCA không?\nE dang nghien cuu ma khong hieu gie het A.Hi em, anh không có tài liệu tiếng Việt nào về 2DPCA cả 😀Anh vu oi! A co biet gi thuat toan 2D-PCA khong?\nA có gợi ý gì cho e với.\nCám ơn Anh thật nhiềuHy dduuyy,\nCách làm của 2DPCA khá đơn giản. Giả sử cho tập ảnh huấn luyện  thì 2DPCA tìm ma trận sau:\ntrong đó  là trung bình cộng của tất cả các ảnh.Sau đó 2DPCA tìm trị riêng và vector riêng của ma trận G, các vector riêng ứng với các trị riêng lớn nhất sẽ là cơ sở cho không gian mới. Trong PCA thì ta dùng ma trận covariance, còn 2DPCA thì dùng ma trận G. Còn lại hoàn toàn tương tự.\nKhông hiểu bạn không rõ chỗ nào.Cam on ban tra loi minh!\nCai minh khong hieu la giua pca va 2dpca, thi cai nao tot hon!\nva muon xin ban thuat toan cua 2dpca.\nMinh lap trinh hoai ma khong duoc, chac tai thuat toan minh bi saiTheo mình hiểu thì 2DPCA có chi phí thấp hơn PCA truyền thống vì ma trận dùng để tính trị riêng của 2DPCA nhỏ hơn nhiều so với ma trận trong PCA. Về hiệu quả thì hình như là tương đương, mình không chắc lắm, bạn có thể xem thêm trong paper của 2DPCA.\nBạn có thể download toolbox này, trong đó có chứa các hàm liên quan đến 2DPCA:\nhttp://www.mathworks.com/matlabcentral/fileexchange/12333-statistical-learning-toolboxCảm ơn A. Vũ nhiều lắm!Một số hạn chế của PCA:Chỉ làm việc với dữ liệu numeric,\nNhạy cảm với các điểm outlier/extreme,\nKhông phù hợp với các mô hình phi tuyến, do PCA hoàn toàn dựa trên các biến đổi tuyến tính.Nhờ anh Vu giải thích dùm em 3 ý trên với  , em còn mơ hồ quá . Cảm ơn anhHi Minh,\n1/ Chỉ làm việc với dữ liệu numeric: PCA không thể sử dụng được cho dữ liệu categorical. Ý là dữ liệu trong ma trận X phải là số thực (liên tục), chứ không phải là các categorical variable.2/ Nhạy cảm với các điểm outlier/extreme: nếu có vài điểm outlier (ngoại lệ) trong dữ liệu ban đầu thì “chất lượng” của PCA có thể sẽ không cao. Chất lượng không cao theo nghĩa là dữ liệu sau khi biến đổi PCA sẽ có thể không giữ được variance cao như ban đầu.3/ Không phù hợp với các mô hình phi tuyến, do PCA hoàn toàn dựa trên các biến đổi tuyến tính: bản chất PCA là thực hiện một biến đổi tuyến tính từ không gian ban đầu sang không gian mới. Theo nghĩa đó thì PCA không “mạnh” bằng các phép biến đổi phi tuyến như RBF v.v…Em cảm ơn anh Vu nhiều lắm .\n^_^anh Vu ơi cho em hỏi chỗ này tí :\nkhi em test bên cửa sổ command matlab Xhat’ thì xuất ra được ma trận nghịch đảo nhưng khi vào code trong hàm thì báo lỗi ngay dòng này V= Xhat’ * Xhat em dọc đủ kiểu mà cũng ko tìm được ma trận nghịc đảo\nChi tiết lỗi :\n??? Error using ==> ctranspose\nTranspose on ND array is not defined.Error in ==> mypca at 18\n  V = Xhat’ * Xhat;Cảm ơn anh đã đọcHi em,\nAnh vừa thử lại nhưng không thấy có lỗi gì cả. Có thể ma trận input X của em có nhiều hơn 2 chiều nên mới có lỗi trên. Trước khi gọi mypca(), em thử gọi size(X) xem kết quả là gì nhé.\nEm có thể download file mypca.m ở đây: https://www.box.com/s/l7jgipl8eyqdwzlxnr2o\nMatlab đã cài đặt sẵn PCA trong hàm princomp (http://www.mathworks.fr/fr/help/stats/princomp.html). Nếu em dùng PCA trong chương trình của em thì nên dùng hàm của Matlab. Cài đặt của anh chỉ để minh họa cho thuật toán thôi.ah hình của em 3 chiều lun RGB hèn chi .anh Vu có code nào trong matlab để nhận dạng khuôn mặt bằng thuật toán PCA có sử dụng Yale Face Database hok. Thuật toán PCA thì em đọc của anh đã nắm rõ . Nhưng khi áp dụng bộ dữ liệu Yale Face Database thì em chưa bít . Nếu anh có thì gửi mail cho em với (minhchi_a4@yahoo.com) . Cảm ơn Anh Vu nhiềuĐây em: http://www.mathworks.fr/matlabcentral/fileexchange/17032-pca-based-face-recognition-systemem test đc rầu cảm ơn anh Vu đã chia sẽanh Vu ơi giải thích kỹ dùm em vài câu hỏi mà em còn thắc mắc với nha :\ncâu 1 : Tại sao PCA lại dùng vector riêng và trị riêng ? Dùng nó có ích lợi gì ?\ncâu 2 : cách tìm K trị riêng tương ứng với vector riêng của anh viết ở trên  và  cái link anh chia sẽ cho em ở trên nó tìm k = cách lấy những trị riêng tương ứng với vector riêng > 1 thì cách chọn k nào tốt hơn anh Vu … em test thử cách theo cái link này thì có hình trả về đúng , có hình trả về kết quả sai  http://www.mathworks.fr/matlabcentral/fileexchange/17032-pca-based-face-recognition-systemcâu 3: PCA dùng để nén dữ liệu thì mình lưu lại cái gì để mình có thể giải nén chính xác như ban đầu ?Thanks anh đã đọcHi mọi người, e đang làm đề tài về nhận dạng một số thao tác cơ bản của tay người, trước mắt e định làm trên các ảnh tĩnh, sau đó nếu ngon nghẻ thì sẽ làm trên webcome. Hiên e mới biết ngâm cứu thuật toán PCA, bác nào có quyển face recognition using eigenfaces and neural network không, share cho e với.\nMail của e là: tieudoan208@gmail.com  Thanks mọi người rất nhiềuHi anh, anh có thể giải thích em thắc mắc này không ạ.\nEm có một tập dataset có 58 samples, mỗi samples có 166200 features. Em dùng PCA trong matlab đề giảm số lượng features xuống . Sau khi giảm thì số features cao nhất mà em có thể đạt được là 57. Em không hiểu tại sao lại như vậy.hi em,Em dùng hàm nào trong matlab để tính PCA vậy?Hi anh,\nCảm ơn anh đã trả lời câu hỏi của em. Em dùng hàm pca ở trong thư viện cua prtools anh ạ.hi em,\ncó thể là ma trận dữ liệu của e không đúng nên prtools hiểu là e có 166200 samples và 58 features. Em thử transpose ma trận đó xem.Ngoài ra e có thể dùng hàm princomp (http://www.mathworks.fr/fr/help/stats/princomp.html) của matlab để kiểm tra kết quả.Minh Khởi\nChào bạn!\n– kết quả của output_args  là gì sao trong code không thấy bạn trả về giá trị cho nó?\n– Nếu mình cho pca cho 1 ảnh thì kết quả mình được gì? lợi ít gì? có cón nhìn thấy ảnh với hàm imshow được không?Chào a Vũ\nPCA có thể áp dụng để nhận dạng ra người và không phải người dựa vào silhouette đươc không a?…\nAnh có thể hướng dẫn em cách áp dụng đươc không ạ?..\nAnh có code C trong opencv về vấn đề này không a, em đang rất muốn tham khảo để hiểu về nó.\nCảm ơn a.a Vũ ơi, em thấy trong opencv đã định nghĩa sẵn lớp PCA rồi, vậy không biết nó có khác gì so với những nội dung anh đề cập đến ở đây không anh? nếu em muốn sử dụng PCA trong opencv thì có cần định nghĩa lại không anh?…Hi em,Em có thể xem ví dụ ở đây nhé: http://www.bytefish.de/blog/pca_in_opencv/Chào anh. Bài viết rất bổ ích mà em đang cần..\nEm đang làm 1 project nhận dạng ra những đối tượng đã lưu sẵn trong Tranning Set.Em định nhận dạng đối tượng đó  luôn bằng cách đo  khoảng các Eclid và tìm MIN nó trong trong tập\nTranning Set đa được chuẩn hóa theo PCA.Theo anh thì nó có khả thi và hiệu quả cao không ạ. ? Thanks !Hi em,\nTùy vào dữ liệu của em như thế nào. Nói chung dùng PCA thì feature thu được có vẻ hơi “low-level”, nên độ chính xác có thể không cao. Em có thể xem xét dùng Histogram of Gradients (HoG) để rút đặc trưng (thay vì PCA), và dùng SVM để phân lớp (thay vì khoảng cách Euclide, tương ứng kNN với k=1).Tuy nhiên còn tùy vào dữ liệu của e nữa.Nếu sử dụng Normed PCA mà độ lệch chuẩn sigma(i)=0 thì giải quyết thế nào hả anh? Liệu có còn sử dụng phương pháp này được nữa ko?Trong trường hợp đó anh thấy người ta hay dùng smoothing bằng cách cộng 0.01 vào variance:trong đó  là hàm tính variance của các cột trong ma trận X (giống hàm var() trong matlab).Cách này sẽ đảm bảo  và em có thể chia bình thường. Nhớ là nếu dùng smoothing thì em phải cộng 0.01 vào variance của tất cả các cột, chứ không phải chỉ cộng vào những cột có .Đương nhiên em có thể dùng giá trị khác thay vì 0.01, nhưng không nên chọn giá trị nhỏ quá hoặc lớn quá.Em cảm ơn anh rất nhiều vì vừa có cách giải quyết thỏa đáng lại vừa nhanh, đây là vấn đề em đang gặp thực tế ạ.chào anh. bài viet cua a thuc su rat bo ich.\nem muốn hỏi anh 1 vấn đề ạ: em dùng thuật toán PCA để phát hiện khuôn mặt thì cái ngưỡng thường dùng để quyêt định xem đó có phải là khuôn mặt hay không thường được chọn như thế nào ạ?? e ko hiểu chỗ đó.mong a giải đáps<α thì H là bức ảnh khuôn mặt ( do H đủ gần với không gian mặt). cai nguong a o day lấy giá trị như thế nào là hợp lí ??Hi em,\nCâu hỏi của em thuần tuý phụ thuộc vào mô hình máy học em đang dùng. Cụ thể anh không biết “s” được tính thế nào nên không thể trả lời chính xác được.Tuy nhiên anh đoán là em sử dụng thuật toán nearest neighborhood (s là khoảng cách Euclid hoặc đại loại vậy). Trong trường hợp đó thì em cần chọn  sao cho tỉ lệ nhận dạng trên tập validation set là cao nhất.Tuy nhiên em nên sử dụng các thuật toán mạnh hơn, chẳng hạn SVM.Em chào a ạ.\nEm nghe cô giáo em nói thì PCA làm giảm số chiều theo kiểu % chính xác. vậy em muốn kiểu như pca(X, %) thì em phải cài như thế nào ạ. em cảm ơn a ạNhư anh nói ở phần 2 bài này: https://phvu.net/2011/11/15/pca-intuition-maths/\nEm dựa vào “% chính xác” để chọn giá trị cho , trong đó  là số trị riêng lớn nhất còn giữ lại sau PCA.Chào anh.\nEm đang tìm hiểu về PCA. Anh cho em hỏi từ trị riêng và vector riêng có mối liên hệ nào với input (X) không? và nhận biết được thành phần nào quan trọng trong X?Chào Anh!\nEm cũng đang nghiên cứu về PCA, em có ví dụ cụ thể: các nhân tố: GPD, dân số, giới, tỷ lệ hộ nghèo, số trạm y tế, cơ sở hạ tầng,… trong các nhân tố đó Em có thể dùng phương pháp PCA để xác định nhân tố nào bị ảnh hưởng nhiều khi lũ về không Anh?\nEm rất vui khi nhận được hồi âm của Anh ^^. Chúc Anh thành công!%eigenvectors\n  eivec2 = zeros(size(eivec));\n  for i=1:size(eivec, 2)\n    eivec2(:, i) = eivec(:, size(eivec, 2) – i + 1);\n  end\n  eivec = eivec2;\n  display(‘Full eigenvectors:’);\n  display(eivec);anh cho em hỏi dòng lệnh trên có phải là sắp xếp eivec2 tăng dần không ạ? em đọc mà không hiểu lắm, nếu vậy em có thể thay bằng dòng eivec2 = sort(eivec,2); không anh?Gửi anh Vũ,Em là sinh viên năm cuối và đang làm đồ án về tìm hiểu phương pháp PCA để nhận biết nguồn gốc dầu. Từ hôm nhận đồ án em đã tìm kiếm tài liệu về PCA mà không tìm được đúng. Đến khi đọc được bài viết này của anh, em thực sự có động lực để làm tiếp đồ án cũng như em đã có cái nhìn tổng quát PCA là gì. Anh Vũ có thể cho em xin thêm tài liệu về PCA được không ạ? Em muốn tìm hiểu thêm về các bước giải quyết bài toán bằng PCA, từ tập hợp số liệu cho đến khi đưa vào không gian có số chiều ít hơn (làm thế nào để chuyển đổi số liệu từ không gian nhiều chiều đưa vào không gian có số chiều ít hơn) và đánh giá kết quả. Em rất mong nhận được hồi âm của anh! Chân thành cảm ơn anh!Anh có thể chỉ em cách làm PCA trên MiniTab 17 Không ạ 🙂em xem cái này chưa: http://support.minitab.com/en-us/minitab/17/topic-library/modeling-statistics/multivariate/principal-components-and-factor-analysis/perform-pca-with-varimax-rotation/#ad cho e hỏi dòng acc_eival = 0; hiểu là gì vậynó là accumulated eigen values.vâng ạ, a giải thích giúp em vòng lặp for ở dòng 30 được ko, em bối rối quáAnh cho em hỏi trong bước 2 xây dựng không gian mới,  ma trận hiệp phương sai cov(X,Y) sẽ được theo cách khác chứ không phải theo công thức (2), vì (2) thực chất là ma trận tự tương quan của các đặc trưng.\n– Trong phần code của anh, có đoạn\n% correlation matrix\n  V = Xhat’ * Xhat;\n  fprintf(1, ‘Correlation between features (columns) of normed data:’);\n  V\nnhư vậy tên bước với tính toán có vẻ không thống nhất a..\nRất mong anh giải đáp thắc mắc của em. Thank anh!Ah tại vì đối với các biến ngẫu nhiên đã chuẩn hoá (phương sai bằng 1) thì ma trận correlation chính là covariance.\nTrong bước 1 ta chuẩn hoá các đặc trưng bằng phương sai, nên correlation hay covariance cũng như nhau.Dạ vâng, em cảm ơn anh ạ. Chúc anh khỏeAnh ơi cho em hỏi 1 tí. Theo lý thuyết thì cái không gian mới là xắp xếp các vecto riêng theo trị riêng có độ lớn giảm dần. Và theo dòng code thì từ dòng 27 -> 46 là anh đang đảo ngược thứ tự các vecto riêng. Tức là dòng 23 khi xuất các vecto rieng nó đã theo thứ tứ tăng dần của các trị riêng sẵn, nên chỉ cần đảo ngược lại sẽ được giảm dần theo giống lý thuyết . Vậy cho em hỏi là vì sao khi xuất các trị riêng như trên, nó đã được sắp xếp theo tăng dần. Em test thử với 1 ma trận bất kỳ, thì em thấy trị riêng xuất ra không phải tăng dần. Nhưng nếu là trị riêng của tích (ma trận  và chuyển vị của nó) giống như là V= (X * chuyển vị X) như đoạn code ở trên, thì các trị riêng sẽ xuất hiện theo thứ tự tăng dần. Cho em hỏi là tại sao lại như v ạ, và các trị riêng khi tính = matlab thì sẽ xuất hiện theo thứ tự như thế nào ạCái này phụ thuộc vào cài đặt của matlab thôi em. Trong documentation của matlab không nói chắc chắn là kết quả của eig() đã được sort. Thông thường thứ tự của các eigenvalues trả về theo thứ tự tính được trong thuật toán.Nếu em muốn chắn chắn là các eigenvalues có thứ tự thì có thể dùng thêm hàm sort:dạ em cảm ơn anh. Vậy mình có thể thay đoạn code từ  dòng 27->36 = 3 dòng code trên của anh với [eival,I] = sort (diag(eival),’descend’); thì cũng được kết quả tương tự phải không anh. Tại em thấy đoạn for đó nó rối quáKhông, đoạn code từ 27->36 là để tính eigenInfo, chứa vài thông tin lặt vặt về các trị riêng để debug (và vì mục đích giáo dục 🙂 ) thôi, em không nhất thiết phải viết y như vây.dạ em cảm ơn anh, em tưởng anh đang xây dựng lại các vecto riêng theo thứ tự giảm dần trị riêng ^^. Em hiểu r ạ, cảm ơn vì những chia sẽ và sự nhiệt tình của anh, nó thật sự giúp em rất nhiều ^^Fill in your details below or click an icon to log in: You are commenting using your WordPress.com account. ( Log Out / Change ) You are commenting using your Twitter account. ( Log Out / Change ) You are commenting using your Facebook account. ( Log Out / Change ) You are commenting using your Google+ account. ( Log Out / Change )Connecting to %s Notify me of new comments via email. \n\nEnter your email address to subscribe to this blog and receive notifications of new posts by email.Join 144 other followers\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t",
          "relevence": "no"
        },
        {
          "url": "https://sachviet.edu.vn/threads/may-vecto-tua-support-vector-machine-svm-va-ung-dung-trong-viec-xac-dinh-tham-so-do-tham.56232/",
          "title": "Luận Văn Thạc Sĩ - Máy Vectơ Tựa (Support Vector Machine-SVM) Và Ứng Dụng Trong Việc Xác Định Tham Số Độ Thấm | Sách Việt Nam",
          "content": "\n\tDiscussion in 'Chuyên Ngành Công Nghệ Phần Mềm' started by nhandang123, Aug 3, 2016.\nDanh sách eBook Sách ViệtSeparate names with a comma.Tất cả sách số hóa giới thiệu tại diễn đàn được các thành viên sưu tầm từ nhiều nguồn miễn phí trên mạng, vì vậy rất có thể chưa có sự đồng ý từ tác giả. Cho nên, quí tác giả không cho đăng tác phẩm của mình vui lòng liên hệ BQT diễn đàn để được gỡ bỏ. Xin cảm ơn!Follow us",
          "relevence": "no"
        },
        {
          "url": "http://doc.edu.vn/tai-lieu/khoa-luan-bai-toan-phan-lop-van-ban-va-ap-dung-phan-lop-du-lieu-tai-chinh-ngan-hang-8312/",
          "title": "\r\n\tKhóa luận Bài toán phân lớp văn bản và áp dụng phân lớp dữ liệu tài chính ngân hàng - Tài liệu, ebook, giáo trình\r\n",
          "content": "Tài liệu - EbookThư viện tài liệu, ebook, đồ án, luận văn, giáo trình tham khảo cho học sinh, sinh viên\r\n                    \r\n\t\t\t.cs95E872D0{text-align:left;text-indent:0pt;margin:0pt 0pt 0pt 0pt}\r\n\t\t\t.cs5EFED22F{color:#000000;background-color:transparent;font-family:Times New Roman; font-size:12pt; font-weight:normal; font-style:normal; }\r\n\t\t\r\n\t\r\n\t\r\n\t\tMỤC LỤC LỜI MỞ ĐẦU .1Chương 1. BÀI TOÁN PHÂN LỚP VĂN BẢN.31.1. Khái niệm .31.2. Phân loại bài toán phân lớp văn bản .51.3. Mô hình phân lớp văn bản .51.3.1. Mô hình phân lớp văn bản.51.3.2. Quá trình xây dựng bộphân lớp văn bản .61.3.3. Quá trình tiền xửlý dữliệu .71.3.3.1. Phương pháp biểu diễn tài liệu.81.3.3.2. Phương pháp lựa chọn thuộc tính.101.3.4. Đánh giá .121.3.4.1. Đánh giá cho bài toán phân lớp.121.3.4.2. Đánh giá dựa vào độtương tự.14Chương 2. CÁC PHƯƠNG PHÁP PHÂN LỚP VĂN BẢN .172.1. Thuật toán K người láng giềng gần nhất .172.2. Mô hình cây quyết định (Decision Tree) .182.3. Thuật toán máy hỗtrợvector (SVM – Suport Vector Machine) .212.4. Mô hình Entropy cực đại .262.4.1. Định nghĩa nguyên lý entropy cực đại .262.4.2. Các ràng buộc và đặc trưng.272.4.3. Mô hình Entropy cực đại.272.3.4. Entropy cực đại cho phân lớp văn bản .28Chương 3. BÀI TOÁN PHÂN LỚP VĂN BẢN TÀI CHÍNH NGÂN HÀNG TIẾNG VIỆT.303.1. Một số đặc trưng của dữliệu tài chính ngân hàng trong tiếng Việt.303.2. Xây dựng một sốlớp trong lĩnh vực tài chính ngânhàng .313.3. Bài toán phân lớp văn bản tài chính ngân hàng trong Tiếng Việt .33 3.3.1. Phát biểu bài toán: .333.3.2. Phương pháp phân lớp.343.3.3. Mô hình của bài toán phân lớp văn bản tài chính ngân hàng.34Chương 4. THỰC NGHIỆM VÀ ĐÁNH GIÁ .384.1. Dữliệu và chương trình .384.2. Môi trường thực nghiệm.394.3. Thiết kếvà kết quảthực nghiệm.404.3.1. Thiết lập thông sốcho Entropy cực đại.404.3.2. Kết quảthực nghiệm .404.4. Đánh giá kết quảthực nghiệm .44KẾT LUẬN .45TÀI LIỆU THAM KHẢO.46Tài liệu Tiếng Việt .46Tài liệu Tiếng Anh .46DANH SÁCH CÁC TỪDỪNG.49\r\n                MỤC LỤC LỜI MỞ ĐẦU .1Chương 1. BÀI TOÁN PHÂN LỚP VĂN BẢN.31.1. Khái niệm .31.2. Phân loại bài toán phân lớp văn bản .51.3. Mô hình phân lớp văn bản .51.3.1. Mô hình phân lớp văn bản.51.3.2. Quá trình xây dựng bộphân lớp văn bản .61.3.3. Quá trình tiền xửlý dữliệu .71.3.3.1. Phương pháp biểu diễn tài liệu.81.3.3.2. Phương pháp lựa chọn thuộc tính.101.3.4. Đánh giá .121.3.4.1. Đánh giá cho bài toán phân lớp.121.3.4.2. Đánh giá dựa vào độtương tự.14Chương 2. CÁC PHƯƠNG PHÁP PHÂN LỚP VĂN BẢN .172.1. Thuật toán K người láng giềng gần nhất .172.2. Mô hình cây quyết định (Decision Tree) .182.3. Thuật toán máy hỗtrợvector (SVM – Suport Vector Machine) .212.4. Mô hình Entropy cực đại .262.4.1. Định nghĩa nguyên lý entropy cực đại .262.4.2. Các ràng buộc và đặc trưng.272.4.3. Mô hình Entropy cực đại.272.3.4. Entropy cực đại cho phân lớp văn bản .28Chương 3. BÀI TOÁN PHÂN LỚP VĂN BẢN TÀI CHÍNH NGÂN HÀNG TIẾNG VIỆT.303.1. Một số đặc trưng của dữliệu tài chính ngân hàng trong tiếng Việt.303.2. Xây dựng một sốlớp trong lĩnh vực tài chính ngânhàng .313.3. Bài toán phân lớp văn bản tài chính ngân hàng trong Tiếng Việt .33 3.3.1. Phát biểu bài toán: .333.3.2. Phương pháp phân lớp.343.3.3. Mô hình của bài toán phân lớp văn bản tài chính ngân hàng.34Chương 4. THỰC NGHIỆM VÀ ĐÁNH GIÁ .384.1. Dữliệu và chương trình .384.2. Môi trường thực nghiệm.394.3. Thiết kếvà kết quảthực nghiệm.404.3.1. Thiết lập thông sốcho Entropy cực đại.404.3.2. Kết quảthực nghiệm .404.4. Đánh giá kết quảthực nghiệm .44KẾT LUẬN .45TÀI LIỆU THAM KHẢO.46Tài liệu Tiếng Việt .46Tài liệu Tiếng Anh .46DANH SÁCH CÁC TỪDỪNG.49Các file đính kèm theo tài liệu này:41 trang | Lượt xem: 1227 | Lượt tải: 109 trang | Lượt xem: 877 | Lượt tải: 0125 trang | Lượt xem: 633 | Lượt tải: 032 trang | Lượt xem: 807 | Lượt tải: 1115 trang | Lượt xem: 1554 | Lượt tải: 886 trang | Lượt xem: 6315 | Lượt tải: 3337 trang | Lượt xem: 1074 | Lượt tải: 375 trang | Lượt xem: 1037 | Lượt tải: 3107 trang | Lượt xem: 841 | Lượt tải: 944 trang | Lượt xem: 1118 | Lượt tải: 9Copyright © 2014 Doc.edu.vn\r\n            \r\n            \r\n        ",
          "relevence": "no"
        }
      ]
    },
    {
      "query": "stop word là gì",
      "description": "Tìm hiểu về định nghĩa và thông tin của khái niệm stop word",
      "sites": [
        {
          "url": "https:hoidap.thachpham.com/chu-de/giup-minh-khac-phuc-loi-stop-word-trong-khi-seo-bai-viet.14884/",
          "title": "Tư vấn - Giúp mình khắc phục: lỗi stop word trong khi seo bài viết | Cộng đồng hỏi đáp - SEO - Hosting và Server",
          "content": "Đăng đồng hỏi đáp - SEO - Hosting và trợ đáp báo đàn không khích cầu hỗ trợ qua Đọc topic hãy đặt tiêu đề rõ ràng. Đọc vấn Giúp mình khắc phục: lỗi stop word trong khi seo bài luận trong 'Hỏi đáp SEO' bắt đầu bởi 1Thành gia thích:0Câu trả lời hay sử dụng Seo Yoats để hỗ trợ seo trong khi viết bài thì gặp hợp slug for this page a stop word, không rõ cái stop word là như nào và làm thế nào để bỏ nó bạn ai biết thì giúp mình với. Xin cám = || trai nhất viên gia thích:2,230Câu trả lời hay đọc trong link đó cũng ra mà, là các từ the, is, at, which, and on,... nhưng nếu bài viết bạn không có thì không cần quan tâm, cứ bỏ thích bài 1Thành gia thích:0Câu trả lời hay Tuấn đọc trong link đó cũng ra mà, là các từ the, is, at, which, and on,... nhưng nếu bài viết bạn không có thì không cần quan tâm, cứ bỏ to ơn bạn, những từ nhiều như vậy thì trong bài viết sao mình có thể tránh khi viết. Vậy những từ đó có ảnh gì tới seo bài viết không? Hay có ảnh gì khác không? Mình có nhất thiết phải tìm và xóa từ đó trai nhất viên gia thích:2,230Câu trả lời hay phải là có, mà là đặt những từ đó ở cuối câu, ta gọi là stop word bạn 1Thành gia thích:0Câu trả lời hay cũng bị lỗi như bạn đây là lần đâu tiên gặp lỗi vậy, URL ở trên stop word là từ nào vậy bạn Anh Tuấn, mình lo lắng nó sẽ ảnh đến seo từ trai nhất viên gia thích:2,230Câu trả lời hay ảnh gì hết bạn nha, đừng quan tâm tới nó 1Thành gia thích:2Câu trả lời hay word mình hiểu là mấy từ chung chung, ít giá trị, bỏ thì link ngắn lại tốt hơn, còn ko bỏ thì thôi. mấy từ mà ko phải từ khóa bỏ đi cũng thấy mấy web nổi tiếng nó để link chỉ có từ khóa chính chứ ko phải là cả tên tiêu đề Tuấn thích bài phải Đăng nhập hoặc Đăng ký để trả lời bài Ignored sẻ trang nàyLog in with tài khoản hoặc địa chỉ đã có tài khoản vào đây để đăng Mật khẩu của tôi đã quên mật khẩu? Duy trì đăng mới trên chứng chỉ SSL miễn phí từ Let’s Encrypt lên dẫn cài lại khi bị dính mã thế nào để xin cấp EV SSL chỉ số mở rộng) cho Doanh 0-day của 4.7.4 và nó có nguy hại Cài đặt trên máy chủ dùng chỉ IP có thật sự ảnh tới thứ hạng dụng nhiều giao diện trên một website đầu viết lại sau nửa năm tạm Sử dụng domain riêng cho website Tạo website con và cài cùng khu túi kỹ thuật cá độ siêu posted 21/9/17 lúc nhựa pvc posted 20/9/17 lúc dẫn cho posted chỉ giáo liên kết nội posted hợp posted ơnTự Tay ân các thành viên hoạt động có ích ở diễn đồng hỏi đáp - SEO - Hosting và trợ đáp báo đàn không khích cầu hỗ trợ qua Đọc topic hãy đặt tiêu đề rõ ràng. Đọc kết kiếm diễn viết gần kết viên tiêu viên đã đăng truy Profile kiếm Chỉ tìm trong tiêu gửi bởi thành cách tên bằng dấu hơn ngày: Search this thread only Search this forum only Hiển thị kết quả dạng Chủ kiếm hữu viết gần ViệtXenforo skin by chủLên đầu dụng mã nguồn Xenforo / NGINX với và VPS tại AZDIGI span{display: Add-ons by Brivium ™© Brivium { 0 0},_lightBoxUniversal: 1,_animationSpeedMultiplier: 10%,speed: rgb(0, 0, 0.6,loadSpeed: {bb_code:true,thread_view:true,message:true,best_answer:true,message_user_info:true,likes_summary:true,share_page:true,wf_default:true,login_bar:true,google:true,js  /brivium  /AutoLink  /click_handler.js?_v=dab01622:true},_cookieConfig: { path: /, domain: , prefix: ,_csrfRefreshUrl: dab01622,_noSocialLogin: Hủy giây phút %minutes% phút nay lúc qua, lúc %day% lúc Chủ Thứ Thứ Thứ Thứ Thứ Thứ Tháng tư,Tháng tám,Tháng mười mười CN,T2,T3,T4,T5,T6,T7,following_error_occurred: Có lỗi sau sảy xa với yêu cầu của The server did not respond in time. Please try Đang đăng Xem ảnh Show hidden content by Javascript = = = || = 0,hours: 1,type: {title: Your is Chúng tôi phát hiện ra bạn đang sử dụng phần mềm để ngăn chặn hiển thị quảng cáo. Tuy nhiên nếu các bạn muốn ủng hộ forum tốt hơn, xin hãy thêm domain vào danh sách của AdBlock và chúng tôi rất biết ơn vì điều giúp diễn đàn từ những việc nhỏ Đóng}};XenForo.loadJs('js/rellect/AdblockDetector/handler.min.js?v=1.1.2');",
          "relevence": "yes"
        },
        {
          "url": "https:ongxuanhong.wordpress.com/2016/12/03/sentiment-analysis-co-ban/",
          "title": "Sentiment cơ bản – Ông Xuân Hồng",
          "content": "Ông Xuân sẻ kiến thức và thông tin về Machine to learningExploratory Data trìnhPythonSparkRWekaKiến lý ngôn ngữ tự nhiên – Natural Mười Hai 3, Tám 3, Xuân phản – hay phân tích tâm lý của đối – là một chủ đề thách thức trong Machine Mọi thể hiện cảm nhận của mình thông qua ngôn ngữ tự nhiên có bản chất nhập mơ hồ đã gây không ít khó khăn cho việc xử lý cho máy tính hiểu. Chưa kể, họ sử dụng các cách chơi chữ, ẩn ý hay các kí hiệu như:), :(, giải bày cảm xúc của bài viết này, tôi sẽ sử dụng tập dữ data: Amazon Fine Foods việc áp dụng kĩ thuật Đây là cơ bản dành cho các bạn mới bắt đầu nghiên cứu vấn đề này. Ở đây, ta sẽ sử dụng tiếp cận Bag of đổi dữ liệu văn bản thành ma trận vector từ đó có thể đưa vào các mô hình phân lớp để code: xử lý dữ bạn dữ liệu về, sẽ có delmartianreview/helpfulness: Good Quality Dog I have bought several of the canned dog food and them all to be of good The product looks more like a stew than meat and it smells better. My is finicky and she better than đây, ta chỉ quan tâm đến các thông là mã sản là điểm đánh giá của dùng cụ là đánh giá chung về sản là phản hồi của dùng về sản tra thông tin file bằng dòng lệnh, tập dữ liệu này có hơn 500,000 dòng, hơn 56 triệu từ, và có kích MB.cat | wc 5116093 tiên, ta sẽ viết hàm để đổi định dạng plain text ban đầu thành file lần đọc ra 9 các thông tin thành một dòng ngăn cách với nhau bởi dấu summary và text ta sẽ làm sạch dữ bỏ các HTML bỏ các kí số, kí tự đặc biệt như dấu chấm, dấu lại vào file thời gian đổi 5 phút với cấu hình Macbook như bên reimport import numpy as pandas as pdfrom import import t2):timediff = t2 - t1mins = / 60)secs = % 60, +  mins and  + +  Remove HTML=# Remove=  ,return =with r) as f1, w) as f2:i = True:= 9))if process get info# remove special from summary and text= for line inif in line:+= + ,elif in line:+= + ,elif in = + ,+= review/text: in = + += text # print statusi += 1if i % 10000 == 0:print %d reviews % iprint  %s - %s % theo, ta tiến hành quan sát tập dữ liệu vừa mới = found Data dimensions: 4)print List features: 'score' First train[summary][0], |, review: Good Quality Dog Food | I have bought several of the canned dog food and have found them all to be of good product looks more like a stew than a meat and it smells is finicky and she this product better ta sẽ loại bỏ các từ không liên quan bằng danh sách trong tiếng Anh. Đây là những từ hay xuất hiện trong câu nhưng không góp phần vào quá trình học của hệ thống như hoặc mỗi dòng dữ liệu quan sát ta tiến hành loại bỏ như sang chữ và tách thành danh sách các từ riêng dụng để lọc ra danh sách các từ có ý lại vào file trình này mất 4 convert a raw review to a string of review # 1. Convert to lower case, split into =## 2. In Python, a set is much faster than# a list, so convert the stop words to a = 3. Remove stop = [w for w in words if not w in 4. Join the words back into one string by space,# and return the  file_name):t0 =# Get the number of reviews based on the column size= an empty list to hold the clean []# Loop over each i in If the index is evenly by 1000, print a (i + 1) % 10000 == 0:print Review %d of %d  % (i + 1,# Call our for each one, and add the result to the list of# clean str(dataset[productId][i])score = = = , + score + , + summary + , + text +  )print Writing clean train w) as review %  %s - Write file %s % dụng Bag of words để tạo of Words model xây dựng bộ từ vựng thông qua tập các văn bản, sau đó mô hình hóa từng văn bản (vector hóa) bằng cách đếm số lần xuất hiện của các từ xuất hiện trong văn bản đó. Ví dụ, ta có hai câu 1: “The cat sat on the 2: “The dog ate the cat and the hai câu trên, bộ từ vựng của chúng ta sẽ là{ the, cat, sat, on, hat, dog, ate, and }Để of words, ta sẽ đếm số lần xuất hiện của từng từ trong từng câu. Trong câu 1, xuất hiện 2 lần, các từ và đề xuất hiện 1 lần, nên ta có feature vector cho câu 1 làCâu 1: { 2, 1, 1, 1, 1, 0, 0, 0 câu 2: { 3, 1, 0, 0, 1, 1, 1, 1}Ta tiến hành vector hóa cho tập dữ liệu đã xử lýDo dữ liệu khá lớn, nên ta sẽ lấy 1000 dòng quan sát để thực Khi mọi thứ đã chạy ổn, ta sẽ chạy lại cho tất cả để rút ra mô hình cuối cùng cho hệ sẽ loại bỏ bớt các review có điểm = 3.0 để phân biệt rõ ràng hơn giữa phản hồi tích cực và tiêu cực đây, ta đánh giá một phản hồi là tích cực khi có điểm đánh giá >= theo, ta sẽ phân chia tập dữ liệu train và test theo tỉ lệ dùng để phát sinh vector Bag of cùng, ta sử dụng hàm để đổi thành ma trận làm input cho các hàm phân ra, bạn có thể gọi hàm để in ra danh sách các từ kèm tần suất xuất hiện của import import Take a look at the words in =print Words in vocab# Sum up the counts of each = For each, print the word and the number of times it# appears in the Words tag, count in count, = ignore all 3* = != 3]# = 4* or 5* = >= 4train, test = Creating the bag of = = = convert to = = = = có lẽ là bước mọi mong chờ nhất. Ta sẽ sử dụng tập các hàm phân lớp khác nhau và để chọn ra mô hình cho kết quả chính xác cao import import import import import import import RBFfrom import import import import SVCfrom import = Neighbors, Linear SVM, RBF SVM, Process, Tree, Random Forest, Neural Net, Naive Bayes, = [ warm_start=True), iterate over = {}for name, clf in Training  + name +  = ---------------------------print resultsprint sorting results and print name inprint name +  %0.3f % thúc quá trình huấn luyện và đánh giá, ta có thể thấy hàm phân lớp Process cho kết quả cao nhất là Process Tree 0.719Naive Bayes SVM Net SVM 0.773Random Forest accuracy: bạn đã làm theo đến đây và chạy trình thành công thì xin chúc mừng, bạn đã xây dựng cho mình một hệ thống cơ bản. Bạn có thể thử nhiều cách khác nhau để cải thiện độ chính xác của mô hình. Bạn có thể làm sạch dữ liệu theo nhiều cách khác như giữ lại các biểu cảm xúc hay các từ viết tắt phổ biến, thay đổi số từ vựng của Bag of Words để xem độ chính xác có thay đổi ra bạn cũng nên áp dụng kỹ thuật này lên các tập dữ liệu khác để tạo ra nhiều ứng dụng thú vị hơn ra nhà hàng nào review tốt nhất trong thành phố của bạn để ghé ăn cuối thập dữ liệu từ một trang web ăn uống và xây dựng cho mình ứng dụng phân tích review các món ăn.Thu thập dữ liệu từ các diễn đàn iPhone và Samsung thử đoán xem sản phẩm nào đánh giá cao hơn khi áp dụng hệ thống của dựng ứng dụng đánh giá phim, bài hát tự động từ những phản hồi của dụng cho tiếng Việt bằng cách sử dụng các thư viện do cộng đồng Xử lý ngôn ngữ tự nhiên Việt Nam đóng góp an open source for natural A Text word bài viết này, bạn có thể nghiên cứu thêm các vấn đề liên quan False có ảnh thế nào đến các ứng dụng như chẩn đoán bệnh hay lọc thư nào là bias, dữ liệu càng nhiều thì độ chính xác của mô hình có tăng lên kĩ thuật đánh giá mô hình div.u > div.u > sectionId:26942, o = Criteo) {var p = 'block', () {var o = } });} else 'none', o = Criteo) {var p = 'block', () {var o = } });} else 'none', lượt thích Đang án, bài Python – thư viện code →10 on Buingoc Mười Hai 6, 2016 lúc 2:11 thư viện nào để thế anh lượt lượt Xuân Hồng Mười Hai 6, 2016 lúc 2:13 emSố lượt lượt Một 13, 2017 lúc 7:35 ơi. A có thể cho em các file python không anh. Em copy về mà không chạy lượt lượt Văn Thế Anh Một 14, 2017 lúc 3:55 Hồng ơi cái để kiểm tra chéo các models này cài đặt thế nào anh? Em newbie đang nghiên cứu cái này. Em cảm ơnSố lượt lượt Xuân Hồng Một 14, 2017 lúc 9:11 tham khảo trang này nhé lượt lượt Bảy 10, 2017 lúc 4:24 đây anh lập trình bằng nào thế lượt lượt Xuân Hồng Bảy 10, 2017 lúc 9:42 pandas e lượt lượt Hung Bảy 28, 2017 lúc 7:33 lỗi anh, em có làm theo dẫn trên nhưng gặp lỗi takes exactly 1 (0 rõ để khởi chạy file py này bắt đầu từ hàm main như thế lượt lượt Xuân Hồng Bảy 28, 2017 lúc 7:48 này nhận một đối số là vector feature đó em.# convert to = = = = lượt lượt Hung Bảy 28, 2017 lúc 10:11 sángEm đã train rồi. Cảm ơn anhSố lượt lượt lời Hủy trả bình luận của bạn tại bạn điền thông tin vào ô dưới đây hoặc kích vào một biểu để đăng điện tử (bắt buộc) (Địa chỉ của bạn giấu (bắt web Bạn đang bình luận bằng tài khoản Bạn đang bình luận bằng tài khoản Twitter Bạn đang bình luận bằng tài khoản Bạn đang bình luận bằng tài khoản Google+ to %svar = input = 'input' = jQuery( );if ( in input ) jQuery( label' );} Expando Mode: start small, then on first click + text '#comment-form-subscribe' ).hide();comment.css( { } ).one( {var timer = 10 this { } 100 { timer ); n(); } '#comment-form-subscribe' ).slideDown();});}jQuery(document).ready( ); Notify me of new via email. Thông báo cho tôi bằng email khi có bài đăng mới. Tìm bài kiếm Không Thể Sống Một ngày mai tớ trở thành nổi Gặp tớ giữa bạn đừng ngại tớ nghe. Cứ nhìn tớ như ngày xưa bạn thấy. Đừng nên nghĩ tớ quá xa vời. Nếu một ngày tớ là kẻ ăn xin. Thì bố thí cho tớ vài kỷ niệm. Dù rất nhỏ nhưng đừng nên khinh tớ. Vì tớ vẫn là tớ của ngày xưa. Nếu ngày mai tớ chẳng biết về. Hãy chỉ giùm tớ con đi đúng đắn. Đừng để tớ sa chân nơi bùn nhớp. Lúc rút chân lên thì đã bẩn mất rồi. Nếu ngày mai tớ gặp bạn giữa Thì ta sẽ gặp nhau và cười nhé. Đừng nên trách và đừng nên đánh khi có một số lướt qua cuộc đời bạn và ngay tức khắc bạn nhận ra rằng sự có mặt của họ ý nghĩa như thế nào. Họ đã dạy bạn những bài học, đã giúp bạn nhận ra giá trị của chính mình hoặc trở thành con mà bạn từng mơ ước. Có lẽ bạn sẽ không biết những con này từ đâu đến ( bạn cùng phòng, hàng xóm, vị giáo sư, bạn mất liên lạc từ lâu hay thậm chí là một hoàn toàn xa lạ ). Nhưng khi bạn thờ ơ với họ, hãy nhớ rằng trong từng khắc họ sẽ ảnh rất sâu sắc đến cuộc đời bạn. sống không hề làm khó bạn mà chính bạn đã tự làm khó mình bằng cách đặt ra những câu hỏi đại loại như: sao lại thế Tại sao lại thế Không ai có thể trả lời giúp bạn mà chính bạn phải tự đi tìm lời giải đáp cho mình. Hãy luôn nghĩ về khác khi nghĩ đến bản thân, chắc chắn lúc đó bạn sẽ hài lòng với câu trả lời của chính mình. Và đừng quên luôn đặt câu hỏi: đã làm gì?” khi tự hỏi: đã nhận gì?” nhé! Tôi tin là bạn sẽ thành ra trên đời, con luôn cần có nhau để giải quyết rất nhiều vấn đề. Nhờ những lúc qua lại giúp đỡ đó mà hoá giải bao muộn nảy sinh các tình cảm, cùng trải biết bao điều thú vị, hiểu biết thêm nhiều tri thức mới. Đây chính là cuộc thế hãy trân trọng những đang “làm bên cạnh bạn nhé! Đừng để đến khi mất đi rồi mới nghẹn ngào lại bảo “giá như” GS. Thích Bụt dõi Email khi có bài viết buộc)Top bài viết việc với Spark – Truy vấn nâng cao Chín 15, kê ứng dụng 3: Các vấn đề trong Chín 14, kê ứng dụng 2: Suy luận Chín 12, kê ứng dụng 1: Quan sát Chín 7, 2017AI, Machine Deep phân biệt như thế nào Chín 4, – Bài toán rút trích thông tin trong Tám 28, đầu nghiên cứu big data từ đâu và như Tám 3, Science – Mỏ vàng của Kỉ Tám 3, 2017SMA 2017 – Lý ra Sáu 17, 2017SMA 2017 – Lý tập thô (P4) – Rút trích luật Sáu 9, 2017Big Data Chia sẻ Data Science Deep Dự án Data Getting and data Kiến thức Lập trình Machine Python R Spark Toán Weka Xử lý ngôn ngữ tự nhiên - Natural (NLP) This việnTháng Chín Tám Sáu Năm Ba Một Mười Hai Mười Một Mười Chín Tám Bảy Năm Tư Ba Hai Một Mười Hai Mười Một Mười Chín Tám Bảy Sáu chuyên Data  (7)   Deep Data and án  (9)Kiến lý ngôn ngữ tự nhiên – Natural trình  (46)   Python  (13)   R  (28)   Spark  (3)   Weka  (4)/* {var = cat {if ( ].value > 0 ) = + = ]]> Mười Hai luận mới xu ly du lieu |… on Tiền xử lý dữ liệu (Horse on Các thuật ngữ trong Xử lý việc với Spark D… on Làm việc với Spark kê ứng dụng 2:… on Thiết kê ứng dụng 2:… on kê ứng dụng 2:… on kê ứng dụng 2:… on A/B testing kê ứng dụng 2:… on Kiểm kê ứng dụng 2:… on Xuân Hồng on Việt Phạm on AI, Machine Deep Khôi on Xuân Hồng on Con học tập Machine on Con học tập Machine Xuân Hồng on Trải tập dữ liệu Big thứcChia ánAboutBlog tại */var WPGroHo = ]]> and attach to all $ ) {if (typeof === ( typeof !== ) = hash, id ) hash, id = 'body', );});/* */var = In  u2026,submittingText:  u0110  u0103ng lu  u1eadn,connectingToText:Connecting to   u0111ang %2$s,logoutText:  u0110i ra nh  u1eadp,connectURL:https:  /  /ongxuanhong.wordpress.com  /public.api  /connect  /?action=request,logoutURL:https:  /  /ongxuanhong.wordpress.com  /wp-login.php?action=logout&_wpnonce=5bbdcd0015,homeURL:https:  /  /ongxuanhong.wordpress.com  /,postID:10171,gravDefault:monsterid,enterACommentError:Please enter a vui ch  u1ec9 email email vui b  u1ea1n,gravatarFromEmail:This picture will show you leave a Click to tin   u0111  u1ed5i,changeAccount:Change ]]> */Post toHủy = 'ready ).on( {if ( !== typeof ){  If there's another sharing window open, close = 'href' ), false;});});var 'ready ).on( {if ( !== typeof ){  If there's another sharing window open, close = 'href' ), false;});});var 'ready ).on( {if ( !== typeof ){  If there's another sharing window open, close = 'href' ), false;});});var 'ready ).on( {if ( !== typeof ){  If there's another sharing window open, close = 'href' ), false;});});(function(){var corecss = = = ( ) rel, type, href, );} else = = document.getElementById(syntaxhighlighteranchor) );var = ( ) rel, type, href, );} else = = document.getElementById(syntaxhighlighteranchor) = '+ expand = = = find brush for: = 'Brush for option: = = scroll $ ) {$( ).on( );} );/* */var = t  u1ea3i...};/* ]]> ** */var = H  u1ed3ng,siteURL:http:  /  /ongxuanhong.wordpress.com,icon:<img alt='' height='50' type=  hidden   name=  _wpnonce     />,referer:https:  /  /ongxuanhong.wordpress.com  /2016  /12  /03  /sentiment-analysis-co-ban  /,canFollow:1,feedID:5686260,statusMessage:,customizeLink:https:  /  /ongxuanhong.wordpress.com  /wp-admin  /customize.php?url=https%3A%2F%2Fongxuanhong.wordpress.com%2F2016%2F12%2F03%2Fsentiment-analysis-co-ban%2F,postID:10171,shortlink:http:  /  /wp.me  /p2Rzzn-2E3,canEditPost:,editLink:https:  /  /wordpress.com  /post  /ongxuanhong.wordpress.com  /10171,statsLink:https:  /  /wordpress.com  /stats  /post  /10171  /ongxuanhong.wordpress.com,i18n:{view:View d  u00f5i,following:Following,edit:Edit,login:  u0110  u0103ng k  u00fd,customize:T  u00f9y this theme: shortlink,copied:Copied,followedText:New posts from this site will now appear in your <a r  u1ed9ng,unfoldBar:Expand this k  u00ed theo trang post in me your email 405 other have a <a in k  u00ea}};/* ]]> ** */var = lu  u1eadn,post_comment:G  u1eedi a B  u00ecnh full size <span class=  photo-size-times  >  u00d7<  /span>{1}<  /span>,no_comment_text:Please be sure to submit some text with your provide an email address to provide your name to but there was an error posting your Please try again comment was comment is in Speed,focal_length:Focal for=  email  >Th  u01b0 <input name=  email   jp-carousel-comment-form-text-field   for=  author  >T  u00ean <input name=  author   jp-carousel-comment-form-text-field   for=  url  >Trang <input name=  url   jp-carousel-comment-form-text-field     /><  /fieldset>,reblog:Reblog,reblogged:Reblogged,reblog_add_thoughts:Add your here... Reblog,stats_query_args:blog=42320789&v=wpcom&tz=7&user_id=0&subd=ongxuanhong,is_public:1,reblog_enabled:};/* ]]> ** */var = = ]]> ** */var = ]]> */ {try{if ( in {if {var jl = var s like = || [];_stq = || {'crypt':'UE5XaGUuOTlwaD85flAmcm1mcmZsaDhkV11YdTdvUG14Q2VDQTR4LlUsLi82dU1mai9BMnMzSitqWkpxbS9HK0NIb2t1W0krTU9WQ3JpZE43RjVQaURGXXM5d0RMJSxSbHh6JUxXOFdhLV9iSk9TYTZCZ3lOSUlfJUhubFBpSjRTVzNFP0R3XU9oTURLTkwvSE1XODNPaXhSSldxQWxsWEF3eC9SRE4rbUViU1VyRHkxdEtWZGs9SFJmcExkVTglLGtXOU5oRzk/dXp8UG9fblJORlJCZTV2QS4tcjlbRm0ycy9wZUdmUFR5SXdSVnJMRDRKWU9LLnJoSlF4U1VaOVFjMUs1ZmdNTGRZZ0p5ZD9HLGcsLk4mTTNzJlExV3NpeE58aUVDQTV4MmU0L35MQzNtaHBiYjAsMHh+amZ8LXFoZzZEL1BZRVdpWjlQT2o3MFd+dWdu'}]);_stq.push([ '10171' ]);if ( === typeof ) = ;if( false !== += &x_ + + '=' + false !== += &x_ + + '=' + += &x_ + + '=' +  != ) {new = + + + + Math.random();}}",
          "relevence": "yes"
        },
        {
          "url": "http:tonngokhong.com/tag/stop-words-la-gi",
          "title": "stop words là gì - Tôn Ngộ Không",
          "content": "Skip links Skip to primary Skip to content Skip to primary Ngộ Ngộ Không có 72 phép thần thông biến thuậtstop words là gìSpin bài viết và Stop words đang gây cản trở SEO July 5, 2016 by admin Leave a Comment Một trong những thủ thuật SEO của dân Việt là rất hay spin bài viết và sử dụng rất nhiều Stop Words. Chính điều này gây cản trở rất lớn đến thứ hạng SEO và kết quả trên bài viết là gì ?Spin nghĩa là trộn, lặp, xào, viết các từ ngữ có điểm chung, có ý nghĩa tương tự để tạo nên các bài viết mang tính duy nhất (unique nhằm qua mặt bộ máy tìm kiếm. Ưu điểm của Spin chỉ duy nhất là nội dung hay số tứ ngữ bài viết đó sẽ trở thành Duy nhất, và đánh lừa thuật toán Panda của Google. điểm lớn của spin là ngữ pháp, chất câu từ lủng củng, không rõ ràng và gây khó hiểu cho dụng spin là một dạng SEO. Tuy nhiên, điều này lại đang phổ biến ở thị SEO do cách thức để tạo ra một bài SPIN quá dễ dàng : Chi phí thấp (thậm chí là 0 ), nội dung của khác có sẵn để dùng spin, tạo nhanh 1 bài báo chí cũng hay sử dụng spin để viết bài, xào lại bài của khác, nhưng ở cấp độ cao hơn khiến cho đọc không nhận ra đó là bài xào lại. Và các cũng ngụy biện rằng họ là một chính hiệu chuyên tạo ra các bài viết hay và họ khẳng định họ là những cung cấp giải pháp nội dung tốt nhất. Cái ranh rới giữa spin bài và rất mong manh và khó nhận ra với 1 duyệt non kinh Các công ty hay thuê đội ngũ sản xuất nội dung bên ngoài để lấy số content bù đắp cho tổng nội dung 1 trang hoặc vệ tinh. ta lầm những bài viết đó sẽ đem lại một kết quả tốt và họ sẵn sàng chèn link vô tội vạ, anchor text đủ các loại để hòng thống trị thứ hạng trên các kết quả tìm kiếm. Nhưng họ quên mất rằng, thuật toán Panda của Google đã nâng cấp lên mức độ cực kì tinh vi. Việc sử dụng các từ đồng nghĩa, các câu lặp, các đoạn mở đầu, thao thao bất tuyệt nhằm cố kéo dài nội dung, đã mang đến một bất lợi cho website về mặt Chất từ khóa. Các lập trình viên của Google rất hiểu việc này nên thuật toán của họ đã nâng cấp và lọc ra những Stop Words, sự tương quan trong ngữ cảnh và ngữ pháp của câu từ. Nếu bạn để ý, ngày xưa, bộ máy phiên dịch của Google rất lởm, nhưng đến nay họ đã khắc phục rất nhiều về ngữ pháp tiếng Việt khi dịch sang tiếng Anh. Đây là kết quả của những nghiên cứu không ngừng nghỉ nhằm cho ra 1 sản phẩm tốt nhất – Một kết quả tìm kiếm phù hợp với dùng words là gì ?Để tiết kiệm không gian lưu trữ và gia tăng tốc độ tìm kiếm, các công cụ tìm kiếm sẽ không ghi nhận lại những từ quá phổ biến, quá chung chung và những từ này gọi là stop Kiệm Không GianXem ví dụ sau:The way to the school is long and hard when walking in the xuất hiện 3 lần. Để tiết kiệm không gian, các SE có thể thay thế nó bởi một “ký tự đánh Ví dụ trên sau khi thay thế sẽ như sau:* way to * shool is long and hard when walking in * giải thích đơn giản này có lẽ đã giúp bạn hiểu rõ vấn đề (tuy nhiên “ký tự đánh đề cập ở trên không nhất thiết phải là ký tự *). Tuy ví dụ trên lưu trữ theo nhưng vẫn không làm mất đi ý nghĩa và sự liên Tăng Tốc Độ Tìm Search Engines đều lưu tất cả các từ trên trang web của bạn nhưng không nhất thiết chúng sẽ tìm kiếm tất cả các từ đó khi có truy vấn từ sử dụng. Xem ví dụ bên piano ta sẽ đơn giản hóa mô hình tìm kiếm như sau – các SE sẽ tìm 3 lần – Đầu tiên chúng sẽ tìm xem có từ không, sau đó chúng tìm tiếp từ và cuối cùng là Nhưng rất có thể, các SE chỉ tìm 2 từ cuối là đủ để xác định sự liên quan có trong nội dung một trang web và điều này chắc chắn sẽ làm gia tăng tốc độ tìm tự như câu cú tiếng Việt, tôi giả sử bạn viết 1 đoạn câu như sau :Thực ra, đây có thể là một trận đấu tốt của Real có thể nhận thấy, các stop words của câu trên quá những từ như : Thực ra, có thể là. Là những từ Google sẽ ưu tiên loại bỏ trong Những từ Google chú ý : Trận đấu tốt, Real thế, thay vì bạn spin bài như cách trên, bạn hãy viết chuẩn lại : Real Madrid có một trận đấu tốt. – Một câu chuẩn ngữ pháp, đầy đủ ý nghĩa và loại bỏ Stop cứ làm như vậy cho đến hết tất cả các đoạn câu, đừng bao giờ mong mỏi số từ trong bài viết nhiều lên là bài viết đó chất lên. Nếu bạn sử dụng đúng ngữ pháp, loại ra các câu từ nhàm chán thì dù bài viết có 500 từ, bạn vẫn sẽ Google ưu tiên trên kết quả tìm kiếm ở rất nhiều từ khóa mở vì sử dụng các Stop words để kéo dài nội dung hay để spin bài viết cho vừa ý khách hàng, hãy suy nghĩ để tạo ra một bài viết tổng hợp có nhiều góc nhìn và ví dụ cụ thể. Điều này làm đọc cảm thấy thỏa mãn vì tìm thông tin hữu Under: SEO Tagged With: spin bài viết, stop words là cách SEO cho trang tin tức – Tự nhiên hóa là sức lý 301 hay 404 hay 410 trong SEO đặc thù ?Có nên tuyển nhiều SEO cho công ty ?Spin bài viết và Stop words đang gây cản trở SEO[5] Lên top 1,2,3 Youtube trong vòng 3 phút không mua views, on Phân loại ứng viên SEO chỉ với 2 câu hỏi trong phỏng Standa on Phân loại ứng viên SEO chỉ với 2 câu hỏi trong phỏng on Phong cách SEO cho trang tin tức – Tự nhiên hóa là sức Tran on Phong cách SEO cho trang tin tức – Tự nhiên hóa là sức on [5] Lên top 1,2,3 Youtube trong vòng 3 phút không mua views,= ·Genesis · · Log in= = = = = */var tocplus = ]]> */",
          "relevence": "yes"
        },
        {
          "url": "https:youcannow.vn/stop-words-trong-seo-la-gi",
          "title": "Stop words trong SEO là gì?",
          "content": "ClosejQuery('a.sidr-class-close-this-menu').click(function(){jQuery('div.sidr').css({'right': chúng ảnh tổng quan trung ngũ tư vấn học hiện thực hành quay thực hành kỹ xảo âm học in học in kiến học hệ với chúng khai học in quà tặng - học MC cơ bản - chuyên học MC đám học MC học chữa ngọng L & NKhóa học chụp ảnh cơ học chụp ảnh sản học cơ học nâng họa - Chỉnh sửa ảnh - Video - chức sự tin tổng PR - cầu tư hệ - Đăng ký  Tin sinh khóa học MC cơ bản học Thứ 7, Chủ Nhật khai giảng 16 tháng 09 năm sinh khóa học MCkhai giảng 13 tháng 09 năm học Make up - Trang điểm cá sinh khóa học MC Nhí, kỹ năng MC cho trẻ em cơ bản tại You Can sinh khóa học Công nghệ in Quà sinh khóa học MC đám giảng words trong SEO là words là words là những từ mà công cụ tìm kiếm cho là những từ quá phổ biến, chung chung không có ý nghĩa cụ thể. Bên cạnh đó để gia tăng tốc độ tìm kiếm ,tiết kiệm không gian lưu trữ giúp việc tìm kiếm dễ dàng cụ tìm kiếm sẽ không ghi nhận những từ cho là stop Kiệm Không ví dụ way to the school is long and hard when walking in the xuất hiện 3 lần. Để tiết kiệm không gian, các SE có thể thay thế nó bởi một “ký tự đánh Ví dụ trên sau khi thay thế sẽ như way to * shool is long and hard when walking in * giải thích đơn giản này có lẽ đã giúp bạn hiểu rõ vấn đề (tuy nhiên “ký tự đánh đề cập ở trên không nhất thiết phải là ký tự *). Tuy ví dụ trên lưu trữ theo nhưng vẫn không làm mất đi ý nghĩa và sự liên Tăng Tốc Độ Tìm lưu tất cả những từ trên website của loại bỏ stop words giúp việc tìm kiếm trở nên nhanh chóng và chính xác : The ta sẽ đơn giản hóa mô hình tìm kiếm như sau – các SE sẽ tìm 3 lần – Đầu tiên chúng sẽ tìm xem có từ không, sau đó chúng tìm tiếp từ và cuối cùng là Nhưng rất có thể, các SE chỉ tìm 2 từ cuối là đủ để xác định sự liên quan có trong nội dung một trang web và điều này chắc chắn sẽ làm gia tăng tốc độ tìm Sách Stop Words Tiếng Sách Stop Word Tiếng                TannXem Core là gì - ngữ scheme là máy tìm kiếm của Google hoạt động thế bằng chiến lược xây dựng sai lầm khi xây dựng bước để từ khóa Seo lên top đào học viên Học viên khóa học In nhiệt Quý IV năm 2016 Những học viên in nhiệt cuối cùng của năm 2016 vừa kết thúc khóa học đáp về chữa ngọng L N ngọng dấu và nguyên âm E cho ngọng có phải đã trở thành 1 nét văn hóa, 1 nếp sinh hoạt quá quen sinh khóa học chữa nói ngọng dấu ngã dấu hỏi dấu sắc chữ e cho lớn Tại Việt Nam do đặc trưng vùng miền nên có nhiều bạn bị ngọng dấu hỏi viên In nhiệt You Can Now quý III năm 2016Ba tháng vừa qua là 3 tháng liền kề mùa quà tặng, nhiều học viên đã the plugin itemContainer,perPage:: : 1,links : thêmHọc viên khóa học In nhiệt Quý IV năm 2016 Những học viên in nhiệt cuối cùng của năm 2016 vừa kết thúc khóa học đáp về chữa ngọng L N ngọng dấu và nguyên âm E cho ngọng có phải đã trở thành 1 nét văn hóa, 1 nếp sinh hoạt quá quen sinh khóa học chữa nói ngọng dấu ngã dấu hỏi dấu sắc chữ e cho lớn Tại Việt Nam do đặc trưng vùng miền nên có nhiều bạn bị ngọng dấu hỏi viên In nhiệt You Can Now quý III năm 2016Ba tháng vừa qua là 3 tháng liền kề mùa quà tặng, nhiều học viên đã thêm Khóa học thiết kế đồ học nâng nâng tâm đào tạo You Can Now học làm phim9 bước cơ bản trong làm bước cơ bản trong làm phim ngắn là thiết bị, kịch bản, phân cảnh, phục những lỗi cơ bản khi quay với những bạn mới bắt đầu làm quen với máy quay thì mắc lỗi nhãn phim ngắn về thành phố đáng sống nhất Việt Nam Phim ngắn về thành phố đáng sống nhất Việt Nam cho thấy một TP Đà Nẵng thức ngàng với những bức ảnh và sau khi chỉnh để chụp ảnh sáng quyết giúp bạn có bức ảnh đẹp hơn trong giờ tâm đào tạo Chỉ: 391 Chinh -(Gần Ngã Tư Sở) Thanh Xuân - Hà- - -Hoặc xã để chúng tôi giúp bạn làm tâm đào tạo Chỉ: 391 Chinh -(Gần Ngã Tư Sở) Thanh Xuân - Hà - có trình đặc biệt giảm 20% học phí cho học viên đăng ký khai giảng 7 ngày. Góc đào tạo - Góc học học thiết kế đồ họa - Chỉnh sửa học nhiếp học PR - học quản lý và tổ chức sự thức về phim, quay dựng - kỹ thức đồ họa - chỉnh sửa thức Digital PR - thức - Kỹ thuật nhiếp thức tổ chức sự tin tổng thức in quà học học in nhiệt hình học làm thức MC - dẫn học pha chế Chinh/HN - Tel: Xem Bản với chúng với chúng 0px;z-index: {float: {float: hidden;position: 0px;}.facebook-inbox-tab {top: -40px 0px 0px 250px;border: 1px 0px 0px 0px #ffffff;}(function (d, s, id) {var js, fjs = = js.id = = = ",
          "relevence": "yes"
        },
        {
          "url": "http:xltiengviet.wikia.com/wiki/VnTokenizer",
          "title": "VnTokenizer | Xử lý tiếng Việt Wiki | FANDOM powered by Wikia",
          "content": "lý tiếng Việt new Changed lý tiếng Việt sách phần mềm tự sách sản phẩm sách stop lục tách nguyênTài sách stop Việt NamHọ Việt liệu Lưu Tuấn blog từVnTokenizer Xây đồ thị biểu diễn tất cả các khả năng tách từ của một câu (mỗi đi từ đầu đến cuối câu tương ứng với một cách tách từ) Số cách quá lớn, tăng theo hàm mũ của số âm tiết trong câu nên phải lọc ra một số nhỏ ứng cử viên để đánh giá. Cụ thể là chọn những cách tương ứng với đi ngắn nhất. Tính xác suất của từng cách theo một mô hình ngôn ngữ, ở đây là unigram hoặc bigram. Chọn cách có xác suất cao from blocker is a site that makes money from We have a for viewers using ad is not if made further Remove the custom ad blocker rule(s) and the page will load as từAdd () () Xử lý tiếng Việt hình ngôn of of SitemapCommunityCommunity ScoreHelpCan't find a you love? Create your own and start a your fandoms with you and never miss a lý tiếng Việt Wiki is a FANDOM Content is under 2Sept ",
          "relevence": "no"
        },
        {
          "url": "http:xltiengviet.wikia.com/wiki/Danh_s%C3%A1ch_stop_word",
          "title": "Danh sách stop word | Xử lý tiếng Việt Wiki | FANDOM powered by Wikia",
          "content": "InDon't have an a lý tiếng Việt new Changed lý tiếng Việt sách phần mềm tự sách sản phẩm sách stop lục tách nguyênTài sách stop Việt NamHọ Việt liệu Lưu Tuấn blog sách stop word sách này tạo bằng cách lọc ra những từ chức năng hoặc ít ý nghĩa đáng chú ý từ những danh sách những từ phổ biến nhất trong bộ ngữ liệu 2 triệu âm không phải bộ dữ liệu chuẩn hay phù hợp với mọi ứng dụng. Bạn có thể thêm bớt các từ tùy ý cho phù hợp với nhu Xem thêm Edit Danh sách stop word Github: Tham khảo Edit↑ from blocker is a site that makes money from We have a for viewers using ad is not if made further Remove the custom ad blocker rule(s) and the page will load as () () Xử lý tiếng Việt hình ngôn of WikisFollow of SitemapCommunityCommunity ScoreHelpCan't find a you love? Create your own and start a your fandoms with you and never miss a lý tiếng Việt Wiki is a FANDOM Content is under 2Sept sách stop nguyên],wgBreakFrames:false,",
          "relevence": "no"
        },
        {
          "url": "http:xltiengviet.wikia.com/wiki/N-gram",
          "title": "N-gram | Xử lý tiếng Việt Wiki | FANDOM powered by Wikia",
          "content": "lý tiếng Việt sách phần mềm tự sách sản phẩm sách stop lục tách nguyênTài sách stop Việt NamHọ Việt liệu Lưu Tuấn blog liệuN-gram niệm về là tần suất xuất hiện của n kí tự ( hoặc từ ) liên tiếp nhau có trong dữ liệu của n = 1 và tính trên kí tự, ta có thông tin về tần suất xuất hiện nhiều nhất của các chữ cái. Điều này ứng dụng để làm các phím hay xuất hiện nhất sẽ ở những vị trí dễ sử dụng n = 2, ta có khái niệm bigram. Ví dụ với các chữ cái tiếng Anh, là các cặp kí tự hay xuất hiện nhất. Ngoài ra, ta có thể biết thêm rằng sau kí tự ‘q’ thì phần lớn đều là kí tự n = 3, ta có Nhưng vì n càng lớn thì số hợp càng lớn nên ta chỉ sử dụng với n = 1,2 hoặc đôi lúc là 3. Ví dụ với các kí tự tiếng Anh, tiếng Anh sử dụng 26 kí tự, vậy với n = 1 thì số hợp là 26, n = 2 thì số hợp là 26^2 = 676 hợp, n = 3 có 17576 sử dụng nhiều trong việc phân tích hình thái (từ, cụm từ, từ loại) cho các ngôn ngữ khó phân tích như tiếng Việt, tiếng Nhật, tiếng Trung, … Dựa vào tần suất xuất hiện cạnh nhau của các từ, ta sẽ tính cách chia 1 câu thành các từ sao cho tổng bigram là cao nhất có thể. Với thuật giải phân tích hình thái dựa vào trọng số nhỏ nhất, ta sử dụng n = 1 để xác định tuần suất xuất hiện của các từ và tính trọng số. Tham khảo Edit Khái yếu về corpus - Lưu Tuấn from blocker is a site that makes money from We have a for viewers using ad is not if made further Remove the custom ad blocker rule(s) and the page will load as () () Xử lý tiếng Việt hình ngôn of WikisFollow of SitemapCommunityCommunity ScoreHelpCan't find a you love? Create your own and start a your fandoms with you and never miss a lý tiếng Việt Wiki is a ",
          "relevence": "yes"
        },
        {
          "url": "http:vnseosem.com/threads/phan-biet-filter-word-va-stop-word-trong-seo.50535/",
          "title": "Phân biệt Filter word và Stop word trong SEO | Diễn Đàn SEO & SEM: Công Nghệ & Thông Trực Tuyến !",
          "content": "Đăng Đàn SEO & SEM: Công Nghệ & Thông Trực Tuyến !Trang ENGINE - thuật - Chiến lược biệt Filter word và Stop word trong luận trong 'Thủ thuật - Chiến lược SEO' bắt đầu bởi soát ung cx =var gcse === true;= == ? : ++ cx;var s 267 Thành đến nay, chúng ta vẫn luôn bị nhầm lẫn giữa 2 khái niệm này. Đa phần mọi hiện đều đang nghĩ rằng stop word là filter word. Tuy nhiên 2 khái niệm này lại hoàn toàn khác word là word là nhứng từ chung chung dùng để kết nối các ý trong một câu ví dụ như: nhưng, thì, là, mà, khi mà, ở gọi là lọc) vì chúng sẽ bị tất cả các công cụ tìm kiếm lọc đi trong quá trình tìm kiếm nội word bị bỏ qua bởi Google và công cụ tìm kiếm lớn khác cũng như các số và chữ cái đơn, bởi vì chúng có xu làm chậm lại việc tìm kiếm thông tin và không cải thiện kết quả tìm này có ý nghĩa gì với trang web của các bạn đã biết, công cụ tìm kiếm xem xét các từ khóa có trong tiêu đề, trong nội dung phần của phần body trên trang như là một tiêu chí xếp hạng quan vì vậy, hãy cẩn thận khi bạn chọn từ khóa của bạn đặt vào tiêu đề và các phần còn lại của trang web. Nếu tiêu đề trang web của bạn có chứa quá nhiều các filter word, sẽ có khả năng website bạn sẽ không xuất hiện trên SERPs. Việc sử dụng nhiều filter word trong các vị trí quan trọng sẽ rất lãng phí bởi bạn hoàn toàn có thể dành vị trí của chúng cho các từ khóa phụ, tránh làm loãng tiêu chí, nhiều công cụ tìm kiếm đã đề nghị quản trị viên phải hạn chế số các từ filter word trong tiêu đề và trong các thẻ số từ có thể khiến bạn có bị BAN trên các công cụ tìm khi các filter words có thể pha loãng hiệu quả của việc tối ưu onpage, thì một số từ thậm chí còn có thể khiến website bạn bị BAN (bị cấm) trên các công cụ tìm kiếm. Những từ này gọi là các từ bị cấm - stop từ hoặc cụm từ có chứa nội dung khiêu dâm hoặc bạo lực là những stop word phổ biến nhất trong SEO. Tuy nhiên, một số công cụ tìm kiếm đã xây dựng lên một danh sách stop word tùy chỉnh để trừng phạt các trang web nhồi nhét từ khóa quá đà. Một số các trang web cố tình nhồi nhét từ khóa trên trang, nhằm mục tiêu lên top với từ khóa này, thậm chí có khi nó còn chẳng liên quan gì đến nội dung của nhiên, dù sớm hay muộn công cụ tìm kiếm cũng sẽ tìm ra các trang web này và họ sẽ áp dụng một hình phạt nào đó lên trang. Cũng có một số hợp chủ quản website vô tình sử dụng một từ khóa là từ khóa dài) trên trang nhiều lần, và thật không may, họ cũng đã bị coi là spam công cụ tìm là lý do tại sao phân tích từ khoá và mật độ từ khóa phù hợp là rất quan trọng cho một trang web thành công. Nếu mật độ từ khóa là quá thấp, bạn sẽ không có một thứ hạng tốt, nếu nó quá cao, trang web của bạn có thể bị buộc tội nhồi từ sử dụng các từ khóa chính quan trọng nhất của bạn ở đầu tiêu đề, thẻ meta, văn bản liên kết và nội dung dùng các từ khóa bộ lọc (filter word) để có thể tăng hiệu quả cho việc SEO từ khóa của bảo rằng trang web của bạn có một mật độ từ khóa phù hợp và không chứa các từ khóa cấm - stop nhóm từ khóa để SEO (pha trộn từ khóa) ngay trên trang web của bạn sẽ giúp cải thiện thứ hạng của bạn trên tất cả các công cụ tìm kiếm thêm : Bạn cần biết về cách loại bỏ ung gia khoa dao tao seo tai Ha Noi chuyen nghiep cung Cong ty SEO Luận ViênSố bài thích:3Điểm thành Google nó cũng không làm căng đến thế đâu. Đa số diễn đàn bị spam quá không kiểm soát mới bị dính đòn chứ còn website cá nhân thì dù bạn viết có trúng lỗi này nó cũng tha. Vì cái chính là nó quét nội dung spam bị nhân ra một cách vô ích thôi mạnh nhờ sữa ong chúa dạng viên. Nói bạn bè về kinh du lịch đà nẵng phải Đăng nhập hoặc Đăng ký để trả lời bài Ignored viết liên quan11 bước để SEO mại điện tử thành công - bước để SEO mại điện tử thành công - bước để SEO mại điện tử thành công - sẻ trang tài khoản hoặc địa chỉ đã có tài khoản vào đây để đăng Mật khẩu của tôi đã quên mật khẩu? Duy trì đăng đề quan tâm Không Liệu Có Lên posted 21/9/17 lúc khóa Top 1 có nên làm posted 21/9/17 lúc đề mới Onpage và SEO Offpage là replied 24/9/17 lúc Không Liệu Có Lên replied 24/9/17 lúc - đàn ông lý replied 24/9/17 lúc GẤP NHÂN VIÊN replied 23/9/17 lúc lấy lại dữ liệu web đã bị replied 23/9/17 lúc kết bạn với làm seo replied 22/9/17 lúc có biết rằng nghề SEO như replied 22/9/17 lúc ai kiếm từ replied 22/9/17 lúc mô hình nào thì SEO hiệu replied 22/9/17 lúc sẻ diễn đàn cho phép để replied 22/9/17 lúc đề ngẫu trao đổi liên posted 21/9/17 lúc Không Liệu Có Lên posted 21/9/17 lúc khóa Top 1 có nên làm posted 21/9/17 lúc trao đổi liên kết, đặt posted 19/9/17 lúc Profile nước posted mạnh của google posted học seo từ posted seoer làm tất cả các posted website lĩnh vực du posted ra đi 1 web tâm posted mừng sinh is 33 viên đang 346 (Thành viên: 3, Khách: 269, Robots: 74) Like Ủng Hộ Diễn Đàn SEO & SEM: Công Nghệ & Thông Trực Tuyến !Trang đàn>SEARCH ENGINE - thuật - Chiến lược đàn kết kiếm diễn viết gần viênThành kết viên tiêu viên đã đăng truy động gần Profile tạo SEOTìm kiếm Chỉ tìm trong tiêu gửi bởi thành cách tên bằng dấu hơn ngày: Search this thread only Search this forum only Hiển thị kết quả dạng Chủ kiếm hữu viết gần tin trên diễn đàn SEO xây dựng bởi chính các thành diễn đàn với mục đích học tập và trao đổi kiến thức về Seo và Sem. BQT diễn chịu trách nhiệm về mọi hành vi, lời nói, tính hợp lệ của nội sao chép về nội dung vui lòng trích lại nguồn bài viết từ quyền © 2014 cộng đồng Seo và Sem Việt đàn sử dụng mã nguồn đang trong giai đoạn chạy thử (BETA). Mọi chi tiết vui lòng liên hệ: báo đối tác liên kết: Từ ngày chúng tôi gỡ một số liên kết xuyên trang trên diễn đàn seo này. Do chúng tôi không liên lạc với toàn bộ các đối tác đã trao đổi liên kết; vậy xin thông báo bằng bản tin này để đề nghị các bạn đối tác đồng thời gỡ bỏ liên kết của trên website của các bạn. Xin trân trọng cám ơn các bạn vì sự hợp tác tốt đẹp đã có giữa hai bên và mong các bạn thứ lỗi vì những bất tiện nếu có. Xin chân thành cám ơn ! Follow Engine dan SEO dich vu SEO là gìTin tức về các Bộ máy tìm dựng liên thuật - Chiến lược SeoDien dan seo tập viên Việt toàn hayTang like bìa cover hay đẹp dễ Engine ưu hóa Mạng Xã cứu cụ vụ SeoTư vấn Seo cho doanh luận hỏi đáp trợ giúp SeoTên hay lạ đẹp nhap bia depStt dang nhap vao dong SEO hang dau viet nam | Cộng đồng SEO Việt Nam | Cộng đồng SEO Việt Nam |Diễn đàn SEO SEM | Auto like | Đổi tên | Like fanpage | Quảng cáo | Tăng like | Đăng ký | Lấy lại mật khẩu | Ký tự Fb đặc biệt | Forum Seo Time | Chia sẻ sẻ 247;var= 41;var Array(img,img,table,textarea,div,hr,h ,code,object,a,b,strong);var = = { 0 0},_lightBoxUniversal: 1,_animationSpeedMultiplier: 10%,speed: rgb(255, 255, 0.6,loadSpeed: {thread_view:true,message:true,bb_code:true,message_user_info:true,BRETI_extra_thread:true,MoreThread_main:true,share_page:true,wf_default:true,login_bar:true},_cookieConfig: { path: /, domain: , prefix: ,_csrfRefreshUrl: 25db75cc});jQuery.extend(XenForo.phrases,{cancel: Hủy giây phút %minutes% phút nay lúc qua, lúc %day% lúc Chủ Thứ Thứ Thứ Thứ Thứ Thứ Tháng tư,Tháng tám,Tháng mười mười CN,T2,T3,T4,T5,T6,T7,following_error_occurred: Có lỗi sau sảy xa với yêu cầu của The server did not respond in time. Please try Đang đăng Xem ảnh Show hidden content by Javascript = = var s=document.createElement('script');s.async=async;s.type=text/javascript;s.src='http:tcr.tynt.com/ti.js';var= { true,version: id){ var js, fjs = if js = js.id = id; js.src = fjs); 'facebook-jssdk'));",
          "relevence": "yes"
        },
        {
          "url": "https:en.wikipedia.org/wiki/Stop_words",
          "title": "Stop words - Wikipedia",
          "content": "Stop the free to be with stop words are words which are out before or after of natural data Though stop words usually refers to the most common words in a there is no single list of stop words used by all natural tools, and indeed not all tools even use such a list. Some tools avoid these stop words to support phrase group of words can be chosen as the stop words for a given For some search these are some of the most common, short words, such as the, is, at, which, and on. In this case, stop words can cause when for phrases that include them, in names such as The Who, The The, or Take That. Other search engines remove some of the most common lexical words, such as a query in order to improve Peter Luhn, one of the in is with coining the phrase and using the The phrase stop word, which is not in Luhn's 1959 and the terms stop list and appear in the shortly concept was used in some For the first Hebrew Me’ir nativ, a list of words, with and which are similar to modern stop miningInformation engine Rajaraman, A.; Ullman, J. D. (2011). Data Mining. Mining of Massive (PDF). Stackoverflow: One of our major for the related query is the top 10,000 most common English words (as by Google search) before the query to the SQL Server 2008 full text engine. It’s how little is left of most posts once you remove the top 10k English words. This helps limit and narrow the which makes the query Luhn, H. P. (1959). Index for (KWIC Index). NY: Corp. Flood, Barbara J. (1999). note: The Start of a Stop List at Journal of the Society for 50 (12): 1066. 16 Bella Hass (2004). of in the domain of (PDF). Second on the History and of and Retrieved 17 of English Stop Words (PHP array, in Stop Words Stop Stop Stop Words and another list of German stop Stop of stop words in 29 [1]A of Stop Words by Kavita corpusStopwordsBag-of-wordsAI-completen-gram segmentationPart-of-speech term resolutionSentiment extractionTruecasingAutomatic simplificationMachine data character Dirichlet essay checkerSyntax online engine standardMeta marketingDisplay engine media inclusionPay per engine farmLink linkingPeopleDanny SchwartzOtherGeotargetingHuman search farmRetrieved from menuPersonal logged historyMoreSearchNavigationMain articleDonate to portalRecent links pagesPermanent this a as links This page was last edited on 15 2017, at is under the Commons terms may using this site, you agree to the Terms of Use and Privacy Policy. is a of the Inc., a WikipediaDisclaimersContact view(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({wgPageParseReport:{limitreport:{cputime:0.100,walltime:0.137,ppvisitednodes:{value:498,limit:1000000},ppgeneratednodes:{value:0,limit:1500000},postexpandincludesize:{value:34448,limit:2097152},templateargumentsize:{value:96,limit:2097152},expansiondepth:{value:7,limit:40},expensivefunctioncount:{value:0,limit:500},entityaccesscount:{value:0,limit:400},timingprofile:[100.00%111.3551 -total, 52.43% 58.3801 33.13% 36.8962 26.51% 29.5181 10.7862 Template:Natural_Language_Processing,6.62%7.3762 Template:SearchEngineOptimization,3.40%3.7891 Template:Main_other]},scribunto:{limitreport-timeusage:{value:0.040,limit:10.000},limitreport-memusage:{value:2755107,limit:52428800}},cachereport:{origin:mw1276,timestamp:20170915054443,ttl:1900800,transientcontent:false}}});});(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({wgBackendResponseTime:59,wgHostname:mw1255});});",
          "relevence": "yes"
        },
        {
          "url": "https:cuoilennaocacban.wordpress.com/2012/03/14/w05/",
          "title": "[W05] Sử dụng Tab trong Word | Office",
          "content": "Microsoft Office Tips, for now. Check back soon, it's to 365Về tác Sử dụng Tab trong / Có gì mà khó sử vậy, Tab rất dễ sử dụng, nhưng nó còn có nhiều tính năng hơn thế. Tab trong trên là Left Tab, tức tab trái. Có các loại tab sau:Tab Trái: Mọi chữ sẽ hiển thị bên trái của Tab này Tab Phải: Mọi chữ sẽ hiển thị bên phải của Tab này Tab Giữa: Mọi chữ sẽ căn giữa với Tab này Hãy cùng xem chức năng các tab này chuột trên thanh Ruler, click một sẽ thấy có một biểu nhỏ xíu xuất hiện, cái đó gọi là Tab click nó, một hộp thoại hiện thoại này cho phép bạn tùy chỉnh các kiểu của Tab Stop, thậm chí thay đổi cả loại Tab đang chọn dòng 3.19” ở Ô textbox bên trên cho phép bạn chỉnh lại vị trí của Tab trên Ruler, và danh sách phía dưới liệt kê toàn bộ Tab có trong Section chọn, là toàn bộ văn bản (Xem bài viết Đặt Header và Footer khác nhau trong cùng văn bản để biết thêm Section của một văn bản là gì chọn dòng mục Leader chọn số 2, sau đó bấm nút bấm OK, hộp thoại biến mất, văn bản hoàn toàn bình không có gì thay đổi bàn phím của bạn, nhấn nút Tab, woa la, điều kỳ diệu xảy raVà không chỉ có vậy, đặt con trỏ vào cuối chữ Test Tab, hãy gõ vài chữ vào đó đi. Bạn sẽ thấy hàng chấm tự động thay đổi độ dài cho vừa, không hề bị đẩy ra ngoài như ta dụng điều này ở đâu? Trong các văn bản hành chính thông ta để một chỗ trống cho bạn điền vào, đó là ứng dụng của nó.Tab Double Click vào Tab Stop đó, và Set nó lại thành Right TabVăn bản vẫn như cũ, điều gì đã xảy ra?Hãy đặt con trỏ vào cuối dòng chấm, gõ vài chữ, và bạn sẽ bộ chữ bạn gõ bị đẩy về phía trái, bên phải không dài ra, và hàng chấm bị ngắn lại cho còn Tab giữa dùng để canh giữa bất kỳ thứ gì đặt xuống dòng, hủy các Tab có đó bằng cách kéo Tab đã có trên ruler xuống dưới tờ giấy, nó sẽ biến nó, Kéo một số từ như sau vào dòng Đại học Công nghệ Thông tinKý 2 dòng một lần vào biểu Tab để nó thành Tab 1 vị trí thích hợp trên Ruler, click một lần để đặt con trỏ ở đầu dòng chữ Đại học Công nghệ Thông tin, bấm TabSau đó, đặt con trỏ ở đầu dòng Ký tên, bấm Tab một lần bản đã canh giữa, đúng với vị trí của Tab Giữa bạn đã đặt điện this:Số lượt thích Đang in MS Word. Tab, Left Tab, Right Tab, Tab, Word TabBài viết liên chìm trong Word và footer khác nhau trong cùng một văn bản mục lục tự động cập nhật bài Sử dụng Office Web Viết Blog bằng Word →3 on Sử dụng Tab trong at 8:33 job, keep in at 9:21 this on and hình là tar mới sang word 10. hô at 9:08 thêm bài mới đi bạn ơi. Các bài viết trên trang này rất bổ lời Hủy trả bình luận của bạn tại bạn điền thông tin vào ô dưới đây hoặc kích vào một biểu để đăng điện tử (bắt buộc) (Địa chỉ của bạn giấu (bắt web Bạn đang bình luận bằng tài khoản xuất / Thay Bạn đang bình luận bằng tài khoản Twitter xuất / Thay Bạn đang bình luận bằng tài khoản xuất / Thay Bạn đang bình luận bằng tài khoản Google+ xuất / Thay to %svar = input = 'input' = jQuery( );if ( in input ) jQuery( label' );} Expando Mode: start small, then on first click + text '#comment-form-subscribe' ).hide();comment.css( { } ).one( {var timer = 10 this { } 100 { timer ); n(); } '#comment-form-subscribe' ).slideDown();});}jQuery(document).ready( ); Notify me of new via email. Thông báo cho tôi bằng email khi có bài đăng mới. Lưu học Tháng Mười Một Chín Ba Mười Chín Tám Bảy Một Mười Hai 2010 Thánh địa bí kết blog: Tuan Tran's – 1 – – Install to Docker from Editor for với List và thêm mới item dùng Blog via your email address to follow this blog and receive of new posts by 12 other - Bài một website miễn phí hoặc 1 blog với */var WPGroHo = ]]> and attach to all $ ) {if (typeof === ( typeof !== ) = hash, id ) hash, id = 'body', );});/* */var = In  u2026,submittingText:  u0110  u0103ng lu  u1eadn,connectingToText:Connecting to   u0111ang %2$s,logoutText:  u0110i ra nh  u1eadp,connectURL:https:  /  /cuoilennaocacban.wordpress.com  /public.api  /connect  /?action=request,logoutURL:https:  /  /cuoilennaocacban.wordpress.com  /wp-login.php?action=logout&_wpnonce=5bbdcd0015,homeURL:https:  /  /cuoilennaocacban.wordpress.com  /,postID:426,gravDefault:identicon,enterACommentError:Please enter a vui ch  u1ec9 email email vui b  u1ea1n,gravatarFromEmail:This picture will show you leave a Click to tin t  u00e0i kho  u1ea3n   u0111  u1ed5i,changeAccount:Change ]]> */Post toHủy = 'ready ).on( {if ( !== typeof ){  If there's another sharing window open, close = 'href' ), false;});});Send to Email Email Address = ''; Hủy was not sent - check your email check failed, please try your blog cannot share posts by */var = Office,siteURL:http:  /  /cuoilennaocacban.wordpress.com,icon:<img alt='' height='50' type=  hidden   name=  _wpnonce     />,referer:https:  /  /cuoilennaocacban.wordpress.com  /2012  /03  /14  /w05  /,canFollow:1,feedID:964553,statusMessage:,customizeLink:https:  /  /cuoilennaocacban.wordpress.com  /wp-admin  /customize.php?url=https%3A%2F%2Fcuoilennaocacban.wordpress.com%2F2012%2F03%2F14%2Fw05%2F,postID:426,shortlink:http:  /  /wp.me  /s1fKlJ-w05,canEditPost:,editLink:https:  /  /wordpress.com  /post  /cuoilennaocacban.wordpress.com  /426,statsLink:https:  /  /wordpress.com  /stats  /post  /426  /cuoilennaocacban.wordpress.com,i18n:{view:View d  u00f5i,following:Following,edit:Edit,login:  u0110  u0103ng k  u00fd,customize:T  u00f9y this theme: shortlink,copied:Copied,followedText:New posts from this site will now appear in your <a r  u1ed9ng,unfoldBar:Expand this k  u00ed theo trang   u0111  u1ecdc,viewReadPost:View post in me your email have a <a in k  u00ea}};/* ]]> ** */var = lu  u1eadn,post_comment:G  u1eedi a B  u00ecnh full size <span class=  photo-size-times  >  u00d7<  /span>{1}<  /span>,no_comment_text:Please be sure to submit some text with your provide an email address to provide your name to but there was an error posting your Please try again comment was comment is in Speed,focal_length:Focal for=  email  >Th  u01b0 <input name=  email   jp-carousel-comment-form-text-field   for=  author  >T  u00ean <input name=  author   jp-carousel-comment-form-text-field   for=  url  >Trang <input name=  url   jp-carousel-comment-form-text-field     /><  /fieldset>,reblog:Reblog,reblogged:Reblogged,reblog_add_thoughts:Add your here... Reblog,stats_query_args:blog=18529427&v=wpcom&tz=7&user_id=0&subd=cuoilennaocacban,is_public:1,reblog_enabled:};/* ]]> ** */var = ]]> */ {try{if ( in {if {var jl = var s = = like = || [];_stq = || {'crypt':'UE40eW5QN0p8M2Y/RE1wamRFTUJvcCVmdVtvcCUrditEcC5rdHw4JUQsQU4vR3dMJT82VTRnczRMU105WE5qZWtoXWczU3FUeU5sTDU2SEZscG5MS1srT09+OVQvdTRzeTBRZmlLb0ZxMS1FZzdKUENyRGYxZDQuTkZ3JklzY1tifHV+dkxoMmRRJUlIenFnTkFwfGZhQTMuMD1zcHRCUDVKVHprbnRYeVRDeE9ddiZ0ZlNhdlRWNE4/V2ZwS0Y0WlYwVHoyV1E4JXFxYTc1OXhPOHZVLz1LZkh2Rkt4JjV3VSZbYlU3S3VjZE5hQyxFdDB4MzdFQnxYbFd5eVgmWU8lZStEa2xCT2JucmJTWUdXN0M3JXNjPUN+bVEreEFXMW1fOWw2MG9CeXZlMWZua20leGkwMnNnMDB2YWN1ODdXMHZYLS89bHY='}]);_stq.push([ '426' ]);if ( === typeof ) = ;if( false !== += &x_ + + '=' + false !== += &x_ + + '=' + += &x_ + + '=' +  != ) {new = + + + + Math.random();}}",
          "relevence": "no"
        },
        {
          "url": "https:englishstudyforvms.wordpress.com/2007/10/27/english-phonetics/",
          "title": "English | Let's Study English - Nào ta cùng học tiếng Anh :)",
          "content": "Body – Great on English Anh cho on Tiếng Anh cho đã Prompt: on …[le anh th… on Kinh học on English your email address to to this blog and receive of new posts by 51 other we talked 2012 (4)June 2012 (4)March 2010 (2)September 2010 (1)June 2008 (2)April 2008 (1)December 2007 (4)August 2007 (18)English on October 27, 2007 by mới học một ít về English thấy thầy dạy rất dễ hiểu nên post lên thời chưa vẽ hình niệm cơ bản: hết ta định nghĩa một số thuật ngữ Mỹ dùng chỉ những bộ phận của khuôn mặt dùng vào nói tiếng Anh: (để hôm nào scan hoặc vẽ lại hình cho mọi dễ hình ngữ: ridge (tính từ của nó là chỉ phần vòm họng kề với (tính từ labial) : (tính từ dental ): palate (tính từ: : phần tiếp theo của vòm họng, palate hay velum (tính từ velar): phần sau của vòm họng, (tính từ: lingua) : cavity (tính từ: nasal) : ống từ miệng lên (tính từ: đây là từ chỉ bộ phận quan trọng nhất cho việc phát âm. Bộ phận này nằm ngay cần cổ và thật sự là một vòng tròn. Trên vòng tròn đó có 2 màn mỏng bình tách ra, nhưng khi ta nói thì hai màng mỏng này khép lại, không khí đi xuyên qua lỗ hở tạo dao đó phát ra âm dụ: Nguyên âm gọi là labial stop Đièu này có nghĩa là muốn phát âm p thì phải chận luồng không khí tại vị trí một cách hoàn toàn (stop) , và không phát ra âm thanh Phần sau sẽ nói rõ hơn làm cách nào thực hiện như on 18, gắng hoài nhưng vẫn không vẽ hình vậy nếu bạn nào thích tham khảo hình và cách phát âm từng từ thì vào đây sẽ tiếp tục trình bày một số điểm trong phát âm tiếng Anh.1) Phân biệt voice và như thế những cặp từ tiếng Anh mà khi phát âm thì lưỡi cùng vị trí, chỉ khác nhau ở voice hay là Chẳng hạn như p/b, t/d… (xem thêm trong link ở hỏi đặt ra là nghĩa là không có âm, thì làm sao ta phát đề là như sau: Khi chúng ta phát âm bình trong tiếng Việt thì hầu như mọi từ có thể xem như là voice trong tiếng Anh. Do đó khi Việt phát âm chữ t, thì Mỹ sẽ nghe giống như d.Làm cách nào Mỹ phát âm những âm Trả lời: Đối với những âm thì Mỹ dùng hơi thở để phát âm. Tức là không phát ra âm nhưng dùng luồng khí để tạo ra âm.2) Từ tiếng Anh dài, làm cách nào họ đọc một từ tiếng lời: Chẳng hạn xét từ Khi Mỹ đọc họ sẽ cắt nó ra thành nhữn phần nhỏ, mỗi phần thông có một phụ âm và một nguyên âm để dọc. Chẳng hạn từ trên sẽ tách ra thành Cho chi tiết chính xác từ nào tách ra thế nào thì hãy xem từ on 24, câu hỏi gặp là tại sao Mỹ nói nhanh như vậy? Chúng ta chỉ cần nhớ những từ sau khi nói tiếng Mỹ: thought group and focus word, -ed and -s. Cuối cùng là Thought group and focus word: khi nêu định nghĩa và thực hành về thought group and focus word, tôi cho một ví dụ về thế nào là thought group: Xét đoạn văn sauFor if one note has a of 400 Hz, the note an octave above it is at 800 Hz, and the note an octave below is at 200 Hz. The ratio of of two notes an octave apart is 2:1. Further octaves of a note occur at 2n times the of that note (where n is an such as 2, 4, 8, 16, etc. and the of that series. For 50 Hz and 400 Hz are one and two octaves away from 100 Hz because they are (or 2 − 1) and 4 (or 22) times the 300 Hz is not a whole number octave above 100 Hz, despite being a of 100 trên có thể chia ra làm thought groups bởi nhữnh dấu /, và focus word (để giữa hai dấu ) như sau (tôi không biết làm cách nào để sử dụng hiệu ứng của Word ở if one [note] [has] a ofy bắt đầu thought groups những dấu câu như chấm hỏi ngã do vì sao thì dễ những words như: and, or, do là vì những từ này có thể bắt đầu một ý đề tính từ (sau hạn ta xét đoạn the unison, the octave is the in music. The human ear tends to hear both notes as being “the For this reason, notes an octave apart are given the same note name in the Western system of music name of a note an octave above A is also A. This is called octave and is closely related to This is similar to and less so and, less still, the latter of which is used only in musical set theory, or atonal theory. Thus all C♯s, or all 1s (if C=0), in any octave are part of the same pitch class. Octave is a part of most musics, but is far from in and early music (e.g., Nettl, 1956; Sachs & Kunst, 1962). monkeys octave and its basis is an octave mapping of neurons in the of the brain [1] and the of octave in neural can form through to pitched notes, without any this being derived from the of those notes 2003, cited in thể phân nó thành thought groups như the the octave is the in music./ The human ear tends to hear both notes/ as being “the For this notes an octave apart are given the same note name in the Western system of music name of a note an octave above A is also A/. This is called octave and is closely related to This is similar to and less so and, less still,/ the latter of which is used only in musical set or atonal Thus all C♯s,/ or all 1s /(if C=0)/, in any octave are part of the same pitch class./ Octave is a part of most but is far from in and early music/ (e.g., Nettl, 1956; Sachs & Kunst, 1962)/. monkeys octave and its basis is an octave of neurons in the of the brain [1]/ and the of octave in neural can form through to pitched notes,/ without any this being derived from the of those notes 2003, cited in 1: Cố gắng đọc một cách liên tục những từ trong cùng một thought 2: Đừng chia talk groups ngắn quá (làm nghe cảm thấy câu văn trúc hay dài quá (vì bạn phải có đủ hơi để đọc một 3: Nên có một ngừng ngắn khi kết thúc một thought 4: có một số cấu trúc song song như A and B, C and D thì không nên chọn thought group bắt đầu từ 5: bạn có thể chọn thought group linh hoạt theo ý Focus word: Focus word là những từ mang ý nghĩa chính của một thought group. Có thể nói như sau: Nếu trong một đoạn văn bạn bỏ hết các từ khác, chỉ giữ lại focus word thì nghe vẫn hiểu từ là focus word word (như and, but, danh từ xuất hiện lần phản thân dụ trong đoạn trên nếu chỉ giữ lại focus words thì như octave music./ human ear tends hear both notes/ as being For this notes octave apart given same name Western system note an octave above also/. called and closely related similar and less so and, less still,/ latter used only musical set or atonal/ Thus all C♯s,/ or all /(if C=0)/, any octave part same pitch class./ Octave part most but far from and early/ (e.g., Nettl, 1956; Sachs & Kunst, 1962)/. monkeys and its basis an octave neurons brain / and of octave neural can form pitched notes,/ without any being derived those notes 2003, cited in nhấn mạnh focus word bằng cách đọc to với những từ sung: nhấn mạnh -self trong on March 10, bày rõ hơn về các âm tiếng Anh:1. Làm sao phát âm một âm Stop (gồm định nghĩa thì: Stop là “the air flow moves, then stop, then moves Như vậy để đọc chữ p thì cho luồng hơi di nhưng khi luồng hơi ra khỏi miệng thì ngừng lại một chút, rồi cho luồng hơi di trở và thêm 4 âm klhác nhưng tôi không biết cách nào để luồng hơi cũng bị chận lại nhưng không hoàn toàn như stop, do đó tạo ra âm thanh giống như tiếng ồn (tS, dZ): là kết hợp giữa stop và Sở dĩ như vậy là vì",
          "relevence": "no"
        },
        {
          "url": "http:www.24and24.com/2017/06/spin-bai-viet-la-gi-stop-words-la-gi.html",
          "title": "SPIN BÀI VIẾT LÀ GÌ? STOP WORDS LÀ GÌ? CÁC SEOer CẦN PHẢI HIỂU RÕ",
          "content": "{vars: UA-85342774-1},triggers: {on: pageview}}} 24 & CẢ CÁC BÀI 24 & 24XEM KẾT QUẢ XỔ SỐ 24 & 2424 & 24, 24 AND 24, 24 / 2424 & 24, 24 AND 24, 24 / 24, WELCOME TO 24 & 24, WEBSITE & SERVERS OPEN 24 HOURS24 & CẢ CÁC BÀI 24 & 24 & & 2424 AND 2424 VÀ & AND BÀI BÀI VIẾT LÀ GÌ? STOP WORDS LÀ GÌ? CÁC SEOer CẦN PHẢI HIỂU RÕSPIN BÀI VIẾT LÀ GÌ? STOP WORDS LÀ GÌ? CÁC SEOer CẦN PHẢI HIỂU RÕ24 & 24 24 & bài viết là nghĩa là trộn, lặp, xào, viết các từ ngữ có điểm chung, có ý nghĩa tương tự để tạo nên các bài viết mang tính duy nhất ( unique article ) nhằm qua mặt bộ máy tìm kiếm. Ưu điểm của Spin chỉ duy nhất là nội dung hay số tứ ngữ bài viết đó sẽ trở thành Duy nhất, và đánh lừa thuật toán Panda của Google. điểm lớn của spin là ngữ pháp, chất câu từ lủng củng, không rõ ràng và gây khó hiểu cho dụng spin là một dạng SEO. Tuy nhiên, điều này lại đang phổ biến ở thị SEO do cách thức để tạo ra một bài SPIN quá dễ dàng: Chi phí thấp ( thậm chí là 0 ), nội dung của khác có sẵn để dùng spin, tạo nhanh 1 bài trang web báo chí cũng hay sử dụng spin để viết bài, xào lại bài của khác, nhưng ở cấp độ cao hơn khiến cho đọc không nhận ra đó là bài xào lại. Và các cũng ngụy biện rằng họ là một chính hiệu chuyên tạo ra các bài viết hay và họ khẳng định họ là những cung cấp giải pháp nội dung tốt nhất. Cái ranh rới giữa spin bài và rất mong manh và khó nhận ra với 1 duyệt non kinh Các công ty hay thuê đội ngũ sản xuất nội dung bên ngoài để lấy số content bù đắp cho tổng nội dung 1 trang hoặc vệ tinh. ta lầm những bài viết đó sẽ đem lại một kết quả tốt và họ sẵn sàng chèn link vô tội vạ, anchor text đủ các loại để hòng thống trị thứ hạng trên các kết quả tìm kiếm. Nhưng họ quên mất rằng, thuật toán Panda của Google đã nâng cấp lên mức độ cực kì tinh vi. Việc sử dụng các từ đồng nghĩa, các câu lặp, các đoạn mở đầu, thao thao bất tuyệt nhằm cố kéo dài nội dung, đã mang đến một bất lợi cho website về mặt chất từ khóa. Các lập trình viên của Google rất hiểu việc này nên thuật toán của họ đã nâng cấp và lọc ra những Stop Words, sự tương quan trong ngữ cảnh và ngữ pháp của câu từ. Nếu bạn để ý, ngày xưa, bộ máy phiên dịch của Google rất lởm, nhưng đến nay họ đã khắc phục rất nhiều về ngữ pháp tiếng Việt khi dịch sang tiếng Anh. Đây là kết quả của những nghiên cứu không ngừng nghỉ nhằm cho ra 1 sản phẩm tốt nhất – Một kết quả tìm kiếm phù hợp với dùng words là tiết kiệm không gian lưu trữ và gia tăng tốc độ tìm kiếm, các công cụ tìm kiếm sẽ không ghi nhận lại những từ quá phổ biến, quá chung chung và những từ này gọi là stop Kiệm Không GianXem ví dụ sau:The way to the school is long and hard when walking in the xuất hiện 3 lần. Để tiết kiệm không gian, các SE có thể thay thế nó bởi một “ký tự đánh Ví dụ trên sau khi thay thế sẽ như sau:* way to * shool is long and hard when walking in * giải thích đơn giản này có lẽ đã giúp bạn hiểu rõ vấn đề ( tuy nhiên “ký tự đánh đề cập ở trên không nhất thiết phải là ký tự * ). Tuy ví dụ trên lưu trữ theo nhưng vẫn không làm mất đi ý nghĩa và sự liên Tăng Tốc Độ Tìm Search Engines đều lưu tất cả các từ trên trang web của bạn nhưng không nhất thiết chúng sẽ tìm kiếm tất cả các từ đó khi có truy vấn từ sử dụng. Xem ví dụ bên piano ta sẽ đơn giản hóa mô hình tìm kiếm như sau – các SE sẽ tìm 3 lần – Đầu tiên chúng sẽ tìm xem có từ không, sau đó chúng tìm tiếp từ và cuối cùng là Nhưng rất có thể, các SE chỉ tìm 2 từ cuối là đủ để xác định sự liên quan có trong nội dung một trang web và điều này chắc chắn sẽ làm gia tăng tốc độ tìm kiếm. SE có nghĩa là viết tắt của ( Search Engines tự như câu cú tiếng Việt, tôi giả sử bạn viết 1 đoạn câu như ra, đây có thể là một trận đấu tốt của Real có thể nhận thấy, các stop words của câu trên quá những từ như: Thực ra, có thể là. Là những từ Google sẽ ưu tiên loại bỏ trong Những từ Google chú ý: Trận đấu tốt, Real thế, thay vì bạn spin bài như cách trên, bạn hãy viết chuẩn lại: Real Madrid có một trận đấu tốt. – Một câu chuẩn ngữ pháp, đầy đủ ý nghĩa và loại bỏ Stop cứ làm như vậy cho đến hết tất cả các đoạn câu, đừng bao giờ mong mỏi số từ trong bài viết nhiều lên là bài viết đó chất lên. Nếu bạn sử dụng đúng ngữ pháp, loại ra các câu từ nhàm chán thì dù bài viết có 500 từ, bạn vẫn sẽ Google ưu tiên trên kết quả tìm kiếm ở rất nhiều từ khóa mở 1 sự thật bài viết và Stop words đang gây cản trở trong những thủ thuật SEO của dân Việt là rất hay spin bài viết và sử dụng rất nhiều Stop Words. Chính điều này gây cản trở rất lớn đến thứ hạng #SEO và kết quả trên vì sử dụng các Stop words để kéo dài nội dung hay để spin bài viết cho vừa ý khách hàng, hãy suy nghĩ để tạo ra một bài viết tổng hợp có nhiều góc nhìn và ví dụ cụ thể. Điều này làm đọc cảm thấy thỏa mãn vì tìm thông tin hữu có dịp nói với một giám đốc kinh doanh của một tờ báo điện tử. Họ nói rằng, vừa rồi họ đã đuổi cả 1 team SEO do những kết quả không tốt và không thấy sự phát triển hay tín hiệu gì từ đội SEO đó. Tôi hỏi vì sao, câu trả lời là: 1 năm team SEO ngồi làm và không đưa ra định phát không bám sát vào 1 cốt lõi gì, đặc biệt cách làm SEO có vẻ quê mùa như là đi link diễn đàn là chủ khi xem qua tôi nhận thấy có vẻ Google đã bắt đầu các hình phạt cho website này. Traffic 3 tháng sụt bất ngờ, chỉ bằng 1 nửa traffic cũ. Dò hỏi ra thì biết, họ có 50% nội dung tự viết, 50% còn lại là copy và từ các website khác. Nếu vậy thì hẳn là dính đòn từ Google rồi! Chỉ cần bạn crawl trong vòng 3 tháng, hoặc muộn nhất là 6 tháng, Google sẽ phạt website của bạn và không thể nào bạn thay đổi điều gì với traffic hoặc từ khóa, chỉ còn cách đổi tên miền và làm lại từ đầu. Google rất ghét các website crawl và thuật toán của Google biết rõ site nào đang cố tình làm việc trở lại câu tuyển dụng từ công ty kia, họ đang cần 1 leader SEO có thể điều hành, định Seo mới và tuyển 1 đội quân mới. Tôi khuyên luôn là họ nên tuyển hẳn 1 SEO Expert với mức lương cao hẳn và không cần tuyển thêm bất kỳ 1 ông seo nào nữa. Lý do rất đơn giản: SEO càng ngày càng chân thực, và dần mất hẳn khái niệm SEO vốn là, nếu làm seo tới mức đỉnh cao, thì SEO sẽ thất sẽ không có việc gì cho ông SEO làm nữa, thay vào đó chỉ ngồi nghiên cứu và phát triển các tính năng sản phẩm, định các chiến lược mới cho công việc Seo phù hợp với xu bây giờ chính là: Tập trung vào nội dung, không nên dùng bất kỳ thủ đoạn nào hòng đoạt thống trị về từ khóa. SEO hiện đại không phải là làm sao lên top từ khóa, mà làm sao tăng traffic đều theo tháng – Traffic organic từ tìm kiếm không trả tiền. Bên cạnh đó, phát triển độ phủ trên các mạng xã hội bằng các hình thức như Group cộng hệ thống tips nhỏ nhặt như viết 1 bài hay, share lên fanpage để kéo 1 IP click vào bài viết đó, kiểm tra lại, từ khóa chính của bài viết đó sẽ có top. Google có vẻ thích sự hào hứng, traffic đổ dồn vào bài viết hơn là cái để bot điều này ảnh đến chiến lược nhân sự 1 vài công ty khi họ có bước đi khôn ngoan hơn, tập trung vào giá trị nội dung nhiều hơn là chỉ chăm chăm từ có thể lên top 1 – 10 từ khóa quan trọng bằng cách đi các nhưng bạn quên mất 1 việc là các bài viết, các page trên trang càng nhiều lên, thì bạn sẽ nhận rất nhiều traffic vào các page đó thông qua các từ khóa rất bất ngờ. Dĩ nhiên bài viết phải đạt độ unique và đem lại 1 giá trị thông tin cho đọc. Bằng việc lên hạng các từ khóa nhỏ, từ khóa vớt vát, hay từ khóa dài trải rộng trên toàn điểm trust website của bạn tăng dần và Google nhận ra điều đó, sẽ tính điểm cho site bạn ở một mức nào đó trong các kết quả tìm kiếm. Vì thế, hãy cung cấp nội dung mà bạn biết rằng có thể mất khá nhiều chi phí cho việc này, nhưng đó là việc cần làm, là điều kiện cần tiên quyết để bạn có thể chen chân vào hệ thống tìm kiếm của nếu bạn để ý, bạn đã cung cấp rất nhiều nội dung như vậy, đến một nào đó, traffic của bạn không thể bật lên vì sao ? Vì Google nhận ra bạn chưa có thay đổi đáng kể nào trong cả quá cần làm gì lúc này? Hãy tái cơ cấu mở chuyên mục mới, mở lĩnh vực content mới, viết cái mới, viết cái mà không ai viết, hoặc phát triển các kênh fanpage để lấy tương tác... Nghĩa là, bạn cần làm gì đó thay đổi sự nhàm chán của Sự thay đổi này rất cần thiết để Google bắt đầu nâng điểm cho bạn. Luật chơi luôn là như vậy! Nếu bạn không tái cơ cấu, bạn đã để đối thủ của bạn ( mà nó luôn sinh ra ) lấn át. Đây cũng là 1 bài toán trong kinh doanh, tái cơ cấu, cùng với việc phát triển chiến lược kinh doanh, để tiếp tục cung cấp các giá trị mới cho tục câu của công ty trên. Tôi có đề xuất sự cải tiến cho website của họ ở một vài chỗ onpage trên toàn trang, và đặc biệt nhấn mạnh: Stop ngay việc copy hay mua ngay mấy cái fanpage cùng các chủ đề với website để phát Giá mua, tôi biết 100K likes, 4 - 5 triệu tương tác cao. Nếu bạn mua với giá trên mức này, có thể bạn đang bị làm giá, hoặc bị lừa gạt. Tôi đang nói đến fanpage like thật, tương tác thật mà trên thị đang vào đó, đào tạo biên tập viên phải tối ưu onpage, tức là biết SEO onpage ngay khi đăng bài, viết bài, hoặc tuyển cộng tác viên viết bài ( giá thị : 20K – 50K / bài ). Việc của ông SEO là theo dõi Google giám sát onpage, google và quản lý các Rất dễ phải không? Nhưng đây là 1 công việc đòi hỏi sự dũng cảm dám vứt đi những kiến thức SEO cũ, những kiến thức SEO học ở đâu đó hoặc tư seo thống trị từ khóa. Đó là điều không dễ dàng chút làm SEO lên một mức cao hơn, bạn sẽ thấy, vấn đề của website không phải chỉ quanh quẩn 1 vài từ khóa, mà là hệ thống quản lý dùng trên site, sự thay đổi của nhu cầu dùng ngoài đời sống, hay cái gọi là product ( quản lý, phát triển sản phẩm ). Cái website của bạn chính là 1 sản phẩm cung cấp giá trị nào đó cho đọc. Việc của bạn là suy nghĩ xem tại sao ta dừng lại ở chỗ này, click vào chỗ kia, tại sao quảng cáo không thu hút, tại sao bài viết không có rating, lý do ta không ở lại trang lâu hơn... rất nhiều vấn đề sẽ mổ xẻ để tiến dần đến 1 giá trị mới tốt hơn cho khách hàng ( dùng, đọc ).Như vậy, bạn có cần thiết phải tuyển 1 đội SEO hoành tráng chuyên đi link, chuyên bôi bôi vẽ vẽ các từ khóa, nhồi nhét vào bài viết không? Những khái niệm như meta tags, H1, h2, bôi đậm, tỷ lệ từ khóa bao nhiêu %,… sẽ không còn là vấn đề của bạn SEO tốt nhất HIỆN NAY & TƯƠNG LAI chính là đẩy các tiến độ tự nhiên của việc Google sẽ đánh giá tổng thể website của bạn nhanh hơn, làm cho mọi thứ thật tự nhiên nhưng trên cơ sở rút ngắn quá trình tự nhiên & 24BÀI VIẾT CÓ HỮU SẺ BÀI VIẾT LÊN DÕI CHÚNG TÔI TRÊN GOOGLE & 24,24 AND 24,24 VÀ & 24H,24H AND BÀI đăng tiếp đăng xem CHỮ CÁI TIẾNG VIỆT VÀ TIẾNG ANH MỚI LÀ THỨ TỰ 24 CHỮ CÁI TIẾNG VIỆT VÀ TIẾNG ANH MỚI ! 1. Thứ tự bảng chữ cái tiếng Việt( Hình trên ) Thứ tự bảng chữ cái NGHĨA LÀ GÌ? 24/7 KHÁC 24/24 THẾ NGHĨA LÀ GÌ? 24/24 NGHĨA LÀ GÌ? & 24/7 KHÁC 24/24 THẾ NÀO? là hoạt động với công suốt 24 giờ  cả ngày lẫn đêm NGÀY, VÀ 12 THÁNG TRONG TIẾNG ANH VIẾT NHƯ VÀ CÁC THÁNG CÒN LẠI TRONG TIẾNG ANH LÀ THÁNG MẤY? TRONG TIẾNG ANH VIẾT NHƯ NÀO? Câu trả lời của January trong tiếng anh là MA KINH DỊ: CÓ CON VỚI MA - PHẦN 1 BỊ MA XÂM viết này không phải gieo rắc hoang mang cũng không phải là mê tín, những ai tin thì hoàn toàn có thể lý giải còn những ai không ti...24 VỊ CÔNG THẦN LĂNG YÊN Yên các ( giản thể: bính âm: Lingyan ), dịch tiếng Việt là gác Lăng Yên , là một ngôi lầu nhỏ nằm bên cạnh Điện Tam Thanh ở tây DẪN CÁCH CHÈN ĐÁNH GIÁ XẾP HẠNG 5 SAO CHO ( BLOGGER ) quá trình các bạn tìm kiếm thông tin trên các công cụ tìm kiếm như Google Search ... hay các bạn phải làm việc trên các công GỠ ÔNG CHỦ VIỆT SỞ HỮU RẤT NHIỀU TÊN MIỀN ĐẸP & NGẮN GỌN NỔI tất cả các bạn! Hôm nay 24 & tôi có cuộc gặp gỡ nói trực mạng với 1 Việt, 1 ông Trùm tên NGÀY CÓ BAO NHIÊU GIÂY, BAO NHIÊU PHÚT VÀ CÓ BAO NHIÊU LẦN KIM GIỜ TRÙNG KIM hỏi: 1 ngày có bao nhiêu giây? & 1 ngày có bao nhiêu phút? & 1 ngày có bao nhiêu lần kim giờ trùng kim phút? Câu PHỤC LỖI KHÔNG HIỆN ẢNH DO NHÀ MẠNG chào tất cả mọi 24 & 24 hôm nay sẽ dẫn cho mọi cách khắc phục lỗi không hiện ảnh do bị nhà mạng chặn. ...CỬ NHÂN ĐẠI HỌC LUẬT THẤT ĐI TRỘM CHÓ... ÔI XEM MÀ cho 1 cử nhân đại học luật quá các bạn, vì thất nên anh này đã đi đánh bả trộm chó, mèo khiến chó, mèo nằm chết rải rác khắp ký nhận bài viết hãy nhập địa chỉ Email để đăng ký nhận bài viết mới mục24 & MIỀN23FACEBOOK21GOOGLE19DOMAIN18WEBSITE11KIẾN WEB7BLOGSPOT6NĂNG TIỀN TRÊN XÃ KÝ TÊN VỤ LÁI XE Ô SNIPPETS2TRIẾT MIỀN VIỆC CON VĂN HÀNG RỘNG MỐI QUAN BLOG KẾ TRANG MA1TÂM BÀO BACK GIÁ 5 HIỆU 24 & 24 ĐÃ ĐĂNG KÝ BẢN : EMAIL: & & ÍCH1. XỔ SỐ KIẾN THIẾT 24 & 242. CÁCH CÀI ĐẶT CÔNG CỤ CHECK KIỂM TRA TRANG WEB CHUẨN AMP HAY CHƯA3. SỬA LỖI KHÔNG HIỆN ẢNH DO NHÀ MẠNG KIẾM HIỆU 24 & 24 VỚI TỪ KHÓA: 24 & 24, 24 AND 24, 24 / 24LÊN ĐẦU TRANG",
          "relevence": "yes"
        },
        {
          "url": "https:www.linkedin.com/pulse/big-data-t%E1%BB%95ng-quan-v%E1%BB%81-elasticsearch-donald-trung-manh-nguyen",
          "title": "[Big Data] Tổng quan về | ",
          "content": "Sign inJoin now Main content starts below. r, a) {r = = || {};== = {}; _nS = g}var _qS = qs = ''; for(var qs += (qs ? '&' : '') + return qs ? ('?' + qs) : ''}var _s = _wA = method:r.method,type:r.method,url:r.url,absoluteURL: _s('ws',s)+'www.linkedin.com'+r.url}}}_nS('com.linkedin.pulse.controllers.EditorController'); = {return url:/pulse/ + + == null ? null : {return trk)), == null ? null : {return (hashtag == null ? null : {return = {return url:/pulse/ + = {return url:/pulse/ + + == null ? null : {return (isMock == null ? null : (asJson == null ? null : (filter == null ? null : {return = {return url:/pulse + == null ? null : {return (count == null ? null : {return = {return url:/pulse/ + + {return v})(id, + /edit + == null ? null : {return = || = || {};var extend = function (original, {for (key inifif === || = Data] Tổng quan về on August 24, Trung Manh Trung Manh in to follow this Senior Java at SITA | J2EE | Adobe CQ5 AEM Expert | AWS Cloud là một nền tảng tích hợp làm cho quá trình data diễn ra nhanh chóng, dễ dàng và an toàn. Trong bài viết này, tôi muốn giới thiệu một số chi tiết cơ bản về Version 2.0.0 vừa phát hành và đây là thời điểm tuyệt vời để nhắc lại lại công cụ xuất sắc này. Tôi sẽ viết về một số hợp sử dụng cho các khái niệm chính của nó, và một số cân nhắc để xem có nên sử dụng nó hay không. Ngoài ra tôi sẽ cố gắng mô tả một số chi tiết về cách store và làm thế nào để extract chúng một cách tốt nhất. Tôi hy vọng rằng bạn sẽ dễ dàng làm quen với một số vấn đề kỹ thuật của ES và có thể bắt đầu áp dụng vào dự án của mình. là gì? Điều đầu tiên cần hỏi chính là: hiểu chính xác là gì? ES là một Rõ ràng nhiệm vụ của nó chính là store và Trong ES, tất cả các hiển thị trong JSON format. Nó xây dựng trên Lucene – phần mềm tìm kiếm và trả với hơn 15 năm kinh về full text and Nếu bạn đã từng sử dụng nghĩa là bạn bạn sẽ dễ dàng quen với JSON storage Nhưng điều làm cho ES thực sự đặc biệt chính là nhờ vào khả năng phục hồi thông tin của nó. Sự kết hợp của storage và service đã làm cho ES thực sự đặc biệt và khác xa 1 công cụ chỉ lưu trữ văn bản. Với bản chất của nó, một điều quan trọng cần biết đó là: khi nào thì nên sử dụng Bạn không nên đổi SQL Chúng có những mục đích khác nhau và mỗi cái đều có ưu và điểm riêng. Một số hợp nên sử dụng ES: Tìm kiếm text thông - for pure text kiếm text và dữ liệu có cấu trúc - text and data search by name +Tổng hợp dữ liệu - kiếm theo tọa độ - Geo trữ dữ liệu theo dạng JSON - JSON niệm cơ bản Chúng ta hãy nhìn vào những khái niệm chính của Một tập hợp Nodes chứa tất cả các dữ Một server duy nhất chứa một số dữ liệu và tham gia vào Hãy quên SQL Indexes đi. Mỗi ES Index là 1 tập hợp Tập con của Một Index có thể chia thành nhiều Một định nghĩa về schema of a bên trong một Index (Index có thể có nhiều JSON object với một số dữ liệu. Đây là basic unit trong ES.Ưu và điểm của hết tôi muốn nói về một trong những lý do cần phải xem xét để sử dụng ES trên hệ thống của bạn. Điều đầu tiên là tốc độ. ES có rất tốt. Như tôi đã nói, nó xây dựng trên Lucene và khả năng mở rộng truy vấn song song bên trong một cluster rất tốt queries in inside a Một ưu điểm khác của thể sắp quả truy vấn (sự liên quan). Theo mặc ES sử dụng thuật toán TF/IDF tương tự để tính toán Nếu bạn không biết là gì, hãy xem trang này trong tài liệu ES. Cuối cùng, ES có thể rất hữu ích để tạo ra số liệu thống kê tổng hợp và với chút ít nỗ lực, Search API có thể linh hoạt đáp ứng yêu cầu của bạn. Nhưng nó cũng có một số rất tốt trong việc tìm kiếm và tổng hợp data, nhưng nếu bạn đang sở hữu môi xuyên ghi dữ liệu ES có thể sẽ không phải lựa chọn tốt nhất của bạn. Ngoài ra, nó không có bất kỳ nào cả. Nhưng nếu bạn không dựa vào nó như nơi lưu trữ dữ liệu chính data bạn cũng sẽ ổn thôi, chẳng vấn đề gì cả. Rest API sử dụng một REST API cho việc tìm kiếm và lưu trữ Dưới đây là một ví dụ về a $ curl -XPUT -d '{  lucas,   tags: [java, web],  A Fancy  context: A nice post http:localhost:9200/ là địa chỉ của ES node. Với câu lệnh này, chúng ta đang tạo ra một blog post trong index blog với một type post  và một id=1. Lưu ý rằng nó có thể sử dụng một truy vấn POST thay vì PUT để tự động tạo ra id. Chính một JSON bình Dưới đây là request để lấy về một $ curl -XGET -d '{    query : {        term : { author : lucas }    }}' Nhìn vào sẽ rất dễ hiểu chúng ta đang truy vấn điều gì. Một thể gửi bằng GET hoặc POST . blog xác định index nào mà ta đang truy vấn và post là type Các tham số _search chỉ ra hành động mà chúng ta muốn thực hiện. Phần query sẽ xác định loại truy vấn và Trong hợp này, chúng ta đang tìm kiếm bất kỳ mà giá trị của author có từ khóa lucas. API có rất nhiều chi tiết và khả năng, đây chỉ là những điều cơ bản. Hãy xem trong tài liệu ES để tìm hiểu thêm về hai vấn đề này. khi đi sâu vào chi tiết về a Hãy cùng tìm hiểu về cách ES thực hiện truy bạn gửi một query đến ES query sẽ nhấn một trong các node và sau đó nó xác định shard nào nên truy vấn. Sau đó, query cho mỗi shard để thực hiện truy vấn song song. Sau khi lần lượt từng node trả lời các truy vấn với các kết quả từng phần, node sẽ merge kết quả và gửi trở lại cho user. Một trong những điều làm cho ES query rất nhanh chính là nó đã thực hiện truy vấn song song theo mặc định (đi qua shard). ES còn có những điểm khác giúp hiệu quả hơn. Khi lưu trữ trong ES, nó tạo ra một số data làm cho query perfom tốt hơn. Tôi sẽ nói về một số chi tiết cơ bản về ES Mỗi gửi tới ES lưu trữ qua một thuật toán và sau đó gửi đến shard. ES cố gắng để phân thông qua các shard. Khi lưu trữ ES tạo ra index, khóa xuất hiện trong này tới chính sử dụng index, nó có thể tìm kiếm thông qua terms như một binary tree (sử dụng thứ tự chữ cái) làm giảm thời gian tìm kiếm. Một điều quan trọng khi lưu trữ đó là quyết định cách tốt nhất để lưu trữ chúng giúp nâng cao tốc độ truy vấn. Khi thiết kế các giải pháp sử dụng điều đáng lưu tâm nhất khi lưu trữ chính là: tôi sẽ truy vấn này như thế nào? này tiếp cận với việc sử dụng cho tất cả các khả năng ES để thực hiện truy vấn cực kỳ nhanh chóng. là text input a chunk of text and outputs Một trong những tính năng tốt nhất của ES là đi kèm với rất nhiều Bạn hãy thử một chức năng mà có mỗi từ trong text trả về của mỗi từ. Hoặc một chức năng mà phải lấy text và remove tất words. Tùy thuộc vào những gì bạn cần, bạn có thể sử dụng một hoặc nhiều để thành text. Trong ES, rất hữu ích trong việc xây dựng index index) và đẩy nhanh tiến độ tìm kiếm thông qua của chúng ta. Một tính năng mạnh khác của đó chính là cung cấp tất cả query type. Có gần 40 query type và có lẽ là một trong những loại này sẽ đáp ứng hoàn hảo như cầu của bạn. Chúng tôi có phrase queries for textual search, geo queries based on numeric range Chúng có thể sẽ rất hữu ích cho các dữ liệu tổng hợp và nhiều hơn nữa. Kết luận Rất khó để viết về Tuy nhiên, nó khá đơn giản để hiểu và sử dụng, cũng như có rất nhiều tính năng để bàn đến. Đôi khi bạn có nhiều cách để chỉ index hoặc query Chỉ có sử dụng ES, bạn sẽ thấy nó tốt hơn. Một trong những lợi ích của ES là nó rất dễ cài đặt và trong nhiều hợp sử dụng, nó sẽ không phải là primary của bạn, vì vậy bạn hãy bớt lo lắng về việc thử dùng nó. Như đã đề cập, điều quan trọng là phải biết những ưu và điểm mà ES mang lại. Tuy không mới nhưng nó đang phát triển nhanh chóng. Các nhà phát triển đang triển khai và thêm nhiều tính năng mới trên tất cả các Nhưng cái chính là nó vẫn thống nhất nội dung và thực tế là nó xây dựng trên Lucene, vì vậy bạn có thể tự tin hơn khi sử dụng nó. dịch từ Trung Manh in to follow this Trung Manh Senior Java at SITA | J2EE | Adobe CQ5 AEM Expert | AWS Cloud for more of the latest on more upHelp Careers Talent Bahasa Bahasa Dansk Deutsch English Norsk Polski Svenska Tagalog Jobs Groups Titles © PolicyGuidelinesCookie r, a) {r = = || {};a = = || {};= r, a) {r = = || {};=== 'c5'; r, a) {r = = || {};= 'Looks like using a browser not <a href=  https:  /  /linkedin.com  /help  /linkedin  /answer  /4135?lang=en  >Learn more about you can r, a) {r = = || {};= {url:a = = || {};= {ie: 'v10', safari: 'v42','v7', 'v2.3', 'v0'}; if { = {}; } LI.RUM = LI.RUM || {}; win) { var doc = = || {}; = = = n5000; = = || {}; = = = || {}; = = = = = true; function getRumScript() { var node = || || script = =[https:static.licdn.com/scds/concat/common/js?h=ed29nkjpsa16bhrjq4na16owq-1mucgfycc664m7vmhpjgqse65-1l5rurej3h44qodo5rn0cdvyn-8om6v2ckrxsbnwf40t9ta8a7e-8jlhg6lqacthgadello7fgxzm-28w7d5j2k2jtil9ncckolke4m-9jzlwicvu376y9q4vjq77y5ks-1m0whdrwis44c1hoa9mrwhlt4-1uvutm1mpyov7rqhtcf8fksby-aac54ic1fmca5xz1yvc5t9nfe-1hn40w0bomeivihj9lopp4hp2-c0121povror81d0xao0yez4gy] [0]; } if { } else { } { var = true; var = { { } }; if { = { }; } var = { var = if { c1: 2, c2: c3: '', c4: '', c5: '', c6: '', c15: '' }); } }; var uc = = (new var = { if (typeof === { } else {  Default if not read from LIX. if { var img = new 1); = = function () { = = null; }; img.src = [ &rp=, timeStamp } } }; var track = { var et; if (_e) { } else if (_r && typeof === { try { function { try { et = } catch (e) { } }); } catch (e) { } } else if (_r && { try { et = } catch (e) { } } else { } }; track); { function go() { var a = = 'none'; a.src = if { } } if { go, false); } else if { go); }",
          "relevence": "no"
        },
        {
          "url": "https:laptrinhx.com/topic/10753/full-text-search-voi-hibernate-va-springmvc-phan-2-search-tieng-nhat",
          "title": "Full Text Search với và Phần 2: Search tiếng Nhật | | Dịch vụ chia sẻ kiến thức về lập trình miễn phí",
          "content": "Điều kiếm Tìm Top browser does not seem to support As a result, your viewing experience will be and you have been placed in a browser that or enable it if it's (i.e. Text Search với và Phần 2: Search tiếng Text Search với và Phần 2: Search tiếng đề này đã bị xóa. Chỉ ban quản trị mới xem chỉnh sửa lần cuối bởi bạn chưa xem P1 có thể xem lại tại phần 2 này, mình sẽ chia sẻ về Full text search với tiếng Nhật, khá thiết thực khi làm việc trong dự án Nhật. Những kiến thức mình chia sẻ dưới đây dừng lại ở mức cơ bản vì mình cũng mới tìm hiểu thôi, nhưng mình nghĩ nó cũng sẽ giúp ích bước đầu cho những bạn newbie như vào vấn đề thể hiểu nôm na như liệu đầu vào cần Lucene phân tích và lưu vào Lucene Index. Khi đó, tồn tại song song 2 vùng dữ liệu: và Lucene tạo mới, update, hay delete trên Lucene cần lại để đảm bảo tính đồng bộ dữ liệu (cái này làm tự search: Lucene parse từ khóa và tìm kiếm Lucene index- > Trả về kết quả là list các id đã sắp xếp theo thứ tự liên quan đến từ khóa -> lấy ra những cần thiết theo id đã nhận và trả để search tiếng Nhật (hay các ngôn ngữ khác) thì chỉ cần tác động vào bước data sao cho hợp lý là số khái niệm hay tìm hiểu về Lucene hay Search các bạn sẽ bắt gặp những khái niệm sau, mình tổng hợp lại để tiện cho các bạn nghiên cứu sau này. Còn nếu bạn chỉ muốn chạy sample thì bỏ qua phần này cũng Quá trình index Kết quả của quá trình sẽ chia nhỏ data ban đầu ra thành những thành phần nhỏ hơn, gọi là Đôi khi việc khá đơn giản nên không đủ cho việc tìm kiếm, khi đó cần đến biện pháp phân tích sâu hơn -> gọi là Có 2 cơ chế khi và sau khi and có thể bao gồm việc cắt bỏ HTML, biến đổi hoặc xóa bỏ text theo Có nhiều loại, mỗi loại có 1 tác dụng khác nhau. Trong Lucene kế thừa từ lớp Các loại tiêu biểu là:⋅ Thay thế từ ban đầu bởi từ gốc của nó. Ví dụ: bikes -> bike⋅ Stop Words Những từ như a, and, an, the gọi là stop word. CHúng cho là không có ý nghĩa cho việc search, vậy nên bước này xóa bỏ stop words đi để giảm index size và tăng tìm kiếm: Tăng tốc độ và giảm Synonym Thêm những token với từ đồng nghĩa với từ gốc để tăng hiệu quả tìm vào yêu cầu cụ thể mà chúng ta lựa chọn kiểu Solr: Search server viết bằng đã nói ở trên, tất cả những gì chúng ta cần làm là tìm cách để tiếng Nhật. Để làm điều đó, ta sử dụng thư viện Thư viện này phát triển bởi công ty atilika Nhật Bản (repo gốc) nhưng sau đó đã tích hợp vào Lucene từ version pom.xml cần thêm sẽ có dạng như này, đầy đủ có trong sample project cuối --> <!-- Apache lucene có rất nhiều các ý nghĩa của mỗi loại các bạn có thể tra cứu tại đây class có đuôi Sử dụng filter nào thì tùy vào bài viết này mình sẽ sử dụng các loại Filter với ý nghĩa như Thay thế các tính từ, động từ bằng từ gốc, ví dụ: từ (mua) sẽ thay thế bởi từ Loại bỏ những bằng pháp gán nhãn từ loại Danh sách tiếng Nhật các bạn xem tại Chuẩn hóa ký tự và Loại bỏ Chuẩn hóa cách đọc của những từ thông dụng. Trong tiếng Nhật, là chữ để phiên âm những từ tiếng nước ngoài cho Nhật dễ đọc, vì vậy 1 từ tiếng Anh có thể phiên âm nhiều cách. Ví dụ manager có thể phiên âm thành hoặc Nếu trong văn bản có chứa ký tự latinh thì nó sẽ chuẩn hóa về ký tự viết Nếu các bạn muốn tìm kiếm theo cách đọc của chữ kanji thì sử dụng thêm Filter này. Các chữ kanji sẽ theo cách đọc dựa vào ngữ cảnh tương dụng các Filter này, các bạn khai báo bộ và filter (gọi là custom chain) trong Entity class bằng Sử dụng tại nào mà các bạn muốn bằng = nào không có thì sẽ dùng sau khi khai báo như = = = = = = = = = = = = class Book = Integer = title, false, length = Analyze.YES, store= = String = false, length = = String = author, false, length = = String Getter and các bạn vì phần này mình đã viết rồi nhưng ko hiểu sao khi public lại bị mất. Kết quả dưới đây do mình nhớ và viết lại kiếm với từ 買います,author: lorem lorem lorem kiếm với từ 天気,author: lorem lorem lorem source code sample tại đây: (Nên dẫnTải thêm các bài gửi khác 1Bài lời dưới dạng chủ đăng nhập để trả / Text Search với và Phần 2: Search tiếng Text Search với và Phần 2: Search tiếng bạn chưa xem P1 có thể xem lại tại phần 2 này, mình sẽ chia sẻ về Full text search với tiếng Nhật, khá thiết thực khi làm việc trong dự án Nhật. Những kiến thức mình chia sẻ dưới đây dừng lại ở mức cơ bản vì mình cũng mới tìm hiểu thôi, nhưng mình nghĩ nó cũng sẽ giúp ích bước đầu cho những bạn newbie như /> OK, vào vấn đề thể hiểu nôm na như liệu đầu vào cần Lucene phân tích và lưu vào Lucene Index. Khi đó, tồn tại song song 2 vùng dữ liệu: và Lucene tạo mới, update, hay delete trên Lucene cần lại để đảm bảo tính đồng bộ dữ liệu (cái này làm tự search: Lucene parse từ khóa và tìm kiếm Lucene index- &gt; Trả về kết quả là list các id đã sắp xếp theo thứ tự liên quan đến từ khóa -&gt; lấy ra những cần thiết theo id đã nhận và trả để search tiếng Nhật (hay các ngôn ngữ khác) thì chỉ cần tác động vào bước data sao cho hợp lý là số khái niệm hay tìm hiểu về Lucene hay Search các bạn sẽ bắt gặp những khái niệm sau, mình tổng hợp lại để tiện cho các bạn nghiên cứu sau này. Còn nếu bạn chỉ muốn chạy sample thì bỏ qua phần này cũng Quá trình index Kết quả của quá trình sẽ chia nhỏ data ban đầu ra thành những thành phần nhỏ hơn, gọi là Đôi khi việc khá đơn giản nên không đủ cho việc tìm kiếm, khi đó cần đến biện pháp phân tích sâu hơn -&gt; gọi là Có2 cơ chế khi và sau khi and có thể bao gồm việc cắt bỏ HTML, biến xóa bỏ text theo Có nhiều loại, mỗi loại có 1 tác dụng khác nhau. Trong Lucene kế thừa từ lớp Các loại tiêu biểu là:<br /> ⋅ Thay thế từ ban đầu bởi từ gốc của nó. Ví dụ: bikes -&gt; bike<br /> ⋅ Stop Words Những từ như a, and, an, the gọi là stop word. CHúng cho là không có ý nghĩa cho việc search, vậy nên bước này xóa bỏ stop words đi để giảm index size và tăng tìm kiếm: Tăng tốc độ và giảm /> ⋅ Synonym Thêm những token với từ đồng nghĩa với từ gốc để tăng hiệu quả tìm /> Tùy vào yêu cầu cụ thể mà chúng ta lựa chọn kiểu Search server viết bằng đã nói ở trên, tất cả những gì chúng ta cần làm là tìm cách để tiếng Nhật. Để làm điều đó, ta sử dụng thư viện Thư viện này phát triển bởi công ty Nhật Bản (<a rel=  nofollow  >reponhưng sau đó đã tích hợp vào Lucene từ version 3.6.<br /> File cần thêm sẽ có dạng như này, đầy đủ có trong sample project cuối --&gt;  &lt;!-- Apache lucene có rất nhiều các ý nghĩa của mỗi loại các bạn có thể tra cứu tại <a rel=  nofollow  >đây<  /a> class có đuôi Sử dụng filter nào thì tùy vào bài viết này mình sẽ sử dụng các loại Filter với ý nghĩa như Thay thế các tính từ, động từ bằng từ gốc, ví dụ: từ (mua) sẽ thay thế bởi từ Loại bỏ những bằng pháp nhãn từ tagging). Danh sách tiếng Nhật các bạn xem tại <a rel=  nofollow  >đây<  /a>.<  /li> <li><strong>CJKWidthFilter<  /strong>: Chuẩn hóa ký tự và Loại bỏ Chuẩn hóa cách đọc của những từ thông dụng. Trong tiếng Nhật, là chữ để phiên âm những từ tiếng nước ngoài cho Nhật dễ đọc, vì vậy 1 từ tiếng Anh có thể phiên âm nhiều cách. Ví dụ manager có thể phiên âm thành hoặc Nếu trong văn bản có chứa ký tự latinh thì nó sẽ chuẩn hóa về ký tự viết Nếu các bạn muốn tìm kiếm theo cách đọc của chữ kanji thì sử dụng thêm Filter này. Các chữ kanji sẽ theo cách đọc dựa vào ngữ cảnh tương dụng các Filter này, các bạn khai báo bộ và filter (gọi là custom chain) trong Entity class bằng Sử dụng tại nào mà các bạn muốn bằng = nào không có thì sẽ dùng sau khi khai báo như = = = = = = = = = = = = class Book = Integer = false, length = Analyze.YES, store= = String = false, length = = String = false, length = = analyze=Analyze.YES, String Getter and các bạn vì phần này mình đã viết rồi nhưng ko hiểu sao khi public lại bị mất. Kết quả dưới đây do mình nhớ và viết lại kiếm với từ ipsum, &quot;loremkiếm với từ ipsum, &quot;lorembạn source code sample tại đây: <a rel=  nofollow  >https:drive.google.com/file/d/0B9SCEcmTeQHBLTBBVUU0ejV6VE0/view?usp=sharing<  /a><  /p> <h1>Tham rel=  nofollow  >https:speakerdeck.com/atilika/japanese-linguistics-in-lucene-and-solr<  /a>đọc)<  /strong><br /> <a rel=  nofollow  >https:speakerdeck.com/atilika/language-support-and-linguistics-in-lucene-solr-and-elasticsearch-and-the-eco-system<  /a><br /> <a rel=  nofollow  >http:hibernate.org/search/documentation/getting-started/<  /a><br /> <a rel=  nofollow  >http:www.lucenetutorial.com/index.html<  /a><br Viet đàn,description:Dịch vụ chia sẻ kiến thức về lập trình miễn vụ chia sẻ kiến thức về lập trình miễn col-xs-6,imageClass:cover,totalPostCount:15490,totalTopicCount:15460},thread_tools:[],isFollowing:false,isNotFollowing:true,isIgnoring:false,bookmark:null,postSharing:[{id:facebook,name:Facebook,class:fa-facebook,activated:true},{id:twitter,name:Twitter,class:fa-twitter,activated:true},{id:google,name:Google+,class:fa-google-plus,activated:true}],deleter:null,deletedTimestampISO:,related:[],unreplied:true,deleted:false,locked:false,pinned:false,icons:[],breadcrumbs:[{text:[[global:home]],url:/},{text:Diễn Text Search với Hibernate và Phần 2: Search tiếng page-topic-full-text-search-voi-hibernate-va-springmvc-phan-2-search-tieng-nhat,widgets:{}} Error.&#x3C;/strong&#x3E;&#x9;&#x3C;p&#x3E;:( ! Hình như có trục trặc gì IF error ENDIF error vẻ như bạn đã mất kết nối tới vui lòng đợi một lúc để chúng tôi thử kết nối lại.require(['forum/footer']);",
          "relevence": "no"
        },
        {
          "url": "https:techtalk.vn/big-data-tong-quan-ve-elasticsearch.html",
          "title": "[Big Data] Tổng quan về | Tech Talk",
          "content": "Công trình ứng trình & gia Sign nhập khẩu của your Get help tạo của khẩu đã gửi vào email Talk Công = new = = c  u1ea3,color_preset:,border_top:,class:td_uid_2_59c7d145e215b_rand,offset:,css:,live_filter:,live_filter_cur_post_id:,live_filter_cur_post_author:}';block_td_uid_2_59c7d145e215b.td_column_number = = = = = = = mặt của đồng Bitcoin và câu đằng sau đang tìm kiếm thời gian để trở thành Desktop 1.0 đã chính thức phát thiết bị kì lạ có thể đào hiểu Azure Lập = new = = c  u1ea3,color_preset:,border_top:,class:td_uid_3_59c7d145e9d3e_rand,offset:,css:,live_filter:,live_filter_cur_post_id:,live_filter_cur_post_author:}';block_td_uid_3_59c7d145e9d3e.td_column_number = = = = = = = trình ứng trình dụng MySql Binary Log để giải quyết vấn Web Service cung hoàng đạo đơn quen với PAYMENT REQUEST – hệ thống 23 mẫu Design Tools & Tipsvar = new = = c  u1ea3,color_preset:,border_top:,class:td_uid_7_59c7d145f222b_rand,offset:,css:,live_filter:,live_filter_cur_post_id:,live_filter_cur_post_author:}';block_td_uid_7_59c7d145f222b.td_column_number = = = = = = = cách giúp lập trình viên tăng năng suất làm trình viên, liệu bạn đã đủ trong[TÀI LIỆU] Mobile App with React Apply và Bind liệu] Ebook Kotlin for Android Sự = new = = c  u1ea3,color_preset:,border_top:,class:td_uid_8_59c7d146026e5_rand,offset:,css:,live_filter:,live_filter_cur_post_id:,live_filter_cur_post_author:}';block_td_uid_8_59c7d146026e5.td_column_number = = = = = = = sự kiện lập trình bạn nên tham gia kết Bot Battle – Đọng lại những kiện Windows Day năm nay sẽ bổng] Cơ hội học tiếng Nhật miễn phí Web Summit 2017- Chuỗi sự kiện cộng đồng Chuyên gia nóivar = new = = c  u1ea3,color_preset:,border_top:,class:td_uid_9_59c7d146064b3_rand,offset:,css:,live_filter:,live_filter_cur_post_id:,live_filter_cur_post_author:}';block_td_uid_9_59c7d146064b3.td_column_number = = = = = = = và ứng dụng Real Time cải thiện dụng kỹ thuật Analysis trong dụng ký pháp BEM cho giờ đã bình chọn là xu tuyển dụng thiếu lập trình viên sai… Tâm sự = new = = c  u1ea3,color_preset:,border_top:,class:td_uid_10_59c7d1460a561_rand,offset:,css:,live_filter:,live_filter_cur_post_id:,live_filter_cur_post_author:}';block_td_uid_10_59c7d1460a561.td_column_number = = = = = = = trình viên và học ngoại code vì trái của ngành kiểm thử phần Tar Pit – Vũng trong nghề lập tôi không có khái niệm đi ChủCông nghệ Tổng quan Data] Tổng quan 22, sẻTweettweet là một nền tảng tích hợp làm cho quá trình data diễn ra nhanh chóng, dễ dàng và an bài viết này, tôi muốn giới thiệu một số chi tiết cơ bản về Version 2.0.0 vừa phát hành và đây là thời điểm tuyệt vời để nhắc lại lại công cụ xuất sắc sẽ viết về một số hợp sử dụng cho các khái niệm chính của nó, và một số cân nhắc để xem có nên sử dụng nó hay không. Ngoài ra tôi sẽ cố gắng mô tả một số chi tiết về cách store và làm thế nào để extract chúng một cách tốt nhất. Tôi hy vọng rằng bạn sẽ dễ dàng làm quen với một số vấn đề kỹ thuật của ES và có thể bắt đầu áp dụng vào dự án của là đầu tiên cần hỏi chính là: hiểu chính xác là gì?ES là một Rõ ràng nhiệm vụ của nó chính là store và Trong ES, tất cả các hiển thị trong JSON format. Nó xây dựng trên Lucene – phần mềm tìm kiếm và trả retrieval software) với hơn 15 năm kinh về full text and bạn đã từng sử dụng nghĩa là bạn bạn sẽ dễ dàng quen với JSON storage Nhưng điều làm cho ES thực sự đặc biệt chính là nhờ vào khả năng phục hồi thông tin của nó. Sự kết hợp của storage và service đã làm cho ES thực sự đặc biệt và khác xa 1 công cụ chỉ lưu trữ văn bản chất của nó, một điều quan trọng cần biết đó là: khi nào thì nên sử dụng Bạn không nên đổi SQL Chúng có những mục đích khác nhau và mỗi cái đều có ưu và điểm riêng. Một số hợp nên sử dụng ES:Tìm kiếm text thông – for pure text kiếm text và dữ liệu có cấu trúc – text and data search by name + hợp dữ liệu – Data kiếm theo tọa độ – Geo trữ dữ liệu theo dạng JSON – JSON niệm cơ ta hãy nhìn vào những khái niệm chính của Một tập hợp Nodes chứa tất cả các dữ Một server duy nhất chứa một số dữ liệu và tham gia vào and Hãy quên SQL Indexes đi. Mỗi ES Index là 1 tập hợp các Tập con của Một Index có thể chia thành nhiều Một định nghĩa về schema of a bên trong một Index (Index có thể có nhiều Một JSON object với một số dữ liệu. Đây là basic unit trong ES.Ưu và điểm tin cho tôi muốn nói về một trong những lý do cần phải xem xét để sử dụng ES trên hệ thống của đầu tiên là tốc độ. ES có rất tốt. Như tôi đã nói, nó xây dựng trên Lucene và khả năng mở rộng truy vấn song song bên trong một cluster rất tốt queries in inside a ưu điểm khác của thể sắp quả truy vấn (sự liên quan). Theo mặc ES sử dụng thuật toán TF/IDF tương tự để tính toán Nếu bạn không biết là gì, hãy xem trang này trong tài liệu cùng, ES có thể rất hữu ích để tạo ra số liệu thống kê tổng hợp và với chút ít nỗ lực, Search API có thể linh hoạt đáp ứng yêu cầu của nó cũng có một số rất tốt trong việc tìm kiếm và tổng hợp data, nhưng nếu bạn đang sở hữu môi xuyên ghi dữ liệu ES có thể sẽ không phải lựa chọn tốt nhất của ra, nó không có bất kỳ nào cả. Nhưng nếu bạn không dựa vào nó như nơi lưu trữ dữ liệu chính data bạn cũng sẽ ổn thôi, chẳng vấn đề gì sử dụng một REST API cho việc tìm kiếm và lưu trữ Dưới đây là một ví dụ về a curl -XPUT -d ‘{  [“java”, “A Fancy “A nice post là địa chỉ của ES node. Với câu lệnh này, chúng ta đang tạo ra một blog post trong index với một type  và một Lưu ý rằng nó có thể sử dụng một truy vấn thay vì để tự động tạo ra id. Chính một JSON bình mật đằng sau công nghệ tìm kiếm của Đăng kí đây là request để lấy về một curl -XGET -d ‘{    : {        : { : }    vào sẽ rất dễ hiểu chúng ta đang truy vấn điều gì. Một thể gửi bằng hoặc . xác định index nào mà ta đang truy vấn và là type Các tham số chỉ ra hành động mà chúng ta muốn thực hiện. Phần query sẽ xác định loại truy vấn và Trong hợp này, chúng ta đang tìm kiếm bất kỳ mà giá trị của có từ khóa API có rất nhiều chi tiết và khả năng, đây chỉ là những điều cơ bản. Hãy xem trong tài liệu ES để tìm hiểu thêm về hai vấn đề khi đi sâu vào chi tiết về a Hãy cùng tìm hiểu về cách ES thực hiện truy bạn gửi một query đến ES query sẽ nhấn một trong các node và sau đó nó xác định shard nào nên truy vấn. Sau đó, query cho mỗi shard để thực hiện truy vấn song song. Sau khi lần lượt từng node trả lời các truy vấn với các kết quả từng phần, node sẽ merge kết quả và gửi trở lại cho user. Một trong những điều làm cho ES query rất nhanh chính là nó đã thực hiện truy vấn song song theo mặc định (đi qua còn có những điểm khác giúp hiệu quả hơn. Khi lưu trữ trong ES, nó tạo ra một số data làm cho query perfom tốt hơn. Tôi sẽ nói về một số chi tiết cơ bản về ES technique.Mỗi gửi tới ES lưu trữ qua một thuật toán và sau đó gửi đến shard. ES cố gắng để phân thông qua các lưu trữ ES tạo ra index, khóa xuất hiện trong này tới chính sử dụng index, nó có thể tìm kiếm thông qua terms như một binary tree (sử dụng thứ tự chữ cái) làm giảm thời gian tìm điều quan trọng khi lưu trữ đó là quyết định cách tốt nhất để lưu trữ chúng giúp nâng cao tốc độ truy vấn. Khi thiết kế các giải pháp sử dụng điều đáng lưu tâm nhất khi lưu trữ chính là: tôi sẽ truy vấn này như thế nào? này tiếp cận với việc sử dụng cho tất cả các khả năng ES để thực hiện truy vấn cực kỳ nhanh là text input a chunk of text and outputs Một trong những tính năng tốt nhất của ES là đi kèm với rất nhiều hãy thử một chức năng mà có mỗi từ trong text trả về của mỗi từ. Hoặc một chức năng mà phải lấy text và remove tất words. Tùy thuộc vào những gì bạn cần, bạn có thể sử dụng một hoặc nhiều để thành ES, rất hữu ích trong việc xây dựng index index) và đẩy nhanh tiến độ tìm kiếm thông qua của chúng tính năng mạnh khác của đó chính là cung cấp tất cả query type. Có gần 40 query type và có lẽ là một trong những loại này sẽ đáp ứng hoàn hảo như cầu của tôi có phrase queries for textual search, geo queries based on numeric range Chúng có thể sẽ rất hữu ích cho các dữ liệu tổng hợp và nhiều hơn mật đằng sau công nghệ tìm kiếm của Đăng kí khó để viết về Tuy nhiên, nó khá đơn giản để hiểu và sử dụng, cũng như có rất nhiều tính năng để bàn đến. Đôi khi bạn có nhiều cách để chỉ index hoặc query Chỉ có sử dụng ES, bạn sẽ thấy nó tốt trong những lợi ích của ES là nó rất dễ cài đặt và trong nhiều hợp sử dụng, nó sẽ không phải là primary của bạn, vì vậy bạn hãy bớt lo lắng về việc thử dùng đã đề cập, điều quan trọng là phải biết những ưu và điểm mà ES mang lại. Tuy không mới nhưng nó đang phát triển nhanh chóng. Các nhà phát triển đang triển khai và thêm nhiều tính năng mới trên tất cả các Nhưng cái chính là nó vẫn thống nhất nội dung và thực tế là nó xây dựng trên Lucene, vì vậy bạn có thể tự tin hơn khi sử dụng hiểu rõ thêm về công nghệ này! Hãy cùng tham gia sự kiện  Bí mật đằng sau công nghệ tìm kiếm của Đăng kí via CHIA js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=platform.twitter.com/widgets.js;fjs.parentNode.insertBefore(js,fjs);}}(document,script,twitter-wjs);Bài viết dụ Clean trong kế Beauty: Recipes & BestAi Hien var = new = = c  u1ea3,color_preset:,border_top:,class:td_uid_13_59c7d14614da9_rand,offset:,css:,live_filter:cur_post_same_tags,live_filter_cur_post_id:13934,live_filter_cur_post_author:14}';block_td_uid_13_59c7d14614da9.td_column_number = = = = = = = VIẾT LIÊN QUANXEM THÊM3 sự kiện lập trình bạn nên tham gia ngay cuối tháng 9 này! Chung kết Bot Battle – Đọng lại những khắc Vietnam Web Summit 2017- Chuỗi sự kiện cộng đồng phát triển Web mong chờ nhất trong năm đã chính thức quay trở (max-width:767px) span 100% iframe {width: 100% -4pxs, id) {var js, fjs =if = js.id = = 'facebook-jssdk'));Facebook By Powered Byvar = new = = gia td_uid_17_59c7d14616ab6_rand,offset:,css:,live_filter:,live_filter_cur_post_id:,live_filter_cur_post_author:}';block_td_uid_17_59c7d14616ab6.td_column_number = = = = = = = gia lệch kỹ năng trong lập 4, trình viên hạnh phúc9, Giấc ngủ ngắn nhưng hiệu quả cho lập trình 22, 2017 var = new = = coder,custom_url:https:  /  /techtalk.vn  /category  /tam-su-coder,show_child_cat:,sub_cat_ajax:,ajax_pagination:load_more,header_color:#,header_text_color:#,ajax_pagination_infinite_stop:,td_column_number:1,td_ajax_preloading:,td_ajax_filter_type:,td_ajax_filter_ids:,td_filter_default_txt:All,color_preset:,border_top:,class:td_block_widget = = = = = = = sự việc yêu máy tính là sai trái, thì tôi cũng chẳng 28, 2017 Một ngày làm việc của Lập Trình Viên 23, 2017 Tâm sự con gái làm coder9, 2016 Xem thêm var = new = = ki  u1ec7n,custom_url:https:  /  /techtalk.vn  /category  /su-kien,show_child_cat:,sub_cat_ajax:,ajax_pagination:load_more,header_color:#,header_text_color:#,ajax_pagination_infinite_stop:,td_column_number:1,td_ajax_preloading:,td_ajax_filter_type:,td_ajax_filter_ids:,td_filter_default_txt:All,color_preset:,border_top:,class:td_block_widget = = = = = = = sự kiện lập trình bạn nên tham gia ngay cuối tháng 9 2017Chung kết Bot Battle – Đọng lại những 2017Sự kiện Windows Day năm nay sẽ tổ chức vào ngày 10 tháng 1021, 2017Xem thêm VỀ CHÚNG TÔI• Giấy phép thiết lập Mạng xã hội số do Bộ Thông Tin và Thông Cơ quan chủ quản: Công ty Cổ phần sở: 179 Đình Chính, 11, Quận PN, TP.HCM Liên hệ: - Tel: 028 6264 CHÚNG TÔI {tdAjaxCount.tdGetViewsCountsAjax(post,[13934]);});html_jquery_obj = && || path = = = = > 1) {var = = (var i = 0; i < i++) {if (i > 0) = + ' ' + + + = function {return ' + + '/' + += <style> + + = > > > > > > > li > > >> >> >> >> >ul >ul > a,ul > a,ul > a,ul > a, .wpb_button:hover,div,.td-current-sub-category, .td-post-category,.widget_product_search input#submit:hover,.checkout .cart div,.bbp-pagination#bbp-single-user-details a,.slide-meta-cat { .button:hover,.woocommerce-error {#ec1f27}.product.woocommerce.widget {none}ul li a {none repeat scroll 0 0 > > > p,blockquote p,blockquote a,.td-subcat-list.td-subcat-dropdown a,.td_mod_wrap:hover a, > .td-pulldown-category-filter-link:hover,.td-subcat-dropdown.td-category-siblings a.td-current-sub-category,.widgettfoota.added_to_cart:hover,li.bbp-header span a:hover,.bbp-topic-freshness li a:hover,.bbp-topic-started-in .sticky .bbp-topic-permalink,.bbp-author-name,.bbp-author-name,.footer-email-wrap a,li .td-smart-list-button:hover,.td-read-more a:hover,a:hover,a ul li a:hover {color: div,.td-category-header a:hover,.bbp-pagination {#ec1f27}.td-drop-down-search:before #ec1f27}> span,> a,li:hover .vc_tta-tab.vc_active > .vc_tta-tab:hover > .td-cur-simple-item,.product {#ec1f27;}div.product ul.tabs {color:.entry-title{rgba(236, 31, 39, 0.7);}> span,> a,li:hover .vc_tta-tab.vc_active > .vc_tta-tab:hover > .td-cur-simple-item,.product {#ec1f27;}div.product ul.tabs ul.tabs:before {#ec1f27;}.current-menu-item > > > a,li a:hover {color: > li > {color:}@media 767px) {body {#ffffff}}@media 767px) {body {color: .entry-title a {color:} p {color:}> li > { li a { }> > a { }ul a { a { a { > li > a { a {> span,> a, a,.vc_tta-color-grey.vc_tta-tabs-position-top.vc_tta-style-classic > div.product ul.tabs li a,.woocommerce .product h2 { a { { { { a,.td-subcat-list span { }{ .entry-title a,.entry-title.post .entry-title { }p,{ }.post p,.page p,blockquote p { }.post p,.page p { }.post p,.page p { }li { }h1 { }h2 { }h3 { }h4 { }h5 { }h6 {.post a { }.post header a { }.post header { }.post header header { }.post a,.post span { }.post span { }.post a { }.post a { }.post a { }.post { .entry-title a { }.post { { { { > p,p { }h1,h1 { }h2,h2 { }h3,h3 { }h4,h4 { }h5,h5 { }h6,h6 ul li a,.bbp-breadcrumb { a { { }a,span { { }a, .product-categories }input[type=submit],.td-read-more #respond p { s, id) {var js, fjs =if = js.id = = 'facebook-jssdk'));!function(f,b,e,v,n,t,s){if(f.fbq)return;n=f.fbq=function(){n.callMethod?n.callMethod.apply(n,arguments):n.queue.push(arguments)};if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';n.queue=[];t=b.createElement(e);t.async=!0;t.src=v;s=b.getElementsByTagName(e)[0];s.parentNode.insertBefore(t,s)}(window,document,'script','https:connect.facebook.net/en_US/fbevents.js');fbq('init',  Insert your pixel ID undefined){var = {};= 3,token: po = po.type = = =var s = s); var s1=document.createElement(script),s0=document.getElementsByTagName(script)[0];s0.parentNode.insertBefore(s1,s0);})();",
          "relevence": "no"
        },
        {
          "url": "http:it.die.vn/f/full-text-search/",
          "title": "Full text search là gì? Khái niệm, định nghĩa",
          "content": "Nơi lưu trữ những khái niệm, định nghĩa liên quan đến công nghệ thông ngữ tin game Kiếm Tool $$ CƠ HỘI KIẾM TIỀN LỚN VÀ ĐỌC SÁCH BẢN QUYỀN / Tìm hiểu: Full định nghĩa Full text search là 2 năm bởi top9xy và 1474 Views đơn giản dễ hiểu, full text search (gọi tắt là FTS) là cách tự nhiên nhất để tìm kiếm thông tin, hệt như Google, ta chỉ cần gõ từ khóa và nhấn enter thế là có kết quả trả về. Phạm vi bài viết này chỉ đề cập, giới thiệu sơ lược về FTS trong MySQL mà không bàn về các FTS engine như Sphinx hay Solr. ##II. Tại sao chúng ta phải dùng Full text search? Bình chúng ta sẽ sử dụng câu truy vấn dạng như sau để tìm kiếm dữ FROM book WHERE title LIKE cách truy vấn này có một số hạn chế như ý: đây là những hạn chế chung trong MySQL, ngay cả full text search của MySQL cũng không giải quyết triệt để các vấn đề này mà phải dùng các search engine ngoài như Solr, Sphinx v.v Nhưng mình muốn nêu lên ở đây để chúng ta có cái nhìn rõ ràng về những điểm Không chính nhiễu cao: Giả sử, bạn có câu truy vấn với mệnh đề LIKE như sau: Title LIKE Thì nó sẽ có thể trả về những kể quả sau: one, zone, money, phone nói chung là không chính xác vì dải kết quả trả về sẽ rộng và có thể chứa nhiều kết quả nhiễu không mong đồng nghĩa Như chúng ta đã biết, ngôn ngữ nào cũng có những từ đồng nghĩa, ví dụ như trong tiếng Việt là xe hơi – ôtô, bao thư – phong bì v.v. Tiếng Anh thì đơn giản thì có v.v Nếu như dùng LIKE hay = (thậm chí search của MySQL) thì tất nhiên không thể giải quyết vấn từ đồng nghĩa cấu tạo bằng chữ đầu của cụm từ Đôi lúc với những cụm từ dài và phổ biến chúng ta viết tắt ví dụ như THPT, CNTT, US, IT. Nhưng khi dùng tìm kiếm thì họ có thể nhập khác với trong chúng ta lưu trữ (viết thu gọn – viết đủ và lại) cho nên đây cũng là một khó khăn mà chúng ta gặp phải khi làm chức năng search. Mong muốn của dùng là họ tìm thấy kết quả mong muốn cho dù họ viết tắt hay viết đầy Tốc độ truy vấn chậm, không dùng như ta đặt ‘%’ ở phía thì MySQL sẽ thực hiện câu truy vấn mà không dùng index, MySQL sẽ thực hiện scan toàn bộ dữ liệu của nó từ đầu đến cuối, cho nên câu truy vấn sẽ rất chậm so với search trên index. Giống như ta tìm từng trang trong một cuốn sách thay vì tìm trong trang index đằng sau quyển sách đó vậy. Để hiểu rõ hơn vì sao dùng index lại nhanh hơn chúng ta sẽ tìm hiểu nó trong các phần sau. The index also can be used for LIKE if the to LIKE is a string that does not start with a Vấn đề với tìm kiếm tiếng Việt có dấu và không sử ta lưu tiếng Việt có dấu trong nhưng dùng nhập tiếng Việt không dấu thì mệnh đề LIKE chắc chắn sẽ không tìm ra dữ liệu ta cần. Có một số giải pháp ví dụ như lưu 2 field, một có dấu và một không dấu, nhưng cách này xem ra không tối ưu và không hỗ trợ search gần đúng. Nếu như dùng nhập “co be mua dogn” thì dùng mệnh đề LIKE sẽ không search ra “Cô bé mùa nhưng FTS có thể giải quyết vấn đề Sơ lược MySQL lược thì MySQL FTS hiện tại chỉ có trên storage engine MyISAM và mới có trên InnoDB (>=5.6 2 chế độ tìm kiếm đó là BOOLEAN MODE và NATURAL MODE. Trong BOOLEAN MODE thì không có default và trong chế độ này thì ta có thể qui định từ khóa nào sẽ xuất hiện, và từ khóa nào không xuất hiện trong kết quả trả về. Còn NATURAL MODE thì tìm kiếm những kết quả thích hợp hơn là chính xác keyword định thì MySQL có một list các nghĩa là các từ mà MySQL sẽ bỏ qua không search nếu gặp phải nó (ví dụ: the, and, or, for v.v). Tham khảo stop words list của MySQL ở ra thì mặc định MySQL FTS chỉ tìm những từ có độ dài tối thiểu là 4 ký tự (global = 4). Từ thực tế, nếu như ta search những chữ có độ dài bé hơn 4 (ví dụ “Hà “Cà ký” “The way I am”) thì sẽ không có kết quả nào trả về, cho nên ta phải lưu ý tới thiết lập này của LIKE SHOW LIKE Valueft_boolean_syntax| + ||| 84 ||| 4|| | 20 || | rows in set (0.11 Ứng dụng tham TABLE IF NOT EXISTS `jobs` ( `id` INT(11) NOT NULL INT(11) DEFAULT NULL, `title` COLLATE NOT NULL, COLLATE NOT NULL, TEXT COLLATE NOT ) * FROM `jobs ` WHERE IN NATURAL MODE); * FROM `jobs ` WHERE IN BOOLEAN postNextBiết đâu cũng FDDI (Fiber (Fully Domain hosts, 2017 - Thuật ngữ công nghệ thông tin tổng hợp sticky_navigation_offset_top = = =  our current from the > { 'left':0, });} else 'relative','padding-top':0, '#42586a' }); }run our function on and run it again every time you ]]> */ 'http:it.die.vn/index.php?ajax=1&s='});});/* */var = the Top};/* ]]> */",
          "relevence": "no"
        },
        {
          "url": "https:www.researchgate.net/publication/259903895_Mot_giai_phap_tom_tat_van_ban_tieng_Viet_tu_dong",
          "title": "Một giải pháp tóm tắt văn bản tiếng Việt tự động (PDF Available)",
          "content": "var node=document.getElementsByTagName(script)[0];node.parentNode.insertBefore(gads,node);})();For full of it is to are the how to enable in your webSee all ›1 all ›18 Reddit Một giải pháp tóm tắt văn bản tiếng Việt tự Reads Một số vấn đề chọn lọc của Công nghệ thông tin và Thông, At Hà Nội, Việt NamCite this Tho Quang bài báo này chúng tôi đề xuất mô hình tóm tắt văn bản tiếng Việt tự Văn bản biểu diễn dưới dạng đồ thị, mỗi đỉnh trong đồ thị biểu diễn một câu trong văn bản, các cạnh nối giữa các đỉnh biểu diễn sự tương tự về ngữ nghĩa giữa hai đỉnh (câu). Giá trị tương tự biểu diễn dưới dạng trọng số của các cạnh. Chúng tôi sử dụng 3 thuật toán thống kê dựa trên từ vựng để tính độ tương tự câu là Jaro, Model và Độ quan trọng của đỉnh (câu) tính bởi thuật toán một giải thuật toán học dựa trên đồ thị, tùy biến để tích hợp độ tương tự câu. Hệ thống sẽ tự động chọn các câu quan trọng nhất (mặc định là 25% tổng số câu) để đưa vào kết quả tóm tắt. Để kiểm chứng tính chính xác của mô hình đề xuất, chúng tôi so sánh kết quả tóm tắt tự động với kết quả tóm tắt của chuyên gia vì thế dữ liệu thực sử dụng là khá khiêm tốn (gồm 5 văn bản thuộc các chủ đề khác nhau). Kết quả tóm tắt của hệ thống có độ tin cậy cao vì đánh giá bởi tập dữ liệu đánh giá tổng hợp từ 12 nhà khoa học uy tín. Kết quả cho thấy việc kết hợp các thuật toán thống kê với thuật toán xếp hạng dựa trên đồ thị có tích hợp độ tương tự câu cho độ chính xác khá cao, trong đó thuật toán model và Jaccard cho kết quả tóm tắt tốt nhất (51.5 và 52%). Ngoài ra, chúng tôi cũng đã thực trên tập các bài viết thu thập từ các trang báo mạng với kết quả khả the world'sHội thảo quốc gia lần thứ XV: Một số vấn đề chọn lọc của Công nghệ thông tin và thông- Hà Nội, giải pháp tóm tắt văn bản tiếng Việt tự Quốc học Cần Thơ, Việt Quang Nông & Sinh học ứng học Cần Thơ, Việt Trong này chúng tôi đề xuất mô hình tóm tắt văn bản tiếng Việt tự Văn bản biểu diễn dưới dạng đồ thị, mỗi đỉnh trong đồ thị biểu diễn một câu trong văn bản, các cạnh nối giữa các đỉnh biểu diễn sự tương tự về ngữ nghĩa giữa hai đỉnh (câu). Giá trị tương tự biểu diễn dưới dạng trọng số cạnh. Chúng tôi sử dụng toán dựatrên từ vựng làJaro,ModelvàĐộquantrọngcủađỉnh(câu)tính bởi thuật giảithuật toán học dựa trên đồ thị, tùy biến để tích hợp độ tương tự câu. sẽ tự động chọn các câu quan trọng nhất (mặc 25% tổng số câu) để đưa vào kết quả tóm tắt. Để kiểm chứng tính chính xác của mô hình đề xuất, chúng tôi so sánh kết quả tóm tắt tự động với kết quả tóm tắt của chuyên gia vì thế dữ dụnglà tốn (gồm 5văn các chủ đề khác nhau). Kết quả tóm tắt của hệ thống có độ tin cậy cao vì đánh giá bởi tập dữ liệu đánh giá tổng hợp từ 12 nhà khoa học uy tín. Kết quả cho thấy việc các thuật toán thống kê với thuật toán xếp hạng dựa trên đồ thị có tích hợp độ tương tự câu cho độ chính xác khá cao, trong toánmodel và Jaccard cho tắttốt và ra,chúng tôi cũng đã thực trên tập các bài viết thu thập từ các trang báo mạng với kết quả khả khóa : tóm tắt, đồ thị, độ đo tương GIỚI tắt văn bản [1] đã trở thành một công cụ quan trọng và hữu ích để hỗ trợ và trích chọn thông tin văn bản trong thời đại thông tin phát triển nhanh chóng ngày nay. Tóm tắt văn bản thủ công thực hiện bởi con đôi khi là một nhiệm vụ khó khăn khi phải làm việc với một văn bản lớn, chứa nhiều thông tin. Nếu phân loại tóm tắt theo tiếp cận, văn bản có thể phân thành các loại như: tóm tắt trích chọn vàtóm (abstractive). tiếp cận tóm tắt trừu [2] có nghĩa là hệ thống cố gắng chínhcủa tài sau giảichúng dưới dạng ngôn ngữ tự nhiên. Tóm tắt trích dựng bằng xuất vịvăn bản quan trọng (câu văn) bản trên từ, tần số, vị trí hoặc các từ gợi ý để xác định tầm quan trọng của các đơn vị và từ đó trích xuất các đơn vị quan trọng nhất như là tóm tắt, trên đã có nhiều công trình nghiên cứu áp dụng các pháp tóm tắt khác nhau [4]: pháp TF-IDF , pháp phân cụm based), pháp tiếp cận máy học, pháp ngữnghĩa tiềm ẩn (LSA), tạo mờ pháp hồi quy pháp dựa trên truy vấn (Query 50 năm qua đã có nhiều công trình nghiên cứu tạo tóm tắt tự động văn bản tiếng Anh, Nhật, Hoa. Một số công trình tiêu biểu: [6] đã thử 3 tiêu chí đánh giá mới cho các câu để tạo ra tóm tắt tự hai trong số đó sử dụng cấu trúc văn bản; công trình của Marcu [7] thì quan tâm đến việc sử dụng pháp phân tích cấu trúc diễn ngôn hoặc để tạo tóm tắt tự công trình của Radev và cộng sự [8] sử dụng khái niệm tâm” để tóm tắt đa văn bản bằng cách trích trình thì sử dụng thuật toán dựa trên đồ thị để tạo tóm tắt tự Đối với các nghiên cứu về tóm tắt tự động văn bản tiếng Việt, gần đây cũng có một số công trình nghiên cứu công bố như: Lê Minh tóm tắt văn bản tiếng Việt bằng pháp phân cụm SVM Vector [10]; Đỗ Phúc và cộng sự rút trích nội dung chính thôngđiệp trên diễn đàn bằngpháp gom cụm đồ thị [11]; Trọng Phúc và cộng sự thì trình bày pháp vănbản tiếng Việt dựa trên ngôn [12]. Tuy nhiên, kết quả của các nghiên cứu này vẫn chưa đánh giá cụ thể. Đồng thời một số công cụ có sẵn thì không thích hợp cho tiếng Việt nên kết quả tóm tắt rất thấp, không đáp ứng yêu cầu dụ như công cụ của phần mềm Word. Vì vậy trong nghiên cứu này chúng tôi đề xuất pháp tóm tắt văn bản tiếng Việt tự động theo tiếp cận rút trích các câu quan trọng của văn bản để đưa vào tóm tắt dựa trên mô hình đồ phần 2 của bài báo chúng tôi trình bày mô hình tóm tắt văn bản bao gồm các nội dung: quản lý đầu vào; tính tự;tínhđiểmxếphạng.Dữ pháp đáng giá và kết quả thiệu trong phần 3. Phần 4 trình bày kết luận và kiến HÌNH TÓM 1 trình bày mô hình tóm tắt văn bản tự động chúng tôi đề thảo quốc gia lần thứ XV: Một số vấn đề chọn lọc của Công nghệ thông tin và thông- Hà Nội, 1. Mô hình tóm tắt văn bản tự lý đầu bản đầu vào có định dạng *.txt hoặc *.doc. Văn bản sẽ đưa quabộ lọc bỏtừ dừng những từ này mang ít nghĩa hoặc không có nghĩa, loại bỏ các ký tự không phải chữ cái hoặc chữ số. Quản lý đầu vào còn có nhiệm vụ tách văn bản thành các câu và các từ riêng lẻ để sử dụng cho mục đích tính toán sau này.1) Tách câu và tách từ: trong nghiên cứu này chúng tôi sử dụng công cụ do nhóm nghiên cứu về xử lý ngôn ngữ tự nhiên của Khoa Công nghệ - Đại học Quốc gia Hà Nội nghiên cứu và xây dựng [13]. Chức năng chính của gói này như văn bản → Gán nhã câu → Tách từ → Gán nhãn từ loại → Từ tôi sử dụng cho giai đoạn lập chỉ mục cho văn bản vì công cụ này có thể nhận biết các danh từ riêng, có thể nhận biết từ đơn và từ ghép và có độ chính xác trung bình khi tách từ khá cao 94,5%). Chúng tôi sử dụng túitừ (bag of words) diễn chính nhờ việc phân biệt từ đơn và từ ghép mà ngữ nghĩa của văn bản không mất đi hoàn toàn khi sử dụng mô hình này. bỏ từ lĩnh vực khoa học máy tính định nghĩa là một tập hợp các từ xuất hiện rất phổ biến trong văn bản nhưng lại không cần thiết cho phân tích ngôn ngữ học, hoặc là xuất hiện rất ít lần trong tập ngữ liệu nên cũng không đóng góp nhiều về mặt ý nghĩa. Vì là các từ không mang nhiều ý nghĩa nên có thể loại bỏ khỏi văn bản một cách an toàn. Một nguyên nhân cần loại bỏ các từ có tần suất xuất hiện cao nhưng lại không mang nhiều ý nghĩa là vì sự tồn tại của các từ này có thể làm sai lệch kết quả khi pháp chúng tôi đề xuất có dựa trên việc phân tích tần suất của từ. Ví dụ các từ như “như “sau “chỉ”, … là những từ sẽ loại bỏ, chẳng những đến kết quả cuối cùng mà còn có thể tăng độ chính xác. Chúng tôi sử dụng danh sách gồm 570 đề xuất bởi độ tương nghiên cứu của chúng tôi, văn bản biểu diễn bằng đồ thị. Mỗi đỉnh trong đồ thị tương ứng với một câu trong văn bản, mỗi cạnh nối hai đỉnh trong đồ thị biểu diễn mối liên hệ giữa hai câu. Trọng số của mỗi cạnh chính là giá trị độ tương tự (value of giữa hai câu. Độ tương cạnh)tính bằng một trong ba pháp: Jaro, Model và cách Jaro một độ đo tương tự giữa hai cách Jaro djcủa giữa câu s1và câus2 tính như đó m là số từ giống nhau, t là 1/2 số bước sẽ thực hai từ giống nhau trong hai câu s1 và s2 có cách không lớn hơn giá Mỗi từ trong câu s1 so sánh với tất cả các từ trong câu s2. định nghĩa là số từ giống nhau giữa hai câu (nhưng thứ tự trong chuỗi khác nhau) chia cho 2.2) Mô hình tương phản model): Chúng tôi sử dụng mô hình tương phản của Tversky [15] để tính độ tương B) = – – thức ở trên có thể sử dụng để tính độ tương tự giữa hai câu A và B. Trong đó biểu diễn cho các từ chung giữa A và B, g(A-B) biểu diễn cho các từ riêng của A, g(B-A) biểu diễn cho các từ riêng của B. α, β,γ trọng số xác định trong quá trình thử thuật Hệ số Hệ số tương tự Jaccard [16] là một độ đo tương tự của các tập hợp dựa trên pháp thống kê. Chúng tôi sử dụng hệ số này để đo độ tương tự giữa hai câu A và B như B) xếp tôi sử dụng thuật toán thuật để tính điểm xếp hạng (độ quan các đỉnh trong đồ thị. Tuy nhiên, thuật toán gốc áp dụng trên đồ thị có chúng tôi hiệu chỉnh để có thể áp dụng trên đồ thị vô Thuật toán sẽ áp dụng Hội thảo quốc gia lần thứ XV: Một số vấn đề chọn lọc của Công nghệ thông tin và thông- Hà Nội, đồ thị vô có trọng số biểu diễn văn bản, trong đó trọng số của cạnh nối các đỉnh là độtương tựcủa hai câu biểu diễn bởi hai đỉnh tương ứng. Thuật toán xếp hạng thực hiện các lần lặp để cập nhật giá trị xếp hạng cho các đỉnh trong đồ thị. Quá trình lặp sẽ kết lỗi hội (STANDARD_ERROR_THRESHOLD) đã vượt quá giá trị định sẵn (tỷ lệ với số đỉnh của đồ thị). Giá trị lỗi tính là độ lệch chuẩn của các giá trị xếp hạng mới và cũ của các đỉnh trong đồ thị. Ngoài việc phụ thuộc vào số các cạnh vào và cạnh ra của các đỉnh trong đồ thị đã xây dựng ở thành phần tính độ tương tự, do đây là đồ thị có trọng số nên trọng số cạnh cũng sẽ tích hợp vào mô hình tính điểm xếp hạng của như sau (trong đó WATi là trọng số cung nối đỉnh A và đỉnh = 0.25 + 0.85 * (WAT1 * + … + WATn * VÀ ĐÁNH GIÁ KẾT pháp tóm tắt mà chúng tôi đề xuất trong nghiên cứu này là rút trích các câu quan trọng nhất trong văn bản để đưa vào tóm tắt. Khi đã xác định danh sách các câu quan trọng nhất (mặc định là 25% số câu của văn bản), chúng tôi sẽ thực hiện sắp xếp các câu này theo thứ tự xuất hiện trong văn bản để có tóm tắt của văn đánh giá độ tốt của giải pháp đề xuất, chúng tôi đã thực hiện đánh giá theo hai cách: 1- Thu thập các đoạn văn bản thô thuộc nhiều chủ đề khác nhau. Chọn lựa cộng tác viên tham gia tóm tắt các văn bản đã thu thập ở bước so sánh kết quả tóm tắt bởi các cộng tác viên và của hệ 2- Thu bài viết trên các trang báo điện tử theo tiêu chí các bài viết này phải tóm tắt theo cách sử câu văn trong nội dung văn bản. đối chiếu tóm tắt của hệ thống với tòm tắt của văn bản thu liệu thực liệu thực dùng cho pháp đánh giá thứ nhất là 5 đoạn văn bản có độ dài khác nhau và thuộc các chủ đề khác nhau. Do cần nhờ đến các cộng tác viên thực hiện tóm tắt các đoạn văn bản để so khớp nên số mẫu thực cho pháp này là nhỏ. Chủ đề và số câu của mỗi mẫu kiểm thử cho trong Bảng 1. DỮ LIỆU THỰC CHO PHÁP 1Tên văn bản Chủ số chuyên gia muốn Apple thu hồi Iphone4 Công nghệ 275 mất tích trong bảo đã tìm thấy Xã hội 30Barca tăng chất thép cho cánh trái Thể thao 18Dự án LMF Kỹ thuật 18Lão ngư dân và biển cả Văn học nghệ thuật 78Dữ liệu thực cho pháp thứ hai là 25 bài viết thu thập từ các trang báo mạng như và Do các bài viết phải đáp ứng yêu cầu là có tóm tắt rút trích từ nội dung của bài viết nên thực tế số cũng không nhiều và không phong phú về chủ đề. Đa số các bài viết thu thập thuộc chuyên mục “Tâm sự” và của hai tờ báo điện tử pháp đánh giá1) Cách 1Dữ liệu dùng để đánh quả trình trong cách 1 này là các bản tóm tắt thực hiện thủ công do các nhà khoa học thực hiện trên 5 văn bản dùng để thực như đã đề cập ở mục A của phần III (Bảng 1). Mặc dù kết quả tóm tắt từ mỗi nhà khoa học có độ tin cậy khá cao, tuy nhiên để đảm bảo tính khách quan của kết quả tóm tắt, chúng tôi tiến hành thu thập tóm tắt từ 12 nhà khoa học (Bảng 2)khác nhau và việc tóm tắt thực hiện độc 2. CÁC NHÀ KHOA HỌC THAM GIA ĐÁNH GIÁ HỆ Họ tên Email1. GS.TS. Võ Thị Gương PGS.TS. Minh Thủy PGS.TS. Lê Thị Mến GS.TS. Văn Thu TS. Thị Hồng Nhân TS. Thị Thu Nga TS. Lê Vĩnh Thúc ThS. Xuân Việt ThS. Văn Ây ThS. Thu Tâm ThS. Lê Minh Lý ThS. Phạm Thị Thảo chính xác của kết quả tóm tắt định nghĩa như sau: (số câu trùng lắp giữa kết quả thuật toán và kết quả chuyên gia) / (số câu tóm tắt cần Chúng tôi đề xuất pháp đo như sau: sử dụng pháp bầu chọnra một Gold-standard là một tập hợp gồm các câu nằm trong tóm tắt nhiều bầu chọn nhất. Gọi result (i) là kết quả tóm tắt văn bản thứ i, công thức để tính độ chính xác của mỗi pháp áp dụng trên văn bản thứ i như tắt của các nhà khoa học không phải lúc nào cũng trùng khớp với nhau, vì thế chúng tôi đề xuất sẽ lựa chọn các câu nào nhiều nhà khoa học chọn nhất sẽ đưa vào tóm tắt và xem như là tóm tắt của các nhà khoa học. Tỷ lệ thống nhất giữa các nhà khoa học cao nhất là 67% và thấp nhất là tôi cho hệ thống thực hiện tóm tắt trên 3 độ đo đã giới thiệu ở mục B phần II. Giá trị các tham số sử dụng cho từng độ đo cho trong bảng 3. GIÁ TRỊ THAM SỐ THỰC pháp Thuật toán tính độ tương (xây dựng cạnh nối giữa các pháp Jaro Jaro pháp Model Model pháp Jaccard Jaccard thảo quốc gia lần thứ XV: Một số vấn đề chọn lọc của Công nghệ thông tin và thông- Hà Nội, hiện so sánh kết quả đạt khi sử dụng các độ đo khác nhau, chúng tôi có thể kết luận rằng độ đo Jaccard cho kết quả tốt nhất nhưng không khác biệt nhiều so với mô hình tương phản model), xem hình pháp phápJaccardHình 2. Kết quả thực theo cách cũng thực ảnh của quá trình tiền xử lý đối với pháp đề xuất. Thật vậy, để có thể tính toán chính xác độ tương đồng giữa các câu đòi hỏi quá trình tách từ phải có khả năng nhận biết đúng các từ sử dụng trong ngữ cảnh của câu. Có nghĩa là cần phân biệt từ đơn và từ ghép. Vì bản chất tiếng việt có nhiều từ ghép nên không thể đơn giản sử dụng trắng để tách từ, kết quả minh họa bởi hình 3 cho thấy rõ điều này.So sánh với các hệ thống đã có trên 5 văn bản thực cũng cho thấy hệ thống chúng tôi xây dựng cho độ chính xác cao hơn (Bảng 4). áp dụng cho tiếng Việt: là kết quả nghiên cứu đề xuất bởi [18] áp dụng cho văn bản tiếng Anh. Thực tóm tắt tương tự như cách thực đã áp dụng cho hệ thống do chúng tôi đề Word 2003): Thực tóm tắt tương tự như cách thực đã áp dụng cho hệ thống do chúng tôi đề 4. SO SÁNH KẾT QUẢ CỦA HỆ THỐNG ĐỀ XUẤT VỚI CÁC HỆ THỐNG pháp Tên pháp Độ chính xác xuất pháp Jaro pháp Tên pháp Độ chính xác pháp Model pháp Jaccard 52.0Có sẵn áp dụng cho tiếng Việt Word 2003 12.42) Cách 2Chúng tôi thu thập 25 bài viết trên 2 trang báo điện tử và theo điều kiện các bài viết cần có tóm tắt theo kiểu rút trích nguyên văn một số câu từ nội dung của bài viết. Chúng tôi cũng đã lựa chọn các bài viết có số câu tóm tắt là khá ít, dao động trong từ 1 đến 3 câu. Kết quả thực theo cách 1 cho thấy độ đo Jaccard có kết quả tốt hơn cả nên ở cách 2 này chúng tôi chỉ thực với độ đo Hình 4 minh họa độ chính xác của pháp tóm tắt đối với từng văn bản cũng như độ chính xác trung bình trên tập 25 văn tích kết quả đạt chúng tôi nhận thấy có 7 văn bản có kết quả tóm tắt trùng khớp 100%, phần nhiều vẫn là trùng khớp với tỷ lệ 50%, tuy nhiên vẫn còn có một số văn bản tỷ lệ trùng khớp là 0%. Tỷ lệ trùng khớp trung bình trên tập 25 văn bản là 55.3%, tỷ lệ này cũng gần với tỷ lệ thực ở cách 1.KẾT LUẬN VÀ KIẾN bài tôi giới trích chọn tóm tắt từ nội dung văn bản theo tiếp cận sử dụng cấu trúc đồ thị để biểu diễn văn bản, đây là tiếp cận mới của thế giới trong những năm gần đây. Chúng tôi cũng đề xuất ứng dụng các độ khác nhau để tính độ tương tự câu trong hệ thống tóm tắt văn bản. Trong đó: 1- đây là công trình nghiên cứu lần đầu tiên tại Việt Nam sử dụng 3 Jaro, Model và công việc tóm tắt văn bản và cho kết quả khả quan; 2- đây cũng là công trình nghiên cứu đầu tiên trên thế giới tích hợp thuật toán Model vào hệ thống tóm tắt văn bản, thuật toán này thể hiện độ chính xác cao trên tập dữ liệu nghiên cứu. Kết quả thực (ngay cả khi tập dữ liệu kiểm thử có kích nhỏ) đã chứng minh phần nào tính khả thi trong việc ứng dụng kết quả nghiên cứu vào thực tiễn. Kết quả khả quan của pháp đề xuất có thể lý giải từ nhiều nguyên nhân: 1- Sử dụng ưu điểm của pháp chỉ mục từ tiếng Việt do công cụ cung cấp. Hình 4. Kết quả thực theo cách 2Hình 3 Kết quả thực khi có sử dụng và không sử thảo quốc gia lần thứ XV: Một số vấn đề chọn lọc của Công nghệ thông tin và thông- Hà Nội, vậy, trong nghiên cứu dựa trên tiếp cận “mô hình túi từ - bag of để biểu diễn nội dung văn bản, pháp này có ưu điểm là cài đặt đơn giản nhưng có hạn chế lớn là làm mất đi ngữ nghĩa của văn bản vì không quan tâm đến vị trí của từ mà chỉ quan tâm đến tần hiện của sử dụng công cụ có khả biết chính xác từ đơn và từ ghép nên ngữ nghĩa của văn bản phần nào giữ lại so với việc xem nội dung văn bản là tập hợp các từ đơn (từ gồm 1 chữ); 2- Thuật toán dùng để xếp hạng các trang web đã chứng tỏ tính khả thi khi ứng dụng thành công bộ kiếm thông tin web. Khi ứng dụng vào ngữ cảnh này, tỏ ra hiệu quả ngay cả thị web là một đồ thị không có trọng số. Vì thế chúng tôi tin rằng sự kết hợp thuật toán xếp hạng với các độ đo tương tự (gán trọng số cho cạnh) sẽ mang lại kết quả khả quan và kết quả thực đã phần nào chứng minh nhận xét trên khi mà độ đo Jaccard và độ đo Model đã cho kết quả tóm tắt vượt trên các hệ thống sẵn có, đặc biệt là khi so sánh với pháp có tiếp tự là Một ưu điểm khác của pháp chúng tôi đề xuất là quá trình tóm tắt không cần tập ngữ liệu huấn cũng như không cần xem xét tính ngữ nghĩa và cấu trúc ngữ pháp của câu và việc tóm tắt áp dụng trên từng văn bản kết quả đạt bước đầu là rất khả quan nhưng để có thể khẳng định chắc chắn hơn tính khả thi của giải pháp chúng tồi cần thêm thời gian thu thập dữ liệu thực cũng như cần thêm thời gian và sự đóng góp của bạn bè đồng trợ hiện tóm tát các đoạn văn bản như kênh thông tin kếtquả của pháp. Chúng tôi cũng đề xuất áp dụng giải pháp tóm tắt văn bản tự động như là một công đoạn của phân nhóm tài liệu. Thay vì phân nhóm văn bản dựa trên toàn bộ nội dung của nó thì ta có thể phân nhóm dựa vào tóm tắt của nó, và nếu giải pháp này thành công thì sẽ giúp tăng đáng kể tốc độ của các ứng dụng phân nhóm văn bản theo chủ LIỆU THAM Karel Jezek and Josef Text Vaclav Snasel (Ed.): 2008, ISBN FIIT STU a G Erkan and R. Radev, as in Text Journal of Re-search, Vol. 22, pp. 457-479 and Pooya (2008), Text Based on Fuzzy of IEEE, of Shahid Bahonar Kerman, UK, pp. (2010),“ASurvey ofText Journal of in Web Vol 2, No 3 (2010), Mohamed Abdel Fattah, Fuji Ren, “GA, MR, FFNN, PNN and GMM based models for text Speech & 23(1): 126-144 H. Methods in J. ACM 16(2): 264-285 Theory and of Parsing and A Book, MIT Press, R. Radev, Hongyan Jing, Stys, and Daniel Tam ,of Processingand vol. 40, 2004.[9] R., ranking for applied to text ACL 2004 on forLinguistics, NJ, USA, pp. Nguyen,L.M., A.,Ho, T.B., Phan, X.H., S., supportvector World of the Federation for on Mining from Large Kobe, 15-17 S5-2-4, Đỗ Phúc, Mai Xuân Hùng, Thị Kim “Gom cụm đồ thị và ứng nội dung chính của trên diễn đàn thảo Tạp chí phát triển Khoa học Công nghệ, Tập 11, Số 05 - 2008, pp 21-32, Nguyen Trong Phuc, Le Thanh Huong, TheICT.rda Hanoi, Nguyen Processing W. E., Metrics and Rules in the Model of ”. of the Section on Survey Methods 1990.[15] A., of Review, 84, Paul laoraledansune portion des Alpes et des In del la Socit des volume 37, pages L.Page, S. T. “The Bringing order to the web”, G Erkan and R. Radev, as in Text Journal of Re-search, Vol. 22, pp. can be by several (1) of a text covers the main ideas of the whole text so it can be used to the topic; (2) by using the right words method library in this work) we d",
          "relevence": "yes"
        },
        {
          "url": "http:hethongphapluatvietnam.net/quy-chuan-ky-thuat-quoc-gia-qcvn-41-2016-bgtvt-ve-bao-hieu-duong-bo.html",
          "title": "Quy chuẩn kỹ thuật quốc gia QCVN về báo hiệu bộ",
          "content": "Hệ thống pháp luật Việt trang văn bản pháp luật và tư 599 881 Toggle THỦ TỤC HÀNH VĂN BẢN PHÁP HỎI ĐÁP PHÁP  điện dụcGiao thông - Vận chấtKế toán - Kiểm động - Tiền vực dân hữu trí chính nhà nguyên - thao - Y tục Tố - Phí - Lệ tệ - Ngân nhiệm hình hóa - Xã hộiVi phạm hành dựngXây dựng - Đô nhập văn bản Công ướcCông vănChỉ lệĐiều ước quốc phápHiệp địnhNghị định quyếtPháp chếQuy địnhQuyết lệnhSắc báoThông tưThông tư liên thuậnTiêu chuẩn chuẩn Việt chuẩn bản hợp bản bản kết VNbản Quy chuẩn kỹ thuật quốc gia QCVN về báo của huyện (ĐH) là nối tâm hành chính của huyện với trung tâm hành chính của xã, cụm xã tâm hành chính của huyện lân cận; có vị trí quan trọng đối với triển kinh tế - xã hội của xã (ĐX) là nối trung chính của xã với các thôn, làng, ấp, bản và đơn vị tương hoặc với các xã lân cận; có vị trí quan trọng đối với sự phát triển - xã hội của đô thịlớn hơn 120 km/h. Ở những đoạn ngắn, có thể không hạn độ mà chỉ nên sử dụng biển cảnh báo đi chậm. Không sử dụng biển số cách tràn lan khi không có nghiên cứu cụ Trong hợp tiếp từ giá độ lớn xuống giá trị tốc độ nhỏ mà sự chênh lệch giữa hai giá trị tốc lớn thì nên đặt biển hạn chế tốc độ tối đa trung gian. Đoạn gian quy định là không nhỏ hơn 250 m cho việc tiếp về dụng biển số P.127a cho một số khu đông dân cư vào ban đêm nhằm mục đích nâng cao tốc độ vận hành ít xe chạy. Biển chỉ có hiệu lực trong thời gian ghi trên biển và vi từ vị trí đặt biển đến vị trí biển số R.421 đoạn qua dân cư”. Biển đặt sau vị trí biển số R.420 qu Bigiao của phải, rẽ trái. Nếu có quy định cấm về thời gian hoặc loại xe thì sử báo phụ thời gian hoặc hình vẽ loại xe cấm.- Vạch chéo màu đỏ đè lên mũi tên màu B.37 - Biểcó góc lớn hơn hay bằng 45° hoặc có bán kính nhỏ hơn hay bằng 100 m.- Ở vùng núi, cong có góc ở lớn hơn hay bằng 45° hoặc có bán kính nhỏ hơn hay bằng 40 m.c) Ở những vùng mà việc quan sát của gia giao thông gặp khó khăn như vùng cây rậm rạp, vùng có sương mùthì các vị trí cong không phân biệt độ lớn góc ở tâm hoặc bán kính là chỗ ngoặt nguy Sau đoạn thẳng dài từ 1 km trở lên cong đầu tiên không phân biệt độ lớn góc ở tâm hoặc bán kính đều coi ngoặt nguy Biển số W.201a b) Biển số C.1 - Biển Biển số W.201 (c,d) Chỗ hiểm có nguy cơ lật xeĐể báo sắp đến chỗ ngoặt nguy hiểm năng gây lật các xe có trọng tâm cao và tải trọng lớn như xe tải, xe nằm, xe chở chất lỏng, v.v... phải đặt biển số Biển số W.201c chỗ ngoặt nguy hiểm có nguycơ lật xe bên phải khi cong vòng sang trái ;- Biển số W.201d chỗ ngoặt nguy hiểm có nguycơ lật xe bên trái khi cong vòng bên C.1a - Biển và Biển số W.202 (a,b) Nhiều nguy hiểm liên Để báo sắp đến hai chỗ ngoặt nhau liên tiếp phải đặt biển số 202 (a,b):- Biển số W.202a đặt trong hợp có từ 2chỗ ở gần nhau trong đó có ít nhất một chỗ ngoặt nguy hiểm mà chỗ tiên vòng bên trái;- Biển số W.202b đặt trong hợp số 202a nhưng vòng bên Hai chỗ ngoặt gọi là gần nhau khi từ tiếp cuối của cong đến tiếp đầu của cong tiếp hơn 160 m.Hình C.2 - Biển Biển số W.203 (a,b,c) bị Để báo sắp đến một đoạn bị đột ngột phải đặt biển số W.203 Biển số W.203a đặt trong hợp thu hẹp cả hai bên;- Biển số W.203b hoặc biển số W.203c hợp bị thu hẹp về phía trái hoặc phía Đoạn bị thu hẹp là đoạn xe chạy bị thu hẹp lại, các làn xe đi chiều nhau gặp khó khăn, và khả năng thông qua giảm đột ngột so với đoạn đó.c) Sau khi đặt biển số W.203 (a,b,c) bị thu hẹp đến mức không có khả năng thông qua cho hai xe đi phải đặt vị trí thu hẹp các biển xác định quyền ưu tiên của chiều số P.132 và biển số Ở tất cả những vị trí bị hẹp, gia giao thông phải chú ý quan sát giao thông Xe đi ở bị thu hẹp phải cho xe đi Nếu vị trí bị thu hẹp có đặt biển thì phải cho xe chạy nếu đặt biển số I.406, thì ưu tiên qua hẹp và xe ch của điều khiển giơ thẳng đứng vuông mặt đất báo hiệu tham gia giao thông  ",
          "relevence": "no"
        },
        {
          "url": "http:vietmmo.club/threads/cac-loi-trong-yoast-seo.8242/",
          "title": "Đã hỗ trợ - Các lỗi trong Yoast Seo | VMC",
          "content": "VMCTrang vực giúp cho đang kiếm tiền online ?, hãy đăng ký tài khoản Vietmmo tại thông hỗ trợ Các lỗi trong Yoast luận trong 'Trợ giúp cho mới' bắt đầu bởi = = ? : = ? : = = = ? : = ? : =gia:6/4/16Bài = = ? : = ? : = muốn hỏi là cái lỗi STOP word là cái gì thế ? Đọc chú thích mà không hiểu gì = = ? : = ? : = = = ? : = ? : = trợ viết:4,531Cống = = ? : = ? : = muốn hỏi là cái lỗi STOP word là cái gì thế ? Đọc chú thích mà không hiểu gì to click vào cái stop word xem nó ra link nào, nội dung thế = = ? : = ? : = = = ? : = ? : = trợ hiến:5,013var = = ? : = ? : = click vào thử xem nó phân tích sao a, với lại cái đó đang nói về slug chứa stop words. A cho e xem cái slug = = ? : = ? : = = = ? : = ? : = trợ hiến:6,090var = = ? : = ? : = muốn hỏi là cái lỗi STOP word là cái gì thế ? Đọc chú thích mà không hiểu gì to word là những từ nối hoặc giới ví dụ như: a, an, about, the, tham khảo danh sách các STOP words ở đây: làm sản phẩm gì vậy? từ khoá của bạn như thế and like = = ? : = ? : = = = ? : = ? : = Bất Dung viết:1,288Cống = = ? : = ? : = Nguyen word là những từ nối hoặc giới ví dụ như: a, an, about, the, tham khảo danh sách các STOP words ở đây: làm sản phẩm gì vậy? từ khoá của bạn như thế to Huy sp chất đấy Nguyen thích bài = = ? : = ? : = = = ? : = ? : =gia:6/4/16Bài = = ? : = ? : = nói các bác ứ tin chứ e đang viết bài cho sản phẩm đầu thu kỹ thuật số DVB T2 mà đang cắt sóng analog ấy. Thank Trang Đinh Nguyen thích bài = = ? : = ? : = popupWindow = = ? : = ? : = trợ hiến:6,090var popupWindow = = ? : = ? : = nói các bác ứ tin chứ e đang viết bài cho sản phẩm đầu thu kỹ thuật số DVB T2 mà đang cắt sóng analog Trang Đinh Click to từ tiếng việt không dấu mình có nhiều từ dễ bị hiểu nhầm thành STOP words của tiếng anh lựa chọn từ khoá bạn phải chú ý một chút  Huy thích bài phải Đăng nhập hoặc Đăng ký để trả lời bài Ignored tài khoản hoặc địa chỉ đã có tài khoản vào đây để đăng Mật khẩu của tôi đã quên mật khẩu? Duy trì đăng đàn>Khu vực giúp cho Add-ons by Brivium ™© Brivium chủDiễn đànDiễn kết viết gần chủ đề kết Đăng CHÚNG tạo ra nhằm mục đích chia sẻ và thảo luận về kiếm tiền online và đến một diễn đàn về kiếm tiền trên mạng ĐÚNG NGHĨA hàng đầu Việt KẾT Sách Bảo HệQUYỀN HẠN - CẤP HiệuEventsCONTACT US by XenForo Ltd. Tiếng định và Nội đầu XenForo,{visitor: { 0 0},_lightBoxUniversal: 1,_animationSpeedMultiplier: 10%,speed: rgb(255, 255, 0.6,loadSpeed: {thread_view:true,message:true,bb_code:true,message_user_info:true,BRETA_awards:true,likes_summary:true,share_page:true,login_bar:true,notices:true,panel_scroller:true,chipxf_riu_form:true,js  /vfchh  /vfchh.js?_v=28811712:true,js  /chipxf  /remoteimageuploader  /jquery-file-upload  /js  /vendor  /jquery.ui.widget.js?_v=28811712:true,js  /chipxf  /remoteimageuploader  /jquery-file-upload  /js  /jquery.iframe-transport.js?_v=28811712:true,js  /chipxf  /remoteimageuploader  /jquery-file-upload  /js  /jquery.fileupload.js?_v=28811712:true,js  /chipxf  /remoteimageuploader  /main.js?_v=28811712:true},_cookieConfig: { path: /, domain: , prefix: ,_csrfRefreshUrl: 28811712,_noSocialLogin: Hủy giây phút %minutes% phút nay lúc qua, lúc %day% lúc Chủ Thứ Thứ Thứ Thứ Thứ Thứ Tháng tư,Tháng tám,Tháng mười mười CN,T2,T3,T4,T5,T6,T7,following_error_occurred: Có lỗi sau xảy xa với yêu cầu của The server did not respond in time. Please try Đang đăng Xem ảnh Show hidden content by Javascript = =: Bạn vừa thêm ảnh này,: Queue to Have an error : Retry,: Chèn = || = { : :: true,: 3};",
          "relevence": "no"
        },
        {
          "url": "https:vi4.ilovetranslation.com/yweRR3JXqfZ=d/",
          "title": "Stopwords Very used words in a to occur - Very used words in a Expected to occur Việt làm thế nào để nói",
          "content": " Văn Very used words 468x15);Stopwords Very used words in a Expected to occur in 80 percent or more of the of, to, a, and, in, said, for, that, was, on, he, is, with, at, by, and must be before can be for Very used words in a Expected to occur in 80 percent or more of the of, to, a, and, in, said, for, that, was, on, he, is, with, at, by, and must be before can be for hiện ngôn Tế ArmeniaTiếng Ba Ba Bồ Ðào Do GujaratTiếng Hy LatviaTiếng Mông SesothoTiếng Xu-đăngTiếng Ai-lenTiếng Bantu (Ðông Bengali (Ấn CatalanTiếng Creole ở GaliciaTiếng Hin-đi (Ấn Hà IndonesiaTiếng MacedoniaTiếng MaoriTiếng Mã Na Nam PhilippinTiếng Phần SloveniaTiếng Thổ Nhĩ Thụy Tây Ban Xéc-biTiếng Xứ Ðan Ả-rậpKlingonKlingon Tế ArmeniaTiếng Ba Ba Bồ Ðào Do GujaratTiếng Hy LatviaTiếng Mông SesothoTiếng Xu-đăngTiếng Ai-lenTiếng Bantu (Ðông Bengali (Ấn CatalanTiếng Creole ở GaliciaTiếng Hin-đi (Ấn Hà IndonesiaTiếng MacedoniaTiếng MaoriTiếng Mã Na Nam PhilippinTiếng Phần SloveniaTiếng Thổ Nhĩ Thụy Tây Ban Xéc-biTiếng Xứ Ðan Ả-rậpTừ:-Sang:-document.getElementById(fy_form).value = = quả Việt) Rất sử dụng từ ngữ trong một ngôn ngữ Dự kiến sẽ xảy ra trong 80% hoặc nhiều hơn các tài đến, một, và, trong, nói, cho rằng, là, trên, ông, là, với, tại, bởi, và bỏ phải thực hiện khi lập chỉ vấn có thể preprocessed để loại bỏ dịch, vui lòng 728x90);ggdm_duqu(ggwz___4, zishiying);Kết quả Việt) Rất từ sử dụng trong một ngôn ngữ Dự kiến sẽ diễn ra trong 80 phần trăm hoặc nhiều hơn các văn bản , của, cho, một, và, trong, cho biết, cho, mà, là, trên, anh, là, với, tại, bởi , và nó bỏ phải thực hiện khi đánh chỉ số truy vấn có thể xử lý để loại bỏ từ vô dịch, vui lòng quả Việt) 3:đang dịch, vui lòng 336x280);ggdm_duqu(ggwz___6, 200x200);ggdm_duqu(ggwz___8, Các ngôn ngữ trợ công cụ dịch Klingon Phát hiện ngôn ngữ, Quốc Tế Ngữ, Aixơlen, Ba Lan, Ba Tư, Bosnia, Bồ Ðào Nha, Do Thái, George, Hungari, Hy Lạp, Java, Khmer, Latvia, Mông Cổ, Nga, Rumani, Séc, Urdu, Xu-đăng, Ý, Zulu, Tiếng Ai-len, Tiếng Tiếng Anh, Tiếng Bantu (Ðông Phi), Tiếng Basque, Tiếng Bengali (Ấn Ðộ), Tiếng Tiếng Tiếng Tiếng Tiếng Tiếng Creole ở Haiti, Tiếng Tiếng Tiếng Tiếng Hausa, Tiếng Hin-đi (Ấn Ðộ), Tiếng Hmong, Tiếng Hà Lan, Tiếng Hàn, Tiếng Igbo, Tiếng Tiếng Kazakh, Tiếng Latinh, Tiếng Lào, Tiếng Tiếng Tiếng Tiếng Malta, Tiếng Maori, Tiếng Tiếng Mã Lai, Tiếng Na Uy, Tiếng Nam Phi, Tiếng Nepal, Tiếng Nhật, Tiếng Tiếng Pháp, Tiếng Phần Lan, Tiếng Punjab, Tiếng Slovak, Tiếng Tiếng Tamil, Tiếng Telugu, Tiếng Thái, Tiếng Thổ Nhĩ Kỳ, Tiếng Thụy Tiếng Trung, Tiếng Tây Ban Nha, Tiếng Tiếng Việt, Tiếng Tiếng Xứ Wale, Tiếng Yoruba, Tiếng Ðan Mạch, Tiếng Ðức, Tiếng Ý, Tiếng dịch ngôn Sung một tập đoàn nước ngoài với lí do mang tính chủ quan đó là tôi tkhoong quá nóng hoặc không quá sẽ để bạn một mình, đó là mong muốn selling of in shirt is too big for of in follow you has a lot of the most popular ra nhiều tệ nạn xã both terms must be either movie starhe is getting ©2017 I Love rights sj_cp);ggdm_duqu(ggwz___10, 'UA-49927087-1', 'pageview');",
          "relevence": "no"
        },
        {
          "url": "http:giongrieng.edu.vn/news/print/Goc-cong-nghe-thong-tin/Stop-Word-tu-dong-tao-khoang-trong-auto-space-khi-danh-tieng-Viet-243/",
          "title": "User Error",
          "content": "User sorry. The you are using to access our website is not Some of this are e-mail and that to your hard drive. If you feel you have gotten this error, please send an e-mail to admin. Your I.P. address has been logged. youhave any about this contact the site for more information",
          "relevence": "yes"
        },
        {
          "url": "http:butchiso.com/2011/11/mysql-full-text-search-p1.html",
          "title": "MySQL Search - P1 « Bút Chì Số",
          "content": "Bút Chì MySQL Search - P1Nov 26, 2011 text search là đơn giản dễ hiểu, full text search (gọi tắt là FTS) là cách tự nhiên nhất để tìm kiếm thông tin, hệt như Google, ta chỉ cần gõ từ khóa và nhấn enter thế là có kết quả trả về. Phạm vi bài viết này chỉ đề cập, giới thiệu sơ lược về FTS trong MySQL mà không bàn về các FTS engine như Sphinx sao chúng ta phải dùng Full text chúng ta sẽ sử dụng câu truy vấn dạng như sau để tìm kiếm dữ FROM book WHERE title LIKE cách truy vấn này có một số hạn chế như ý: đây là những hạn chế chung trong MySQL, ngay cả full text search của MySQL cũng không giải quyết triệt để các vấn đề này mà phải dùng các search engine ngoài như Solr, Sphinx v.v Nhưng mình muốn nêu lên ở đây để chúng ta có cái nhìn rõ ràng về những điểmKhông chính nhiễu cao Giả sử, bạn có câu truy vấn với mệnh đề LIKE như LIKE nó sẽ có thể trả về những kể quả sau: one, zone, money, phone nói chung là không chính xác vì dải kết quả trả về sẽ rộng và có thể chứa nhiều kết quả nhiễu không mong đồng nghĩa chúng ta đã biết, ngôn ngữ nào cũng có những từ đồng nghĩa, ví dụ như trong tiếng Việt là xe hơi - ôtô, bao thư - phong bì v.v. Tiếng Anh thì đơn giản thì có v.v Nếu như dùng LIKE hay = (thậm chí search của MySQL) thì tất nhiên không thể giải quyết vấn từ đồng nghĩa cấu tạo bằng chữ đầu của cụm từ lúc với những cụm từ dài và phổ biến chúng ta viết tắt ví dụ như THPT, CNTT, US, IT. Nhưng khi dùng tìm kiếm thì họ có thể nhập khác với trong chúng ta lưu trữ (viết thu gọn - viết đủ và lại) cho nên đây cũng là một khó khăn mà chúng ta gặp phải khi làm chức năng search. Mong muốn của dùng là họ tìm thấy kết quả mong muốn cho dù họ viết tắt hay viết đầy độ truy vấn chậm, không dùng như ta đặt ‘%’ ở phía thì MySQL sẽ thực hiện câu truy vấn mà không dùng index, MySQL sẽ thực hiện scan toàn bộ dữ liệu của nó từ đầu đến cuối, cho nên câu truy vấn sẽ rất chậm so với search trên index. Giống như ta tìm từng trang trong một cuốn sách thay vì tìm trong trang index đằng sau quyển sách đó vậy. Để hiểu rõ hơn vì sao dùng index lại nhanh hơn chúng ta sẽ tìm hiểu nó trong các phần sau.The index also can be used for LIKE if the to LIKE is a string that does not start with a đề với tìm kiếm tiếng Việt có dấu và không sử ta lưu tiếng Việt có dấu trong nhưng dùng nhập tiếng Việt không dấu thì mệnh đề LIKE chắc chắn sẽ không tìm ra dữ liệu ta cần. Có một số giải pháp ví dụ như lưu 2 field, một có dấu và một không dấu, nhưng cách này xem ra không tối ưu và không hỗ trợ search gần đúng. Nếu như dùng nhập “co be mua dogn” thì dùng mệnh đề LIKE sẽ không search ra “Cô bé mùa nhưng FTS có thể giải quyết vấn đề Sơ lược MySQL lược thì MySQL FTS hiện tại chỉ có trên storage engine MyISAM và mới có trên 2 chế độ tìm kiếm đó là BOOLEAN MODE và NATURAL MODE. Trong BOOLEAN MODE thì không có default và trong chế độ này thì ta có thể qui định từ khóa nào sẽ xuất hiện, và từ khóa nào không xuất hiện trong kết quả trả về. Còn NATURAL MODE thì tìm kiếm những kết quả thích hợp hơn là chính xác keyword tìm.Mặc định thì MySQL có một list các nghĩa là các từ mà MySQL sẽ bỏ qua không search nếu gặp phải nó (ví dụ: the, and, or, for v.v). Tham khảo stop words list của MySQL ở ra thì mặc định MySQL FTS chỉ tìm những từ có độ dài tối thiểu là 4 ký tự (global = 4). Từ thực tế, nếu như ta search những chữ có độ dài bé hơn 4 (ví dụ “Hà “Cà ký” “The way I am”) thì sẽ không có kết quả nào trả về, cho nên ta phải lưu ý tới thiết lập này của LIKE SHOW LIKE Valueft_boolean_syntax| + ||| 84 || 4|| | 20 || | rows in set (0.11 Ứng dụng tham TABLE IF NOT EXISTS `jobs` ( `id` INT(11) NOT NULL INT(11) DEFAULT NULL, `title` COLLATE NOT NULL, COLLATE NOT NULL, TEXT COLLATE NOT ) * FROM `jobs ` WHERE AGAINST IN NATURAL MODE); SELECT * FROM `jobs ` WHERE AGAINST IN BOOLEAN này ta chỉ tìm hiểu sơ lược về MySQL full text search, entry sau chúng ta sẽ tìm hiểu kỹ hơn về full text search syntax, NATURAL MODE, BOOLEAN =  replace example with your forum/* * * DON'T EDIT BELOW THIS LINE * * */{var dsq = = = = '' + * * DON'T EDIT BELOW THIS LINE * * */() {var s = s.async = =s.src = '' + enable to view the powered by by NeoFull PostsSOLID - 5 nguyên tắc của thiết kế đốiLaravel : và IoCTừ đồng nghĩa hoá bất đối xứng RSAin ES6 and solving 2016 Hung Neo Date();a=s.createElement(o),'UA-39549683-1','pageview',",
          "relevence": "no"
        },
        {
          "url": "https:viblo.asia/p/elasticsearch-phan-tich-va-tim-kiem-du-lieu-tieng-viet-p1-3P0lPveoKox",
          "title": "[Elasticsearch] Phân tích và tìm kiếm dữ liệu tiếng Việt [P1] - Viblo",
          "content": "No yet.All Sign In/Sign up+12Le Xuan tích và tìm kiếm dữ liệu tiếng Việt Aug 29th, 3:58 pm 442 12 1 as 14:55:13Marked as on post is in the học học mãi học Phân tích và tìm kiếm dữ liệu tiếng Việt [P1] bài viết của mình có liên with docker bài viết mình có giới thiệu qua về việc tìm kiếm cơ bản với tuy nhiên dữ liệu mới chỉ dừng lại ở tiếng Anh vậy còn tiếng Việt thì đã thử google tìm kiếm để xem các bài dẫn hay tìm hiểu của đi về việc tìm kiếm trên dữ liệu tiếng Việt nhưng gần như không có bài dẫn hay demo cụ áp dụng những xử lý với tiếng Việt giống như với tiếng Anh mình đã thử và kết quả tìm kiếm mà mình nhận về không tốtBài viết hôm nay mình xin giới thiệu về phần xử tiền xử lý với dữ liệu tiếng Việt và sẽ demo ứng dụng hoàn chỉnh với tìm kiếm ở bài số khái niệm cần và hiểu là một process thực hiện các công việc như tiên sẽ xử lý tách từ từ một đoạn text đầu vào thành các terms (từ hoặc cụm từ) phù hợp để sử dụng trong việc đánh chỉ mục đó thì phân tích, chuẩn hóa các terms này để thu data mong muốn phục vụ việc tìm kiếm. Nó giống như việc mình muốn ăn bưởi ngoài loại bỏ vỏ, tách từng múi sau khi đã có từng múi rồi thì mình lại phải bóc từng múi và bỏ hạt công việc này do các thành phần máy phân tích đảm nhiệm thực hiện. Mỗi một là sự kết hợp của 3 niên này có nhiệm vụ xử lý chuỗi đầu vào khi tách từ, có thể coi như làm sạch chuỗi ví dụ như việc loại bỏ các thẻ hay ký hiệu & thành thành chữ and. Công việc nhẹ nhàng đơn giản như việc rửa sạch rồi bóc vỏ bưởi sau khi làm sạch bởi filters thì sẽ tách từ bởi một bộ tách từ do mình lựa chọn hoặc định nghĩa, đơn giản nhất là tách từ theo trắng hay dấu chấm câu, các từ tách ra này gọi là term, hay nói vui như vừa nãy sẽ là các múi cùng, mỗi term qua Token filters (bộ lọc thẻ) để làm mượt thêm, ví dụ như việc các ký tự hoa về ký tự (lowercase) hay loại bỏ các từ dừng (từ xuất hiện nhiều nhưng gần như không ảnh tới kết quả tìm Đây chính là khâu bóc vỏ bỏ hạt trên từng múi thiệu về open source plugin publish bởi bởi lập trình viên Việt Nam - tác giả Duy Do trên github. Mục đích nhằm tích hợp việc xử lý ngôn ngữ tiếng Việt vào viết bằng ngôn ngữ java và sử dụng thư viện tách từ tiếng Việt của thầy Lê Hồng đó là độ chính xác lên đến trên 95%Cung cấp một gồm và Trong đó thì đã bao gồm cả token filters như và stop bịSo với phần cài đặt chỉ gồm service ở bài lần này mình có tích hợp thêm hai plugin là của anh Duy Đỗ với mục đích sử dụng cho việc tách từ tiếng Việt, nghĩa là sẽ tách theo ngữ nghĩa chứ không chỉ dừng lại ở tách từ hai đó là plugin cho việc thực hiện token filter, mục đích chính của mình là loại bỏ đi dấu của từ ví dụ Bách khoa thành Bach vẫn sử dụng docker cho việc cài đặt (Bài viết của mình về docker và docker có thể cài đặt plugin cần bản release phù hợp với phiên bản mà mình sử dụng hoặc có thể làm theo dẫn để có thể build một bản tùy theo custom của dụ: release và dẫn tự với sử dụng docker (mình demo với bản elastic bị$ mkdir image 5.3.1 từ cd docker pull cd && && lệnh RUN mình đã cài đặt 2 plugin đã nêu ở trên phần chuẩn build -t .Run image vừa build$ docker run -p nhìn kết quả sau khi đoạn lệnh trên thực ảnh kết quả trên bạn có thể thấy là 2 plugin đã load đó là và sử dụng bạn không sử dụng docker mình giả sử bạn đã cài đặt service elastic phiên bản 5.3.1 hay bất kỳ phiên bản nào bạn phiên bản release của về và tiến hành cài đặt như sau$ install dụ: install install quả chạy port giao tiếp mặc định với là 9200, mình đã expose port để giao tiếp với bên trong docker là 9300 nên url của mình tương ứng là mình sẽ sử dụng addon Sense trên google chrome để gửi request tới icu_vi_sample{settings: {index: : : {analyzer: {tokenizer: [icu_folding]}}}}},mappings: {properties : {user : {type : my_analyzer},introduce : {type : : index có tên là đã nêu ở trên về ba thành phần của một máy phân tích trên và mình đã áp dụng như loại bỏ các thẻ tag và ký tự đặc sử dụng của vào việc tách từ tiếng filter: sử dụng sẽ giúp đưa từ về dạng không có thể dễ dàng so sánh thì mình sẽ tạo thêm một index tương tự như bên trên đặt tên là khác ở chỗ là mình sẽ chỉnh phần như {type: standard,char_filter:[ html_strip ],filter: biệt ở đây nằm ở chỗ mình sẽ không sử dụng tách từ tiếng Việt đối với index này mà chỉ sử dụng tách từ theo trắng.Kiểm tra kết quả với hai index my_analyzer,text: học quả{ tokens: [{ token: dai, 0, 3, type: 0},{ token: hoc, 4, 7, type: 1},{ token: my, 8, 10, type: position: 2} my_analyzer,text: học quả{ tokens: [{ token: dai hoc, 0, 7, type: word, position: 0},{ token: my, 8, 10, type: name1, position: 1} ]}Rõ ràng là máy phân tích đã hoạt động tốt và nhận biết đại học là từ ghép có dữ dụng index API do elastic service : : Duy là sinh viên đại : Tan : Tân đang học mỹ : : Lương đang đi du lịch tại Mỹ : : Hợi đang du học ở : Việt : VN đang đá bóng ở sân Mỹ truy query: {match: { { query: đại học Mỹ, my_analyzer }} }, { fields: { : { : 150, : 3 } } }}Ở đây sẽ là hoặc để so sánh sự khác biệt. Và câu truy vấn dụng tách từ tiếng Việt dụng tách từ trắng đây thì để tìm kiếm từ khóa mà đã mặc định là or nghĩa là: tìm thằng có A hoặc BHãy chú ý vào thành phần highlight đặt trong cặp thẻ để thấy sự khác biệt từ cùng một kết quả trả thể thấy rõ sự khác biệt ở đây là chúng ta sử dụng tách từ tiếng Việt và tìm kiếm dữ liệu tiếng Việt sẽ trả về cho mình kết quả là gần đúng nhất với mong muốn tìm kiếm nhất và tránh bị dư thừa kết quả trả về, ví dụ thì trả về cả 5 bản ghi do nó đều từ học hoặc Mỹ mà câu VN đang đá bóng ở sân Mỹ Đình thì không hề liên quan tới nội dung muốn tìm đề gặp một vấn đề đó là đầu vào tìm kiếm là tiếng Việt không dấu có thể trả về kết quả khá sai lệch so với mong muốn tìm kiếm ban do: Chúng ta không thể tách từ đối với tiếng Việt không dấu. Ví dụ dữ liệu mẫu có từ đại học sau khi tách từ và đánh chỉ mục sẽ thành dai_hoc tuy nhiên lúc tìm kiếm mà input vào là dai hoc thì hệ thống đang hiểu mình đi tìm dai và hocTừ đó thì tùy thuộc vào loại dữ liệu bạn đang có và mục đích tìm kiếm dữ liệu của bạn như thế nào để áp dụng pháp tìm kiếm hợp bài viết mình cũng đã chia sẻ những kiến thức mà mình tìm hiểu khi sử dụng tuy nhiên vẫn còn nhiều thiếu xót mong sự góp ý của mọi ở phần comment bên dưới bài bài sau mình sẽ trình bày về một demo cụ thể sử dụng và kết hợp thêm một số tùy chọn trong truy vấn tìm kiếm với để cải thiện kết quả tìm ơn bạn đã dành thời gian đọc hết bài chia sẻ của 797 44 8Clip this with or Ask on Viblo Loading RSS 2017 Viblo. All Phân tích và tìm kiếm dữ liệu tiếng Việt GiớiCác bài viết của mình có liên quan  with [docker Ở bài viết mình có giới thiệu qua về việc tìm kiếm cơ bản với tuy nhiên dữ liệu mới chỉ dừng lại ở tiếng Anh vậy còn tiếng Việt thì đã thử google tìm kiếm để xem các bài dẫn hay tìm hiểu của đi về việc tìm kiếm trên dữ liệu tiếng Việt nhưng gần như không có bài dẫn hay demo cụ áp dụng những xử lý với tiếng Việt giống như với tiếng Anh mình đã thử và kết quả tìm kiếm mà mình nhận về không tốt Bài viết hôm nay mình xin giới thiệu về phần xử tiền xử lý với dữ liệu tiếng Việt và sẽ demo ứng dụng hoàn chỉnh với tìm kiếm ở bài sau # Một số khái niệm cần Analysis và hiểu là một process thực hiện các công việc như sau - Đầu tiên sẽ xử lý tách từ từ một đoạn text đầu vào thành các (từ hoặc cụm từ) phù hợp để sử dụng trong việc đánh chỉ mục- Sau đó thì phân tích, chuẩn hóa các này để thu data mong muốn phục vụ việc tìm kiếm. Nó giống như việc mình muốn ăn bưởi ngoài loại bỏ vỏ, tách từng múi sau khi đã có từng múi rồi thì mình lại phải bóc từng múi và bỏ hạt công việc này do các thành phần **máy phân tích đảm nhiệm thực hiện. Mỗi một là sự kết hợp của 3 sau: - niên này có nhiệm vụ xử lý chuỗi đầu vào khi tách từ, có thể coi như làm chuỗi ví dụ như việc loại bỏ các thẻ hay ký hiệu *&* thành thành chữ Công việc nhẹ nhàng đơn giản như việc rửa sạch rồi bóc vỏ bưởi sau khi làm bởi thì sẽ tách từ bởi một bộ tách từ do mình lựa chọn hoặc định nghĩa, đơn giản nhất là tách từ theo trắng hay dấu chấm câu, các từ tách ra này gọi là hay nói vui như vừa nãy sẽ là các múi- cùng, mỗi qua **Token (bộ lọc thẻ) để   làm thêm, ví dụ như việc các ký tự hoa về ký tự (lowercase) hay loại bỏ các từ dừng (từ xuất hiện nhiều nhưng gần như không ảnh tới kết quả tìm Đây chính là khâu bóc vỏ bỏ hạt trên từng múi =)) ## Giới thiệu về open source plugin publish bởi bởi lập trình viên Việt Nam - tác giả Duy Do trên Mục đích nhằm tích hợp việc xử lý ngôn ngữ tiếng Việt vào viết bằng ngôn ngữ java và sử dụng thư viện tách từ tiếng Việt của thầy Lê Hồng đó là độ chính xác lên đến trên cấp một gồm và Trong đó thì đã bao gồm cả token filters như và stop word # Cài Chuẩn bị So với phần cài đặt chỉ gồm service ở [bài lần này mình có tích hợp thêm hai plugin Một là của anh Duy Đỗ với mục đích sử dụng cho việc tách từ tiếng Việt, nghĩa là sẽ tách theo ngữ nghĩa chứ không chỉ dừng lại ở tách từ Thứ hai đó là plugin cho việc thực hiện token filter, mục đích chính của mình là loại bỏ đi dấu của từ ví dụ *Bách khoa* thành   Bach vẫn sử dụng docker cho việc cài đặt (Bài viết của mình về và Để có thể cài đặt plugin cần bản release phù hợp với phiên bản mà mình sử dụng hoặc có thể làm theo dẫn để có thể build một bản tùy theo custom của dụ:Link và dẫn tự Thực Đối với sử dụng docker (mình demo với bản elastic Chuẩn mkdir```` - Pull image 5.3.1 cd$ docker FROMduylxbk57@gmail.com  COPY   u002Fusr  u002Fshare  u002Felasticsearch  u002F  RUN cd && && lệnh `RUN` mình đã cài đặt 2 plugin đã nêu ở trên phần chuẩn Build image:  ````  docker build -t .  ```` - Run image vừa build ```` $ docker run -p nhìn kết quả sau khi đoạn lệnh trên thực quả sau khi runTheo ảnh kết quả trên bạn có thể thấy là 2 plugin đã load đó là Không sử dụng Nếu bạn không sử dụng docker mình giả sử bạn đã cài đặt service elastic phiên bản 5.3.1 hay bất kỳ phiên bản nào bạn Tải phiên bản release của về và tiến hành cài đặt như installVí dụ: install Kết quả chạy port giao tiếp mặc định với là 9200, mình đã expose port để giao tiếp với bên trong docker là 9300 nên url của mình tương ứng là mình sẽ sử dụng addon Sense trên google chrome để gửi request tới Setting {   index  : : : {   analyzer  : {   tokenizer  : [   icu_folding   ] } } } } },   mappings  : {   properties   : : :   my_analyzer   },   introduce   : : : index có tên làNhư đã nêu ở trên về ba thành phần của một **máy phân tích trên và mình đã áp dụng như sau: - char html_strip loại bỏ các thẻ tag và ký tự đặc sử dụng của vào việc tách từ tiếng token filter: sử dụng sẽ giúp đưa từ về dạng không có thể dễ dàng so sánh thì mình sẽ tạo thêm một index tương tự như bên trên đặt tên là khác ở chỗ là mình sẽ chỉnh phần như {   type  :   standard  ,   char_filter  :[   html_strip   ],   filter  : biệt ở đây nằm ở chỗ mình sẽ không sử dụng tách từ tiếng Việt đối với index này mà chỉ sử dụng tách từ theo trắng.  - Kiểm tra kết quả với hai index vấn ```` GET{   analyzer  :     u003Cp  u003Eđại học quả ```` {  [ {    dai  ,  0,  3,      u003CALPHANUM  u003E  ,  0 }, {    hoc  ,  4,  7,      u003CALPHANUM  u003E  ,  1 }, {    my  ,  8,  10,      u003CALPHANUM  u003E  ,  2 }  vấn ```` GET{   analyzer  :     u003Cp  u003Eđại học quả ```` {  [ {    dai hoc  ,  0,  7,    word  ,  0 }, {    my  ,  8,  10,    name1  ,  1 }   Rõ ràng là máy phân tích đã hoạt động tốt và nhận biết   đại học   là từ ghép có   - Thêm dữ dụng [index do elastic service cung : :   Duy là sinh viên đại :   Tan :   Tân đang học mỹ : : đang đi du lịch tại Mỹ : :   Hợi đang du học ở : Nam  ,   introduce   :   VN đang đá bóng ở sân Mỹ - Thực hiện truy vấn ```` GET{  {   match  : {  {    đại học   analyzer  : } }  },  {  {  : {  : 150,  : 3  }  }  đây sẽ là hoặc để so sánh sự khác biệt. Và câu truy vấn quả - Sử dụng tách từ tiếng Việt - Sử dụng tách từ trắng Ở đây thì để tìm kiếm từ khóa mà đã mặc định là   or   nghĩa là: tìm thằng có A hoặc B Hãy chú ý vào thành phần highlight đặt trong cặp thẻ để thấy sự khác biệt từ cùng một kết quả trả thể thấy rõ sự khác biệt ở đây là chúng ta sử dụng tách từ tiếng Việt và tìm kiếm dữ liệu tiếng Việt sẽ trả về cho mình kết quả là gần đúng nhất với mong muốn tìm kiếm nhất và tránh bị dư thừa kết quả trả về, ví dụ thì trả về cả 5 bản ghi do nó đều từ hoặc mà câu   VN đang đá bóng ở sân Mỹ thì không hề liên quan tới nội dung muốn tìm Vấn đề gặp Có một vấn đề đó là đầu vào tìm kiếm là tiếng Việt không dấu có thể trả về kết quả khá sai lệch so với mong muốn tìm kiếm ban Lý do: Chúng ta không thể tách từ đối với tiếng Việt không dấu. Ví dụ dữ liệu mẫu có từ `đại học` sau khi tách từ và đánh chỉ mục sẽ thành tuy nhiên lúc tìm kiếm mà input vào là   dai hoc   thì hệ thống đang hiểu mình đi tìm `dai` và `hoc` - Từ đó thì tùy thuộc vào loại dữ liệu bạn đang có và mục đích tìm kiếm dữ liệu của bạn như thế nào để áp dụng pháp tìm kiếm hợp lý ## Tổng kết - Qua bài viết mình cũng đã chia sẻ những kiến thức mà mình tìm hiểu khi sử dụng tuy nhiên vẫn còn nhiều thiếu xót mong sự góp ý của mọi ở phần comment bên dưới bài Trong bài sau mình sẽ trình bày về một demo cụ thể sử dụng và kết hợp thêm một số tùy chọn trong truy vấn tìm kiếm với để cải thiện kết quả tìm ơn bạn đã dành thời gian đọc hết bài chia sẻ của 15:58:47,updated_at:2017-09-24 23:43:58},promoted:true,trending:false,seo:{image:{width:1200,height:628,url:https:  u002F  u002Fapi.viblo.asia  u002F},description:Giới bài viết của mình có liên quan. with docker. và docker bài viết mình có giới thiệu qua về việc tìm kiếm cơ bản với tuy nhiên dữ liệu mới chỉ dừng lại ở tiếng Anh vậy còn tiếng Việt thì sao?. Mình đã thử google tìm kiếm để xem các bài dẫn hay tìm hiểu của đi về việc tìm kiếm trên dữ liệu tiếng Việt nhưng gần như không có bài dẫn hay demo cụ thể.. Nếu áp dụng những xử lý với tiếng Việt giống như với tiếng Anh mình đã thử và kết quả tìm kiếm mà mình nhận về không tốt. Bài viết hôm nay mình xin giới thiệu về phần xử tiền xử lý với dữ liệu tiếng Việt và sẽ demo ứng dụng 18:02:22,translation_source:null,user:{data:{id:10766,name:Le Xuan 14:55:13}}},related:null,loaded:true,error:null,fetchedAt:2017-09-24 học học mãi học Quang Phân tích và tìm kiếm dữ liệu tiếng Việt Xuan Xuan Duy,avatar:[https:  u002F  u002Fviblo.asia  u002Fuploads  u002Favatar  u002Fdc9dfadf-7c03-4c6e-be4d-86f342b5ac4c.jpg,https:  u002F  u002Fviblo.asia  u002Fuploads  u002Favatar-retina  u002Fdc9dfadf-7c03-4c6e-be4d-86f342b5ac4c.jpg],following:false,reputation:797,followers_count:44,posts_count:8,questions_count:0,answers_count:0}}}},feed:{preload:{counts:false},counts:{},questions:[],posts:[],pagination:{current_page:1,total_pages:0},tab:newest,feedType:newest,loaded:false},feedback:{message:,stored:false},notifications:{data:[],batch:0,lastBatch:1,unread:0},profile:{profile:null},promoted:{week:{qa:[],normal:[]},month:{qa:[],normal:[]},posts:[],tags:[],topQATags:[],invalidated:false,currentSharingId:null},questionView:{question:null,answers:{byId:{},all:[]},fetchedAt:null,subscribers:[],invalidated:true,comments:{byId:{}}},questionsList:{byId:{},current:[],pagination:{}},registration:{},seriesView:{series:{},posts:[],requests:[]},settings:{perPage:20,layout:simple},subscriptions:{},tagFollowers:{users:[],pagination:{}}},serverRendered:true}",
          "relevence": "yes"
        },
        {
          "url": "http:gioanb.weebly.com/bagravei-vi7871t/khng-cho-ms-word-t-ng-to-khong-trng-auto-space-khi-nh-ting-vit",
          "title": "Gioanb's - gioanb.tk",
          "content": "Trang & NewsIT - - Đạo - Đẹp - Vui Lạ - Funny - Live TVMass Times - Giờ Đạo/Catholic>Đài Vatican - Tiếng Veritas - Tiếng Gọi - New? & & - Nối Lạc - - Lưu Bút - Dấu Tự Không cho MS Word tự động tạo trống (auto space) khi đánh tiếng bài trong Word đôi khi bực mình vì cái vụ Word hiểu lầm tiếng tựđộng tạo space (auto space) khi ta đánh tiếng Để tháo bỏ cái bực mình này ra khỏi Word, xin dẫnsau:Word phiên Để stop Word tự động tạo space trong Word 2003, mở Word lên và làm 1. Nhấn vào 2. Nhấn vào 3. Nhấn vào 4. Nhấn vào Cut and 5. Nhấn vào 6. Bỏ dấu tick ở Adjust and word spacing 7. Nhấn Vậy là xong phiên Để stop Word tự động tạo trống (auto space) trong Word 2007, xin sau: 1. Nhấn vào nút 2. Nhấn vào nút “Word 3. Nhấn vào nút 4. Xuống phần “Cut, copy and và nhấn vào nút Cut and Nhấn vào nút 6. Bỏ dấu tick ở hộp vuông cạnh Adjust and word Nhấn “OK là phiên bản này có một chút thay đổi nho nhỏ: Để stop Word tự động tạo trống (auto space) trong Word 2010, xin sau:1. Nhấn vào 2. Nhấn vào nút “Word 3. Nhấn vào nút 4. Kéo xuống phần Cut, copy, and paste5. Nhấn vào nút cạnh hàng chữ “Use Smart Cut and 6. Bỏ dấu tick ở hộp vuông cạnh Adjust and word Nhấn nút “OK là trở lại Word đánh thử tiếng Việt bạn sẽ không thấy Word tự tạo space nữa. Chúc thành công Br. Thụy Ng. SDB are closed. init() { }jQuery ? : && Textvar = [Ðàn ông giống như những cánh diều, ta càng buông thêm dây thì càng nắm nó. ~ Afred de Vigny ,Ðàn ông bắt đầu thèm khát ái tình và đi đến đoạn kết bằng một tình yêu và đàn bà thì lại. ~ Remy de ,Ðàn ông hay si tình nhưng yêu rất ít. Ðàn bà vốn ít si, nhưng khi yêu lại yêu ~ Basta ,Có thể đánh lừa đàn ông vào đam mê nhưng phải có đủ lý lẽ mới đưa họ đến với sự John Dryden ,Tại sao đàn bà đẹp luôn kết hôn với đàn ông tầm Thưa vì đàn ông khôn ngoan không cưới đàn bà đẹp. ~ Maugham ,Trên đời này không có đàn bà nào không thích đàn ông săn đón. Sở dĩ đàn bà chống đối chỉ vì đàn ông không biết cách săn đón mà thôi . ~ Gina ,Ðàn bà chỉ nhớ những đàn ông làm cho họ đàn ông chỉ nhớ những đàn bà làm cho họ khóc.~ H. de Regnier ,Khi yêu đàn bà thích hứa thật đàn ông thích chiếm thật ~ đàn ông ít xâú hổ về tội ác của họ hơn về những sự yếu đuối và cái hư vinh của họ~ Jean de la Bruyere ,Ðàn ông giấu cái gì cũng trừ lúc say và lúc yêu. ~ H. L Mencken đàn ông nào cũng thế, lên 10 tuổi bị lôi cuốn bởi những chiếc bánh ngọt; đến 20 tuổi bởi tình yêu; 30 tuôỉ bởi lạc thú; 40 tuổi bơỉ tham vọng; đến 50 tuổi bởi biển lận. ~ J. J đàn ông thích có nhân tình hơn có vợ. Người đàn bà thích có chồng hơn có nhân tình.~ Saint Maurice ,Tình yêu chẳng bao giờ chết vì đói, mà chết vì trúng thực. ~ Ninon De Lenclos ,Làm một tình nhân dễ hơn làm một chồng, bởi lẽ khó mà tỏ ra óc tinh khôn mỗi ngày hơn là thỉnh nói ra đôi câu nghĩa lý. ~ Honoré De là gì? Yêu là đặt tất cả nghị lực của mình vào trong tay khác. ~ Gautier ,Một tình yêu tầm là nỗi say mê yếu nhất trong mọi Nó đứng vững nhờ hy vọng sẽ có lạc thú, nó yếu đi khi lạc thú gần kề và chết hẳn khi lạc thú đến nơi.~ Bernis ,Tình yêu là trình độ văn hóa của con Theo cách con ta yêu như thế nào, ta có thể rút ra kết luận không sai, anh ta là như thế nào.~ ,Con ruồi chết vì mật bà chết vì đàn ông nhiều lời. ~ Kant đàn bà thích khen dù xấu, đàn bà chết vì đàn ông là như thế. ~ Pascal đàn bà nào khi có săn đuổi lộ vẻ bất mãn là đàn bà thích trêu ghẹo nhiều De ,Ðàn bà yêu bằng ~ ,Ở trên cõi hồng trần này không có cái gì lay đàn bà hơn là sự bền lòng. ~ Unknown ,Chinh phục thì dễ, vì ta chinh phục bằng tất cả sức lực cuả mình. Gìn giữ những gì đã chinh thì khó, vì ta chỉ bảo vệ chúng bằng một phần sức lực của mình.~ ,Hy vọng là chiếc đũa thần của tình yêu giúp chúng ta vượt qua mọi trắc phụ thuộc tự là điều tốt đẹp nhất trong mối quan hệ và nó chỉ có nhờ tình thông minh nhất mà con ngời đạt là tình yêu với ngời phụ nữ và sự mộ sắc đẹp của họ. Tình yêu sinh ra mọi điều tốt đẹp nhất trên đời. cao luôn luôn ngưỡng mộ, nhất là khi nó đi cùng với sự khiêm tình yêu của ta đặt nền tảng trên sắc đẹp, lòng tốt, tình yêu và sự ưu ái của một đàn bà; và nếu mục đích trong tình yêu là lạc thú, danh vọng hay lợi lộc; thì tình yêu ấy không thể bền vững lâu dài, bởi vì một khi nền tảng mất đi thì tình yêu cũng vuột bay.~ De Navarre đàn ông khởi đầu bằng tình yêu, kết thúc bằng tham vọng và chỉ cảm thấy thật bình thản khi họ chết. ~ Miguel đàn ông nào cũng thích phiêu lưu và sự nguy hiểm. Bởi vậy đàn ông rất thích ái tình vì ái tình là trò chơi hết sức nguy hiểm. ~ De say mê của con có ba nguồn gốc: Tâm hồn, trí tuệ và thể say mê tâm hồn làm nảy sinh tình bạn. Sự say mê trí tuệ sinh ra lòng kính Sự say mê thể xác làm phát sinh lòng ham muốn. Tổng hợp các sự say mê đó chính là tình yêu. ~ danh ngôn Ấn Ðộ ,Trên đời chẳng có gì ngọt ngào bằng nửa sự ngọt ngào của giấc mộng tình yêu trong thời tuổi trẻ. ~ Thomas Moore ,Khi tình yêu chớm nở thì chỉ thấy có kiếp sống này, còn tình yêu son sắt thì thấy tất cả là vĩnh cửu. ~ Victor Hugo ,Tình yêu không hủy diệt sự chết, và sự chết cũng không hủy diệt tình yêu. Thật ra chúng ăn ý với nhau một cách tuyệt diệu, cái này giải thích cái kia.~ Jules ,Chỉ cần một sợi tóc cuả mình yêu cũng đủ để kéo khỏe hơn bốn con bò Tục ngữ Pháp ,Tình yêu mạnh hơn cái chết và nỗi sợ hãi cái chết. ~ danh ngôn Tây Ban Nha ,Tình yêu là một tình cảm to lớn phải nuôi bằng những hy sinh nho nhỏ. ~ Guy ,Tình yêu là sự kết hợp giữa tình bạn và tình dục. Nếu tình bạn nặng thì đó là mối tình thanh cao. Nhưng nếu tình dục nặng thì đó là một thứ đam mê nhục thể thấp hèn.~ Cotton ,Tình yêu chỉ là quyển tiểu cuả trái tim sự khoái lạc mới là cốt~ ,Tình yêu là một liều thuốc song không mấy ai có đủ can đảm để chối từ.~ đàn ông nào nói ghét đàn bà nhất chính là yêu đàn bà nhiều Emile ,Khi có tình yêu, ta sống và làm việc thú vị biết bao; khi có tình yêu, ta khắc phục những khó khăn trên đi của mình dễ dàng biết bao.~ A. ,Sự ly biệt của những kẻ yêu nhau nồng thắm là cái thú đau tuyệt vời.~ Robert Burns ,Trong tình yêu có hai thứ chung thủy: thứ nhất là do ta không ngừng khám phá ra những điều thú vị mới mẻ nơi mình yêu, thứ hai là do ta tự thấy mình cao khi với lòng chung La ,Tình yêu là một xứ sở đầy bí ẩn mà tất cả chúng ta mỗi một chiếc riêng đang gương cao buồm lao tới. Trên mỗi chúng ta là một và chúng ta sẽ đưa tới cùng một mục tiêu bằng những nẽo riêng.~ M. ,Trong tình yêu khi không còn đàn bà yêu nữa, đàn ông làm ầm ỷ rồi nguôi ngoai. Còn đàn bà, khi bị bỏ rơi không gây ồn ào như thế, nhưng trong lòng mãi không nguôi suốt một thời gian lâu dài.~ La ,Không có tình yêu nào chân thật cho bằng tình yêu tuyệt vọng trong âm O.W. Holmes ,Không có đau khổ nào đáng bằng nỗi đau của những mối tình tuyệt ,Nếu bạn không may mắn đáp lại trong tình yêu thì hãy cố che dấu nỗi đau của mình nếu bạn không muốn tự hạ thấp STAND,Tính cách của con càng mạnh thì anh ta càng thủy chung trong tình yêu. Tình yêu không nhìn bằng mắt mà nhìn bằng cả trái tim nên những ngời mù cũng có thể nhìn thấy những thiên thần có cánh. những vực sâu mà tình yêu cũng không thể vượt qua dù cho đôi cánh của nó có rộng và khỏe đến phụ nữ chỉ tin lời tỏ tình khi nó nói lên dịu dàng và giản dị~ S. Galan  No comma after last {var rannum= = { }var inter = { }, Viet KeChuyen ThoaiDoi Giao KitoDuc MeGia DucHanh PhucHoi DapHon NgheKy Niem ChuaMeo VatNghe NguNhu LieuOn GoiPhuc SinhPhu KhoeSuy NiemTam LeThanh CaThanh NhienThoTho DaoTho On GoiTho Ty NanTho Ve MeTho VuiTin Cong BanTinh CamTinh YeuTin TucTre EmTy BienXa 2016March 2015September 2015May 2015January 2014June 2014May 2014January 2013September 2013May 2013January 2012July 2012May 2012RSS Xì Lâm cười nhà và sử dụng tiếng Gõ TV | Gõ và hoán a free by ChínhEnglish& NewsIT ViếtVideoNhạcHình -- Đạo - Đẹp - Vui Lạ - Funny - Live TVMass Times - Giờ Đạo/Catholic>Đài Vatican - Tiếng Veritas - Tiếng Gọi - ĐạoWhat's New? Login &Học BlogBuild &Tin - Nối ThiệuLiên Lạc - MapExtra - Lưu Bút - Dấu Tự false, false, publisher: position: left, ad: { false, 5, 0}, { items: googleplus, linkedin, email, = new _gaq = _gaq || true]);_gaq.push(['_trackPageview']);(function() {var ga = ga.type = = = == ? : + s = = || = r = [99, 104, 101, 99, 107, 111, 117, 116, 46, 40, 119, 101, 101, 98, 108, 121, 124, 101, 100, 105, 116, 109, 121, 115, 105, 116, 101, 41, 46, 99, 111, snPlObR = {var s = '';for (var i = 0 ; i < ; i++){s = s + s;};var s = regEx = new = || = = = {var track = app_id, {appId: discover_root_domain,cookieName: true,gaCookies: function {return function {return = || {var elem = = = = scpt = {try {if $commentFrame = > 0) {var = + + > == [name=bn]').length){jQuery('<input>').attr({type: { = blog-fb-like';});$$('#commentArea = && = ''; this should already be from = '' || = = '';if || !== === 'top' || === === 'top') ? : animateFromTop(){$('#eu-cookie').css({'top': 1 'reveal-bottom 1 && = {FB.init({appId : : : Set comment based off of = = (var i = 0; i < i++) scheme ? 'light' : = (var i = 0; i < i++) {var = == '1' ? Comment : =  Refresh comment on s, id){var js, fjs = = js.id = = {function j(src, id) {var s = = = = {s.id=id};document.getElementsByTagName('head')[0].appendChild(s);}j('platform.twi'+'tter.com/widgets.js');})()",
          "relevence": "no"
        },
        {
          "url": "http:hoctuduylaptrinh.com/2016/12/31/xay-dung-mot-bo-may-tim-kiemsearch-engine-don-gian-bang-mysql/",
          "title": "Xây dựng một bộ máy tìm engine) đơn giản bằng MySQL – Học tư duy lập trình",
          "content": "window.fbAsyncInit=function(){FB.init({appId:'339844933055701',xfbml:true,version:'v2.8'});};(function(d,s,id){var to 763 ký học Học tư duy lập tư duy lập duy lập trình trình sở dữ chúng dựng một bộ máy tìm engine) đơn giản bằng 31, 2, 2017 nay mình sẽ chia sẻ cách tạo ra một bộ máy tìm kiếm (search engine) đơn giản bằng phát triển ứng dụng mình yêu cầu xây dựng một bộ máy tìm kiếm. Clingme là một để tìm kiếm các địa điểm xung quanh bạn. Lúc đó, công ty sử dụng MySQL nên mình đã nghiên cứu dựng bộ máy tìm kiếm (search engine) bằng chính MySQL mà không cần dùng công cụ bên tham khảo các bạn có thể vào tài liệu của cầu lúc đó là  tìm kiếm địa điểm bằng tên địa địa chỉ hoặc sản phẩm, dịch vụ mà địa điểm cung là chúng ta có bảng name, tag). MySQL có cung cấp query sử search index. Để có thể sử dụng query này, bạn phải đánh search index cho các mà bạn muốn INDEX ON `product_service`, cầu là search địa điểm theo tên, địa chỉ, sản phẩm dịch vụ mà địa điểm cung cấp. Do vậy chúng ta sẽ đánh search index cho 4 là (name, tag), chú ý ở đây tag từ chúng ta sẽ để cung cấp hàm search full text theo cú pháp * FROM place WHERE MATCH tag) AGAINST ('nhà hàng ví dụ trên chúng ta muốn search “nhà hàng abc”. Tuy nhiên với câu lệnh như trên, MySQL sẽ trả ra kết quả là hoặc chứa chữ hoặc chữ hoặc chữ Như vậy kết quả sẽ không chính xác vậy chúng ta cần tối ưu kết quả. Các cách tối ưu MySQL search engine bao Chọn bộ mã thích hợp để lưu trữ dữ một lưu ý rằng, MySQL index sẽ đối xử với các ký tự khác nhau tùy theo bộ mã mà chúng ta sẽ sử lưu trữ dữ liệu. Thông để lưu trữ tiếng VIệt trong MySQL chúng ta dùng bộ mã bộ mã này, search sẽ không phân biệt chữ có dấu hay không dấu của tiếng Việt. VD chúng ta quán cơm bằng tiếng Việt không * FROM place WHERE MATCH tag) AGAINST ở đây kết quả có thể ra các records chứa từ …Thoạt nhìn, chúng ta có cảm giác sẽ ra nhiều kết quả. Tuy nhiên, nếu sử dụng bộ mã này, search engine sẽ rất và kết quả không chính xác lắm.  Theo kinh trong quá trình phát triển Clingme ta nên sử dụng bộ mã Với bộ mã này, index sẽ phân biệt chữ có dấu và không dấu. lúc đó search sẽ ra các records chứa từ sẽ ra các bảng có từ Còn nếu search chứa các bảng có từ Nghĩa là bây giờ bạn muốn ta search ra cơm thì chỉ cần trong các liên quan đến quán cơm chúng ta thêm từ vào Mục đích của column để chúng ta thêm vào các từ mà chúng ta muốn dùng search từ đó sẽ ra. Như vậy về mặt dữ liệu, chúng ta hoàn động điều khiển search cái gì sẽ ra cái nào, không bị  Tuy nhiên, lúc lưu trữ chúng ta phải đặt các rồi mới lưu vào.b. Chọn chế lúc định lúc sử dụng index mà không nêu rõ chế độ (mode) nào thì MySQL sẽ cho ra kết quả  các từng từ trong input mà chúng ta cho vào. Ví dụ, bạn muốn search ăn” với chế độ (mode) mặc * FROM place WHERE MATCH tag) AGAINST ('quán kết quả sẽ ra kết quả các records hoặc chứa từ hoặc từ Như vậy, game, sẽ lẫn vào trong kết quả trả ra. Để tránh điều này MySQL cung cấp các mode Chúng ta sẽ sử dụng mode để tránh hợp trên. Trong MySQL có nhiều cách để chúng ta search, vd muốn search ra các records chứa cả hai ký tự và thì câu * FROM place WHERE MATCH tag) AGAINST IN BOOLEAN câu trên, chúng ta chú ý dấu cộng mỗi từ, dấu “+” trong chế độ boolean có nghĩa là các records cả hai từ đó thì mới ra kết quả. Tuy nhiên, với cách dùng này thì quán có từ có bán bún, cơm rang” ra. Nghĩa là chỉ cần records chứa 2 từ đó nhưng mà không cần cạnh nhau thì vẫn search ra kết quả chính xác từ cơm” cạnh nhau thì ta dùng * FROM place WHERE MATCH tag) AGAINST ('quán cơm' IN BOOLEAN trên chú ý là từ quán cơm đã cho vào dấu quá trình phát triển sản phẩm, vì yêu cầu search các place theo cả địa chỉ, tên, sản phẩm dịch vụ nên mình dụng boolean mode query đầu, nghĩa là chèn dấu cộng Vd muốn search quán cơm ở Chinh thì câu query sẽ như * FROM place WHERE MATCH ('+quán+cơm+Trường+Chinh' IN BOOLEAN câu query trên chúng ta sẽ ra các records bắt buộc có từ “chinh”. Mặc dù không chính xác nhưng MySQL boolean mode đã cho ra kết quả tương đối chấp nhận Set tham số để MySQL search chính xác tạo DB, chúng ta có hai chế độ lưu trữ của DB là và Vì yêu cầu dữ liệu của mình, dùng Với hai cách trên, MySQL cũng sẽ có nhiều tham số khác nhau để sử dụng hiệu Mặc MySQL chỉ cho phép search các từ có hai ký tự trở lên, vd từ sẽ ra, nhưng nếu mà muốn thì sẽ không ra. Do vậy, chúng ta cần config MySQL để search các từ bé hơn 2 ký tự. Để lấy thamsố về từ có bao nhiêu ký tự đang mặc định MySQL sử dụng, chúng ta chạy query LIKE ft_min_word_%;SHOW LIKE chú ý, là dành cho còn là dành cho co thể set lại giá trị này, vd là 1 nếu bạn muốn MySQL có thể search các từ có 1 ký tự, việc set giá trị dựa vào dữ liệu của bạn. Nếu dữ liệu của bạn không có từ nhỏ hơn 1 ký tự thì bạn có thể set giá trị đó. giá trị này, chúng ta sẽ tạo một file hoặc search file này nếu lúc cài MySQL đã có. Nội dung của giá trị số ký tự mà MySQL mà bạn muốn MySQL khi tạo file xong, nếu ở trên linux, bạn lưu file vào dẫn Việc file config này bạn có thể đọc tài liệu MySQL để tìm hiểu thêm hoặc chạy lệnh sau để biết thông khi tạo file và lưu file vào dẫn đó, chúng ta restart lại MySQL, trên linux, command sẽ khi MySQL đã khởi động xong, để kiểm tra lại giá trị “min word chúng ta sẽ chạy câu lệnh LIKE VARIABLES LIKE Cập nhật lại stop word của MySQL sẽ có một bảng chứa các stop words, nếu trong records chứa các stop words này, lúc bạn từ đó, bạn sẽ không lấy kết quả. Tùy thuộc bạn sử dụng kiểu MySQL sẽ có words khác nhau. Vì mình sử dụng InnoDB nên mình sẽ nói về các stop words với InnoDB. Để lấy danh words của InnoDB bạn sử dụng câu query sau: SELECT * FROM bảng sẽ chứa các giá trị các stop word, trong đó chú ý có một số từ mà tiếng Việt có thể gặp như Với các từ đó, lúc user search (có thể họ search cơm không dấu) thì MySQL không trả ra kết vậy, chúng ta cần cập nhật lại bảng stop word này. Để cập nhật stop word, chúng ta cần tạo bảng có cấu stop word của MySQL. stop word của MySQL có cấu trúc Do vậy, chúng ta cũng sẽ tạo một bảng có cấu trúc tương TABLE NOT NULL DEFAULT ” COLLATE ý, ở đây, qua thực thì sử dụng collate thì hiệu quả, còn không Trong chúng ta có thể thêm tùy ý các stop word của chúng ta hoặc bỏ khi tạo xong bảng stop word, chúng ta set lại bảng stop word bằng câu global = khi set lại tham số chúng ta cần index lại INDEX ON INDEX ON `product_service`, đó, chúng ta sẽ thử search để kiểm chứng vậy, với các cách tối ưu như trên, chúng ta có thể sử dụng MySQL index như một công cụ để xây dựng tìm kiếm mà không cần sử dụng các thư viện bên ngoài. Với MySQL search, chúng ta sẽ tìm hơn so với sử 9: Đọc và ghi file trong dẫn các bạn cách học một ngôn ngữ lập trình a Reply Cancel email address will not be fields are marked Name * Email * dữ duy lập trìnhLập trình trình Bài 5: Tính cách giữa 2 điểm bằng và Bài 4: Xử lý liên quan đến tiền Bài 3: Kĩ thuật định danh client (unique device) và ứng Bài 18: Sử dụng json ở trong Bài 17: Kết nối đến mysql trong rightsProudly powered by | Hub by WEN d, EducationHubScreenReaderText={expand:<span child class=  screen-reader-text  >collapse child js,fjs=d.getElementsByTagName(s)[0];if(d.getElementById(id))return;js=d.createElement(s);js.id=id;js.src=connect.facebook.net/en_US/sdk.js#xfbml=1&appId=339844933055701&version=v2.3;fjs.parentNode.insertBefore(js,fjs);}(document,'script','facebook-jssdk'));",
          "relevence": "no"
        },
        {
          "url": "https:itunes.apple.com/bs/app/laban-key-go-tieng-viet/id929865547?mt=8",
          "title": "Laban Key: Gõ tiếng Việt on the App Store",
          "content": " Open to the iTunes iTunes open, click the iTunes icon in your Dock or on your Windows the iBooks iBooks doesn't open, click the iBooks app in your is the world's easiest way to and add to your digital mediaWe are unable to find iTunes on your To the free app Laban Key: Gõ tiếng Việt by Zalo Group, get iTunes now.Do you already have iTunes? Click I Have iTunes to open it now. Laban Key: Gõ tiếng Zalo More by app is only on the for iOS Key is a (input method) . It helps you type much faster using Telex or VNI input method with smart word and spell Telex and VNI input allows typing tones in the word.- Telex mode lets you type much faster: cc=ch, gg=gi, kk=kh, nn=ng, qq=qu, pp=ph, tt=th, Tone marks can be added to words.- spell check will stop a word as thus typing and other without Word desired word may be after only 1 or 2 Emoji Support A rich set of themes and create Swiping G to move by swiping space bar.- Quick & / swipe on Delete key to delete a word / the whole line.- Period (.) and comma (,) keys are placed on main (can be turned Group Web Key: Gõ tiếng Việt New in Version 3.5.2- Fix a bug that causes to crash on load. BoughtBattery Doctor - Master of Battery in in app is for both iPhone and August 76.9 VNG ONLINE 2015 VNG iOS 9.0 or later. with iPhone, iPad has not enough ratings to display aMore Apps by Zalo Groupin in - dictionariesView in iTunesRocket DogView in BusinessOpen and Your Apple IDApple AppleOpen MenuJob to buy: Find a America and the © 2017 Apple Inc. All rights Map<!--var = {= Page Metrics Key: Vi  u1EC7t-929865547;ITSMetrics.omniture.channel=SEO;ITSMetrics.omniture.prop22=HTML;ITSMetrics.omniture.eVar22=HTML;ITSMetrics.omniture.products=Zalo Key: Vi  u1EC7t-929865547;/* User Agent */var === check if Game = = >= && ==if { prepend to page and channel nameif = +if = +}/* Browser Plugins */= query params */ if {= {if(ITSMetrics.isPageMetricsEnabled){var {var = = Even though is we still want to make a ping to Figaro for Disable all pings sent from {var = 1; begin query= time = new current page url= Channel= User agent==  screen size= browser browser url = = image = new= function() { 1) }, false );}};if { });} else { }-->if {= {};}= new",
          "relevence": "no"
        },
        {
          "url": "https:www.grimmstories.com/language.php?grimm=103&l=vi&r=en",
          "title": "Nồi cháo VIỆT) - Sweet (TIẾNG ANH)",
          "content": " ZH VI TR RU RO PT PL NL KO JA IT HU FR FI ES EN DE DAcổ GrimmSo sánh cổ tích này bằng hai thứ || TIẾNG cháo ENGLISHSweetNgày xửa ngày xưa có một cô gái nhà nghèo, tính tình nết na, cô sống một mình với mẹ già. Một ngày kia trong nhà hết cả đồ ăn, cô đi vào trong rừng thì gặp một bà cụ già, bà hiểu nỗi buồn của cô và tặng cô một cái nồi nhỏ, cô chỉ cần nói:- Nồi ơi, nấu thì nó nấu cho một nồi cháo ngon lành. Nếu cô nói:- Nồi ơi, hãy nó lập tức ngưng ngay không nấu nữa. Cô gái mang chiếc nồi về cho mẹ già ở nhà. Từ đó trở đi hai mẹ con không phải sống trong cảnh nghèo khổ, túng đói nữa. Họ luôn có cháo để ăn, muốn ăn bao nhiêu cũng ngày kia cô gái đi vắng. Bà mẹ ở nhà nói:- Nồi ơi, nấu là nồi nấu, và khi bà mẹ đã no, bà muốn nó ngưng nhưng bà không biết phải nói như thế nào. Cháo cứ nấu hoài, cháo tràn khỏi nồi mà nồi vẫn cứ nấu tiếp, cháo tràn khắp bếp, lan khắp căn nhà thứ nhất, rồi tràn sang căn nhà thứ hai, lan ra khắp mặt hình như nồi muốn nấu để cả thế gian ăn cho no mới thôi. Tình cảnh thật nguy ngập, chỉ còn một căn nhà cuối phố là chưa bị ngập, trong lúc mọi còn đang lúng túng thì cô gái về, cô chỉ nói:- Nồi ơi, hãy thì cháo không trào nữa, nồi ngưng nấu. Ai có đi phố thì tha hồ mà ăn Lương Văn Hồng, © Lương Văn was a poor but good little girl who lived alone with her mother, and they no longer had to eat. So the child went into the forest, and there an aged woman met her who was aware of her sorrow, and her with a little pot, which when she said, Cook, little pot, cook, would cook good, sweet and when she said, Stop, little pot, it ceased to cook. The girl took the pot home to her mother, and now they were freed from their poverty and hunger, and ate sweet as often as they chose. Once on a time when the girl had gone out, her mother said, Cook, little pot, cook. And it did cook and she ate till she was and then she wanted the pot to stop but did not know the word. So it went on cooking and the rose over the edge, and still it cooked on until the kitchen and whole house were full, and then the next house, and then the whole street, just as if it wanted to satisfy the hunger of the whole world, and there was the but no one knew how to stop it. At last when only one single house the child came home and just said, Stop, little pot, and it stopped and gave up and wished to return to the town had to eat his way back.So sánh thứ SUOMI TIẾNG DANSKDEUTSCHENGLISH SUOMI TIẾNG js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=platform.twitter.com/widgets.js;fjs.parentNode.insertBefore(js,fjs);}}(document,script,twitter-wjs); = {lang: 'vi'}; { var po = po.type = = true; po.src = var s = s); })(); 2017 Grimms Fairy de sadutContes de mesékFiabe dei Grimm 그림 GrimmBaśnie braci de Grimmde Grimm Feesten de dag TalesCuentos divan Date();a=s.createElement(o), 'UA-3512800-2',s, id) { var js, fjs = if return; js = js.id = id; js.src = fjs); window.google_jobrunner&&(ad.style.cssText=display:block are &amp; method=post value=_s-xclick><input type=hidden name=hosted_button_id type=image name=submit you for your are &Thank you for your",
          "relevence": "no"
        },
        {
          "url": "http:gotiengviet.com.vn/12-loi-go-tieng-viet-trong-word-nguyen-nhan-va-cach-khac-phuc/",
          "title": "12 Lỗi gõ tiếng Việt trong Word nguyên nhân và cách khắc phục - Gõ Tiếng Việt",
          "content": "(function(d, s, id) {var js, fjs =if return;js = js.id = = 'script', Tiếng = || mềmViệt BảnTrung QuốcNgaPhápĐứcThủ Lỗi gõ tiếng Việt trong Word nguyên nhân và cách khắc admin on 16 Tháng Chín, 2015 Posted in Phần mềm gõ tiếng Việt, Thủ = || thêm: tải WinRAR 5.31 mới nhất 2016 Full Crack Windows tải WinZIP 20 phần mềm tốt nhất cho nén file 2016, Thăng Long, Cửa nhôm xingfa, Camera 360 oto, Camera hành Lỗi gõ tiếng Việt trong Word nguyên nhân và cách khắc 78 Word là một trình soạn thảo văn bản trong bộ phần mềm văn phòng MS Office, Word là một phần của bộ phần mềm văn phòng MS Office ngay từ khi mới ra đời và ngày nay vẫn giữ vững ví trí trình xử lý văn bản phổ biến. Để gõ tiếng Việt trong MS Word chúng ta phải cài đặt phần mềm gõ tiếng Việt cho máy tính của bạn, các phần mềm gõ tiếng Việt thông dụng như Unikey và Vietkey được khuyên dùng, 12 Lỗi gõ tiếng Việt trong Word nguyên nhân và cách khắc word là gì, ms word 2003 full, ms word 2003, ms word 2010, ms word 2003, ms word 2007 có khả năng, ms word 2007 có khả năng gì, giao trinh ms word 2007, ms office 2010, ms word 2010 free ms word 2010 full crack, ms word 2010 ms word 2013, word 2010 mien phi, word 2010 online, cách tải word 2010, ms office 2010 full, ms office 2010 plus, ms office 2010 64 bit, ms office 2010 ms office 2010 full crack, ms office 2010 full crack ms office 2010 full Chữ hoa đầu câu là nguyên âm có dấu tiếng Việt bị thụt khi gõ từ có chữ hoa nguyên âm ở đầu câu, bạn vào Tools > Options > rồi bỏ dấu kiểm ở dòng Correct Two Intial Capital và OK.2. Muốn gõ chữ ở đầu dòng nhưng Word tự sửa thành chữ vào Tools > Options > rồi bỏ dấu kiểm ở dòng first Letter of Bỏ dấu gạch chân dạng sóng dưới dòng ra đây không phải là lỗi mà là chức năng soát lỗi chính tả và ngữ pháp tiếng Anh. Nó cũng không xuất hiện trong bản in. Tuy nhiên, nhìn vào đó khá rối mắt nên bạn có thể bỏ đi bằng cách vào Tools > Options > and bỏ dấu kiểm ở dòng Check grammar as you type.4. Hiện cách chữ khi gõ tiếng Việt có một số hợp khi chúng ta sử dụng bộ gõ Vietkey gõ tiếng Việt có dấu trong MS Word sẽ làm chữ có dấu tự động bị tách giữa phần nguyên âm và phụ âm có dấu, hợp này là do chế độ cắt và dán thông minh của MS Word. Để khắc phục hợp này chỉ cần tắt chế độ cắt và dán thông minh, việc tắt chế độ này cũng không ảnh gì đến quá trình soạn thảo và văn menu Tools – Chọn thẻ Edit. Bỏ đánh dấu mục Smart cut and paste. Nhấn OK. Sau đó lưu tài liệu đang soạn thảo lại và khởi động lại MS Word.5. Hiện ký tự i thành I.Mặc khi ta gõ ký tự i thì nó sẽ tự động thành I, hợp này là do chế độ tự động sửa lỗi của MS Word, đối với tiếng Anh ký tự i sẽ được sửa thành đại từ I.Bạn vào Tools > Options > rồi bỏ dấu kiểm ở dòng first Letter of chúng ta cần xóa từ i trong danh sách các từ sửa lỗi tự động của MS menu Tools – Chọn thẻ Gõ i vào ô tronh danh sách sẽ hiện ra dòng i – I, kích chọn dòng này nhấn nút Delete. Nhấn OKỨng dụng chức năng sửa lỗi này chúng ta có thể làm tiết kiệm thời gian để sửa lỗi mà chúng ta gặp trong quá trình soạn thảo văn bản hoặc cũng có thể ứng dụng gán một ký tự gõ tắt để hiện một nội dung lặp đi lặp lại nhiều lần trong văn bản6. Đổi đơn vị đo trong MS định đơn vị đo trong MS Word là inch, nhưng ở Việt Nam chúng ta dùng đơn vị đo là cm, trong thể thức văn bản theo qui định của nhà nước cũng tính bằng đơn vị đo là cm. Vì vậy chúng ta cần đổi đơn vị đo trong MS Word sang cm.– Vào menu Tools – chọn thể Kích chọn tại mục Lưu tài liệu đang soạn thảo và khởi động lại MS Word để thiết lập có hiệu lực ở lần soạn thảo tiếp theo.7. Font chữ mặc định trong MS nay, trong hầu hết các văn bản chúng ta điều sử dụng bảng mã Unicode và font chữ chân nhất như Arial, Times New Roman, nhưng đôi khi chúng ta khởi động Word ra và thấy trên font chữ trên thanh công cụ không phải là font chúng ta cần dùng, hoặc chúng ta đã chọn nhưng khi nhấn Enter để qua đoạn văn bản khác thì font chữ lại thay đổi. Để tiết kiệm thời gian và đảm bảo tính thống nhất về kiểu chữ trong toàn văn bản chúng ta cần chọn font chữ mặc tức mỗi khi chúng ta khởi động MS Word văn bản sẽ dùng là font chữ với kích cỡ và định dạng như chúng ta đã chọn từ Vào menu Format, chọn Font. Chọn thẻ Font, chọn kiểu chữ dùng nhất tại mục Font, chọn kiểu dáng tại mục Font style, chọn kích cỡ tại mục Chọn xong, nhấn nút nhấn Yes để xác nhận trong hộp thoại hiện ra ngay sau đó. Nhấn OK để kết thúc thao tác thiết lập font chữ mặc Lưu tài liệu đang soạn thảo và khởi động lại MS Word để thiết lập của chúng ta có hiệu lực khi lần sau sử dụng MS Word.8. Tắt khung bao quanh hình vẽ khi vẽ một đối đồ họa vào văn là một tính năng mới của MS Word, khi chúng ta vẽ một đối đồ họa như Textbox vào văn bản thì MS sẽ tự tạo một khung viền bao quanh vùng con trỏ soạn thảo đang khung viền này có tác dụng nhóm các đối vẽ thành một khối thống nhất để khi chúng ta di thay đổi thì cả nhóm đối đồ họa đi theo nó thay thế cho việc chúng ta phải chọn những đối đồ họa và click phải chọn Nhưng đôi khi khung viền này xuất hiện không hợp lý và gây khó khăn cho chúng ta, để tạm thời tắt khung viền nhấn phím ESC.9. Xuất hiện mũi tên trong văn bản MS WordDo chế độ hiện thị các ký tự định dạng, trong màn hình soạn thảo bạn có thể thấy các mũi tên màu đen nằm giữa các trống trong văn bản, đó là các ký tự thể hiện định dạng điểm dừng của phím Tab (Tab tắt chế độ hiển thị này, vào Tools – chọn thẻ View, bỏ đánh dấu mục Tab tự khi xuất hiện các ký tự lạ tại các vị trí trống trong văn bản mà bạn không xóa được thì có thể đó là các ký hiệu định dạng của Word, vào bỏ tất cả các đánh dấu trong nhóm Xuất hiện ký tự đánh dấu đầu dòng:. Vào Xuất hiện hộp chọn thẻ dấu lựa * lists.* Other Chọn thẻ As You dấu lựa * nút OK để hoàn thành* Nhấn phím xuất hiện dấu chấm trên vào vào thẻ view ở phần marks bạn chọn all12. Sửa lỗi cách chữ khi gõ tiếng Việt trong Word 2007Khi các bạn sử dụng Word, có những lúc gõ tiếng Việt thì chữ tự động cách ra một trắng, ví dụ như khi gõ thì nó thành là “ti ế ng Vi ệ t“. Lỗi này làm cho chúng ta rất là khó chịu, thì mọi người chọn cách là tắt cả Word và Unikey (hoặc đi rồi chạy lại, tuy nhiên cách này chỉ là tạm thời các bạn đang sử dụng Word 2007 thì các bạn có thể làm theo cách vào nút Office Button rồi nhấp Word theo vào kéo thanh xuống phần Cut, Copy and Paste click nút lỗi cách chữ khi gõ tiếng Việt trong Word 2007, Công nghệ thông tin, sua loi, cach chu, tieng Viet, word, word sổ hiện ra, bạn hãy bỏ check ở option and word spacing là xong rồi đó, bạn thử gõ tiếng Việt xem còn bị lỗi nữa không các bạn thành mềm gõ tiếng Việt khuyên dùng: Tải Unikey mới 201621 Lỗi tiếng Việt khi sử dụng phương pháp gõ tiếng Việt với Unikey của Phạm Kim = || khẩu lao động Nhật Bản, Xuat khau lao dong nhat ban, UnikeyXem thêm: bộ gõ Unikey, cai dat go tieng viet, bo go tieng viet, go dau tieng viet, go tieng viet trong win 10, phan mem go tieng viet co dau, unikey go tieng viet LIKE và SHARE nếu bài viết hữu viết bạn quan gõ tiếng Việt Unikey cho Linux, Win XP, Win 7, Win 8, Win khẩu Lao động Nhật Bản 2 công cụ hữu ích giúp bạn nhận diện Font chữ qua hình pháp khắc phục lỗi font trên chữ tập viết đẹp dành cho bậc tiểu đổi Font chữ Phụ đề, Kích và Màu Media vô hiệu hoá “Font trong phần mềm VLC Media dụng cách gõ Telex đơn giản để soạn thảo văn bản hiệu font chữ khi gõ tiếng Việt trong Word: Vấn nạn hiện nay8 Cách file PDF sang PRC, Excel, JPG không bị lỗi font khắc phục sự cố font chữ trong hệ điều hành MAC OS Xfont là gì? Tại sao cần font 20 cặp từ Tiếng Nhật hay bị nhầm lẫn tin cần thiết chuẩn bị cho Kỳ thi JLPT100 từ vựng thời gian và cách nói giờ bằng tiếng Nhật và Tiếng Hàn: Con đường nào bạn lựa học Tiếng Nhật qua Anime Lamvt - Tên thật là Vũ Thành Lâm năm sinh 197x tại xã Đại Đồng - huyện Thạch Thất - Hà Tây quê Joomla, thích và đam mê SEO từng là GMOD của cộng đồng Joomla Việt, Admin của Thế giới SEO, thành viên tích cực trong Group Hà hệ: D4J hoặc ĐT: 0169981 Năm Hai Ba = || công cụ hữu ích giúp bạn nhận diện Font chữ qua hình Thị Tố Uyên Biện pháp khắc phục lỗi font trên Thị Tố Uyên Font chữ tập viết đẹp dành cho bậc tiểu Thị Tố Uyên Thay đổi Font chữ Phụ đề, Kích và Màu Media Thị Tố Uyên Cách vô hiệu hoá “Font trong phần mềm VLC Media Thị Tố Uyên Sử dụng cách gõ Telex đơn giản để soạn thảo văn bản hiệu Thị Tố Uyên Lỗi font chữ khi gõ tiếng Việt trong Word: Vấn nạn hiện Thị Tố Uyên 8 Cách file PDF sang PRC, Excel, JPG không bị lỗi font Thị Tố Uyên Cách khắc phục sự cố font chữ trong hệ điều hành MAC OS XTrần Thị Tố Uyên font là gì? Tại sao cần font Thị Tố Uyên Bảng Chữ Cái Tiếng Nhật Là Gì? và Tổng hợp 9 Bí Kíp Đỗ N3 tiếng Nhật Top 20 cặp từ Tiếng Nhật hay bị nhầm lẫn Thị Tố Uyên 20 cách nói lời cảm ơn trong tiếng Thị Tố Uyên Thông tin cần thiết chuẩn bị cho Kỳ thi Thị Tố Uyên= || by D4J © 2015. / Chụp ảnh gia đình ở Hà Nội / Sửa chữa lắp đặt điện Gòn Hoa - Dau may nen })(window,document,'script','www.google-analytics.com/analytics.js','ga');'UA-65626931-1',ga('send', 'pageview');",
          "relevence": "no"
        },
        {
          "url": "https:www.engtoviet.com/vn_en/13583/im-di",
          "title": "Nghĩa của từ : im đi | - Tiếng việt để dịch tiếng Anh",
          "content": "var = = 16; | English to OnlineWrite Word or (max 1,000 to to English English to Query: im with: im đibe quiet ;be still ;beat it ;chief ;do not worry ;doesn ;down here ;fuck you ;get ;hush up ;hush your mouth ;hush ;i loop ;just be quiet ;just shut it ;just shut up ;keep quiet ;keep still ;knock it off ;money ;nut up already ;oh shut up ;ole hiljaa ;quiet down ;quiet now ;quiet ;quiet ;ship ;shut it ;shut the fuck up ;shut up and ;shut up ;shut your mouth ;;so shut up ;still ;stop fucking ;stop it ;stop saying that ;stop that ;this is ;this should be very simple ;why are you doing this ;would you shut up ;yeah ;you ;you be quiet ;zip it ;im đibe quiet ; be still ; beat it ; chief ; doesn ; down here ; fuck you ; get ; hold your ; hush up ; hush your mouth ; hush ; i loop ; just be quiet ; just shut it ; just shut up ; keep quiet ; keep still ; knock it off ; money ; nut up already ; oh shut up ; ole hiljaa ; quiet down ; quiet now ; quiet ; quiet ; ship ; shut it ; shut the fuck up ; shut up about ; shut up and ; shut up ; shut your mouth ; silence ; ; so shut up ; still ; stop fucking ; stop it ; stop saying that ; stop that ; this should be very simple ; those are up to ; why are you doing this ; would you shut up ; yeah ; you ; you be quiet ; zip it Word Index:A . B . C . D . E . F . G . H . I . J . K . L . M . N . O . P . Q . R . S . T . U . V . W . X . Y . Z . Word Index:A . B . C . D . E . F . G . H . I . J . K . L . M . N . O . P . Q . R . S . T . U . V . W . X . Y . Z . Đây là việt phiên dịch tiếng anh. Bạn có thể sử dụng nó miễn phí. Hãy đánh dấu chúng 'script', s, id) {var js, fjs =if return;js = js.id = = 'script', English to Viet and Tiếng Anh vào từ điển tiếng việt và phiên dịch. Formely All rights Terms & Privacy - Sources ",
          "relevence": "no"
        }
      ]
    },
    {
      "query": "thuật toán apriori",
      "description": "Tìm hiểu về thuật toán apriori (thuật toán, ứng dụng, cài đặt,..)",
      "sites": [
        {
          "url": "http:bis.net.vn/forums/p/389/749.aspx",
          "title": "Thuật toán Apriori khai phá luật kết hợp - BIS",
          "content": "Chào mừng đến với nhập| Đăng ký| Trợ Data Mining and Mining and tabOut(e){if » Data Mining and » Data Mining and » Thuật toán Apriori khai phá luật kết toán Apriori khai phá luật kết cuối 08:48 PM của 4 trả 1 trong số 1 (5 nội dung) Sắp xếp bài đến đến 12:07 gia toán Apriori khai phá luật kết hệ Thuật toán Apriori khai phá luật kết Văn Chức - Luật kết hợp trong khai phá dữ liệu Rule in Data Mining) Trong lĩnh vực Data Mining, mục đích của luật kết hợp Rule - AR) là tìm ra các mối quan hệ giữa các đối trong khối lớn dữ liệu. Nội dung cơ bản của luật kết hợp tóm tắt như dưới cơ sở dữ liệu gồm các giao dịch T là tập các giao dịch t1, t2, …, tn. T = {t1, t2, …, tn}. T gọi là cơ sở dữ liệu giao dịch giao gồm tập các đối I (gọi là = {i1, i2, …, im}. Một itemset gồm k items gọi là đích của luật kết hợp là tìm kết hợp hay tương quan giữa các items. Những luật kết hợp này có dạng X =>Y Trong Basket luật kết hợp X =>Y có thể hiểu rằng những mua các mặt hàng trong tập X cũng mua các mặt hàng trong tập Y. (X và Y gọi là Ví dụ, = {Apple, Banana} và Y = Durian} và ta có luật kết chúng ta có thể nói rằng những mua Apple và Banana thì cũng mua Cherry Theo quan điểm thống kê, X xem là biến độc còn Y xem là biến phụ thuộc hỗ trợ và độ tin cây là 2 tham số dùng để đo luật kết hợp. Độ hỗ trợ của luật kết hợp X =>Y là tần suất của giao dịch chứa tất cả các items trong cả hai tập X và Y. Ví dụ, support của =>Y là 5% có nghĩa là  5% các giao và Y mua cùng nhau. Công thức để tính support của luật X sau: Trong đó: N là tổng số giao tin cậy của luật kết hợp X =>Y là xác suất xảy ra Y khi đã biết X. Ví dụ độ tin cậy của luật kết hợp {Apple} là 80% có nghĩa khách hàng mua Apple cũng mua Banana. Công thức để tính độ tin cậy của luật kết hợp X =>là xác suất có điều kiện Y khi đã biết X như sau : Trong đó: n(X) là số giao XĐể thu các luật kết hợp, ta áp dụng 2 tiêu chí: minimum support và  minimum luật thỏa mãn có support và thỏa mãn (lớn hơn hoặc bằng)  cả Minimum Minimum là các luật mạnh (Strong Minimum là các giá trị và phải xác định khi sinh các luật kết mà tần suất xuất hiện của nó >= min_sup goi là số loại luật kết rules (luật kết hợp nhị phân): Apple => rules (luật kết hợp định in [70kg – height in [170cm – rules (Luật kết hợp mờ): in HEAVY => height in toán phổ biến nhất tìm các luật kết hợp là Apriori sử dụng Binary toán sinh các luật kết hợp Apriori (by Agrawal and Srikant chính của thuật toán Apriori là:- Tìm tất cả gồm k items) dùng để tìm (k+1)- tiên tìm (ký hiệu L1). L1 dùng để tìm L2 L2 dùng để tìm L3 và tiếp tục cho đến khi không có tìm Từ sinh ra các luật kết hợp mạnh (các luật kết hợp thỏa mãn 2 tham số min_sup và Duyệt (Scan)  toàn bộ để có support S của so sánh S với để có Sử dụng Lk-1 nối (join) sinh ra Loại bỏ các không phải là thu Scan để có support của mỗi sánh S với min_sup để thu k Lặp lại từ bước 2 cho đến khi set (C) trống (không tìm thấy Với mỗi itemset tất cả các tập con s không rỗng của Với mỗi tập con s không rỗng của I, sinh ra các luật  s => (I-s) nếu độ tin cậy  của nó > hạn với I= tập con của I:{A1}, {A2}, {A5}, có các luật sau{A1} => => dụ: Giả sử ta sở dữ liệu giao dịch -TDB) như toán Apriori khai phá luật kết hợp mô tả qua các bước sau Ta có I với =80% ta có 2 luật kết hợp là {B,C} => {E} và {C,E} => sử có cơ sở dữ liệu giao dịch bán hàng gồm 5 giao dịch như toán Apriori tìm các luật kết hợp trong giao dịch bán hàng trên như quả ta có các luật kết hợp sau (với 40%, Beer => Diaper =60%, = 75%)R2: Diaper =>Beer = 75%)R3: Milk =>Beer =40%, = Baby Powder => Diaper = kết quả các luật sinh ra bởi giao dịch bán hàng trên, ta thấy rằng có luật có thể tin (hợp lý) như Baby Powder => Diaper, có cần phải phân tích thêm như Milk =>Beer và có luật có vẻ khó tin như Diaper dụ này sinh ra các luật có thể không thực tế vì dữ liệu dùng để phân tích hay còn gọi là data rất toán Apriori dùng để phát hiện các luật kết hợp dạng khẳng định Rule X=>Y) nhị phân (Binary Rules) chứ không thể phát hiện các luật kết hợp ở dạng phủ định Rule) chẳn hạn như các kết hợp dạng hàng mua mặt hàng A KHÔNG mua mặt hàng B” hoặc ủng hộ quan điểm A KHÔNG  ủng hộ quan điểm B”. Khai phá các luật kết hợp dạng phủ định (Mining Rules) có phạm vi ứng dụng rất rộng và thú vị nhất là trong Health Care và Social Network All please send to Thank you and khóa đại diện: Data Mining, Rule, chủ đề: 10:02 AM trả lời gia 70Re: Thuật toán Apriori khai phá luật kết viết dễ hiểu và minh hoạ rất rõ ràng. Cảm ơn bạn Chucnv rất Mình mới tìm hiểu về lĩnh vực này và chuẩn bị làm tốt thấy hay nhưng cũng khó. Đọc các bài viết của bạn mới có cái nhìn cụ thể bạn mọi điều tốt chủ đề: 09:20 AM trả lời gia 430Re: Thuật toán Apriori khai phá luật kết Anh em nghe anh nói anh sẽ giới thiệu về kỹ thuật phân cụm SOM (Self Map), em đang tìm hiểu về kỹ thuật này và trông chờ bài viết của anh về SOM. Nếu anh có tài liệu nào về SOM Anh cũng anh có thể cho em xin với của em sẽ gởi cho anh qua private Em cảm ơn anh chủ đề: 07:06 AM trả lời gia 20Re: Thuật toán Apriori khai phá luật kết ơn về bài viết rất dễ hiểu của chỉ có 1 thắc mắc nhỏ ở bước thứ 2 và bước thứ 3, nếu bước thứ 2 bạn đã lọc itemset rồi thì bản thân nó chính là Ck, sao cần bước thứ 3 làm gì nữa hả chủ đề: 08:48 PM trả lời gia 20Re: Thuật toán Apriori khai phá luật kết hoặc anh chị nào biết thì vui lòng dẫn giúp em cách tính độ chính xác và độ bao phủ cho ví dụ cơ sở giao dịch bán hàng trong bài Thầy đưa raSau khi áp dụng Apriori cho cơ sở giao dịch bán hàng thì các luật kết hợp sau (với 40%, Beer => =60%, = 75%)R2: Diaper =>Beer = 75%)R3: Milk =>Beer =40%, = Baby Powder => Diaper = chân thành cảm chủ đề: 20Trang 1 trong số 1 (5 nội dung) = || All rights == = = true; return 5, false, ]]> ]]> = new 'return trang chủ của 'return weblog của 'return thư viện ảnh của 'return thông tin của 'return bài viết của ]]> = new ]]> = new ]]>",
          "relevence": "yes"
        },
        {
          "url": "https:ongxuanhong.wordpress.com/2015/08/23/khai-thac-luat-tap-pho-bien-frequent-itemsets-voi-thuat-toan-apriori/",
          "title": "Khai thác tập phổ biến với thuật toán Apriori – Ông Xuân Hồng",
          "content": "Ông Xuân sẻ kiến thức và thông tin về Machine to Data and lý ngôn ngữ tự nhiên – Natural thác tập phổ biến với thuật Tám 23, Mười Một 27, Xuân nghĩ gì về bài viết and toán khai thác tập phổ biến là bài toán trọng trong lĩnh vực data toán khai thác tập phổ biến là bài toán tìm tập các hạng mục S có độ phổ biến thỏa mãn độ phổ biến tối .Dựa trên tính chất của tập phổ biến, ta có pháp tìm kiếm theo chiều rộng toán hay pháp phát triển mẫu toán Trong bài viết này, ta sẽ nói về Apriori cùng với một số ví dụ minh họa khi chạy thuật toán khái niệm cơ minh họa cho các khái niệm, ta lấy ví dụ CSDL với các giao dịch sau.TID (mã giao (tập các hạng B, E2B, D3B, C4A, B, D5A, C6B, C7A, C8A, B, C, E9A, B, CHạng mục (item): mặt = apple, B = bread, C = cereal, D = donuts, E = các hạng mục danh sách các hạng giỏ hàng như .Giao dịch tập các hạng mục mua trong một giỏ hàng, lưu kèm với mã giao dịch phổ biến item): là mẫu xuất hiện xuyên trong tập dữ liệu hiện khá nhiều trong các giao mục ví dụ danh sách sản phẩm như , danh sách cặp sản phẩm đi kèm như , danh sách 3 sản phẩm đi kèm như .Độ phổ biến tính bằng .là tập các hạng mục, D là cơ sở dữ liệu (CSDL) giao phổ biến là tập các hạng mục S thỏa mãn độ phổ biến tối thiểu – do dùng xác định như 40% hiện 5 lần). S là tập phổ phổ biến tối đại (max tồn tại |X’| > |X|, với X’ cũng phổ phổ biến đóng (closed tồn tại |X’| > |X| mà = kết hợp rule): kí hiệu , nghĩa là khi X có mặt thì Y cũng có mặt (với xác suất nào đó). Ví dụ, ; ; .Độ tin cậy tính bằng .Ta có thể biến đổi CSDL về dạng nhị phân để dễ tính (mã giao toán khai thác luật kết độ phổ biến tối thiểu và độ tối thiểu do dùng xác tập các hạng CSDL giao dịch , .Bài toán khai thác luật kết bài toán tìm tất luật dạng  (X, Y là tập con của I và X giao Y = mãn độ phổ biến và độ tin cậy tối .Quy trình khai thác luật kết 1: Tìm tất cả các tập phổ biến dựng luật từ các tập phổ với mỗi tập phổ biến S, tạo ra tất cả các khác rỗng của với mỗi tập con khác rỗng A của S (|A| < luật kết tìm bài toán khai thác luật kết toán khai thác tập phổ biến : độ tính toán toán tìm kiếm theo chiều rộng tắc loại bỏ của Nếu không phải phổ biến thì tập bao nó cũng không tất cả các tập phổ biến 1- hạng mục các tập ứng viên kích mục (k từ các tập phổ biến có mục. Ví dụ, tạo ứng tập phổ biến .Kiểm tra độ phổ biến của các ứng viên và loại các ứng viên không phổ biến ta .Dừng khi không tạo tập phổ biến ứng .Ví dụ thuật toán Apriori với minsupp = giả# buoc 2: loai bo de giam so luong Input:# c: tap ung vien kich thuoc k+1# L_k: tap pho bien kich thuoc k# subset in c:if subset not in False# buoc 3: tao tap ung vien muc# gom 2 buoc join + prune# Input:# L_k: tap pho bien kich thuoc k# tap ung vien kich thuoc k+1def tap ung vien kich thuoc = []for i1 in L_k:for i2 in L_k:# by check = # range_k = {0, 1, ..., k-1}for k in i1[k] == i2[k]: # make sure only 1 i1[k] = = LĐánh giá thuật là thuật toán đơn giản, dễ hiểu và dễ cài đặt. Tuy nhiên, Apriori có các duyệt CSDL nhiều lần. Với , số lần duyệt CSDL sẽ là tập ứng viên rất lớn: .Thực hiện việc tính độ phổ biến tiến Apriori : ý số lần duyệt số tập ứng trình tính độ phổ biến thuận tiện khảo div.u > div.u > width:300, o = Criteo) {var p = () {var o = } });} else 'none', o = Criteo) {var p = () {var o = } });} else 'none', lượt thích Đang thức, Machine bài Công cụ Data cho và với tập dữ lời Hủy trả bình luận của bạn tại bạn điền thông tin vào ô dưới đây hoặc kích vào một biểu để đăng điện tử (bắt buộc) (Địa chỉ của bạn giấu (bắt web Bạn đang bình luận bằng tài khoản Bạn đang bình luận bằng tài khoản Twitter Bạn đang bình luận bằng tài khoản Bạn đang bình luận bằng tài khoản Google+ to %svar = input = 'input' = jQuery( );if ( in input ) jQuery( label' );} Expando Mode: start small, then on first click + text ).hide();comment.css( { } ).one( {var timer = 10 this { } 100 { timer ); n(); } ).slideDown();});}jQuery(document).ready( ); Notify me of new via email. Thông báo cho tôi bằng email khi có bài đăng mới. Tìm bài kiếm Không Thể Sống Một ngày mai tớ trở thành nổi Gặp tớ giữa bạn đừng ngại tớ nghe. Cứ nhìn tớ như ngày xưa bạn thấy. Đừng nên nghĩ tớ quá xa vời. Nếu một ngày tớ là kẻ ăn xin. Thì bố thí cho tớ vài kỷ niệm. Dù rất nhỏ nhưng đừng nên khinh tớ. Vì tớ vẫn là tớ của ngày xưa. Nếu ngày mai tớ chẳng biết về. Hãy chỉ giùm tớ con đi đúng đắn. Đừng để tớ sa chân nơi bùn nhớp. Lúc rút chân lên thì đã bẩn mất rồi. Nếu ngày mai tớ gặp bạn giữa Thì ta sẽ gặp nhau và cười nhé. Đừng nên trách và đừng nên đánh khi có một số lướt qua cuộc đời bạn và ngay tức khắc bạn nhận ra rằng sự có mặt của họ ý nghĩa như thế nào. Họ đã dạy bạn những bài học, đã giúp bạn nhận ra giá trị của chính mình hoặc trở thành con mà bạn từng mơ ước. Có lẽ bạn sẽ không biết những con này từ đâu đến ( bạn cùng phòng, hàng xóm, vị giáo sư, bạn mất liên lạc từ lâu hay thậm chí là một hoàn toàn xa lạ ). Nhưng khi bạn thờ ơ với họ, hãy nhớ rằng trong từng khắc họ sẽ ảnh rất sâu sắc đến cuộc đời bạn. sống không hề làm khó bạn mà chính bạn đã tự làm khó mình bằng cách đặt ra những câu hỏi đại loại như: sao lại thế Tại sao lại thế Không ai có thể trả lời giúp bạn mà chính bạn phải tự đi tìm lời giải đáp cho mình. Hãy luôn nghĩ về khác khi nghĩ đến bản thân, chắc chắn lúc đó bạn sẽ hài lòng với câu trả lời của chính mình. Và đừng quên luôn đặt câu hỏi: đã làm gì?” khi tự hỏi: đã nhận gì?” nhé! Tôi tin là bạn sẽ thành ra trên đời, con luôn cần có nhau để giải quyết rất nhiều vấn đề. Nhờ những lúc qua lại giúp đỡ đó mà hoá giải bao muộn nảy sinh các tình cảm, cùng trải biết bao điều thú vị, hiểu biết thêm nhiều tri thức mới. Đây chính là cuộc thế hãy trân trọng những đang “làm bên cạnh bạn nhé! Đừng để đến khi mất đi rồi mới nghẹn ngào lại bảo “giá như” GS. Thích Bụt dõi Email khi có bài viết bài viết việc với Spark – Truy vấn nâng cao Chín 15, kê ứng dụng 3: Các vấn đề trong Chín 14, kê ứng dụng 2: Suy luận Chín 12, kê ứng dụng 1: Quan sát Chín 7, 2017AI, Machine Deep phân biệt như thế nào Chín 4, – Bài toán rút trích thông tin trong Tám 28, đầu nghiên cứu big data từ đâu và như Tám 3, Science – Mỏ vàng của Kỉ Tám 3, 2017SMA 2017 – Lý ra Sáu 17, 2017SMA 2017 – Lý tập thô (P4) – Rút trích luật Sáu 9, 2017Big Data Chia sẻ Data Science Deep Dự án Data Getting and data Kiến thức Lập trình Machine Python R Spark Toán Weka Xử lý ngôn ngữ tự nhiên - Natural (NLP) This Chín Tám Sáu Năm Ba Một Mười Hai Mười Một Mười Chín Tám Bảy Năm Tư Ba Hai Một Mười Hai Mười Một Mười Chín Tám Bảy Sáu chuyên Data and lý ngôn ngữ tự nhiên – Natural {var = cat {if ( ].value > 0 ) = + = ]]> Tám luận mới xu ly du lieu |… on Tiền xử lý dữ liệu (Horse on Các thuật ngữ trong Xử lý việc với Spark D… on Làm việc với Spark kê ứng dụng 2:… on Thiết kê ứng dụng 2:… on kê ứng dụng 2:… on kê ứng dụng 2:… on A/B testing kê ứng dụng 2:… on Kiểm kê ứng dụng 2:… on Xuân Hồng on Việt Phạm on AI, Machine Deep Khôi on Xuân Hồng on Con học tập Machine on Con học tập Machine Xuân Hồng on Trải tập dữ liệu Big ánAboutTạo một website miễn phí hoặc 1 blog với */var WPGroHo = ]]> and attach to all $ ) {if (typeof === ( typeof !== ) = hash, id ) hash, id = 'body', );});/* */var = lu  u1eadn,connectingToText:Connecting to %2$s,logoutText:  u0110i ra enter a vui email email vui picture will show you leave a Click to tin sau,change:Thay ]]> */Post toHủy = 'ready ).on( {if ( !== typeof ){  If there's another sharing window open, close = 'href' ), 'ready ).on( {if ( !== typeof ){  If there's another sharing window open, close = 'href' ), 'ready ).on( {if ( !== typeof ){  If there's another sharing window open, close = 'href' ), 'ready ).on( {if ( !== typeof ){  If there's another sharing window open, close = 'href' ), corecss = = = ( ) rel, type, href, );} else = = );var = ( ) rel, type, href, );} else = = = '+ expand = = = find brush for: = 'Brush for option: = = scroll $ ) {$( ).on( );} );/* */var = ]]> ** */var = alt='' type=  hidden     />,referer:https:  /  /ongxuanhong.wordpress.com  /2015  /08  /23  /khai-thac-luat-tap-pho-bien-frequent-itemsets-voi-thuat-toan-apriori  /,canFollow:1,feedID:5686260,statusMessage:,customizeLink:https:  /  /ongxuanhong.wordpress.com  /wp-admin  /customize.php?url=https%3A%2F%2Fongxuanhong.wordpress.com%2F2015%2F08%2F23%2Fkhai-thac-luat-tap-pho-bien-frequent-itemsets-voi-thuat-toan-apriori%2F,postID:1955,shortlink:http:  /  /wp.me  /p2Rzzn-vx,canEditPost:,editLink:https:  /  /wordpress.com  /post  /ongxuanhong.wordpress.com  /1955,statsLink:https:  /  /wordpress.com  /stats  /post  /1955  /ongxuanhong.wordpress.com,i18n:{view:View k  u00fd,customize:T  u00f9y this theme: posts from this site will now appear in your <a this theo trang post in me your email 406 other have a <a in ]]> ** */var = a full size <span be sure to submit some text with your provide an email address to provide your name to but there was an error posting your Please try again comment was comment is in for=  email  >Th  u01b0 <input jp-carousel-comment-form-text-field   <input jp-carousel-comment-form-text-field   <input jp-carousel-comment-form-text-field   your here... ]]> ** */var = = ]]> ** */var = ]]> */ {try{if ( in {if {var jl = s = ]]>%d like = || [];_stq = || '1955' ]);if ( === typeof ) = ;if( false !== += &x_ + + '=' + false !== += &x_ + + '=' + += &x_ + + '=' +!= ) {new = + + + + Math.random();}}",
          "relevence": "yes"
        },
        {
          "url": "http:ait.edu.vn/Hoc_thuat/Apriori.html",
          "title": "THUẬT TOÁN APRIORI",
          "content": "THUẬT TOÁN APRIORI Về trang chủI. Giới là khả sinh đề xuất bởi R. Agrawal và R. Srikant vào năm 1993 để các tập item đối với các luật kết hợp kiểu bool. Tên của thuật toán việc thuật toán sử dụng tri thức (prior của các thuộc item phổ biến, chúng ta sẽ thấy sau đây. Apriori dùng cách tiếp cận lặp đến như tìm kiếm tập k item dùng để thăm dòcác tập (k+1) item. Đầu tiên, tập 1 item phổ biến tìm thấy bằng cách quét cơ sở dữ liệu để đếm số item, và thu thập những item thỏa mãn độ hỗ trợ tối Tập kết quả L1. Tiếp theo, dùng để tìm L2, tập các tập 2 item phổ dùng để tìm L3, vàcứ thế tiếp tục, cho tới khi tập k item phổ biến không thể tìm thấy. Việc tìm kiếm cho mỗi Lkđòi hỏi một lần quét toàn bộ cơ sở dữ khi đi vào chi tiết của Apriori đầu tiên chúng ta sẽ tìm hiểu xác định một vài thuật ngữ phổ sử dụng trong thuật Itemset là của những item trong cơ sở dữ liệu mà nó xác định bởi in}, trong đó n là số một thành phần cơ sở dữ liệu mà nó bao gồm tập hợp các item. hiệu là T và T chứa tâp hợp các item support support là điều kiện cần đáp ứng bởi các item đề ra để có thể xử lý item kế tiếp có thể. Minimum support có thể xem như là một giúp loại bỏ các tâp không phổ biến trong bất kỳ cơ sở dữ liệu. Minimum support cho mô hình tỷ lệ phần itemset biến) -  các Itemset đáp ứng các điều kiện minimum support thì gọi là tập phổ biến. Nó ký hiệu làLi trong đó i itemset tập phổ biến) - ứng viên tập phổ biến là các item chỉ xem xét xử lý. tập phổ biến là tất cả các kết hợp có thể có của tập phổ biến. Nó ký hiệu Ci trong đó I chỉ – Độ hữu dụng luật có thể đo với sự giúp đỡ của hỗ trợ. Support giúp chúng tađo như thế nào các giao tác có  tập mà nó phù hợp với ý nghĩa cả hai phía cạnh trong luật kết xét hai item A và b. Để support của A->B theo công thức như – sự chắc chắn của các luật. Thông số này cho phép chúng ta đếm mức độ một giao tác của tập phổ biến phù hợp với ý nghĩa cả phía cạnh bên trái cạnh bên phải. các tập phổ biến không đáp ứng các điều kiện trên có thể xét hai itemA và B. Để tính toán của A->B theo công thức ý: không bằng quả của việc phát sinh của tập item phổ biến, một tính trọng gọi là tính chất Apriori giới thiệu dùng để giảm không gian tìm kiếm. Chúng ta sẽ mô tả tính chất này xem một ví dụ minh họa cách sử dụng : các tập con không rỗng của một tập item phổ biến cũng phải là phổ chất Apriori này dựa theo nhận xét sau. Theo nếu một tập item I không độ hỗ trợ tối I không là phổ biến, do đó, P(I)< Nếu một item A thêm vào tập item I, thì tập item tạo thành (vd, thể xuất hiện xuyên hơn I.Do đó, IA cũng không phổ biến; do đó, P(IA) < chất này thuộc đặc biệt của các thuộc tính gọi là điệu rằng nếu một tập không thể qua kiểm tra, tất cả các tập cha của nó cũng sẽ thất bại với một tra tương tự. Đó gọi là điệu là vì thuộc tính này là đơn điệu trong ngữ cảnh của bại một cuộc kiểm tra. II. Thuật toán tiếp cận lặp biết đến như tìm kiếm với các tập k item dùng để thăm dò các tập 1- phổ biến 1 tìm thấy ký hiệu là theo là tính support có nghĩa là sự xuất hiện của các item trong cơ sở Điều này đòi hỏi phải duyệt qua toàn bộ cơ sở dữ bước cắt tỉa thực hiện trên C1 trong đó những item sosánh với thông số minimum Những item thỏa điều kiện minimum support xem xét cho tiến trình tiếp theo ký hiệu là bước phát sinh các bộ ứng viên thực hiện trong đó tập phổ biến 2 ra ký hiệu là nữa, cở sở dữ liệu duyệt để tính toán support của 2 tập phổ biến. các bộ ứng viên tạo ra kiểm tra và chỉ những tập phổ thỏa điều kiện minimum support thì tiếp tục sử dụng tạo ra bô ứng phổ biến trên tiếp đến khi không có tập phổ biến hoặc bộ ứng viên có thể tạo ra. III. Ví dụ thuật toán 1 biểu giao dịch cơ sở dữ liệu có 4 giao là một duy nhất cho mỗi giao hiện bước là chức năng duyệt cơ sở dữ liệu để xác định số sự xuất hiện cho cụ thể. Sau bước đầu tiên chúng ta sẽ có C1 trong Table tiếp theolà bước cắt tỉa, trong đó support tập phổ biến so sánh với Các tập phổ biến thỏa măn minimum support sẽ xử lý tiếp tục. rằng minimum support là 2. Chúng ta sẽ có L1 từ bước 3 cho thấy kết quả cắt giờ bước phát sinh ứng viên thực hiện trong đó tất cả ứng viên có thể có 2 biến ứng viên tạo. Bảng này ký hiệu là C2. TABLE 4 tất cả khả năng kết hợp mà có thể tạo ra từ TABLE 3 tập phổ giờ cắt thực hiện trên cơ sở các điều kiện minimum Từ TABLE 4 hai biến sẽ loại bỏ. Sau khi cắt tỉa chúng ta nhận kết quả như quá tự tiếp tục cho đến khi không có tập phổ biến hoặc bộ ứng viên có ra. Tiến trình mô tả trong TABLE 6 và TABLE (Kết quả cuối giả thuật { itemset of size kLk itemset of size kL1 (k = 1; Lk!=0; k++) from Lk; t in do the count of all in t = in Ck+1 chế của lớn tập phổ tạo ra  làm gia tăng sự phức nhiều lần duyệt cơsở dữ liệu yêu cầu vì số lớn tập phổ biến số lần duyệt cơ liệu nhiều làm gia tăng sự phức tạp thời gian khi cơ sở dữ liệu gia những hạn một điều cần thiết phải đưa ra một đổi trong thuật toán Apriori mà chúng ta sẽ thấy thêm trong phần Demo Ngày cập nhật điều hành: Windows XP, 7,8 4.0File chạy : chi tiết liên hệ: coding thuật toán Apriori theo yêu cầu, đề tài, ngôn ngữ C#)   ",
          "relevence": "yes"
        },
        {
          "url": "http:congdongjava.com/forum/threads/apriori-algorithm.21358/",
          "title": "Apriori | Cộng đồng Java Việt Nam | Java Việt Nam | Java SE, Java ME, Java EE, Android",
          "content": "Đăng Nhập hoặc Đăng đồng Java Việt Nam | Java Việt Nam | Java SE, Java ME, Java EE, in 'Java started by sẻ code thuật toán size: 3.1 phải đăng ký thành viên hoặc đăng nhập vào diễn đàn nếu muốn trả lời bài viết Ignored sẻ trang tài khoản hoặc địa chỉ tài khoản tại diễn đàn Cộng đồng Java tạo một tài khoản ngay bây mật khẩu của tôi mật khẩu? Giữ đăng cho giới thiệu khóa hệ: cho giới thiệu sản phẩm công hệ: đồng Java Việt Nam | Java Việt Nam | Java SE, Java ME, Java EE, kết trong diễn kết ]]>Thành kết truy cập hiện kiếm Chì tìm tiêu bởi thành tên cách nhau bởi dấu thời Search this thread only Search this forum only Hiển thị kết quả như các chủ countrydivcontainer)countries.setpersist(true)countries.setselectedClassTarget(link) link or hệ với Chúng by XenForo khoản & Quy _gaq = _gaq || {var ga = ga.type = = = == ? : + s = { 0 1,_animationSpeedMultiplier: rgb(255, 255, {thread_view:true,attached_files:true,message:true,bb_code:true,message_user_info:true,share_page:true,login_bar:true},_cookieConfig: { path: /, domain: , prefix: 09614c29});jQuery.extend(XenForo.phrases,{cancel: phút phút nay lúc qua lúc %day% lúc Chủ Thứ Thứ Thứ Thứ Thứ Thứ Xuất hiện lỗi sau The server did not respond in time. Please try Logging Click this image to show the Show hidden content by = = true;",
          "relevence": "yes"
        },
        {
          "url": "http:doc.edu.vn/tai-lieu/tieu-luan-tim-hieu-luat-ket-hop-trong-khai-pha-du-lieu-6760/",
          "title": "Tiểu luận Tìm hiểu luật kết hợp trong khai phá dữ liệu - Tài liệu, ebook, giáo trình",
          "content": "Trang hệ Tài liệu - viện tài liệu, ebook, đồ án, luận văn, giáo trình tham khảo cho học sinh, sinh viên Tiểu luận Tìm hiểu luật kết hợp trong khai phá dữ liệu = || 0pt 0pt New Roman; }MỤC dung MỞ ĐẦU 2NỘI DUNG 3I. TỔNG QUAN VỀ KHAI PHÁ DỮ LIỆU 31. Khái niệm: 32. Quá trình khám phá tri thức trong CSDL 33. Các kỹ thuật khai phá dữ liệu 43.1. Các kỹ thuật tiếp cận trong Data mining 43.2. Dạng dữ liệu có thể khai phá 53.3. Ứng dụng của khai phá dữ liệu 53.4. Khai phá luật kết hợp và ứng dụng 5II. LUẬT KẾT HỢP TRONG KHAI PHÁ DỮ LIỆU 61. Khai phá luật kết hợp 62. Lý về luật kết hợp 72.1. Khái niệm 72.2. Một số tính chất liên quan đến các hạng mục phổ biến: 82.2.1. Tập mục phổ biến: 82.2.2. Luật kết hợp: 92.3. Một số tiếp cận trong khai phá luật kết hợp 92.4. Phát hiện luật kết hợp trên hệ thông tin nhị phân Các định nghĩa về hệ thông tin nhị phân Thuật toán phát hiện tập chỉ mục và luật kết hợp nhị phân 13III. MỘT SỐ THUẬT TOÁN PHÁT HIỆN LUẬT KẾT HỢP 151. Thuật toán Apriori 151.1. Ý thuật toán Apriori 151.2. Thuật toán Apriori 151.3. Sinh các luật kết hợp từ tập mục phổ biến: 182. Thuật toán 202.1. Ý thuật toán 202.2. Thuật toán 212.3. Đánh giá thuật toán 23IV. THỬ KHAI PHÁ LUẬT KẾT HỢP 231. Phát biểu bài toán 232. Phân tích trình 25KẾT LUẬN 27TÀI LIỆU THAM KHẢO: 28   = || trang | Chia sẻ: netpro | Ngày: | Lượt xem: 6166 | Lượt tải: 51 Bạn đang xem nội dung tài liệu Tiểu luận Tìm hiểu luật kết hợp trong khai phá dữ liệu, để tải tài liệu về máy bạn click vào nút ở dịch hỗ trợ X trên tổng các giao dịch trong D, nghĩa là: hỗ trợ tối thiểu minsup là một giá trị cho bởi sử dụng. Nếu tập mục X có sup(X) ³ minsup thì ta nói X là một tập các mục phổ biến. Một tập phổ biến sử dụng như một tập đáng quan tâm trong các thuật toán, lại, những tập không phải tập phổ biến là những tập không đáng quan tâm. Các phần sau sẽ sử dụng những cụm từ khác như “X có độ hỗ trợ tối hay “X không có độ hỗ trợ tối cũng để nói lên rằng X thỏa mãn hay không thỏa mãn ³ khoản mục X gọi là nếu lực của X bằng k, tức là luật kết hợp có dạng R: X => Y, trong đó X, Y là tập các mục, X, Y Í I và X ÇY = Æ. X gọi là tiên đề và Y gọi là hệ quả của X => Y tồn tại một độ tin cậy c . Độ tin cậy c định nghĩa là khả năng giao dịch T hỗ trợ X thì cũng hỗ trợ Y. Ta có công thức tính độ tin cậy c như sau: nhiên, không phải bất cứ luật kết hợp nào có mặt trong tập các luật có thể sinh ra cũng đều có ý nghĩa trên thực tế. Mà các luật đều phải thoả mãn một hỗ trợ và tin cậy cụ thể. Thực vậy, cho một tập các giao dịch D, bài toán phát hiện luật kết hợp là sinh ra tất cả các luật kết hợp mà có độ tin cậy conf lớn hơn độ tin cậy tối thiểu minconf và độ hỗ trợ sup lớn hơn độ hỗ trợ tối thiểu minsup tương ứng do dùng xác Khai phá luật kết hợp phân thành hai bài toán toán 1: Tìm tất cả các tập mục mà có độ hỗ trợ lớn hơn độ hỗ trợ tối thiểu do dùng xác Các tập mục thoả mãn độ hỗ trợ tối thiểu gọi là các tập mục phổ toán 2: Dùng các tập mục phổ biến để sinh ra các luật mong muốn. Ý chung là nếu gọi ABCD và AB là các tập mục phổ biến, thì chúng ta có thể xác định luật nếu AB => CD giữ lại với tỷ lệ độ tin cậy: conf ≥ minconf thì luật giữ lại (luật này sẽ thoả mãn độ hỗ trợ tối thiểu vì ABCD là phổ Một số tính chất liên quan đến các hạng mục phổ Tập mục phổ biến: Tính chất 1 (Độ hỗ trợ của tập A và B là tập các mục, nếu A Í B thì sup(A) ³ sup(B) Điều này là rõ ràng vì tất cả các giao tác của D hỗ trợ B thì cũng hỗ trợ A.Tính chất 2:Một tập chứa một tập không phổ biến thì cũng là tập không phổ một mục trong B không có độ hỗ trợ tối thiểu trên D nghĩa là sup(B)< minsup thì một tập con A của B sẽ không phải là một tập phổ biến vì £ < minsup (theo tính chất 1)Tính chất 3: Các tập con của tập phổ biến cũng là tập phổ mục B là mục phổ biến trên D, nghĩa là ³ minsup thì mọi tập con A của B là tập phổ biến trên D vì ³ > Luật kết chất 1:( Không hợp các luật kết có XZ và YZ trong D thì không nhất thiết XÈYZ là hợp X ÇZ =Æ và các tác vụ trong D hỗ trợ Z nếu và chỉ nếu chúng hỗ trợ mỗi X hoặc Y, khi đó luật XÈYZ có độ hỗ trợ tự : XY Ù XZ Þ chất tách XÈYZ thì XZ và YZ chưa chắc xảy raVí dụ hợp Z có mặt trong một giao tác chỉ khi cả hai X và Y cũng có mặt, tức là sup(Z), nếu độ hỗ trợ của X và Y đủ lớn hơn tức là sup(X) > và sup(Y) > thì hai luật riêng biệt sẽ không đủ độ tin nhiên đảo lại: XYÈZ Þ XY Ù chất 3: (Các luật kết hợp không có tính bắc XY và YZ, chúng ta không thể suy ra dụ: giả sử T(X) Ì T(Y) Ì T(Z), ở đó T(X), T(Y), T(Z) tương ứng là các giao dịch chứa X,Y,Z, và độ tin cậy cực tiểu thế thì: < minconf vì minconf < 1, do đó luật XZ không đủ độ tin chất 4:Nếu A(L - A) không thoả mãn độ tin cậy cực tiểu thì luật B (L -B) cũng không thoả mãn, với các tập mục L,A,B và B Í A Ì LVì supp(B) ³ sup(A) (theo tính chất 1) và định nghĩa độ tin cậy, chúng ta nhận như vậy: Nếu có (L-C) C thì ta cũng có luật (L – D)D, với DÍC và vì DÍC nên (L - D) Ê (L - C), do đó sup(L - D) £ Þ³ tính chất này sẽ sử dụng trong thuật toán mô tả trong các Một số tiếp cận trong khai phá luật kết vực khai thác luật kết hợp cho đến nay đã nghiên cứu và phát triển theo nhiều khác nhau. Có những đề xuất nhằm cải tiến tốc độ thuật toán, có những đề xuất nhằm tìm kiếm luật có ý nghĩa hơn… và có một số chính như kết hợp nhị phân là nghiên cứu đầu tiên của luật kết hợp. Hầu hết các nghiên cứu ở thời kỳ đầu về luật kết hợp đều liên quan đến luật kết hợp nhị phân. Trong dạng luật kết hợp này, các mục, thuộc tính, chỉ quan tâm là có hay không xuất hiện trong giao tác của CSDL chứ không quan tâm về xuất hiện. Ví dụ: Trong hệ thống tính cước điện thoại thì việc gọi 10 cuộc điện thoại và một cuộc xem là giống nhau. Thuật toán tiêu biểu nhất khai phá dạng luật này là thuật toán Apriori và các biến thể của nó. Đây là dạng luật đơn giản và các luật khác cũng có thể về dạng luật này nhờ một số pháp như rời rạc hoá, mờ hoá, … Một ví dụ về dạng luật này: liên tỉnh= AND gọi di => gọi quốc tế= AND gọi dịch vụ 108 = với độ hỗ trợ 20% và độ tin cậy kết hợp có thuộc tính số và thuộc tính hạng mục: Các thuộc tính của các CSDL thực tế có kiểu rất đa dạng, như số nhị phân, giá trị định tính, định Để phát hiện luật kết hợp với các thuộc tính này, các nhà nghiên cứu đã đề xuất một số pháp rời rạc hoá nhằm dạng luật này về dạng nhị phân để có thể áp dụng các thuật toán đã có. Một ví dụ về dạng luật này thức gọi = ‘Tự AND giờ gọi IN AND Thời gian đàm thoại IN 300’] => gọi liên tỉnh = , với độ hỗ trợ là 23. 53% , và độ tin cậy là kết hợp tiếp cận theo tập thô: Tìm kiếm luật kết hợp dựa trên lý tập kết hợp nhiều mức: Cách tiếp cận theo luật này sẽ tìm kiếm thêm những luật có dạng “mua máy tính PC => mua hệ điều hành AND mua phần mềm tiện ích văn phòng, …” thay vì chỉ những luật quá cụ thể như “mua máy tính IBM PC => mua hệ điều hành Windows AND mua phần mềm tiện ích văn phòng Office, …”. Như vậy dạng luật đầu là dạng luật tổng quát hoá của dạng luật sau và tổng quát theo nhiều mức khác kết hợp mờ: Với những hạn chế còn gặp phải trong quá trình rời rạc hoá các thuộc tính số các nhà nghiên cứu đã đề xuất luật kết hợp mờ nhằm khắc phục các hạn chế trên và luật kết hợp về một dạng tự nhiên hơn, gần gũi hơn với sử dụng một ví dụ của dạng này là: bao tư nhân = AND thời gian đàm thoại lớn AND cước nội tỉnh = => cước không hợp lệ = với độ hỗ trợ 4% và độ tin cậy 85%”. Trong luật trên, điều kiện thời gian đàm thoại lớn ở vế trái của luật là một thuộc tính đã mờ kết hợp với thuộc tính đánh trọng số: Trong thực tế, các thuộc tính trong CSDL không phải lúc nào cũng có vai trò như nhau. Có một số thuộc tính chú trọng hơn và có mức độ quan trọng cao hơn các thuộc tính khác. Ví dụ khi khảo sát về doanh thu hàng tháng, thông tin về thời gian đàm vùng cước là quan trọng hơn nhiều so với thông tin về thức Trong quá trình tìm kiếm luật, chúng ta sẽ gán thời gian gọi, vùng cước các trọng số lớn hơn thuộc tính thức gọi. Đây là nghiên cứu rất thú vị và đã một số nhà nghiên cứu đề xuất cách giải quyết bài toán này. Với luật kết hợp có thuộc tính đánh trọng số, chúng ta sẽ khai thác những luật (tức là có độ hỗ trợ thấp, nhưng có ý nghĩa đặc biệt hoặc mang rất nhiều ý kết hợp song song: Bên cạnh khai thác luật kết hợp tuần tự, các nhà làm tin học cũng tập trung vào nghiên cứu các thuật giải song song cho quá trình phát hiện luật kết hợp. Nhu cầu song song hoá và xử lý phân tán là cần thiết bởi kích dữ liệu ngày càng lớn hơn nên đòi hỏi tốc độ xử lý cũng như dung bộ nhớ của hệ thống phải đảm bảo. Có rất nhiều thuật toán song song khác nhau đã đề xuất để có thể không phụ thuộc vào phần cạnh những nghiên cứu về các biến thể của luật kết hợp, các nhà nghiên cứu còn chú trọng đề xuất những thuật toán nhằm tăng tốc quá trình tìm kiếm tập phổ biến từ ra, còn có một số nghiên cứu khác về khai thác luật kết hợp như: khai thác luật kết hợp trực khai thác luật kết hợp kết nối trực tuyến đến các kho dữ liệu đa chiều thông qua công nghệ OLAP, MOLAP, ROLAP, Phát hiện luật kết hợp trên hệ thông tin nhị Các định nghĩa về hệ thông tin nhị thông tin nhị các tập O ={o1, o2, …, on} là một tập hữu hạn gồm n đối = {d1, d2, …, dm} là một tập hữu hạn gồm m chỉ báo,B = {0, 1}Hệ thông tin nhị phân định nghĩa là SB = (O, D, B, c) trong đó c là ánh xạ c:O x D → B, c(o,d) = 1 nếu đối o có chỉ báo d và c(o,d) = 0 nếu ánh xạ thông tin nhị hệ thông tin nhị phân SB = (O, D, B, c). Cho P(O) là các tập con của O, P(D) là các tập con của D. Các ánh xạ thông tin nhị phân rB và lB định nghĩa như sau:rB: P(D)  P(O) với ý nghĩa: cho S Ì D, rB(S) = {o Î O|d Î S, c(o, d) = 1}lB: P(O)  P(D) với ý nhĩa: cho X Ì O, lB(X) = {d Î D|o Î X, c(o, d) = 1}Tập chỉ báo phổ biến nhị hệ thông tin nhị phân SB = (O, D, B, c) và một q Î (0, 1).Cho S Í D, S là tập chỉ báo phổ biến nhị phân với q nếu ≥ LB là một tập gồm tất cả các tập chỉ báo phổ biến nhị phân đã phát hiện từ SB, chúng có thuộc tính như sau: S Î LB, T Ì S thì T Î đó LB,h là tập con của LB nếu XÎLB,h thì (với h là số nguyên luật kết hợp phổ biến nhị phân và hệ số tin hệ thông tin nhị phân SB = (O, D, B, c) và một q Î (0, 1). Cho L là một phần tử của X và Y là hai tập con của L, trong đó:L = X È Y, X ≠ {}, Y ≠ {} và X Ç Y = ta xác định các luật kết hợp nhị phân giữa tập chỉ số X và tập chỉ số Y là một ánh xạ thông tin: X  Y. Hệ số tin cậy của luật này biểu diễn là: RB,b là tập tất cả các luật kết hợp phổ biến nhị phân phát hiện từ SB. Trong đó CFB(r) ≥ b,r Î vectơ chỉ báo nhị phân và các phép hệ thông tin nhị phân SB = (O, D, B, c) trong đó O ={o1, o2, …, on} là một tập hữu hạn gồm n đối D = {d1, d2, …, dm} là một tập hữu hạn gồm m chỉ chỉ báo nhị phân: vB(X) = {X1, X2, … , Xn} trong đó: X Ì D là một vectơ với n thành phần, mỗi thành phần chiếm một giá trị trong B. Cho VSB là tập tất cả các vectơ chỉ báo nhị phân của SB, nếu card(X) = 1 thì X là bộ chỉ báo của SB và Xj = c(o, X)Tích vectơ chỉ báo nhị phân: Cho X1, X2 Ì D, vB(X1) = (X11, X12, … , X1n), vB(X2) = (X21, X22, … , X2n) là các phần tử của VSB. Tích vectơ chỉ báo nhị phân vB(X1) và vB(X2) biểu hiện là vB(X3) = vB(X1) QB vB(X2). Trong = (X31, X32, … , X3n) với X3j = X2j), j = 1¸nX3 = X1 È X2 Î DTừ vectơ vB(X3), biết tất cả các đối hiện có trong tập chỉ báo X1 và X2. Chúng ta dùng vB(X1) để trình diễn rB(X1), vB(X2) để trình diễn rB(X2) và vB(X3) để trình diễn hỗ trợ các vectơ chỉ báo nhị phân Cho X1 Ì D, độ hỗ trợ của vB(X1) biểu diễn định nghĩa là: = {o Ì O| d Î X1, c(o, d) = 1} thấy rằng: = (lực của tập hợp): Cho S = {s1, s2, … , sk} là tập con của D. Trong đó sj là bộ chỉ báo của SB, j = 1 ¸ k. Mỗi sj tương ứng với vectơ chỉ báo nhị phân Các yếu tố của rB(S) tính bằng = QB QB … hiệu VSB, h là tập con của VSB chứa chỉ vectơ vB(X) trong đó X Ì D và card(X) = h (h là số nguyên dương cho Thuật toán phát hiện tập chỉ mục và luật kết hợp nhị toán phát triển từ thuật toán Để phát hiện các tập chỉ báo nhị phân phổ biến từ các luật kết hợp nhị phân từ hệ thông tin nhị phân. Thuật toán này làm việc với các bit trong bộ nhớ và không làm việc với CSDL trên đĩa, vì thế có thể cải tiến tốc độ quá trình phát hiện luật. Cho một CSDL và hai độ hỗ trợ tối thiểu minsup và độ tin cậy tối thiểu minconf của luật kết hợp. Thuật toán có hai pha:Pha 1: Phát hiện các tập chỉ báo phổ biến dựa trên minsup cho 2: Xây dựng các luật kết hợp dựa trên một minconf cho ma trận thông tin nhị phân SB = (O, D, B, c) và một q, b Î(0, 1). Trong đó q là minsup và b là tiết thuật toán như sau:Pha 1: Phát hiện tập chỉ mục phổ biến nhị phân1. TraLoi = Æ ;2. Sinh LB,1 từ SB theo thủ tục 1.a. dưới đây ;3. for (k = 2; LB,k {}; LB,k từ LB,k-1 theo thủ tục 2.a. dưới đây = Èk LB,k-1 ;6. }7. Return TraLoi ;1.a. Sinh LB,1 1. LB,1 = Æ ;2. for (i = 1; i <= m; > q * VSB,1) VSB,1)) ;6. }7. TraLoi = LB,1 ;8. Return TraLoi ;  Trong đó m = card(D) là lực của lập D.2.a. Sinh trên thuộc tính S Î LB, T Ì S thì T Î LB, chúng sinh ra LB,k từ LB,k-1. Kết quả như một ma trận có các dòng và cột là các thành phần của LB, k-11. LB,k = Æ ;2. for (Mỗi X Î LB,k-1 && XY)3. {T = X È Y > && card(T) LB,k) ;6. VSB,k)) ;7. }}9. TraLoi = ; 10. Return TraLoi ;Trong LB,k) là một hàm để ghi một tập chỉ báo phổ biến nhị phân T vào VSB,k)) là một hàm để lưu một vectơ chỉ báo phổ biến nhị phân vB(T) vào VSB,k. Dựa vào (1) và (2), ta có thể tính rất nhanh tại bước thứ k của vòng lặp ở trên, từ các phần tử của 2: Phát hiện các luật phổ biến nhị phân1. RB,b = Æ ;  Khởi tạo tập luật ban đầu là for (Mỗi L Î LB)3. X, Y Î L và XÇY ³ RB,b);  ghi luật X=>Y vào ³ RB,b);  ghi luật Y=>X vào }10. TraLoi = RB,b ;11. Return RB,b ;  Kết MỘT SỐ THUẬT TOÁN PHÁT HIỆN LUẬT KẾT HỢP1. Thuật toán Ý thuật toán là một thuật giải do Rakesh Tomasz Arun Swami đề xuất lần đầu vào năm 1993. Thuật toán tìm giao dịch t có độ hỗ trợ và độ tin cậy thoả mãn lớn hơn một giá trị nào đó. Thuật toán tỉa bớt những tập ứng cử viên có tập con không phổ biến khi tính độ hỗ toán Apriori tính tất cả các tập ứng cử của tập k trong một lần duyệt CSDL. Apriori dựa vào cấu trúc cây băm. Tìm kiếm đi xuống trên cấu trúc cây mỗi khi ta chạm lá, ta tìm một tập ứng cử viên có tiền tố chung bao gồm trong giao dịch. Sau đó các tập ứng cử này tìm trong giao dịch đã ánh xạ đó. Trong hợp tìm thấy biến đếm tăng lên 1.Ký hiệu: Giả sử các mục trong mỗi giao dịch lưu giữ theo trật tự từ Gọi số các mục trong một tập mục là kích của nó và gọi tập mục có kích k là tập k-mục (tập k mục). Các mục trong mỗi tập mục cũng giữ ở trật tự từ Ta sử dụng các ký hiệu sau:Lk: Tập các tập k-mục phổ biến (với độ hỗ trợ cực tiểu minsup nào đó)Ck : Tập các tập k-mục ứng cử (các tập mục phổ biến tiềm Thuật toán CSDL D, Tập các tập mục phổ L1 = {Các 1 - itemset phổ k=2; 3. While( Lk-1! =Æ )4. { Ck = các ứng cử mới theo trình con ở dưới dịch tÎ D)6. ứng cử viên chứa trong t7. for ( ứng cử c Î ++;10. }11. Lk={ c Î Ck÷ c.count ³ k++;13. }14. Return L= ÈkLk' ; sinh ứng cử viên mới minsup )1. { for ( itemset l1Î Lk-1)2. for ( itemset l2Î == L1(k-2) == == c= L1 kết nối L2;5. if( Lk-1)) delete add c to for ( sÎ Ï Lk-1) return return FALSE thích: Lần duyệt đầu tiên, sẽ tính số lần xuất hiện của mỗi mục để xác định các 1- itemset phổ biến. Lần duyệt thứ k (k ³ 2) sẽ bao gồm 2 giai phổ biến Lk-1 đã tìm thấy ở lần duyệt thứ k-1 sử dụng để sinh ra các tập ứng cử viên Ck bằng việc sử dụng hàm vào CSDL, tính độ hỗ trợ của các ứng của viên trong Ck. Các ứng cử viên trong Ck mà chứa trong giao dịch t có thể xác định một cách hiệu quả bằng việc sử dụng cây băm mô tả như giai đoạn 2 (giai đoạn sửa, tỉa): xoá bỏ các tập c Î Ck sao cho một vài (k-1) – tập con của c không nằm trong Lk-1. Thủ tục này là đầy đủ bởi đối với bất kì tập nào Lk với độ hỗ trợ tối thiểu thì các tập con kích cỡ (k-1) cũng có độ hỗ trợ tối do đó nếu ta mở rộng mỗi tập trong Lk-1 với tất cả các tập mục có thể và sau đó xoá tất cả các tập mà (k-1) – tập con của nó không nằm trong Lk-1, ta sẽ nhận tập các tập trong kết nối là tương với việc mở rộng Lk-1 với mỗi mục nằm trong CSDL và sau đó xoá bỏ các tập này mà đối với nó (k-1) nhận bằng việc xoá đi mục thứ (k-1) không nằm trong Lk-1. Ở giai đoạn này Ck Ê Lk . Với lập luận như vậy, giai đoạn tỉa là giai đoạn ta xoá khỏi Ck tất cả các tập mà các (k-1) tập con của nó không nằm trong Lk-1 , cũng không xoá bất kỳ một tập nào có thể nằm trong Lk.Hàm Subset: Các tập ứng cử viên Ck lưu trữ trong một cây băm. Một nút của cây này hoặc là chứa một danh sách của các tập (nút lá) hoặc bảng băm ( một nút trong). Trong mỗi một nút trong, mỗi cụm của bảng băm chỉ đến một nút khác. Gốc của cây băm xem ở độ sâu là 1. Một nút trong ở độ sâu d sẽ dẫn đến nút ở độ sâu d+1. Các tập lưu trữ trong các lá. Khi ta bổ sung thêm một tập c, ta bắt từ nút gốc và đi xuống cây cho đến khi ta chạm vào một lá. Tại một nút ở độ sâu d, ta quyết định sẽ đi theo cành nào bằng việc áp dụng hàm băm đối với mục thứ d của tập đó và theo con trỏ trong Bucket tương ứng. Tất cả các nút ban đầu tạo ra như là nút lá. Khi số các tập trong một nút lá vượt quá chọn, nút lá này thành một nút đầu từ nút gốc, hàm Subset tìm tất cả các ứng cử viên chứa trong giao dịch t như sau: Nếu ta bắt đầu tại một lá, ta tìm những tập trong nút lá này chứa trong giao dịch t và bổ sung các mối quan hệ với chúng đối với tập kết quả mong muốn. Nếu ta đang ở một nút trong và ta đến nó bằng việc băm mục i, ta băm trên mỗi mục đi sau i trong t và áp dụng một cách đệ quy thủ tục đó đối với nút này trong Bucket tương ứng. Đối với nút gốc, ta băm theo mỗi mục trong t.Để thấy tại sao hàm Subset trả lại tập các tham khảo mong muốn hãy để ý đến những gì sẽ xảy ra tại nút gốc. Đối với bất kỳ tập c nào chứa trong giao dịch t, mục đầu tiên cần phải có trong t. Tại nút gốc, việc băm mọi mục trong t đảm bảo rằng ta chỉ không biết các tập mà nó bắt đầu với một mục không nằm trong t. Những lí luận tương tự áp dụng cho các mức sâu hơn. Vì các mục trong bất kì tập nào cũng sắp thứ tự, nếu ta đến một nút hiện tại bằng việc băm mục i, ta chỉ cần quan tâm đến những mục trong t nó xuất hiện sau i. Bước tỉa: Xoá bớt tất cả các tập mục c Î Ck mà (k-1) tập con của c không phụ thuộc Lk-1.1. for ( tập mục c Î Ck)2. for ( (k-1) – tập con s của c)3. if(s Ï Lk-1)4. delete c khỏi xét: Thuật toán Apriori với n là độ dài lớn nhất của tập sinh ra. Vậy thì thuật toán sẽ thực hiện duyệt toàn bộ các giao tác n+1 lần. Như vậy, nếu bỏ qua thời gian so sánh tìm sự xuất hiện của một mẫu trong một giao tác thì độ phức tạp của thuật toán Apriori là O(A) > O(n*L) trong đó L là kích CSDL còn n là độ dài cần đạt của các ra, nếu độ hỗ trợ tối thiểu minsup bị thay đổi thì thuật toán sẽ phải thực hiện lại từ đầu, điều này sẽ rất mất thời gian. Thuật toán Apriori xây dựng nhằm phát hiện các luật kết hợp giữa các đối với độ hỗ trợ và độ tin cậy tối 1.3. Sinh các luật kết hợp từ tập mục phổ khi các tập mục phổ biến từ các tác vụ trong CSDL đã tìm thấy, nó có thể sinh ra các luật kết hợp mạnh, ở đó luật kết hợp mạnh (strong rule) là luật thoả mãn cả hai độ hỗ trợ cực tiểu và độ tin cậy cực tiểu. Điều đó có thể thực hiện bằng việc sử dụng tính độ tin cậy của luật, ta nhắc lại: độ tin cậy của luật X  Y là: conf (X  Y) = P(Y/X) = đó là độ hỗ trợ của XÈY và sup(X) là độ hỗ trợ của X.Có thể coi tỷ số trên là tỷ số giữa: số các tác vụ chứa XÈY và số các tác vụ chứa X. Dựa trên biểu thức tính toán đó, các luật kết hợp có thể sinh như mỗi tập mục phổ biến l, sinh ra tất cả các tập con không rỗng của lVới mỗi tập con không rỗng a của l, ta có luật a  (l-a) nếu ³ minconf ở đó minconf là độ tin cậy",
          "relevence": "yes"
        },
        {
          "url": "https:vi.wiktionary.org/wiki/Th%E1%BA%A3o_lu%E1%BA%ADn:apriori",
          "title": "Thảo – tiếng Việt",
          "content": "Thảo điển mở lái, tìmlà thuật toán phổ biến nhất trong khai phá luật kết hợp đưa ta bởi Agrawal và từ đơn cụ cá đăng tài gian hiển đề tàiXem lịch đổi gần từ ngẫu luận ngànhIn/xuất raTạo một quyển về dưới dạng in liên kết đến đổi liên tập tin trang đặc kết tin ngữ định nghĩa Trang này sửa đổi lần cuối lúc 08:48 ngày 22 tháng 3 năm bản phát hành theo Giấy phép Commons Ghi sẻ tương tự; có thể áp dụng điều khoản bổ sung. Xem Điều khoản Sử dụng để biết thêm chi định quyền riêng thiệu phủ phát bố về bản di -total]},cachereport:{origin:mw1272,timestamp:20170914151957,ttl:1900800,transientcontent:false}}});});(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({wgBackendResponseTime:54,wgHostname:mw1243});});",
          "relevence": "yes"
        },
        {
          "url": "https:nguyentuananhtn.blogspot.com/2012/10/code-association-rule.html",
          "title": "Nguyễn Tuấn Anh - Blog: Code Rule ",
          "content": "function val) = val; }, else = val; {if && {var script = = = head = (head) Tuấn Anh - ký học tập - Nghiên trình trình C#Trang kết Rule 1. Một số địa chỉ tham khảo về luật kết hợp  2. Code tham khảo về luật kết toán Apriori mờ (fuzzy Code khai phá luật kết hợp Mình đọc thấy dễ hiểu, trong vòng có một hai hôm mình đã chỉnh sửa và cài đặt trên C# hoàn thuật toán FP - Growth C# FP Growth Tham khảo thuật toán nhóm đăng Anh pmEmail to to to to:Post = { 'lang': 'en-GB' This With ty cổ phần Cơ điện tử ASOCPP - tức công số quan hệ giữa các class trong UMLHôm nay tôi sẽ trình bày về các loại quan hệ giữa các class trong UML , gồm có các 4 quan hệ chính sau 1 - Làm quen với mô hình MVC 4 ASP.NET Bài đầu tiên trong chuỗi bài Lập trình Web qua các ví dụ (Kết hợp giữa ASP.NET MVC 4 - Entity - Jquery) . Chuỗi bài phá luật kết hợp với Weka Khai phá luật kết hợp với Weka Rule Mining with WEKA ) Văn Chức – vực Data Mining, Rule 1. Một số địa chỉ tham khảo về luật kết tắc đặt tên trong SQL Server Cái tên phản ánh và bản chất của đối hoặc biến mànó dùng để đặt tên sao cho nó có ý nghĩa đúng với Update, Insert, Delete dùng Store trong SQL Server 2008 Bài viết dưới đây, tôi dùng câu Store để biểu diễn các câu lệnh SQL nhằm thực thi các hành động cần thiết cho việc phát triển đề tiền xử lý dữ liệu trong Data in Data Mining) Vấn đề tiền xử lý dữ liệu trong Data Mining (Data in Data Mining) Văn Chức – 1. Giới thiệu ASP.NET MVC Đây dịch từ một số bài blog của tác giả Scott Guthrie về ASP.NET MVC về một công nghệ mới ra đời phục lỗi bị chiếm port 80 cho cài đặt xong thì có nhiều hợp không thể Start Apache. Đó là do có một trình nào đó chiếm mất port 80, mà dịch vụ vài thông tinvề mã số chuẩn quốc tế cho tạp chí và sách, về sự phân loại tạp chí khoa học và cách trình bày một bài báo trong tạp chí khoa học Trần Văn Nhung Để có thêm thông tin cho các ứng viên chức danh giáo sư (GS), phó giáo sư (PGS) và Hội đồng Chức danh giáo sư các cấp Tuấn vị: Khoa Công nghệ thông tin - Đại học CNTT và TT - số truy = {lang: ngọt Thanh Bình - Thái ngọt Thanh Bình - Thái (1)June (3)May (1)July (2)June (3)May (1)July (1)June (3)July (1)May (6)August (4)Theo dõi qua thi chất thi chất ty cổ phần cơ điện tử ty cổ phần cơ điện tử ngọt Thanh Bình - Thái ty cổ phần Cơ điện tử trình và Cuộc tâm luyện thi chất cao Hoa Trạng thi trợ trực thiệu về Anh my = Tuấn Anh Blog. Picture Window theme. Powered by = '');}, = 'blog', 'data': Tuấn Anh - Blog', 'url': 'canonicalHomepageUrl': false, true, true, false, '', 'ltr', false, false, false, '', false, Tuấn Anh - Blog - Tuấn Anh - Blog - Tuấn Anh - Blog - Tuấn Anh - Blog - '', '', false, 'view': '', {'platforms': 'Get link', 'key': 'link', 'Get link', ''}, 'key': 'Share to 'key': 'key': 'Share to 'key': 'Share to 'key': 'Share to 'key': 300,   x3d false, 'Read more', 'item', 'Code Rule ', Tuấn Anh - Blog: Code Rule '}}, 'data': 'data': 'Edit', 'Link copied to 'ok': 'Ok', 'Post 'data': false, false, false, 'view', 'data': 'url': 'url': 'url': 'url': 'url': 'url': 'url': false, 'Code Rule ', '1. Một số địa chỉ tham khảo về luật kết 'url': 'type': 'item', true, false, false, false, true, false, false, false, new null, {}, new null, {}, new null, false, 'href': 'Lập trình false, 'href': 'Lập trình C#'}, false, 'href': 'Trang false, 'href': 'id': 'Giới false, 'href': 'id': 'Liên kết false}, new 'main', null, false, true, new null, {}, new null, {}, new null, {}, new null, {}, new null, {}, new null, 'Tổng số truy cập', true, true, true, new null, {}, new null, true}, new null, 'ltr', new null, {}, new null, true}, new null, true}, new null, {}, new null, {}, new null, {}, new null, {}, new null, {}, 'displayModeFull'));",
          "relevence": "yes"
        },
        {
          "url": "http:joshueuni.edu.vn/index.php/TCKHDHH/article/view/2092",
          "title": "CẢI TIẾN THUẬT TOÁN APRIORI TRÊN MÔ HÌNH | CNTT | Hue Journal of Science (HU JOS)",
          "content": "Journal body, 9, 16, 32, );  the font -->Font Journal Systems Tools Print this article How to cite item Finding this article (Login the author (Login a Comment (Login The Thiên Đình filesMultimediaGovernment search Search IssueBy >Vol 106, No 7 (2015) TIẾN THUẬT TOÁN APRIORI TRÊN MÔ HÌNH CNTT, Trần Thiên Phan Đình tắt: Tìm kiếm c{c tập mục phổ biến l| một bước rất quan trọng khai ph{ luật kết từ đó ph{t sinh luật kết hợp. Một trong những thuật to{n khai ph{ luật kết hợp kinh nổi tiếng l| thuật to{n Tuy nhiên, thuật to{n Apriori có yếu điểm l| với cơ sở dữ liệu lớn thì thời gian tính to{n cũng sẽ rất lớn. Vì vậy, trong b|i b{o n|y đề xuất một cải tiến của thuật to{n Apriori trên mô hình lập trình v| tính to{n song Cải tiến thực hiện thông qua việc x}y dựng thuật to{n Apriori trên qua đó đưa ra giải ph{p tìm tất cả tập mục phổ biến thông qua hai pha:pha thứ nhất tìm c{c tập mục phổ biến pha thứ hai tìm tất cả tập mục phổ (k>=2). Với ph{p n|y việc tính to{n đã cải thiện đ{ng kể thời gian khi ứng dụng thuật to{n Apriori khai ph{ cơ sở dữ liệu lớn. Bước đầu thử thuật to{n Apriori ph{t triển trên mô hình cho kết quả nhanh hơn trên cơsở dữ liệu lớn so với thuật to{n Apriori khóa: J. & Kamber M, Data mining and San CA, Inc, Apache. 12, 2012 & of Apriori IOSR Journal Science e-ISSN: p-ISSN: PP 01-04, D. & Sanjay G, data on large In OSDI, pages 137 – R. and Srikant R, Fast for mining rules in large In Proc, 487 – 499, A.M, Hanbook of Cloud for Cloud FL,USA, 83-136, = a_vars = = 1;var = Open Define Terms context when any {var term;if {term = else if {term = else && && == 'text') {var range = = (term != ){if > -1) + + + + Make sure to only open the reading tools when double within the = if = = = new --> ",
          "relevence": "yes"
        },
        {
          "url": "https:vi.wikipedia.org/wiki/Ti%C3%AAn_nghi%E1%BB%87m",
          "title": "Tiên – tiếng Việt",
          "content": "(function(){var href=  #    u003Etắt  u003C/a  u003E]  u003C/div  u003E  u003Cdiv dir=  ltr    u003E  u003Cdiv tài tài mừng mới dẫn bài định và đổi gần đổi gần href=  /wiki/Wikipedia:C%C3%A2u_th%C6%B0%E1%BB%9Dng_h%E1%BB%8Fi   href=  /wiki/%C4%90%E1%BA%B7c_bi%E1%BB%87t:D%E1%BB%8Bch_n%E1%BB%99i_dung   nội tập tin nhiều xem nhất: text   text   hệ luận Chiến lược Phong trào luận chiến chuẩn bài nổi độ nổi độ trung phong trung và đáng tin nguồn đáng tin gì không phải là spam quảng vi phạm bản nang biên nang biên Tháng thi đua viết bài về khoa học tự nhiên trên tiếng tiếng Giải nhất 1,1 triệu 2 giải nhì 810 nghìn, 3 giải ba 530 nghìn, Nature Science Contest Quỹ tài viết bài khoa học tự nhiên text   khoa toàn thư mở tìm thuật toán khai thác dữ liệu, xem bài thuật toán tiên (chữ Hán: 先驗, tiếng Latin: a priori) có nghĩa kinh Trong nhiều cách sử dụng tại Tây hiện đại, thuật ngữ tiên cho là có nghĩa tri thức mệnh đề - loại tri thức có thể có mà không cần hoặc kinh Nó đối lập với tri thức hậu với nghĩa sau kinh - loại tri thức đòi hỏi kinh học và logic coi là các ngành khoa học tiên Các khẳng như 2 + 2 = 4 chẳng hạn, coi là tiên vì chúng là các tư xuất phát chỉ từ tư duy mà khoa học về tự nhiên và khoa học xã hội coi là các ngành khoa học hậu Các câu như kiểu Trời có màu xanh. có thể coi là tri thức hậu lục1 Tư triết học2 Xem thêm3 Tham khảo4 Liên kết triết | sửa mã trong các câu hỏi cơ bản của nhận thức luận là có hay không tri thức tiên không tầm Nói chung, các nhà duy lý tin rằng có, trong khi các nhà kinh chủ nghĩa tin rằng mọi tri thức đều rút ra từ một dạng kinh nào là từ bên nếu không, nó là tri thức tầm theo một nghĩa nào ngữ này đã đạt một vị trí vững chắc nhờ các nhà tư duy lý, chẳng hạn René và những đã lý luận rằng tri thức thu qua lý tính, mà không phải kinh coi tri thức về bản ngã, hay tôi tư duy, do đó tôi tồn tại, là tiên vì ông cho rằng ta không cần viện dẫn đến kinh trong quá khứ để suy xét về sự tồn tại của chính rằng tư duy là một phần của kinh John Locke, đã đưa ra một cơ sở lý luận mà từ đó toàn bộ khái niệm tiên có thể bị loại Hume coi mọi tri thức tiên là một quan hệ của các ý niệm of Ideas), khi ông nhắc đến thuật ngữ này vài lần trong tác phẩm Enquiry Human của sử dụng từ tiên hiện đại bắt đầu với Kant, đã đưa ra sự phân biệt giữa chân lý tổng hợp và chân lý phân tích để bổ sung cho sự phân biệt giữa tri thức tiên và tri thức hậu Ông lý luận rằng các mệnh đề biết là tiên nhất thiết đúng, trong khi các mệnh đề biết là hậu thì còn tùy vì theo Kant tri thức tiên đã luôn luôn đúng hạn 2 + 2 = 4). Các mệnh đề hậu sẽ phụ thuộc vào các điều kiện ngoại cảnh, các điều kiện này có thể thay đổi theo thời gian và làm cho mệnh đề trở nên sai (ví dụ. Bill Clinton là Tổng thống Mỹ, là mệnh đề đã từng đúng nhưng bây giờ là Kripke, khi phê phán Kant trong Naming and (Đặt tên và sự cần thiết - 1980), lý luận rằng tiên là một tính chất nhận thức luận, và không nên kết hợp với vấn đề tách biệt của siêu hình học về sự cần Để hỗ trợ luận cứ này, ông đưa ra một vài kêu gọi tới trực giác. Đầu tiên, ông lý luận rằng một chân lý hậu có thể nhất thiết đúng. Ví dụ, khi nói rằng Sao Hôm là Sao Mai is Câu đó nhất thiết đúng do cả hai đều là tên của Sao Kim, nhưng lại biết là hậu Ông còn lý luận rằng có thể có các mệnh đề tiên tùy Ví dụ, ở Paris có một đoạn đã từng dùng làm tiêu chuẩn của mét. Mệnh đề đi kèm, Đoạn đó dài 1 mét, là tùy thuộc do ta có thể lấy một độ dài khác để định nghĩa mét. Tuy nhiên, nó biết là tiên vì một mét đã định nghĩa là chiều dài của đoạn đó, nên đoạn phải có độ dài 1m (tại thời điểm nó dùng làm tiêu - đây là một phép lặp thừa tác phẩm The of (Các vấn đề triết học), Russell đã coi tri thức tiên là quan hệ giữa các phạm trù Chẳng hạn 2 + 2 = 4, là một nguyên lý tiên cho thấy quan hệ giữa 2, +, =, và 4, theo chúng đều là các phạm triết gia nổi bật thời về tư duy tiên bao gồm Alfred Ayer, và W.V.O. | sửa mã đề phân đề tổng thức | sửa mã kết | sửa mã từ loại: Nhận thức học triết nghĩa kinh niệm triết đơn cụ cá đăng luận cho địa chỉ IP tài gian hiển mã lịch viết chọn viết ngẫu đổi gần hồi thiệu luận sử liên kết đến đổi liên trang đặc kết tin mục dẫn trang raTạo một quyển về dưới dạng để in raNgôn ngữ / / liên kết Trang này sửa đổi lần cuối lúc 00:20 ngày 14 tháng 7 năm bản phát hành theo Giấy phép Commons có thể áp dụng điều khoản bổ sung. Với việc sử dụng trang web này, bạn chấp nhận Điều khoản Sử dụng và Quy định quyền riêng là hiệu đã đăng ký của Inc., một tổ chức phi lợi định quyền riêng thiệu phủ phát bố về bản di -total]},cachereport:{origin:mw1304,timestamp:20170912055055,ttl:1900800,transientcontent:false}}});});(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({wgBackendResponseTime:76,wgHostname:mw1262});});",
          "relevence": "no"
        },
        {
          "url": "http:tailieu.vn/doc/data-mining-and-application-khai-thac-tap-pho-bien-luat-ket-hop-723932.html",
          "title": "DATA MINING AND KHAI THÁC TẬP PHỔ BIẾN & LUẬT KẾT HỢP",
          "content": "DATA MINING AND KHAI THÁC TẬP PHỔ BIẾN & LUẬT KẾT HỢP Mẫuphổbiến:làmẫu(tậpcáchạngmục,chuỗicon,cấutrúccon,đồthịcon,...)xuất hiện thường xuyên trong tập DL–Agrawal,Imielinski,Swami–1993–trong ngữ cảnh bài toán tập phổ biến và luật kết hợp Mục đích :Tìm cách iện tượng thường ",
          "relevence": "yes"
        },
        {
          "url": "http:proceeding.vap.ac.vn/index.php/proceedingvap/article/view/0139",
          "title": "CÁCH TIẾP CẬN KỸ THUẬT KẾT HỢP LUẬT KHÔNG GIAN VÀ THỜI GIAN ỨNG DỤNG CHO BÀI TOÁN DỰ BÁO TRÊN BỘ DỮ LIỆU LỚN | Thiện | of House for Science and Technology",
          "content": "PUBLISHING HOUSE FOR SCIENCE AND body, 9, 16, 32, );  the font -->Font {var e = = = = = >FAIR - NGHIÊN CỨU CƠ BẢN VÀ ỨNG DỤNG CÔNG NGHỆ THÔNG TIN TIẾP CẬN KỸ THUẬT KẾT HỢP LUẬT KHÔNG GIAN VÀ THỜI GIAN ỨNG DỤNG CHO BÀI TOÁN DỰ BÁO TRÊN BỘ DỮ LIỆU Văn Phạm Văn báo này trình bày tiếp cận cho việc giải quyết vấn đề hiệu năng cho việc khai phá bộ dữ liệu có đặc tính không gian – thời gian, qua đó tìm ra những quy luật kết hợp phổ biến sinh ra từ bộ dữ liệu. Trong các kỹ thuật sinh luật thống dựa trên dữ liệu, khai phá dữ liệu từ các giao dịch thực hiện độc lập nhau. Khi sử dụng thuật toán khai phá thông như Apriori hay thì chi phí tính toán tập các phần tử phổ biến, trong đó việc sinh tập các ứng viên, chi phí thời gian thực hiện lớn do quét cơ sở dữ liệu nhiều lần. Bên cạnh đó, việc sinh luật không gian – thời gian phải dựa trên sự phụ thuộc lẫn nhau giữa các giao dịch, nhằm thể hiện mức độ liên quan của các phần tử trong một không – thời gian nào đó. Chúng tôi sử dụng một cửa sổ giúp các giao dịch độc lập vào trong cùng một giao dịch mới gọi là liên giao dịch. Sau đó tiến hành áp dụng một kỹ khai phá mới mà chúng tôi đề xuất cho việc khai phá. Nhằm thể hiện kết quả thực của toán đề xuất chúng tôi chạy trên bộ dữ liệu lớn về thời tiết, đây là loại dữ liệu mang tính chất không gian và thời gian, từ bộ dữ liệu này chúng tôi tìm ra một cách hiệu quy luật phổ biến ứng dụng cho các lĩnh vực dự báo thời tiết và biến đổi khí hậu, giảm đáng kể chi phí thời so sánh với thuật toán giao dịch, cây phần tử, tập phổ biến, tập phổ biến tối đaFull (c) 2016 of House for Science and HOUSE FOR SCIENCE AND ",
          "relevence": "yes"
        },
        {
          "url": "http:luanvan.co/luan-van/luan-van-khai-pha-du-lieu-va-thuat-toan-khai-pha-luat-ket-hop-song-song-35683/",
          "title": "Luận văn Khai phá dữ liệu và thuật toán khai phá luật kết hợp song song - Luận văn, đồ án, đề tài tốt nghiệp",
          "content": "Đăng hệ Luận văn, đồ án, đề tài, tiểu luận, luận đồng chia sẻ luận văn, đồ án, tiểu luận, đề tài tham khảo cho các bạn học sinh, sinh Luận văn Khai phá dữ liệu và thuật toán khai phá luật kết hợp song song MỞ sự bùng nổ và phát triển của công nghệ thông tin đã mang lại nhiều hiệu quả đối với khoa học cũng như các hoạt động thực tế, trong đó khai phá dữ liệu là một lĩnh vực mang lại hiệu quả thiết thực cho con Khai phá dữ liệu đã giúp sử dụng thu những tri thức hữu ích từ những cơ sở dữ liệu hoặc các kho dữ liệu khổng lồ sở dữ liệu trong các đơn vị, tổ chức kinh doanh, quản lý khoa học chứa đựng nhiều thông tin tiềm ẩn, phong phú và đa dạng, đòi hỏi phải có những pháp nhanh, phù hợp, chính xác, hiệu quả để lấy những thông tin bổ ích. Những “ tri thức ” chiết su ất từ n guồn cơ sở dữ liệu trên sẽ là nguồn thông tin hỗ trợ cho lãnh đạo trong việc lên kế hoạch hoạt động hoặc trong việc ra quyết định sản xuất kinh doanh. T iến hành công việc như vậy chính là thực hiện quá trình phát hiện tri thức trong cơ sở dữ liệu in mà trong đó kỹ thuật khai phá dữ liệu (Data Mining) cho phép phát hiện những tri thức tiềm ẩn. Để lấy thông tin mang tính tri thức trong khối dữ liệu khổng lồ, cần thiết phải phát triển các kỹ thuật có khả năng tích hợp các dữ liệu từ các hệ thống giao dịch khác nhau, chúng thành một tập hợp các cơ sở dữ liệu ổn định có chất Các kỹ thuật như vậy gọi là kỹ thuật tạo kho dữ liệu và môi các dữ liệu nhận khi áp dụng các kỹ thuật tạo kho dữ liệu nói trên gọi là kho dữ liệu (Data [19, trong các nội dung cơ bản nhất trong khai phá dữ liệu và rất phổ biến là phát hiện các luật kế t hợp. pháp này nhằm tìm ra các tập thuộc tính xuất hiện đồng thời trong cơ sở dữ liệu và rút ra các luật về ảnh của một tập thuộc tính dẫn đến sự xuất hiện của một (hoặc một tập) thuộc tính khác như thế nào. Bên cạnh đó, nhu cầu song s ong hóa và xử lý phân tán là rất cần thiết hiện nay bởi kích lưu trữ dữ liệu ngày càng nhiều nên đòi hỏi tốc độ xử lý cũng như dung bộ nhớ hệ thống phải đảm bảo. Vì thế, yêu cầu cần có những thuật toán song song hiệu quả cho việc phát hiện luật kết dụng khai phá dữ liệu đã mang lại những lợi ích to lớn trong việc tổng hợp và cung cấp những thông tin trong các nguồn cơ sở dữ liệu lớn. Hơn nữa hiện nay nhu cầu song song hóa và xử lý phân tán là rất cần thiết bởi kích dữ liệu lưu trữ ngày càng lớn nên đòi hỏi tốc độ xử lý cũng như dung bộ nhớ hệ thống phải đảm bảo Vì thế, yêu cầu cần có những thuật toán song song hiệu quả cho luật kết pháp nghiên cứu của luận văn là tổng hợp các kết quả dự a trên các bài báo khoa ọhc trong một số hội thảo quốc tế và các bài báo chuyên ngành, từ đó trình bày các vấn đề khai phá dữ liệu và xây dựng một số thuật toán khai phá luật kết hợp song dung luận văn trình bày trong 3 và phần kết 1: Tổng quan về k hai phá dữ liệu: Giới thiệu tổng quan về quá trình khai phá dữ liệu, kho dữ liệu và khai phá dữ liệu; kiến trúc của một hệ thống khai phá dữ liệu; Nhiệm vụ chính và các pháp khai phá dữ 2: Khai phá luật kết hợp song song: này trì nh bày tổng quan về luật kết hợp; phát biểu bài toán khai phá dữ liệu, phát hiện luật kết hợp; các khái niệm cơ bản luật kết hợp và các pháp khai phá luật kết hợp; khai phá luật kết hợp với một số khái niệm mở 3: Một số pháp khai phá luật kết hợp song song và phân tích đánh giá các thuật toán song song .MỤC phụ bìa cám ơn Lời cam đoan Mục mục các kí hiệu, các chữ viết mục các hình đầu 1: TỔNG QUAN VỀ KHAI PHÁ DỮ LIỆU 31.1. Khái niệm 31.2. Kiến trúc của một hệ thống khai phá dữ liệu 31.3. Các giai đoạn của quá trình khai phá dữ liệu 41.4. Một số kỹ thuật khai phá dữ liệu 61.5. Các cơ sở dữ liệu phục vụ cho khai phá dữ liệu 101.6. Các pháp chính trong khai phá dữ liệu 111.7. Các ứng dụng của khai phá dữ liệu 131.8. Khai phá dữ liệu và các lĩnh vực liên quan 141.9. Các thách thức trong phát hiện tri thức và khai phá dữ liệu 151.10. Kết luận 1 2: KHAI PHÁ LUẬT KẾT HỢP TRONG CƠ SỞ DỮ LIỆU 172.1. Mở đầu 172.2 Luật kết hợp 182.2.1 Các khái niệm cơ bản Khai phá luật kết hợp Cách tiếp cận khai phá luật kết hợp 222.3 Luật kết hợp cơ sở 242.3.1 Phát hiện các tập mục phổ biến 242.3.2 Sinh luật kết hợp 30 2.4. Khai phá luật kết hợp với một số khái niệm mở rộng Giới thiệu Khai phá luật kết hợp trọng số 322.4.3 Khai phá luật kết hợp tổng quát 432.5. Kết luận 2 3: MỘT SỐ PHÁP KHAI PHÁ LUẬT KẾT HỢP 50SONG SONG VÀ PHÂN TÍCH ĐÁNH GIÁ CÁC THUẬT Nguyên lý thiết kế thuật toán song song 503.2. Hư ớng tiếp cận chính trong thiết kế thuật toán khai phá luật kết hợp song song Mô hình song song dữ liệu Mô hình song song thao tác 513.3. Một số thuật toán khai phá luật kết hợp song song 523.3.1 Thuật toán Count (CD) Thuật toán Data (DD) Thuật toán Thuật toán song song 603.3.5 Thuật toán song song Eclat 653.4. Phân tích, đánh giá và so sánh việc thực hiện thuật toán Phân tích và đánh giá thuật toán song song So sánh việc thực hiện các thuật toán 733.5. Kết luận 3 74Kết luận 75Tài liệu tham khảo 77 105 trang | Chia sẻ: | Ngày: | Lượt xem: 1318 | Lượt tải: 11 Bạn đang xem nội dung tài liệu Luận văn Khai phá dữ liệu và thuật toán khai phá luật kết hợp song song, để tải tài liệu về máy bạn click vào nút ở tập mục ứng không cần độ hỗ trợ cho các tập ứng một b ản ghi t, một tập các mục ứng cử C, tìm tất cả các tập mục trong C hỗ trợ bởi tiên, ta phân các tập mục ứng cử vào các nhóm sao cho các tập mục ứng cử trong mỗi nhóm có cùng thuộc tính phân loại. Mỗi nhóm lập thành một cha ứng cử” (super trong đó, mỗi cha ứng c ử” gồm có 2 Phần thứ nhất là các thuộc tính phân loại chung.- Phần thứ hai là một cấu trúc dữ liệu biểu diễn tập các giá rị của đó nhi ệm vụ đếm độ hỗ trợ cho tập ứng cử thực hiện qua 2 Tìm cha ứng cử” hỗ trợ bởi các thuộc tính phân loại trong bản ghi. Ta sử dụng cấu trúc dữ liệu cây băm để giảm bớt số các cha ứng cử” cần phải kiểm tra đối với một bản ghi.(2) Thuộc tính phân loại của cha ứng cử” hỗ trợ bởi một bản ghi cho nên c ần phải tìm các tập mục ứng cử nào trong cha ứng cử” là hỗ trợ. Lưu ý các tập mục ứng cử trong một “ tập cha ứng cử” có cùng các giá trị đối với thuộc tính phân loại, nhưng có các giá trị khác nhau đối với thuộc tính Kết luận 2Trong 2 trình bày tổng quan về luật kết hợp, các định nghĩa, tính chất liên quan đến luật kết hợp: độ hỗ trợ, độ tin cậy, tập mục phổ biến, phát biểu bài toán khai phá luật kết phá luật kết hợp trong CSDL có thể chia thành hai bài toán con: (1) Tìm tất cả các tập mục phổ biến từ CSDL. (2) Sinh ra các luật từ các tập mục phổ biến. Tro ng n ày tìrnh bày một số thuật toán cơ bản phát hiện tập mục phổ biến, phát hiện luật kết hợp từ các tập mục phổ biến nhằm làm tiền đề cho các nghiên cứu sau này như cải tiến thuật toán, thuật toán song song . Ngoài ra trong này còn trình bày một số tiếp cận trong khai phá luật kết hợp như khai phá luật kết hợp trọng số; khai phá luật kết hợp tổng quát; khai phá luật kết hợp định 3MỘT SỐ PHÁP KHAI PHÁ LU ẬT KẾT SONG VÀ PH ÂN T ÍCH Đ ÁNH GI Á CÁC THUẬT Nguyên lý thiết kế thuật toán song thuật toán, trong đó có một số thao tác có thể thực hiện đồng thời gọi là thuật toán song song. Tổng quát hơn, thuật toán song song là một các tập tiến trình hoặc các tác vụ có thể thực hiện đồng thời và có thể trao đổi dữ liệu với nhau để kết hợp cùng giải một bài toán đặt ra [1].Có năm nguyên lý chính trong việc thiết kế thuật toán song so ng:1. Các nguyên lý lập lịch: Giảm thiểu các bộ xử lý sử dụng trong sao cho thời gian tính toán không tăng (xét theo khía cạnh độ phức lý hình ống: Nguyên lý này áp dụng khi bài toán xuất dãy các thao tác {T1, T2,…, Tn}, trong đó Ti+1 thực hiện sau khi Ti kết Nguyên lý trị: Chia bài toán thành những phần nhỏ hơn, tương đối độc lập với nhau và giải quyết chúng một cách song song.4. Nguyên lý đồ thị phụ thuộc dữ liệu: Phân tích mối quan hệ dữ liệu trong tính toán để xây dựng đồ thị phụ thuộc dữ liệu và dựa vào đó để xây dựng thuật toán song song.5. Nguyên lý điều khiển tranh đua: Nếu hai tiến trình cùng muốn truy cập vào cùng một dữ liệu thì chúng phải tương tranh với nhau, nghĩa là chúng có thể cản trở lẫn ra, khi thi ết kế thuật toán song song cần quan tâm đến các vấn đề sau:- Hiệu quả thực hiện của thuật toán song song có thể rất khác nhau, mà yếu tố quan trọng nhất ảnh tới độ phức tạp tính toán là cấu hình tôpô liên kết mạng của các đơn vị xử lý.- Thu ật toán song song phải thiết kế dựa trên những kiến thức về kiến tính, ngôn ng ữ lập trình song song và các pháp tính Hư ớng tiếp cận chính trong thiết kế thuật toán khai phá luật kết hợp song songHai hư ớng tiếp cận chính trong thi ết kế thuật toán khai phá luật ếkt h ợp song song đó là: (1) Mô h ình song song d ữ liệu và (2) Mô hình song song thao Mô hình song song dữ xử lý 1Thao tác 1Tập con DL 1.1Thao tác nTập con DL 1.nBộ xử lý nHình 3.1. Mô hình song song dữ hình song song dữ liệu th ực thi th ao tác gi ống nhau hay thực thi chỉ lệnh trên một tập con dữ liệu cùng một thời Tất cả các bộ xử lý thực hiện trình giống nhau. Tuy nhiên, trong trình này, ta có thể s ử dụng cấu trúc điều khiển if – then – else để chỉ định lệnh nào thực thi với bộ xử lý nào, tức là một số phần ch ương trình chỉ thực hiện trên một hoặc một vài bộ xử mô hình song song ữd liệu, dữ liệu cần phải phân chia thành các tập con dữ liệu để tăng tốc đạt bằng cách giảm khối dữ liệu cần xử lý trên mỗi bộ xử lý.Thu ật toán thiết kế dựa vào mô hình song song d ữ liệu dễ dàng thực thi,",
          "relevence": "yes"
        },
        {
          "url": "http:www.zbook.vn/ebook/khai-pha-du-lieu-va-thuat-toan-khai-pha-luat-ket-hop-song-song-45030/",
          "title": "Khai phá dữ liệu và thuật toán khai phá luật kết hợp song song - Tài liệu, ebook, giáo trình",
          "content": "Trang hệ Thư Viện Tài liệu, Ebook, Luận Văn, Báo Cáo, Tiểu Luận, Giáo án, Giáo viện tài liệu, ebook, đồ án, luận văn, giáo trình tham khảo miễn phí cho học sinh, sinh phá dữ liệu và thuật toán khai phá luật kết hợp song song Số hóa bởi Trung tâm Học liệu – Đại học Thái HỌC THÁI NGUYÊN KHOA CÔNG NGHỆ THÔNG TIN LÊ THỊ VIỆT HOA KHAI PHÁ DỮ LIỆU VÀ THUẬT TOÁN KHAI PHÁ LUẬT KẾT HỢP SONG SONG Chuyên ngành: KHOA HỌC MÁY TÍNH Mã số : LUẬN VĂN THẠC SĨ CÔNG NGHỆ THÔNG TIN dẫn khoa học: PGS.TS ĐOÀN VĂN BAN THÁI NGUYÊN 2008 Số hóa bởi Trung tâm Học liệu – Đại học Thái CẢM ƠNXin chân thành cảm ơn Thầy giáo PGS.TS Đoàn Văn Ban đã86 trang | Chia sẻ: huyen82 | Ngày: | Lượt xem: 1157 | Lượt tải: 3 Tóm tắt tài liệu Khai phá dữ liệu và thuật toán khai phá luật kết hợp song song, để xem tài liệu hoàn chỉnh bạn click vào nút ở trên tận tình chỉ dạy và dẫn tôi trong suốt thời gian học tập và làm luận cũng xin xin lời biết ơn chân thành đến quý Thầy giáo, cô giáo Viện Công nghệ Thông đã tận tình giảng dạy, trang bị cho tôi những kiến thức quý báu trong suốt quá trình học tập tại cảm ơn tất cả các anh chị em học viên Cao học khóa 5, cám ơn cán bộ công chức, giảng viên – Khoa Công nghệ Thông tin - Đại học Thái Nguyên đã tạo điều kiện giúp đỡ tôi trong suốt quá trình học tập và làm luận cùng xin cảm ơn gia đình, bạn bè, đồng đã giúp đỡ tôi trong suốt thời gian học tập và hoàn thành luận văn này. Thái tháng 9 năm 2008 Tác Thị Việt Hoa Số hóa bởi Trung tâm Học liệu – Đại học Thái CAM xin cam đoan đề tài khoa học “Khai phá dữ liệu và thuật toán khai phá luật kết hợp song song” này là công trình nghiên cứu của bản thân tôi. Các số liệu và kết quả nghiên cứu nêu trong luận văn này là trung thực, các tác giả cho phép sử dụng và các tài liệu tham khảo như đã trình bày trong luận văn. Tôi xin chịu trách nhiệm về luận văn của mình. Số hóa bởi Trung tâm Học liệu – Đại học Thái LỤC Trang phụ bìa Trang Lời cám ơn Lời cam đoan Mục lục Danh mục các kí hiệu, các chữ viết tắt Danh mục các hình vẽ Mở đầu 1 1: TỔNG QUAN VỀ KHAI PHÁ DỮ LIỆU 3 1.1. Khái niệm 3 1.2. Kiến trúc của một hệ thống khai phá dữ liệu 3 1.3. Các giai đoạn của quá trình khai phá dữ liệu 4 1.4. Một số kỹ thuật khai phá dữ liệu 6 1.5. Các cơ sở dữ liệu phục vụ cho khai phá dữ liệu 10 1.6. Các pháp chính trong khai phá dữ liệu 11 1.7. Các ứng dụng của khai phá dữ liệu 13 1.8. Khai phá dữ liệu và các lĩnh vực liên quan 14 1.9. Các thách thức trong phát hiện tri thức và khai phá dữ liệu 15 1.10. Kết luận 1 16 2: KHAI PHÁ LUẬT KẾT HỢP TRONG CƠ SỞ DỮ LIỆU 17 2.1. Mở đầu 17 2.2 Luật kết hợp 18 2.2.1 Các khái niệm cơ bản 18 2.2.2. Khai phá luật kết hợp 21 2.2.3. Cách tiếp cận khai phá luật kết hợp 22 2.3 Luật kết hợp cơ sở 24 2.3.1 Phát hiện các tập mục phổ biến 24 2.3.2 Sinh luật kết hợp 30 Số hóa bởi Trung tâm Học liệu – Đại học Thái Khai phá luật kết hợp với một số khái niệm mở rộng 32 2.4.1. Giới thiệu 32 2.4.2. Khai phá luật kết hợp trọng số 32 2.4.3 Khai phá luật kết hợp tổng quát 43 2.5. Kết luận 2 49 3: MỘT SỐ PHÁP KHAI PHÁ LUẬT KẾT HỢP SONG SONG VÀ PHÂN TÍCH ĐÁNH GIÁ CÁC THUẬT TOÁN 50 3.1. Nguyên lý thiết kế thuật toán song song 50 3.2. Hư ớng tiếp cận chính trong thiết kế thuật toán khai phá luật kết hợp song song 51 3.2.1. Mô hình song song dữ liệu 51 3.2.2. Mô hình song song thao tác 51 3.3. Một số thuật toán khai phá luật kết hợp song song 52 3.3.1 Thuật toán Count (CD) 52 3.3.2. Thuật toán Data (DD) 54 3.3.3. Thuật toán 58 3.3.4. Thuật toán song song 60 3.3.5 Thuật toán song song Eclat 65 3.4. Phân tích, đánh giá và so sánh việc thực hiện thuật toán 71 3.4.1. Phân tích và đánh giá thuật toán song song 71 3.4.2. So sánh việc thực hiện các thuật toán 73 3.5. Kết luận 3 74 Kết luận 75 Tài liệu tham khảo 77 Số hóa bởi Trung tâm Học liệu – Đại học Thái MỤC CÁC KÝ HIỆU VIẾT TẮT Ký hiệu Diễn giải Ck Tập các ứng viên kC Tập các ứng viên mà TID của giao dịch sinh ra liên kết với tập mục ứng viên Conf Độ tin cậy CFPT FP-Tree điều kiện cơ sở (Fisst D Cơ sở dữ liệu giao dịch Di Phần thứ i của cơ sở dữ liệu D Item Mục Itemset Tập mục I Tập các mục KDD Phát hiện tri thức trong cơ sở dữ liệu in CSDL Cơ sở dữ liệu Tập mục gồm k mục Lk Tập các phổ biến MPI thông điệp minconf tin cậy tối thiểu minsup hỗ trợ tối thiểu OLAP Phân tích trực tuyến OLTP Xử lý giao dịch trực tuyến SC Số đếm hỗ trợ count) sup Độ hỗ trợ T Giao dịch Tid Định danh của giao dịch Danh sách các định danh của giao dịch X ⇒Y Luật kết hợp (với X là tiền đề, Y là hệ quả) Số hóa bởi Trung tâm Học liệu – Đại học Thái MỤC HÌNH VẼ VÀ BẢNG Trang Hình 1.1. Khám phá tri thức trong cơ sở dữ liệu điển hình 3 Hình 1.2. Các bước của quy trình khai phá dữ liệu 5 Hình 1.3: Cây quyết định 7 Hình 1.4: Mẫu kết quả của nhiệm vụ phân cụm dữ liệu 8 Hình 1.5: Mẫu kết quả của nhiệm vụ hồi quy 8 Hình 1.6: Một số lĩnh vực liên quan đến khai phá dữ liệu 14 Hình 2.1. Sơ đồ tổng quan của thuật toán khai phá tập mục phổ biến 24 Hình 2.2: Ví dụ thuật toán Apriori 28 Bảng 2.1.a. Thông tin của một cửa hàng bán lẻ 33 Bảng 2.1.b. Tập giao dịch D của cửa hàng 33 Hình 3.1. Mô hình song song dữ liệu 51 Hình 3.2. Mô hình song song thao tác 52 Hình 3.3. Sơ đồ thuật toán Count 52 Hình 3.4. Phát hi ện các tập mục phổ biến bởi thuật toán song song CD 54 Hình 3.5. Sơ đồ mô tả thuật toán Data 55 Hình 3.6: Sơ đồ luồng thuật toán Data 56 Hình 3.7: Phát hi ện các tập mục phổ biến bởi thuật toán song song DD 57 Hình 3.8: Các phân hoạch CSDL và các FP-Tree cục bộ ban đầu 61 Bảng 3.1: Các mẫu điều kiện cơ sở và các FP-Tree điều kiện cơ sở 62 Hình 3.9: Quá trình sinh tập phổ biến bởi 2 bộ xử lý P1 và P2 63 Hình 3.10: Quá trình đổi CSDL theo chiều dọc 70 Số hóa bởi Trung tâm Học liệu – Đại học Thái MỞ ĐẦU Với sự bùng nổ và phát triển của công nghệ thông tin đã mang lại nhiều hiệu quả đối với khoa học cũng như các hoạt động thực tế, trong đó khai phá dữ liệu là một lĩnh vực mang lại hiệu quả thiết thực cho con Khai phá dữ liệu đã giúp sử dụng thu những tri thức hữu ích từ những cơ sở dữ liệu hoặc các kho dữ liệu khổng lồ khác. Cơ sở dữ liệu trong các đơn vị, tổ chức kinh doanh, quản lý khoa học chứa đựng nhiều thông tin tiềm ẩn, phong phú và đa dạng, đòi hỏi phải có những pháp nhanh, phù hợp, chính xác, hiệu quả để lấy những thông tin bổ ích. Những “ tri chiết suất từ nguồn cơ sở dữ liệu trên sẽ là nguồn thông tin hỗ trợ cho lãnh đạo trong việc lên kế hoạch hoạt động hoặc trong việc ra quyết định sản xuất kinh doanh. T iến hành công việc như vậy chính là thực hiện quá trình phát hiện tri thức trong cơ sở dữ liệu in mà trong đó kỹ thuật khai phá dữ liệu (Data Mining) cho phép phát hiện những tri thức tiềm ẩn. Để lấy thông tin mang tính tri thức trong khối dữ liệu khổng lồ, cần thiết phải phát triển các kỹ thuật có khả năng tích hợp các dữ liệu từ các hệ thống giao dịch khác nhau, chúng thành một tập hợp các cơ sở dữ liệu ổn định có chất Các kỹ thuật như vậy gọi là kỹ thuật tạo kho dữ liệu và môi các dữ liệu nhận khi áp dụng các kỹ thuật tạo kho dữ liệu nói trên gọi là kho dữ liệu (Data [19, 24]. Một trong các nội dung cơ bản nhất trong khai phá dữ liệu và rất phổ biến là phát hiện các luật kế t hợp. pháp này nhằm tìm ra các tập thuộc tính xuất hiện đồng thời trong cơ sở dữ liệu và rút ra các luật về ảnh của một tập thuộc tính dẫn đến sự xuất hiện của một (hoặc một tập) thuộc tính khác như thế nào. Bên cạnh đó, nhu cầu song song hóa và xử lý phân tán là rất cần thiết hiện nay bởi kích lưu trữ dữ liệu ngày càng nhiều nên đòi hỏi tốc độ xử lý cũng như dung bộ nhớ hệ thống phải đảm bảo. Vì thế, yêu cầu cần có những thuật toán song song hiệu quả cho việc phát hiện luật kết hợp. Ứng dụng khai phá dữ liệu đã mang lại những lợi ích to lớn trong việc tổng hợp và cung cấp những thông tin trong các nguồn cơ sở dữ liệu lớn. Hơn nữa hiện nay nhu cầu song song hóa và xử lý phân tán là rất cần thiết bởi kích Số hóa bởi Trung tâm Học liệu – Đại học Thái dữ liệu lưu trữ ngày càng lớn nên đòi hỏi tốc độ xử lý cũng như dung bộ nhớ hệ thống phải đảm bảo Vì thế, yêu cầu cần có những thuật toán song song hiệu quả cho luật kết hợp. pháp nghiên cứu của luận văn là tổng hợp các kết quả dự a trên các bài báo khoa học trong một số hội thảo quốc tế và các bài báo chuyên ngành, từ đó trình bày các vấn đề khai phá dữ liệu và xây dựng một số thuật toán khai phá luật kết hợp song song. Nội dung luận văn trình bày trong 3 và phần kết luận 1: Tổng quan về khai phá dữ liệu: Giới thiệu tổng quan về quá trình khai phá dữ liệu, kho dữ liệu và khai phá dữ liệu; kiến trúc của một hệ thống khai phá dữ liệu; Nhiệm vụ chính và các pháp khai phá dữ liệu. 2: Khai phá luật kết hợp song song: này trì nh bày tổng quan về luật kết hợp; phát biểu bài toán khai phá dữ liệu, phát hiện luật kết hợp; các khái niệm cơ bản luật kết hợp và các pháp khai phá luật kết hợp; khai phá luật kết hợp với một số khái niệm mở rộng. 3: Một số pháp khai phá luật kết hợp song song và phân tích đánh giá các thuật toán song song . Thái Nguyên 01 tháng 10 năm 2008 Tác Thị Việt Hoa Số hóa bởi Trung tâm Học liệu – Đại học Thái 1 TỔNG QUAN VỀ KHAI PHÁ DỮ LIỆU 1.1. Khái niệm Khai phá dữ liệu là một khái niệm ra đời vào những năm cuối của thập kỷ 80, nó là quá trình tìm kiếm, khám phá d ưới nhiều góc độ khác nhau nhằm phát hiện các mối liên hệ, quan hệ giữa các dữ liệu, đối bên trong CSDL, kết quả của việc khai phá là xác định các mẫu hay các mô hình tồn tại bên trong nhưng chúng nằm ẩn ở các CSDL [3]. Về bản chất nó là giai đoạn duy nhất rút trích và tìm ra các mẫu, các mô hình hay thông tin mới, tri thức tiềm ẩn có trong CSDL chủ yếu phục vụ cho mô tả và dự đoán. Đây là giai đoạn quan trọng nhất trong quá trình phát hiện tri thức từ CSDL, các tri thức này hỗ trợ trong việc ra quyết điều hành trong khoa học và kinh doanh. Khai phá dữ liệu là tiến trình khám phá tri thức tiềm ẩn trong các CSDL, cụ thể hơn, đó là tiến trình lọc, sản sinh những tri thức hoặc các mẫu tiềm ẩn, chưa biết những thông tin hữu ích từ các CSDL lớn. 1.2. Kiến trúc của một hệ thống khai phá dữ liệu Khai phá d ữ liệu là quá trình rút trích thông tin bổ ích từ những kho dữ liệu lớn. ",
          "relevence": "yes"
        },
        {
          "url": "https:ctfgame.blogspot.com/2015/10/data-mining-code-luat-ket-hop.html",
          "title": "KHOA HỌC MÁY TÍNH: Data mining: Code luật kết hợp",
          "content": "function val) = val; }, else = val; {if && {var script = = = head = (head) HỌC MÁY Năm, 8 tháng 10, mining: Code luật kết Code Rule1. Một số địa chỉ tham khảo về luật kết  2. Code tham khảo về luật kết hợp1 toán Apriori mờ (fuzzy khai phá luật kết đọc thấy dễ hiểu, trong vòng có một hai hôm mình đã đọc hiểu chỉnh sửa và cài đặt trên C# hoàn thuật toán FP - Growth FP Growth đăng email bài đăng sẻ lên sẻ lên sẻ lên có nhận xét nhận đăng Mới Nhận xét = { 'lang': 'vi' thiệu bản hồ sơ hoàn chỉnh của trữ mining: Code luật kết đề Đơn giản. tạo bởi = '');}, = 'blog', 'data': 'KHOA HỌC MÁY TÍNH', 'url': 'canonicalHomepageUrl': false, true, true, false, '', 'vi', 'vi', 'ltr', false, false, false, '', false, HỌC MÁY TÍNH - HỌC MÁY TÍNH - HỌC MÁY TÍNH - HỌC MÁY TÍNH - '', '', false, 'view': '', {'platforms': 'Nhận liên kết', 'key': 'link', 'Nhận liên kết', ''}, 'key': 'Chia sẻ với 'key': 'key': 'Chia sẻ với 'key': 'Chia sẻ với 'key': 'Chia sẻ với 'key': 300,   x3d false, 'Đọc thêm', 'item', 'Data mining: Code luật kết hợp', 'KHOA HỌC MÁY TÍNH: Data mining: Code luật kết 'data': 'data': sửa', 'Đã sao chép liên kết vào khay nhớ 'ok': 'Ok', 'Liên kết bài 'data': 'Đơn false, false, false, 'bold', 'bold', 'Bold', 'view', 'data': 'url': 'url': 'url': 'url': 'url': 'url': 'url': false, 'Data mining: Code luật kết hợp', ' Code Rule 1. Một số địa chỉ tham khảo về luật kết ht...', 'url': 'type': 'item', true, false, false, false, true, false, false, false, new null, {}, new null, {}, new 'main', null, false, true, new null, {}, new null, 'ltr', 'Đang new null, {}, 'displayModeFull'));",
          "relevence": "yes"
        },
        {
          "url": "http:thuvien.hce.edu.vn:8080/dspace/handle/DHKTHue_123456789/3482",
          "title": "Thư viện Đại học Kinh tế Huế: Ứng dụng luật kết hợp trong khai phá dữ liệu quản lý nguồn nhân lực tại Huyện ủy A Lưới",
          "content": "Skip Items starts href=# = Việt on to:My DSpace Receive Edit Profile Thư viện Đại học Kinh tế luận tốt Hệ thống thông tin kinh tếKL Tin học kinh use this to cite or link to this dụng luật kết hợp trong khai phá dữ liệu quản lý nguồn nhân lực tại Huyện ủy A Ths Thanh Trần, Lê Thu dụng luật kết ủy A phá dữ liệu quản lý nguồn nhân Đại học Kinh tế TẮT NGHIÊN tài trình bày tổng quan về công tác quản lý nguồn nhân lực tại Huyện uỷ A Giới thiệu tổng quan về Huyện uỷ huyện A Lưới gồm cơ cấu tổ chức, chức năng nhiệm vụ và sơ lược về tình hình nguồn nhân lực, công tác quản lý nguồn nhân lực tại Huyện uỷ A tài nghiên cứu các nội dung chính đó là khai phá dữ liệu, luật kết hợp trong khai phá dữ liệu, thuật toán phần mềm weka và ứng dụng khai phá luật kết hợp trong weka vào cơ sở dữ liệu nguồn nhân lực tại Huyện uỷ A xây dựng luật kết hợp trong bài toán quản lý nguồn nhân lực. Sẽ đánh giá và phân tích các tập luật sinh ra từ quá trình khai phá dữ liệu luật kết hợp xử lý bởi phần mềm Weka. Sử dụng thuật toán Apriori để sinh tập quả chính mà đề tài đạt là sử dụng luật kết hợp - thuật toán apriori và phần mềm Weka nhằm khai phá nguồn thông tin trong cơ sở dữ liệu của Huyện uỷ A Đem đến những nguồn thông tin bổ ích và từ những thông tin đó sẽ đưa ra những giải pháp giúp giải quyết tốt công tác quản lý nguồn nhân lực của đơn luận K44 Tin học kinh tế, tài liệu gồm dung in Tin học kinh in This Lê Thu MBAdobe   Request a full item in DSpace are by with all rights unless tâm Thông tin - Thư viện Đại học Kinh tế 99 Hồ Đắc Di, Điện ; Fax: Email: Góp ý >",
          "relevence": "yes"
        },
        {
          "url": "http:toc.123doc.org/document/183182-so-sa-nh-thua-t-toa-n-apriori-va-apriori-tid.htm",
          "title": "SO SÁNH TOÁN APRIORI VÀ - Tài liệu text",
          "content": ".dc_breadcrumb{height: Nghệ Thông Tin Cơ sở dữ bản đầy đủSO SÁNH TOÁN APRIORI VÀ   SO SÁNH TOÁN APRIORI VÀ = || = || 2px bản đầy TOÁN toán coi như kết hợp giữa Thuật toán Apriori toán Trong thuật toán sử dụng khi lặp và sang khi đã chắc chắn rằng tập C k đã vào bộ Thuật toán coi là tốt hơn so với Apriori Nhờ có nhận xét tinh tế là thuật toán Apriori chạy khá nhanh đầu tiên, còn thuật toán chạy nhanh ở những bước khá chậm ở những bước đầu tiên), Agrawal đề nghị án lai nhất thiết phải chạy tất cả các bước cùng một thuật toán giống nhau. đầu tiên, ông cho chạy thuật toán sau đó khi tập các ứng cử viên sắp chứa đầy trong bộ nhớ tính toán, mới dùng thuật toán đưa ra thêm một nhận xét: thời gian từ thuật toán Apriori toán tương đối tốn THUẬT TOÁN ta đã biết thuật toán Apriori là một bước đột phá về khai thác các xuyên bằng cách sử dụng kỹ thuật tỉa để rút gọn kích của mục ứng cử. Tuy nhiên, trong hợp số tập mục tập mục dài độ hỗ trợ nhỏ thì thuật toán gặp phải hai chi phí Sinh ra số khổng lồ các tập mục ứng cử. Hơn nữa, để phát hiện ra các xuyên có kích n, thuật toán phải kiểm tra 2n-2 các tập xuyên tiềm Phải duyệt qua cơ sở dữ liệu nhiều lần. Số lần duyệt cơ sở dữ liệu của thuật bằng độ dài của tập mục xuyên dài nhất tìm tập mục xuyên dài và cơ sở dữ liệu lớn thì không thể thực hiện toán Apriori phù hợp với cơ sở dữ liệu thưa, còn với cơ sở dữ liệu dày toán kém hiệu Để khắc phục những chi phí lớn của thuật toán Apriori năm 2000 Jiawei pei và Yiwen Yin đã đưa ra thuật toán mới gọi là để mục xuyên bằng cách không sinh các tập mục ứng cử từ các tập xuyên mà vẫn hiệu quả bằng cách sử dụng ba kỹ thuật nhất, thuật toán sử dụng cấu trúc cây mẫu để nén dữ liệu. Cấu trúc FP_Tree là mở rộng của cấu trúc Những nút trong cây là các mục có độ dài là 1, gán nhãn mục và sắp xếp theo tần suất xuất hiện của các mục để các mục cósố lần xuất hiện nhiều thì sẽ chia sẻ nhiều hai, khai thác phát triển từng đoạn mẫu dựa trên từ mẫu xuyên có kích 1 và chỉ kiểm tra trên cơ sở thuộc pattern base), khởi tạo FP_Tree của mẫu phụ hiện khai thác đệ quy trên cây này. Mẫu kết quả nhận qua nối mẫu hậu tố với mẫu mới sinh ra từ FP_Tree phụ ba, dùng kỹ thuật tìm kiếm phân hoạch không gian tìm chia để trị để chia nhiệm vụ khai thác thành những nhiệm vụ nhỏ hơn hạn lại các mẫu làm giảm không gian tìm BẢN Khai thác tập phổ biến không sử dụng hàm tạo ứng Nén cơ sở dữ liệu thành dạng cấu trúc cây FP Duyệt đệ qui cây FP để tạo tập phổ THUẬT TOÁN XÂY DỰNG CÂY FP– Duyệt CSDL, lấy ra tập các item phổ biến F và tính độ phổ biến Sắp xếp các item trong tập F theo thứ tự giảm dần của độ phổ biến, tập kết quả là L.– Tạo nút gốc cho cây T, và tên của nút gốc sẽ là đó duyệt CSDL lần thứ hai. Ứng với mỗi giao tác trong CSDL thực hiện 2công việc các item phổ biến trong các giao tác và sắp xếp chúng tự giảm dần độ phổ biến trong tập hàm để đưa các item vào trong cây T– Thủ tục thêm các mục xuyên vào cây Tree đó p là mục đầu tiên của dãy và P là phần còn lại của dãyIf cây T có nút con N mà = p Then nút mới p; đổi nút liên kết cho p;If p≠∅ MÔ PHỎNG CÁC BƯỚC CỦA THUẬT ra tập phổ biến L18 f(L bao gồm các item phổ biến theo thứ tự giảm dần của độ phổ 2:Tạo nút gốc cho cơ sở dữ 1 giao tác trong cơ sở dữ item phổ biến trong các giao tác &sắp xếp tập L theo thứ tự giảm dần độ phổ hàm tra tác T có nút con N sao cho: = tăng N.count lên lại tạo một nút mới với = 1, liên kết với nútvà của nó liên kết đến các nút có nhãn giống nó T,Gọi đệ qui nếu P không MINH HỌA cơ sở dữ liệu với các giao tác và các mục xuyên trong mỗi giao sắp xếp giảm dần theo độ hỗ trợ (minsup = 60%) thể hiện trong liệu liên luận khai phá dữ liệu Các thuật toán tìm luật kết hợp xuất phát từ HỌA TOÁN PHỎNG CÁC BƯỚC CỦA THUẬT liệu 3: THIẾT KẾ luận DỤNG CHO VAY ƯU ĐÃI TẠI NGÂN HÀNG CHÍNH SÁCH XÃ HỘI TỈNH THANH Ngân hàng CSXH Thanh VỚI HỘ NGHÈO VÀ CÁC ĐỐI CHÍNH SÁCH TẠI NGÂN HÀNG CHÍNH SÁCH XÃ HỘI TỈNH THANH SỐ VẤN ĐỀ LÝ LUẬN VÀ THỰC TIỄN VỀ HIỆU QUẢ TÍN DỤNG CHO VAY ƯU ĐÃI ĐỐI VỚI HỘ NGHÈO VÀ CÁC ĐỐI CHÍNH đồ 2.2 : Tăng vốn huy động của Ngân hàng Hợp tác xã Việt Nam - Chi nhánh Thanh Hóa giai đoạn 2012 - Hiệu quả kinh doanh của Ngân hàng Hợp tác- Chi nhánh Thanh Giới thiệu về Ngân hàng Hợp tác – Chi nhánh Thanh Kinh nâng cao hiệu quả kinh doanh của một số ngân hàng = || = || s, id) {var js, fjs = = js.id = = 'facebook-jssdk'));",
          "relevence": "yes"
        },
        {
          "url": "http:text.123doc.org/document/2635446-mo-phong-thuat-toan-apriori-tim-tap-pho-bien-luat-ket-hop.htm",
          "title": "MÔ PHỎNG THUẬT TOÁN APRIORI TÌM TẬP PHỔ BIẾN & LUẬT KẾT HỢP - Tài liệu text",
          "content": "Tải bản đầy left;}Công nghệ thông tin Hệ thống thông tin = || PHỎNG THUẬT TOÁN APRIORI TÌM TẬP PHỔ BIẾN & LUẬT KẾT = || = || 2px HỌC QUỐC GIA THÀNH PHỐ HỒ CHÍ ĐẠI HỌC CÔNG NGHỆ THÔNG TINKHOA KHOA HỌC MÁY    LUẬN CHUYÊN PHÁ DỮ LIỆU & KHO DỮ TÀI: MÔ PHỎNG THUẬT TOÁN APRIORI TÌM TẬP PHỔ BIẾN & LUẬT KẾT HỢP GIẢNG VIÊN ĐỖ VIÊN THỰC MINH phố Hồ Chí MinhMô phỏng thuật toán Apriori tìm tập phổ biến và luật kết dung MỤC CÁC HÌNH = || NÓI ĐẦU II>Khai phá dữ liệu: niệm: 62/ Quá trình phát hiện tri thức: 63/Các pháp khai phá dữ liệu: 94/Các tác vụ khai phá dữ liệu: kết hợp: số khái niệm cơ bản: phá luật kết hợp: trạng nghẽn cổ chai của thuật toán cải tiến của thuật toán số biến thể của thuật toán IV>Cài đặt thuật toán tìm luật kết hợp: thiệu trình: số đoạn mã chính để xây dựng trình: dẫn sử dụng trình: V>Kết luận & phát triển đề tài: luận : phát triển đề tài: 41TÀI LIỆU THAM KHẢO 42HVTH: Lê Minh Trí Trang 2Mô phỏng thuật toán Apriori tìm tập phổ biến và luật kết MỤC CÁC 1: Quá trình phát hiện tri thức 8Hình 2: Hỗ trợ ra quyết định 8Hình 3: Sơ đồ quá trình khai phá dữ liệu bằng mạng 15Hình 4: Tri thức đạt từ quá trình khai phá 19Hình 5: Sơ đồ so sánh Apriori và 29Hình 6: So sánh 3 thuật toán và 30Hình 7: Các giai đoạn áp dụng khuôn mẫu của thuật toán Apriori DT 31Hình 8: Giao diện chính của trình 32Hình 9: Mở tập tin 38Hình 10: Thêm mặt hàng 38Hình 11: Thêm giao tác 39Hình 12: Độ hỗ trợ và độ tin cậy 39Hình 13: Kết quả thực hiện 40HVTH: Lê Minh Trí Trang 3Mô phỏng thuật toán Apriori tìm tập phổ biến và luật kết NÓI = || bùng nổ, phát triển của công nghệ thông tin (CNTT) và việc ứng dụng CNTT trong nhiều lĩnh vực của đời sống, kinh tế xã hội trong nhiều năm qua cũng đồng nghĩa với việc thu thập và lưu trữ dữ liệu càng ngày càng tăng. Chúng ta bị tràn ngập trong dữ liệu nhưng lại đói khát tri thức. Việc tổ chức quản lý và sử dụng những dữ liệu đó như thế nào trong tương lai lại là một bài toán nan giải của các doanh nói chung và trong lĩnh vực kinh doanh nói riêng. Trong môi cạnh tranh, ta cần có nhiều thông tin với tốc độ nhanh để trợ giúp việc ra quyết định và ngày càng có nhiều câu hỏi mang tính chất định tính cần phải trả lời dựa trên một khối dữ liệu khổng lồ đã có. Với những lý do đó, các pháp quản trị và khai thác cơ sở dữ liệu thống ngày càng không đáp ứng thực tế đã làm phát triển một khuynh kỹ thuật mới đó là kỹ thuật Phát hiện tri thức và Khai phá dữ liệu (KDD - and Data Khai phá dữ liệu bao gồm rất nhiều những kỹ thuật phân tích dữ liệu bên trong như: luật kết hợp, phân loại dữ liệu, gom nhóm dữ liệu, lập mô hình, dự tiềm ẩn quan trọng nhất vẫn là pháp tìm luật kết hợp để tạo ra các tri thức hữu dụng. Ví dụ như chúng ta có thể dự đoán những sản phẩm nào sẽ mua nhiều trong tương lai đối với hệ thống siêu thị hay dự đoán thị đối với lĩnh vực kinh doanh chứng phạm vi bài tiểu luận này nghiên cứu xin trình bày một cách tổng quát nhất về những khái niệm của Data Mining và đồng thời cài đặt thử trình mô phỏng thuật toán Apriori để tìm tập phổ biến, từ đó suy ra các luật kết hợp tương bài thu em xin gửi lời cảm ơn chân thành, sâu sắc đến thầy PGS.TS Đỗ Phúc, đã tận tình đạt cho em những kiến thức sâu rộng, bổ ích về môn Khai phá dữ liệu và kho dữ liệu. Từ đó giúp em nắm vững hơn về cơ sở lý và có một nền tảng kiến thức cơ bản tạo điều kiện thuận lợi để em hoàn thành tốt bài tiểu luận này. Bên cạnh đó em cũng xin gửi lời cảm ơn đến các trong cùng khóa học đã nhiệt tình chia sẽ tài liệu và những thông tin cần thiết trong suốt quá trình Lê Minh Trí Trang 4Mô phỏng thuật toán Apriori tìm tập phổ biến và luật kết nghiên I> Giới bùng nổ của CNTT đồng nghĩa với việc bùng nổ dữ liệu. Theo thống kê, rất nhiều tập đoàn lớn trên thế giới đã lưu trữ một số khổng lồ về CSDL như: Google (90 triệu tìm trung tâm tính toán khoa học nghiên cứu năng quốc gia Mỹ tháng 3/2010 460 TB, Youtube sau hai năm hàng trăm triệu video, dung CSDL của Youtube tăng gấp đôi sau mỗi chu kỳ 5 tháng, tổng giao vận IP trên mạng năm 2010 là 20.396 ước tính từ năm 2009 đến năm 2014 tăng trung bình hàng năm 34% sách trắng Cissco 2010), 13,5 tỷ trang web đánh chỉ số ngày ta đang ngập trong dữ liệu khoa học, dữ liệu y tế, dữ liệu nhân khẩu học, dữ liệu tài chính, và các dữ liệu tiếp Con không có đủ thời gian để xem xét dữ liệu như vậy. Sự chú ý của con đã trở thành nguồn tài nguyên quý giá. Vì vậy, chúng ta phải tìm cách tự động phân tích dữ liệu, tự động phân loại nó, tự động tóm tắt nó, tự động phát hiện và mô tả các xu trong nó, và tự động chỉ dẫn các dị Đây là một trong những lĩnh vực năng động và thú vị nhất của cộng đồng nghiên cứu cơ sở dữ liệu. Các nhà nghiên cứu trong lĩnh vực bao gồm thống kê, trực quan hóa, trí tuệ nhân tạo, và học máy đang đóng góp cho lĩnh vực này. Bề rộng của lĩnh vực làm cho nó trở nên khó khăn để nắm bắt những tiến bộ phi trong vài thập kỷ gần đây (Theo Jim Gray, chuyên gia của giải Turing tin từ khan hiếm tới dư dật. Điều đó mang lại lợi ích mới to tạo nên khả năng làm nhiều việc mà đây không thể thực hiện như: nhận ra các xu kinh doanh, ngăn ngừa bệnh tật, chống tội phạm … quản lý tốt, dữ liệu như vậy có thể sử dụng để mở khóa các nguồn mới có giá trị kinh tế, cung cấp những hiểu biết mới vào khoa học và tạo ra lợi ích từ quản lý (Theo Kenneth những phân tích trên ta có thể dễ dàng thấy việc khai phá dữ liệu để chọn lọc ra những tri thức hữu ích giữ vai trò ngày càng cao. Hiện nay khai phá dữ liệu ứng dụng vào rất nhiều lĩnh vực cụ thể khác nhau như: y tế, dầu khí, khí kinh doanh, y học, web mining, tài chính và thị chứng khoán, bảo hiểm Để có những tri thức hữu ích ấy thì CIO và các chuyên gia phân tích dữ liệu cũng có tầm ảnh quan trọng không Lê Minh Trí Trang 5Mô phỏng thuật toán Apriori tìm tập phổ biến và luật kết II> Khai phá dữ phá dữ liệu định nghĩa như là một quá trình chắt khai phá tri thức từ một lớn dữ liệu. Một ví dụ hay sử dụng là việc khai thác vàng từ đá và cát, ví như công việc Đãi cá",
          "relevence": "yes"
        },
        {
          "url": "http:timtailieu.vn/tai-lieu/de-tai-nghien-cuu-va-ap-dung-mot-so-ky-thuat-khai-pha-du-lieu-voi-co-so-so-du-lieu-nganh-thue-viet-nam-6231/",
          "title": "Đề tài Nghiên cứu và áp dụng một số kỹ thuật khai phá dữ liệu với cơ sơ sở dữ liệu ngành Thuế Việt Nam - Tài liệu, ebook, giáo trình, dẫn",
          "content": "Đăng hệ - Tài liệu, ebook, giáo trình, đồ án, luận - Thư viện tài liệu, ebook, đồ án, luận văn, tiểu luận, giáo trình, dẫn tự Đề tài Nghiên cứu và áp dụng một số kỹ thuật khai phá dữ liệu với cơ sơ sở dữ liệu ngành Thuế Việt Nam Thời ñại phát triển mạnh của Data cùng với triển nhanh trữ ñã tạo ñiều kiện cho các doanh các thu thập và khối thông tin khổng lồ. Hàng triệu CSDL ñã dùng trong quản doanh, quản lý chính phủ, quản lý khoa học và nhiều ứng dụng khác. Với của các các CSDL này càng lớn lên nhanh chóng. Câu mạnh của các CSDL dẫn ñến thiết phải có các và các công hiện ñổi tự ñộng một cách thông minh thành thông tin và tri thức hữu ích” [10] ñã ñặt vấn nhiều bài viết phá thông tin và tri thức CSDL trang | Chia sẻ: nhungnt | Ngày: | Lượt xem: 1663 | Lượt tải: 15 Bạn đang xem nội dung tài liệu Đề tài Nghiên cứu và áp dụng một số kỹ thuật khai phá dữ liệu với cơ sơ sở dữ liệu ngành Thuế Việt Nam, để tải tài liệu về máy bạn click vào nút ở trên THU TRÀ CÔNG NGHỆ THÔNG TIN GIÁO DỤC VÀ ðÀO TẠO ðẠI HỌC BÁCH KHOA HÀ NỘI LUẬN VĂN THẠC SỸ KHOA HỌC NGÀNH: CÔNG NGHỆ THÔNG TIN NGHIÊN CỨU VÀ ÁP DỤNG MỘT SỐ KỸ THUẬT KHAI PHÁ DỮ LIỆU VỚI CƠ SỞ DỮ LIỆU NGÀNH THUẾ VIỆT NAM THU TRÀ Hà Nội 2006 Hà Nội 20062 MỤC LỤC DANH MỤC CÁC KÝ HIỆU VÀ CÁC CHỮ VIẾT DANH MỤC CÁC BẢNG DANH MỤC CÁC HÌNH MỞ ðẦU 1. KHAI PHÁ DỮ LIỆU 1.1. Tổng quan khai phá dữ 12 1.1.1 Dữ 14 1.1.2 Tiền xử lý dữ liệu 16 1.1.3 Mô hình khai phá dữ liệu 18 1.2. Các chức năng cơ bản khai phá dữ liệu 19 1.2.1 Phân lớp 19 1.2.2 Hồi qui 31 1.2.3 Phân 34 1.2.4 Khai phá luật kết 38 2. MỘT SỐ THUẬT TOÁN KHAI PHÁ DỮ LIỆU 2.1. Thuật toán khai phá luật kết 46 2.1.1 Thuật toán Apriori 46 2.1.2 Thuật toán 49 2.1.3 Thuật toán 51 2.2. Cải tiến hiệu quả thuật toán 54 2.2.2 pháp FP-tree 56 2.2.3 Thuật toán PHP 59 2.2.4 Thuật toán 63 2.2.5 Thuật toán PCY nhiều 65 2.3. Thuật toán phân lớp bằng học cây quyết ñịnh 67 2.3.1 Các ñịnh 68 2.3.2 Thuật toán 69 2.3.3 Các mở rộng của C4.5 70 3. ÁP DỤNG KHAI PHÁ TRÊN CSDL NGÀNH THUẾ ..72 3.1. CSDL ngành Thuế 72 3.2. Lựa chọn công cụ khai phá 73 3.2.1 Lựa chọn công 73 3.2.2 Oracle Data Mining (ODM) 76 3.2.3 78 3.3. Mục tiêu khai thác thông tin của ngành 793 3.4. Thử khai phá luật kết hợp 81 3.5. Phân lớp bằng học cây quyết ñịnh 91 3.5.1 Phân lớp ðTNT dựa vào so sánh tỷ suất các năm 93 3.5.2 Phân lớp ðTNT theo số liệu của một 96 4. KẾT LUẬN NGHIÊN CỨU TIẾP TÀI LIỆU THAM KHẢO PHỤ DANH MỤC CÁC KÝ HIỆU VÀ CÁC CHỮ VIẾT TẮT Ký hiệu, chữ viết tắt Ý nghĩa Rules Các luật kết hợp itemset Một itemset trong tập Ck sử dụng ñể sinh ra các large itemset Ck Tập các ở giai ñoạn thứ k ðộ chắc chắn của luật kết hợp = phản ánh khả năng giao dịch hỗ trợ X thì cũng hỗ trợ Y CSDL Cơ sở dữ liệu DM Data mining – Khai phá dữ liệu DW Data – Kho dữ liệu ðTNT ðối nộp thuế, chỉ tới các cá nhân hoặc tổ chức nộp thuế itemset Một itemset có ñộ hỗ trợ >= ñộ hỗ trợ tối thiểu ID Item Một phần tử của itemset Itemset Tập của các item Một itemset có ñộ dài k Lk Tập các Large itemset ở giai ñoạn thứ k ODM Oracle Data Mining – 1 công cụ khai phá dữ liệu TID Unique Giao dịch5 DANH MỤC CÁC BẢNG Bảng 1.1: CSDL ñơn giản gồm các ví dụ huấn luyện 25 Bảng 1.2 Mô hình CSDL giao dịch ñơn giản 39 Bảng 2.1 Cơ sở dữ liệu giao dịch T 56 Bảng 2.2 Bảng các sản phẩm khai phá dữ liệu 746 DANH MỤC CÁC HÌNH VẼ Hình 1.1 Quá trình khám phá tri thức 14 Hình 1.2 Khuôn dạng ñơn bản ghi và ña bản ghi 16 Hình 1.3: Cây quyết ñịnh ñơn giản với các tests trên các thuộc tính X và Y. 22 Hình 1.4: Sự phân lớp một mẫu mới dựa trên mô hình cây quyết ñịnh 23 Hình 1.5 Cây quyết ñịnh cuối cùng cho CSDL T ñã nêu trong bảng 1.1 ....... 29 Hình 1.6 Cây quyết ñịnh ở dạng giả code cho CSDL T (bảng 29 Hình 1.7 Hồi qui tuyến tính 32 Hình 1.8 Gộp nhóm theo pháp k-means ñánh dấu + là tâm) 36 Hình 1.9 Phân hoạch vun ñống hoặc tách dần 37 Hình 1.10 Bước lặp ñầu tiên của thuật toán Apriori cho CSDL DB 41 Hình 1.11 Lần lặp thứ 2 của thuật toán Apriori cho CSDL DB 42 Hình 1.12 Lần lặp thứ 3 của thuật toán Apriori cho CSDL DB 42 Hình 2.1 Thuật toán 46 Hình 2.2 Thuật toán 50 Hình 2.3 Ví 51 Hình 2.4: Thời gian thực hiện cho mỗi lần duyệt của Apriori và 52 Hình 2.5: Một ví dụ của cây phân cấp khái niệm cho khai phá các nhiều 55 Hình 2.6: FP-tree cho CSDL T trong bảng 2.1 57 Hình 2.7 Thuật toán PHP 62 Hình 2.8 Bộ nhớ với 2 lần duyệt của thuật toán PCY 63 Hình 2.9 Sử dụng bộ nhớ cho các bảng băm nhiều 66 Hình 3.1 Công sức cần cho mỗi giai ñoạn khai phá dữ liệu 82 Hình 3.2 Các bước khai phá luật kết hợp trên CSDL ngành Thuế 83 Hình 3.3 Nhánh cây phân cấp ngành nghề 85 Hình 3.4 Các luật khai phá từ ODM (ñộ dài luật = 2) 877 Hình 3.5 Các luật khai phá từ ODM (ñộ dài luật = 3) 89 Hình 3.6 Cây quyết ñịnh dùng ODM – Bài toán phân tích tỷ 95 Hình 3.7 Cây quyết ñịnh dùng See5 – Bài toán phân tích tỷ suất 96 Hình 3.8 Cây quyết ñịnh dùng ODM – Bài toán xét số liệu một 99 Hình 3.9 Cây quyết ñịnh dùng See5 – Bài toán phân tích trong 1008 MỞ ðẦU Thời ñại phát triển mạnh của Data cùng với sự phát triển nhanh về công nghệ lưu trữ ñã tạo ñiều kiện cho các doanh các tổ chức thu thập và sở hữu khối thông tin khổng lồ. Hàng triệu CSDL ñã dùng trong quản trị kinh doanh, quản lý chính phủ, quản lý dữ liệu khoa học và nhiều ứng dụng khác. Với khả năng hỗ trợ mạnh của các Hệ quản trị CSDL, các CSDL này càng lớn lên nhanh chóng. Câu “Sự lớn mạnh của các CSDL dẫn ñến sự cần thiết phải có các kỹ thuật và các công cụ mới ñể thực hiện ñổi tự ñộng dữ liệu một cách thông minh thành thông tin và tri thức hữu ích” [10] ñã trở thành ñặt vấn ñề của nhiều bài viết về khai phá thông tin và tri thức từ các CSDL lớn. Công tác trong ngành Thuế, nơi Công nghệ thông tin áp dụng vào quản lý Thuế từ những năm 1986, CSDL thông tin liên quan ñến các lĩnh vực quản lý Thuế là một CSDL lớn và chắc chắn tiềm ẩn nhiều thông tin quý báu. Với mong muốn bước ñầu áp dụng kỹ thuật khai phá dữ liệu trên CSDL ngành Thuế, luận văn ñã tập trung nghiên cứu về các kỹ thuật khai phá dữ liệu và tiến hành khai phá thử trên CSDL ngành Thuế. Khả năng mở rộng tri thức có ích ẩn trong dữ liệu ñể ñưa ra những hành ñộng cần thiết dựa trên tri thức ñó ñang trở nên ngày càng quan trọng trong thế giới cạnh tranh hiện nay. Toàn bộ quá trình dùng các pháp luận dựa trên tính toán, bao gồm các kỹ thuật mới ñể phát hiện ra tri thức từ dữ liệu gọi là khai phá dữ liệu (data [9] Khai phá dữ liệu là sự tìm kiếm thông tin mới, có giá trị và không tầm trong một khối dữ liệu lớn. Nó là sự phối hợp nỗ lực của con và máy tính. Các kết quả tốt nhất nhận bằng việc cân bằng giữa9 tri thức của các chuyên gia con trong việc mô tả các vấn ñề và mục ñích với khả năng tìm kiếm của máy tính. Hai mục ñích chính của khai phá dữ liệu là ñể dự ñoán và mô tả Dự ñoán bao gồm việc dùng một vài biến hoặc trong tập dữ liệu ñể dự ñoán các giá trị tương lai hoặc chưa biết của các biến cần quan tâm. Còn mô tả tập trung vào việc tìm ra các mẫu mô tả dữ liệu mà con có thể hiểu biên dịch Có thể ñưa các hoạt ñộng khai phá dữ liệu vào một trong hai loại sau:  Khai phá dữ liệu dự báo, tạo ra mô hình của hệ thống mô tả bởi tập dữ liệu cho hoặc  Khai phá dữ liệu mô tả, với việc tạo ra thông tin mới, không tầm dựa trên tập dữ liệu có sẵn. Một số chức năng khai phá dữ liệu chính như:  Mô tả khái niệm: Mô tả ñặc ñiểm và phân biệt. Tìm ra các ñặc ñiểm khái quát hoá, tổng kết, các ñặc ñiểm khác nhau trong dữ liệu.  Kết hợp: xem xét về tương quan và quan hệ nhân quả.  Phân lớp và dự báo and Xác ñịnh mô hình mô tả các lớp riêng biệt và dùng cho dự ñoán tương lai.  Phân tích nhóm Chưa biết nhãn lớp, thực hiện nhóm dữ liệu thành các lớp mới dựa trên nguyên tắc cực ñại hoá sự tương tự trong cùng lớp và cực tiểu hoá sự khác tương tự giữa các lớp khác nhau.  Phân tích nhiễu Hữu ích trong việc phát hiện lỗi, phân tích các sự kiện hiếm.  Phân tích xu và sự phát triển Khai phá dữ liệu là một trong những lĩnh vực phát triển nhanh nhất trong công máy tính. Từ chỗ là một miền quan tâm nhỏ trong khoa học10 máy tính và thống kê, nó ñã nhanh chóng mở rộng thành một lĩnh của riêng nó. Một trong những lớn mạnh nhất của khai phá dữ liệu là sự ảnh trong phạm vi rộng của các pháp luận và các kỹ thuật ứng dụng ñối với một loạt các bài toán, các lĩnh vực. Trong kinh doanh, khai phá dữ liệu có thể dùng ñể khám phá ra những xu mua sắm mới, kế hoạch cho các chiến lược ñầu tư, và phát hiện những sự tiêu dùng không chính ñáng từ hệ thống kế toán. Nó có thể giúp cải tiến các chiến dịch ñể mang lại nhiều hỗ trợ và quan tâm hơn tới khách hàng. Các kỹ thuật khai phá dữ liệu có thể áp dụng ñối với các bài toán thiết kế lại quy trình kinh doanh, trong ñó mục ñích là ñể hiểu các tương tác và quan hệ trong thông lệ kinh doanh và các tổ chức kinh doanh. Nhiều ñơn vị thi hành luật, các ñơn vị ñiều tra ñặc biệt, có nhiệm vụ tìm ra các hành ñộng không trung thực và phát hiện ra các xu phạm tội, cũng ñã sử dụng khai phá dữ liệu một cách thành công. Các kỹ thuật khai phá dữ liệu cũng có thể dùng trong các tổ chức tình báo nơi lưu giữ nhiều nguồn dữ liệu lớn liên quan ñến các hoạt các vấn ñề về an ninh quốc gia. Với mục ñích nghiên cứu một số pháp khai phá dữ liệu và thử khai phá trên CSDL ngành Thuế, luận văn trình bày với các phần sau: 1 – Khai phá dữ liệu: Tìm hiểu các chức năng khai phá dữ liệu. 2 – Một số thuật toán khai phá dữ liệu. Nghiên cứu trên hai kiểu khai phá: Khai phá luật kết hợp - một kỹ thuật thông dụng trong học không giám sát. Phân lớp bằng học cây quyết ñịnh - kỹ thuật học có giám sát. 3 – Áp dụng khai phá trên CSDL ngành Thuế: Thử khai phá luật kết hợp và phân lớp trên CSDL ngành 4 – Kết luận và những kết quả ñạt Cuối cùng là một số nghiên cứu tiếp theo. Em xin chân thành cảm ơn PGS. TS Ngọc Bình ñã dẫn và cho em những ý kiến quý báu, chân thành cảm ơn các thầy cô giáo của ðại học Bách khoa Hà Nội ñã trang bị kiến thức giúp em hoàn thành luận văn này.12 1. KHAI PHÁ DỮ LIỆU 1.1. Tổng quan khai phá dữ liệu Khai phá dữ liệu có nguồn gốc từ các pháp riêng biệt, 2 dạng quan trọng nhất là thống kê và học máy. Thống kê có nguồn gốc từ toán học và do ñó nhấn mạnh ñến ñộ chính xác toán học, mong muốn thiết lập cái mà có thể nhận ra trên nền toán học khi kiểm thử nó trong thực tế. lại, học máy có nguồn gốc rất nhiều trong thực tiễn tính toán. ðiều này dẫn ñến sự thực tiễn, sẵn sàng kiểm thử ñể biết nó thực hiện tốt thế nào mà không cần chờ một chứng minh chính thức. [9] Có thể có ñịnh nghĩa về Khai phá dữ liệu như sau: Khai phá dữ liệu là quá trình phát hiện các mô hình, các tổng kết khác nhau và các giá trị lấy từ tập dữ liệu cho [9] Hay, Khai phá dữ liệu là sự thăm dò và phân tích dữ liệu lớn ñể khám phá từ dữ liệu ra các mẫu hợp lệ, mới lạ, có ích và có thể hiểu [14]. Hợp lệ là các mẫu ñảm bảo tính tổng quát, mới lạ là mẫu chưa biết ñó, có ích là có thể dựa vào mẫu ñó ñưa ra các hành ñộng phù hợp, hiểu là có thể biên dịch và hiểu thấu ñáo các mẫu. Các kỹ năng phân tích của con là không ñầy ñủ do: Kích và chiều của dữ liệu; tốc ñộ tăng của dữ liệu là rất lớn. Thêm vào ñó là những ñáp ứng mạnh mẽ của kỹ thuật về khả năng: thu thập dữ liệu, lưu trữ, năng lực tính toán, phần mềm, sự thành thạo về chuyên môn. Ngoài ra còn có môi cạnh tranh về dịch vụ, chứ không chỉ cạnh tranh về giá (ñối với Ngân hàng, công ty ñiện khách sạn, công ty cho thuê …) với câu “Bí quyết của sự thành công là biết những gì mà không ai khác Onassis [14]). Tất cả những ñiều ñó chính là những nguyên nhân thúc ñẩy Khai phá dữ liệu phát Quá trình khám phá tri thức: tiên, phân biệt giữa các thuật ngữ “mô hình và dùng trong khai phá dữ liệu. Mô hình là một cấu trúc “quy mô có thể là tổng kết các quan hệ qua nhiều hợp (case) (ñôi khi là tất cả các hợp), trong khi mẫu là một cấu trúc cục bộ, thoả mãn bởi một số ít hợp hoặc trong một miền nhỏ của không gian dữ liệu. Trong khai phá dữ liệu, một mẫu ñơn giản là một mô hình cục bộ. Quá trình khám phá tri thức tiến hành theo các bước sau: 1. Xác ñịnh bài toán vụ: tiên phải tìm hiểu lĩnh vực của ứng dụng vụ; Tìm hiểu các tri thức liên quan và các mục ñích của ứng dụng. 2. Khai phá dữ liệu - Lựa chọn dữ liệu: Xác ñịnh các tập dữ liệu ñích và các liên quan - Làm sạch dữ liệu: Xoá bỏ tiền xử lý. Phần việc này có thể chiếm tới 60% công sức. - Giảm bớt dữ liệu và ñổi dữ liệu: Tìm ra những ñặc trưng hữu dụng, giảm bớt các chiều hoặc các biến, biểu diễn lại các ñại bất biến - Lựa chọn chức năng khai phá dữ liệu: Tổng kết, phân lớp, Hồi qui, kết hợp, phân nhóm. - Lựa chọn thuật toán khai phá. - Thực hiện khai phá dữ liệu (Data Tìm kiếm các mẫu quan tâm - ðánh giá các mẫu và biểu diễn tri Hình 1.1 Quá trình khám phá tri thức 3. Áp dụng khám phá tri thức 4. ðánh giá và ño ñạc 5. Triển khai và tích hợp vào các qui trình vụ 1.1.1 Dữ liệu Do có nhiều kiểu dữ liệu, các CSDL sử dụng trong các ứng dụng cũng khác nhau, nên dùng luôn mong ñợi một hệ thống khai phá dữ liệu có thể ñiều khiển tất cả các loại dữ liệu. Thực tế CSDL có sẵn là CSDL quan hệ và hệ thống khai phá dữ liệu cũng thực hiện hiệu quả việc khai phá tri thức trên dữ liệu quan hệ. Với những CSDL của ứng dụng chứa các kiểu dữ liệu phức tạp, như dữ liệu và dữ liệu tạm và không gian dữ liệu kế thừa phải có các hệ thống khai phá dữ liệu riêng biệt xây dựng ñể khai phá cho các kiểu dữ liệu cụ Dữ liệu khai phá có thể là dữ liệu có cấu trúc, hoặc không có cấu trúc. Mỗi bản ghi dữ liệu coi như một hợp hoặc một ví dụ Phân biệt hai kiểu thuộc tính: phân loại và số Các thuộc tính kiểu phân loại là những thuộc tính có các giá trị thuộc vào một số nhỏ các phân loại hoặc các lớp riêng rẽ và giữa chúng không có thứ tự ẩn nào. Nếu chỉ có 2 giá trị, ví dụ là yes và no, hoặc male và female, thuộc tính coi là binary. Nếu có hơn 2 giá trị, ví dụ, nhỏ, vừa, lớn, rất lớn, thuộc tính coi là ña lớp Các thuộc tính số là những thuộc tính lấy các giá trị liên tục, ví dụ, thu nhập hàng năm, hoặc tuổi. Thu nhập hàng năm hoặc tuổi có thể về lý là bất kỳ một giá trị nào từ 0 tới vô hạn, mặc dù mỗi giá trị xuất hiện phù hợp với thực tế. Các thuộc tính số có thể biến ñổi thành Ví dụ, thu nhập hàng năm có thể chia thành các loại: thấp, trung bình, cao. Dữ liệu không có cấu trúc có thể áp dụng các thuật toán khai phá dữ liệu là dữ liệu kiểu Text. Khuôn dạng bảng của dữ liệu có thể thuộc hai loại:  Dữ liệu dạng ñơn bản ghi (còn gọi là kiểu không giao ñây là các bảng dữ liệu quan hệ thông  Dữ liệu dạng ña bản ghi (còn gọi là kiểu giao dùng cho dữ liệu với nhiều thuộc tính. Ở dạng ñơn bản ghi (kiểu không giao mỗi bản ghi lưu trữ như 1 dòng trong bảng. Dữ liệu ñơn bản ghi không ñòi hỏi cung cấp khoá ñể xác ñịnh duy nhất mỗi bản ghi. Nhưng, khoá là cần cho các hợp kết hợp ñể có kết quả cho học có giám sát.16 Trong dạng ña bản ghi (kiểu giao mỗi hợp (case) lưu trong nhiều bản ghi trong một bảng với các cột: dãy số ñịnh danh, tên thuộc tính, giá trị. Hình 1.2 Khuôn dạng ñơn bản ghi và ña bản ghi 1.1.2 Tiền xử lý dữ liệu Dữ liệu chọn lọc sẽ phải qua bước tiền xử lý khi tiến hành khai phá phát hiện tri thức. Bước thu thập và tiền xử lý dữ liệu là bước rất phức tạp. ðể một giải thuật DM thực hiện trên toàn bộ CSDL sẽ rất cồng kềnh, kém hiệu quả. Trong quá trình khai phá dữ liệu, nhiều khi phải thực hiện liên hợp dữ liệu từ rất nhiều nguồn khác nhau. Các hệ thống sẵn có thiết kế với những mục Tài liệu liên quan Đề tài Xây dựng hệ thống thông tin tổ chức, quản lý các giải thi qua mạng trang | Lượt xem: 948 | Lượt tải: 1Bài giảng Phân Tích Log10 trang | Lượt xem: 1009 | Lượt tải: 2Đề tài Hệ thống giao dịch chứng trang | Lượt xem: 941 | Lượt tải: 0Bài tập thực hành - Cơ sở dữ liệu quản lý sinh viên6 trang | Lượt xem: 4117 | Lượt tải: 9Đồ án Phân tích thiết kế hệ thống trình thi trắc qua trang | Lượt xem: 781 | Lượt tải: 2Đề tài Một số pháp khai phá dữ liệu quan hệ trong tài chính và chứng trang | Lượt xem: 1159 | Lượt tải: 6Đề tài Xác thực trong các mạng vô trang | Lượt xem: 1110 | Lượt tải: 12Đề tài Xây dựng mô hình ngôn ngữ cho tiếng trang | Lượt xem: 1414 | Lượt tải: 8Đề tài Phân tích và thiết kế hệ thống quản lý một chi nhánh của ngân hàng tín trang | Lượt xem: 1074 | Lượt tải: 3Đồ án Xây dựng hệ thống quản lý kho hàng ở trang | Lượt xem: 1652 | Lượt tải: 13 © 2016 đang trong thời gian thử chờ xin giấy phép của Bộ TT & TT.Chia sẻ: Thư viện Luận Văn, Tài Liệu và Đồ Án tốt viện Ebook miễn phí, Thư viện giáo án, PDFvar _gaq = _gaq || {var ga = ga.type = = = == ? : + s = ",
          "relevence": "yes"
        },
        {
          "url": "https:fullcode.vn/phuong-phap-luan-ket-hop-va-ung-dung.t615906.html",
          "title": "Thạc Sỹ - pháp luận kết hợp và ứng dụng | Community",
          "content": "Đăng liệu - Luận Học - tham gia Diễn đàn, hãy đăng ký để trở thành thành viên chính thức. Tìm hiểu Sỹ pháp luận kết hợp và ứng luận trong 'Tin Học - CNTT' bắt đầu bởi tài: PHÁP LUẬN KẾT HỢP VÀ ỨNG DỤNG Luận văn dài 69 1 TỔNG QUAN VỀ KHÁM PHÁ TRI THỨC VÀ KHAI PHÁ DỮ LIỆU. 6 1.1. Phát hiện tri thức và khai phá dữ liệu. 6 1.2. Quá trình phát hiện tri thức từ cơ sở dữ liệu . 7 1.2.1. Xác định vấn đề 8 thập và tiền xử lý dữ liệu9 1.2.3. Khai thác dữ liệu . 11 1.2.4. Minh họa và đánh giá 11 1.2.5. Đưa kết quả vào thực tế 11 1.3. Khai phá dữ liệu 12 1.3.1. Các quan niệm về khai phá dữ liệu. 12 1.3.2. Nhiệm vụ của khai phá dữ 1.3.3. Triển khai việc khai phá dữ 1.3.4. Một số ứng dụng khai phá dữ liệu . 15 1.3.5. Các kỹ thuật khai phá dữ liệu 17 1.3.6. Kiến trúc của hệ thống khai phá dữ liệu 19 1.3.7. Quá trình khai phá dữ liệu . 21 1.3.8. Những khó khăn trong khai phá dữ liệu 22 2 LUẬT KẾT HỢP TRONG KHAI PHÁ DỮ 2.1. Bài toán kinh điển dẫn đến việc khai phá luật kết hợp. 25 2.2. Định nghĩa về luật kết hợp . 26 2.3. Một số tiếp cận trong khai phá luật kết hợp32 3 MỘT SỐ THUẬT TOÁN PHÁT HIỆN LUẬT KẾT HỢP 35 3.1. Thuật toán AIS35 3.2. Thuật toán SETM 36 3.3. Thuật toán 37 3.4. Thuật toán 44 toán 3.6. Thuật toán 47 3.7. Thuật toán 95] 55 4 KHAI THÁC LUẬT KẾT HỢP TRONG BÀI TOÁN QUẢN LÝ THIẾT BỊ THPT CHU VĂN AN- THÁI NGUYÊN . 58 4.1. Phát biểu bài toán 58 4.2. Cơ sở dữ liệu của bài toán . 59 4.3. Rời rạc các thuộc tính gốc để tạo thành các thuộc tính nhị Cơ sở dữ liệu dạng nhị phân 62 4.5. Kết quả khai thác luật kết hợp bằng thuật toán Apriori 62 4.6. Kết quả khai thác cơ sở dữ liệu quản lý thiết bị THPT Chu Văn An – Thái Nguyên đính kèm 632.3 KBXem: 2Bài viết cùng chủ đề Phát triển hệ thống cảnh báo dịch bệnh trên cơ sở công nghệ cứu kỹ thuật bảo vệ bản quyền các sản phẩm đồ họa dụng phép dịch lược đồ quan hệ trong cơ sở dữ kiếm mờ và ứng dụng tìm kiếm thông tin trong các văn bản trúc hệ thống quản trị mạng dựa trên chỉnh biến dạng hình học và ứng nghệ vsat ip trong cung cấp thông tin hỗ trợ phát triển kinh tế xã hội cấp cứu một số pháp phân cụm mờ và ứng PHÁP NÂNG CAO ĐỘ AN NINH THÔNG TIN TRONG MẠNG LAN KHÔNG DÂY CHUẨN IEEE cứu một số kỹ thuật lấy tin tự động trên lời vào chủ Ignored sẻ trang with tài khoản hoặc địa chỉ đã có tài khoản vào đây để đăng Mật khẩu của tôi đã quên mật khẩu? Duy trì trạng thái đăng nên có của posted 25/9/17 lúc 16:57In túi giấy đựng bắp rang posted 25/9/17 lúc 10:51In nhanh gọn, posted 25/9/17 lúc 10:34In quà tặng pha lê cao cấp posted 25/9/17 lúc sao nên uống nước ấm pha posted 24/9/17 lúc sẻ trang liệu - Luận Học - kết kết Active kết viên đã đăng truy động gần Profile kiếm Chỉ tìm trong tiêu gửi bởi thành cách tên bằng dấu hơn ngày: Search this thread only Search this forum only Hiển thị kết quả dạng Chủ kiếm hữu đầu 0.0756 5.414 MBDB { 0 1,_animationSpeedMultiplier: rgb(255, 255, {SV_rrssbDefault:true,thread_view:true,attached_files:true,message:true,bb_code:true,message_user_info:true,MoreThread_main:true,share_page:true,sidebar_share_page:true,wf_default:true,login_bar:true,notices:true,panel_scroller:true,dark_azucloud:true,node_list:true,facebook:true},_cookieConfig: { path: /, domain: , prefix: ca88d254,_noRtnProtect: Hủy giây phút phút nay lúc qua, lúc %day% lúc Chủ Thứ Thứ Thứ Thứ Thứ Thứ Tháng tám,Tháng mười mười Có lỗi sau sảy xa với yêu cầu của The server did not respond in time. Please try Đang đăng Xem ảnh Show hidden content by Javascript = = docUrl = = 'Thư viện chia sẻ source code, mã nguồn miễn phí, tổng hợp các mã nguồn và đồ án, source code web, kiến thức lập trình chuyên nghành công nghệ thông tin', 'I thought you might like this', image: title: url: docUrl });",
          "relevence": "yes"
        },
        {
          "url": "http:ait.edu.vn/Hoc_thuat/Apriori.html",
          "title": "THUẬT TOÁN APRIORI",
          "content": "THUẬT TOÁN APRIORI Về trang chủI. Giới là khả sinh đề xuất bởi R. Agrawal và R. Srikant vào năm 1993 để các tập item đối với các luật kết hợp kiểu bool. Tên của thuật toán việc thuật toán sử dụng tri thức (prior của các thuộc item phổ biến, chúng ta sẽ thấy sau đây. Apriori dùng cách tiếp cận lặp đến như tìm kiếm tập k item dùng để thăm dòcác tập (k+1) item. Đầu tiên, tập 1 item phổ biến tìm thấy bằng cách quét cơ sở dữ liệu để đếm số item, và thu thập những item thỏa mãn độ hỗ trợ tối Tập kết quả L1. Tiếp theo, dùng để tìm L2, tập các tập 2 item phổ dùng để tìm L3, vàcứ thế tiếp tục, cho tới khi tập k item phổ biến không thể tìm thấy. Việc tìm kiếm cho mỗi Lkđòi hỏi một lần quét toàn bộ cơ sở dữ khi đi vào chi tiết của Apriori đầu tiên chúng ta sẽ tìm hiểu xác định một vài thuật ngữ phổ sử dụng trong thuật Itemset là của những item trong cơ sở dữ liệu mà nó xác định bởi in}, trong đó n là số một thành phần cơ sở dữ liệu mà nó bao gồm tập hợp các item. hiệu là T và T chứa tâp hợp các item support support là điều kiện cần đáp ứng bởi các item đề ra để có thể xử lý item kế tiếp có thể. Minimum support có thể xem như là một giúp loại bỏ các tâp không phổ biến trong bất kỳ cơ sở dữ liệu. Minimum support cho mô hình tỷ lệ phần itemset biến) -  các Itemset đáp ứng các điều kiện minimum support thì gọi là tập phổ biến. Nó ký hiệu làLi trong đó i itemset tập phổ biến) - ứng viên tập phổ biến là các item chỉ xem xét xử lý. tập phổ biến là tất cả các kết hợp có thể có của tập phổ biến. Nó ký hiệu Ci trong đó I chỉ – Độ hữu dụng luật có thể đo với sự giúp đỡ của hỗ trợ. Support giúp chúng tađo như thế nào các giao tác có  tập mà nó phù hợp với ý nghĩa cả hai phía cạnh trong luật kết xét hai item A và b. Để support của A->B theo công thức như – sự chắc chắn của các luật. Thông số này cho phép chúng ta đếm mức độ một giao tác của tập phổ biến phù hợp với ý nghĩa cả phía cạnh bên trái cạnh bên phải. các tập phổ biến không đáp ứng các điều kiện trên có thể xét hai itemA và B. Để tính toán của A->B theo công thức ý: không bằng quả của việc phát sinh của tập item phổ biến, một tính trọng gọi là tính chất Apriori giới thiệu dùng để giảm không gian tìm kiếm. Chúng ta sẽ mô tả tính chất này xem một ví dụ minh họa cách sử dụng : các tập con không rỗng của một tập item phổ biến cũng phải là phổ chất Apriori này dựa theo nhận xét sau. Theo nếu một tập item I không độ hỗ trợ tối I không là phổ biến, do đó, P(I)< Nếu một item A thêm vào tập item I, thì tập item tạo thành (vd, thể xuất hiện xuyên hơn I.Do đó, IA cũng không phổ biến; do đó, P(IA) < chất này thuộc đặc biệt của các thuộc tính gọi là điệu rằng nếu một tập không thể qua kiểm tra, tất cả các tập cha của nó cũng sẽ thất bại với một tra tương tự. Đó gọi là điệu là vì thuộc tính này là đơn điệu trong ngữ cảnh của bại một cuộc kiểm tra. II. Thuật toán tiếp cận lặp biết đến như tìm kiếm với các tập k item dùng để thăm dò các tập 1- phổ biến 1 tìm thấy ký hiệu là theo là tính support có nghĩa là sự xuất hiện của các item trong cơ sở Điều này đòi hỏi phải duyệt qua toàn bộ cơ sở dữ bước cắt tỉa thực hiện trên C1 trong đó những item sosánh với thông số minimum Những item thỏa điều kiện minimum support xem xét cho tiến trình tiếp theo ký hiệu là bước phát sinh các bộ ứng viên thực hiện trong đó tập phổ biến 2 ra ký hiệu là nữa, cở sở dữ liệu duyệt để tính toán support của 2 tập phổ biến. các bộ ứng viên tạo ra kiểm tra và chỉ những tập phổ thỏa điều kiện minimum support thì tiếp tục sử dụng tạo ra bô ứng phổ biến trên tiếp đến khi không có tập phổ biến hoặc bộ ứng viên có thể tạo ra. III. Ví dụ thuật toán 1 biểu giao dịch cơ sở dữ liệu có 4 giao là một duy nhất cho mỗi giao hiện bước là chức năng duyệt cơ sở dữ liệu để xác định số sự xuất hiện cho cụ thể. Sau bước đầu tiên chúng ta sẽ có C1 trong Table tiếp theolà bước cắt tỉa, trong đó support tập phổ biến so sánh với Các tập phổ biến thỏa măn minimum support sẽ xử lý tiếp tục. rằng minimum support là 2. Chúng ta sẽ có L1 từ bước 3 cho thấy kết quả cắt giờ bước phát sinh ứng viên thực hiện trong đó tất cả ứng viên có thể có 2 biến ứng viên tạo. Bảng này ký hiệu là C2. TABLE 4 tất cả khả năng kết hợp mà có thể tạo ra từ TABLE 3 tập phổ giờ cắt thực hiện trên cơ sở các điều kiện minimum Từ TABLE 4 hai biến sẽ loại bỏ. Sau khi cắt tỉa chúng ta nhận kết quả như quá tự tiếp tục cho đến khi không có tập phổ biến hoặc bộ ứng viên có ra. Tiến trình mô tả trong TABLE 6 và TABLE (Kết quả cuối giả thuật { itemset of size kLk itemset of size kL1 (k = 1; Lk!=0; k++) from Lk; t in do the count of all in t = in Ck+1 chế của lớn tập phổ tạo ra  làm gia tăng sự phức nhiều lần duyệt cơsở dữ liệu yêu cầu vì số lớn tập phổ biến số lần duyệt cơ liệu nhiều làm gia tăng sự phức tạp thời gian khi cơ sở dữ liệu gia những hạn một điều cần thiết phải đưa ra một đổi trong thuật toán Apriori mà chúng ta sẽ thấy thêm trong phần Demo Ngày cập nhật điều hành: Windows XP, 7,8 4.0File chạy : chi tiết liên hệ: coding thuật toán Apriori theo yêu cầu, đề tài, ngôn ngữ C#)   ",
          "relevence": "yes"
        },
        {
          "url": "https:www.rung.vn/dict/en_vn/A_priori",
          "title": "Nghĩa của từ A priori, tra từ A priori là gì. Từ điển Anh - Việt - Rung.vn",
          "content": "Toggle đồng Hỏi điển Anh - điển Việt - AnhTừ điển Anh - AnhTừ điển Pháp - điển Việt - điển Anh - điển Nhật - AnhTừ điển Việt - điển Nhật - điển Hàn - điển Trung - điển Việt - điển Viết văn bản Tìm âm tiếng AnhTừ vựng tiếng qua tiếng Anh qua Các cách tiếng Anh qua BBC tiếng Anh qua nghe tiếng Anh qua video điển Anh - điển Việt - AnhTừ điển Anh - AnhTừ điển Pháp - điển Việt - điển Anh - điển Nhật - AnhTừ điển Việt - điển Nhật - điển Hàn - điển Trung - điển Việt - điển Viết tắt set default bộ () {var dict = = || = || đã = : + word + ' title=' + word + '>' + , ' ') + (w, d) { for (var i = 0, j = k = j[i]; i < k = == && != 1) { = || = || 1); = || = || = || Date); } } var s = s.type = s.async = true; s.src = điển Anh - điển Anh - điển Anh - Anh Từ điển Anh - Nhật A phát s, id) {var js, fjs = = js.id = = lục1 Thông Phó Theo cách suy diễn, theo cách diễn Tiên Chuyên Toán & tiên Điện tiền Thông dụng Phó từ Theo cách suy diễn, theo cách diễn dịch Tiên học) xác xuất tiên Chuyên & lạnh tiền thể khảo & = || điển trực tu điển từ điển đã = : + word + ' title=' + word + '>' + , ' ') + từ xem nhiều nhất trong = || cùng thể to idle section section nổi THU YÊU [VẬN ĐỘNG QUYÊN GÓP - TÀI TRỢ]1 báo giá banner trên báo update đăng nhập Rừng thông qua of nasal 1264Top Bài and and the Beach (w, d) { for (var i = 0, j = k = j[i]; i < k = == && != 1) { = || = || 1); = || = || = || Date); } } var s = s.type = s.async = true; s.src = khoản · Nhóm phát triển · Liên hệ quảng cáo và phản hồi · Trà Sâm Từ điển trực tuyến © hãy Like và Share để ủng hộ cho Rừng s, id) {var js, fjs = = js.id = = ứng dụng Từ điển Rừng, hoàn toàn Miễn phí có bài viết mới đăng câu hỏi, mời bạn ấn vào link này để tham giavào nhóm Cộng đồng hỏi bạn nhập câu hỏi ở đây quên cho thêm ngữ cảnh và nguồn bạn nhé :) ). Bạn vui lòng soát lại chính tả câu phải là thành viên để sử dụng chức năng nàycó bài viết mới ↑ Tạo bài câu bạn vào đây để xem thêm các = () { Handler for link,}]);jwplayer().play();jwplayer().onComplete(function () () {if (!$(this).hasClass(playing)) {var link = else else  true la dang '1',30);$('#toctitle').addClass('fa {$.ajax({url: {dict: () = = = = 90;var topY = && > = = 1;var = 3;var = = '';var = '';var = load for Rung, site: size: 1x1 - display */var _avlVar = _avlVar || = == ? : src=' + + type=text/javascript></scr'+'ipt>');",
          "relevence": "no"
        },
        {
          "url": "https:vi.wikipedia.org/wiki/Ti%C3%AAn_nghi%E1%BB%87m",
          "title": "Tiên – tiếng Việt",
          "content": "(function(){var href=  #    u003Etắt  u003C/a  u003E]  u003C/div  u003E  u003Cdiv dir=  ltr    u003E  u003Cdiv tài tài mừng mới dẫn bài title=  Wikipedia:Quy định và đổi gần đổi gần href=  /wiki/Wikipedia:C%C3%A2u_th%C6%B0%E1%BB%9Dng_h%E1%BB%8Fi   href=  /wiki/%C4%90%E1%BA%B7c_bi%E1%BB%87t:D%E1%BB%8Bch_n%E1%BB%99i_dung   title=  Đặc nội tập tin nhiều xem nhất: text   text   luận    u003EThảo class=  mw-redirect   hệ title=  Wikipedia:Thảo luận Chiến lược Phong trào luận chiến chuẩn bài title=  Wikipedia:Độ nổi độ nổi title=  Wikipedia:Thái độ trung phong trung và title=  Wikipedia:Nguồn đáng tin nguồn đáng tin gì không phải là spam quảng class=  mw-redirect   vi phạm bản nang biên nang biên Tháng thi đua viết bài về khoa học tự nhiên trên title=  Wikipedia tiếng tiếng Giải nhất 1,1 triệu class=  mw-redirect   2 giải nhì 810 nghìn, 3 giải ba 530 nghìn, class=  extiw   Nature Science Contest Quỹ tài viết bài khoa học tự nhiên class=  external text   khoa toàn thư mở tìm thuật toán khai thác dữ liệu, xem bài thuật toán tiên (chữ Hán: 先驗, tiếng Latin: a priori) có nghĩa kinh Trong nhiều cách sử dụng tại Tây hiện đại, thuật ngữ tiên cho là có nghĩa tri thức mệnh đề - loại tri thức có thể có mà không cần hoặc kinh Nó đối lập với tri thức hậu với nghĩa sau kinh - loại tri thức đòi hỏi kinh học và logic coi là các ngành khoa học tiên Các khẳng như 2 + 2 = 4 chẳng hạn, coi là tiên vì chúng là các tư xuất phát chỉ từ tư duy mà khoa học về tự nhiên và khoa học xã hội coi là các ngành khoa học hậu Các câu như kiểu Trời có màu xanh. có thể coi là tri thức hậu lục1 Tư triết học2 Xem thêm3 Tham khảo4 Liên kết triết | sửa mã trong các câu hỏi cơ bản của nhận thức luận là có hay không tri thức tiên không tầm Nói chung, các nhà duy lý tin rằng có, trong khi các nhà kinh chủ nghĩa tin rằng mọi tri thức đều rút ra từ một dạng kinh nào là từ bên nếu không, nó là tri thức tầm theo một nghĩa nào ngữ này đã đạt một vị trí vững chắc nhờ các nhà tư duy lý, chẳng hạn René và những đã lý luận rằng tri thức thu qua lý tính, mà không phải kinh coi tri thức về bản ngã, hay tôi tư duy, do đó tôi tồn tại, là tiên vì ông cho rằng ta không cần viện dẫn đến kinh trong quá khứ để suy xét về sự tồn tại của chính rằng tư duy là một phần của kinh John Locke, đã đưa ra một cơ sở lý luận mà từ đó toàn bộ khái niệm tiên có thể bị loại Hume coi mọi tri thức tiên là một quan hệ của các ý niệm of Ideas), khi ông nhắc đến thuật ngữ này vài lần trong tác phẩm Enquiry Human của sử dụng từ tiên hiện đại bắt đầu với Kant, đã đưa ra sự phân biệt giữa chân lý tổng hợp và chân lý phân tích để bổ sung cho sự phân biệt giữa tri thức tiên và tri thức hậu Ông lý luận rằng các mệnh đề biết là tiên nhất thiết đúng, trong khi các mệnh đề biết là hậu thì còn tùy vì theo Kant tri thức tiên đã luôn luôn đúng hạn 2 + 2 = 4). Các mệnh đề hậu sẽ phụ thuộc vào các điều kiện ngoại cảnh, các điều kiện này có thể thay đổi theo thời gian và làm cho mệnh đề trở nên sai (ví dụ. Bill Clinton là Tổng thống Mỹ, là mệnh đề đã từng đúng nhưng bây giờ là Kripke, khi phê phán Kant trong Naming and (Đặt tên và sự cần thiết - 1980), lý luận rằng tiên là một tính chất nhận thức luận, và không nên kết hợp với vấn đề tách biệt của siêu hình học về sự cần Để hỗ trợ luận cứ này, ông đưa ra một vài kêu gọi tới trực giác. Đầu tiên, ông lý luận rằng một chân lý hậu có thể nhất thiết đúng. Ví dụ, khi nói rằng Sao Hôm là Sao Mai is Câu đó nhất thiết đúng do cả hai đều là tên của Sao Kim, nhưng lại biết là hậu Ông còn lý luận rằng có thể có các mệnh đề tiên tùy Ví dụ, ở Paris có một đoạn đã từng dùng làm tiêu chuẩn của mét. Mệnh đề đi kèm, Đoạn đó dài 1 mét, là tùy thuộc do ta có thể lấy một độ dài khác để định nghĩa mét. Tuy nhiên, nó biết là tiên vì một mét đã định nghĩa là chiều dài của đoạn đó, nên đoạn phải có độ dài 1m (tại thời điểm nó dùng làm tiêu - đây là một phép lặp thừa tác phẩm The of (Các vấn đề triết học), Russell đã coi tri thức tiên là quan hệ giữa các phạm trù Chẳng hạn 2 + 2 = 4, là một nguyên lý tiên cho thấy quan hệ giữa 2, +, =, và 4, theo chúng đều là các phạm triết gia nổi bật thời về tư duy tiên bao gồm Alfred Ayer, và W.V.O. | sửa mã đề phân đề tổng thức | sửa mã kết | sửa mã từ loại: Nhận thức học triết nghĩa kinh niệm triết đơn cụ cá đăng luận cho địa chỉ IP tài gian hiển mã lịch viết chọn viết ngẫu đổi gần hồi thiệu luận sử liên kết đến đổi liên trang đặc kết tin mục dẫn trang raTạo một quyển về dưới dạng để in raNgôn ngữ / / liên kết Trang này sửa đổi lần cuối lúc 00:20 ngày 14 tháng 7 năm bản phát hành theo Giấy phép Commons có thể áp dụng điều khoản bổ sung. Với việc sử dụng trang web này, bạn chấp nhận Điều khoản Sử dụng và Quy định quyền riêng là hiệu đã đăng ký của Inc., một tổ chức phi lợi định quyền riêng thiệu phủ phát bố về bản di -total]},cachereport:{origin:mw1304,timestamp:20170912055055,ttl:1900800,transientcontent:false}}});});(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({wgBackendResponseTime:76,wgHostname:mw1262});});",
          "relevence": "no"
        },
        {
          "url": "https:en.wikipedia.org/wiki/Apriori_algorithm",
          "title": "Apriori - Wikipedia",
          "content": "Apriori the free is an for item set mining and rule over It by the items in the and them to larger and larger item sets as long as those item sets appear often in the The item sets by Apriori can be used to rules which general trends in the this has in domains such as market basket Example 12.2 Example 23 Apriori was by Agrawal and Srikant in 1994. Apriori is to operate on (for of items bought by or details of a website Other are for finding rules in data having no (Winepi and or having no (DNA Each is seen as a set of items (an Given a C}, the Apriori the item sets which are subsets of at least C} in the uses a bottom up where subsets are one item at a time (a step known as and groups of are tested against the data. The when no further are uses search and a Hash tree to count item sets It item sets of length k} from item sets of length k-1}. Then it prunes the which have an sub to the closure lemma, the set all item sets. After that, it scans the to item sets among the pseudo code for the is given below for a T}, and a support of }. Usual set is though note that T} is a C_{k}} is the set for level k}. At each step, the is assumed to the sets from the large item sets of the level, heeding the closure lemma. a field of the data that set c}, which is assumed to be zero. Many details are omitted below, usually the most part of the is the data used for storing the sets, and their {large~1-itemsets} k  gets { extbf      qquad a  in b ot   in c  land   qquad ~t  in   qquad   qquad c  in   qquad   qquad ~c  in   qquad   qquad   qquad   qquad c  in   qquad k  gets { extbf the where each row is a and each cell is an item of the rules that can be from this are the of sets with alpha also contain beta50% of sets with alpha, beta also have of sets with alpha, beta also have thetawe can also this through a variety of that a large tracks sales data by unit (SKU) for each item: each item, such as butter or bread, is by a SKU. The has a of where each is a set of SKUs that were bought the of consist of will use Apriori to the item sets of this To do this, we will say that an item set is if it appears in at least 3 of the the value 3 is the support first step of Apriori is to count up the number of called the of each member item By the for the first time, we obtain the the of size 1 have a support of at least 3, so they are all next step is to a list of all pairs of the the pair {1,2}: the first table of Example 2 shows items 1 and 2 in three of the we say item {1,2} has support of pairs {1,2}, {2,3}, {2,4}, and {3,4} all meet or exceed the minimum support of 3, so they are The pairs {1,3} and {1,4} are not. Now, because {1,3} and {1,4} are not any larger set which {1,3} or {1,4} cannot be In this way, we can prune sets: we will now look for triples in the but we can already exclude all the triples that contain one of these two the there are no -- {2,3,4} is below the minimal and the other were because they were super sets of pairs that were already below the have thus the sets of items in the and how some items were not counted because one of their subsets was already known to be below the while suffers from a number of or which have spawned other large numbers of subsets (the to load up the set with as many as before each scan). subset a of the subset finds any maximal subset S only after all of its proper such as try to the maximal item sets without their and perform jumps in the search space rather than a purely Rakesh Agrawal and Srikant Fast for mining rules. of the 20th on Very Large Data Bases, VLDB, pages Chile, 1994.^ Bayardo Jr, Roberto J. (1998). mining long from (PDF). ACM SIGMOD Record. 27 GPL Java rule mining with GUI, of for of and of rules Java of Eclat and offers Java of Apriori and several such as and other more such as and Borgelt C for Apriori and many other pattern mining (Eclat, etc.). The code is as free under the MIT R package arules Apriori and Eclat and for and data and an data mining suite, widgets for and rules based on Apriori from Data mining with example logged articleDonate to links this a as / links This page was last edited on 29 June 2017, at is under the Commons terms may using this site, you agree to the Terms of Use and Privacy Policy. is a of the Inc., a view(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({wgPageParseReport:{limitreport:{cputime:0.068,walltime:0.180,ppvisitednodes:{value:213,limit:1000000},ppgeneratednodes:{value:0,limit:1500000},postexpandincludesize:{value:1803,limit:2097152},templateargumentsize:{value:81,limit:2097152},expansiondepth:{value:7,limit:40},expensivefunctioncount:{value:0,limit:500},entityaccesscount:{value:0,limit:400},timingprofile:[100.00% 51.7461 51.7461 -total, 79.46% 41.1151 Template:Main_other]},scribunto:{limitreport-timeusage:{value:0.020,limit:10.000},limitreport-memusage:{value:1437886,limit:52428800}},cachereport:{origin:mw1219,timestamp:20170914060354,ttl:1900800,transientcontent:false}}});});(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({wgBackendResponseTime:68,wgHostname:mw1264});});",
          "relevence": "yes"
        },
        {
          "url": "http:englishteststore.net/index.php?option=com_communityanswers&view=question&id=451012:apriori-dich-la-gi&catid=578:dich-anh-viet&Itemid=238",
          "title": "Apriori dịch là gì?",
          "content": " 6Grade 7Grade 8Grade 9Grade 10Grade 11Grade 12TOEIC - PDFChat IOSFor Android OS Việt phápDe thi Anh Pho lỗi TA đề thi TNPT các đề thi ĐH các tra TA nhân thi chứng chỉ A, B, C, A1, A2, B1, B2, C1, C2, TOEFL, TOEIC, English School in the same meaning English or British your grammar the English your English your English or English Grammar = || tra trình độ Tiếng tra trình độ tra trình độ tra trình độ tra trình độ Ngữ tra Tiếng Anh xin TOEFL - Picture know your English level? Check Latest Most Popular Featured View all users order, = = = = a Click hereAsk try our dịch là gì?Em muốn hỏi là Apriori dịch là by Dịch Anh - year one year Add it on +1 on Google, Tweet it or share this on other links = || (1)0 Apriori là: Tiên -Một cụm thuật ngữ miêu tả quá trình suy luận phán đoán từ giả ban đầu cho đến kết luận. pháp này có thể đối lập với cách tiếp cận dựa trên những cứ liệu rút ra từ thực tế quan one year Add this Article Image Page Break Read your = || password?RegisterCancelLog vs Silent vs Hope and Perfect and and and and and and time vs vs Test { var var t = t.src = g; t.id = r; = 1;var n);} else 'ic', 'cbola', Errors? Report Us. © All Rights of use | Privacy Policy | Notice to Topvar _gaq = _gaq || {var ga = ga.type = = = == ? : + s = s);})();",
          "relevence": "no"
        },
        {
          "url": "http:adnet.ucoz.co.uk/board/kinh_te/apriori/9-1-0-1535",
          "title": "Apriori nghĩa là gì - Kinh tế- Từ vựng chuyên ngành - Tiếng Anh",
          "content": " Tiếng Anh Từ vựng chuyên hỏi câu Tiếng AnhHome » Từ Kinh tế[ Thêm từ mới nghĩa là gì? Apriori  Tiên { top left li{ li a { li a:hover { left { { { { { { { { { { { top:0 left:0 left }var = mark) {if = Added by: admin |Views: 16 | Rating: = nhập để bình Đăng ký Guest! Sign Up | Log Trị Nhân tếTin họcY InLog in with uIDOld login class=myWinLoad></div></div>');_uPostForm('frmLg549309',{type:'POST',url:'/index/sub/',error:function() align= cannot be at this time, please try again cannot be at this time, please try again Lost | Sign UpNew Tài liệu tiếng Anh giao tiếp hay#1  Ebook + Audio sách luyện nghe: Tactics For For For For tự điển sách tiếng Anh cho bé (trẻ em) sách tiếng Anh cho bé (trẻ em) David - Wide - for the © 2017. Powered by '<a title=uCoz alt= + + '?01' + + ' ",
          "relevence": "no"
        },
        {
          "url": "https:nhannguyen95.github.io/2017/08/12/bai-tap-khai-pha-tap-pho-bien-bang-thuat-toan-apriori",
          "title": "Bài tập khai phá tập phổ biến bằng thuật toán Apriori",
          "content": "Bài tập khai phá tập phổ biến bằng thuật toán Apriori bảng dữ liệu bao gồm các giao dịch (tid) 3 42002 3 53001 2 3 54002 5Tìm các tập mục có độ hỗ trợ ≥ 0.5 (tức tần số sup. ≥ 2).Lý định nghĩa cơ hạng mục sử I là một tập hữu hạn, mỗi phần tử của I gọi là một hạng mục tập hạng mục X là một tập con của I.X gọi là một tập hạng mục mức k nếu X chứa k hạng dụ: I = {1 2 3 4 5}; 1, 2, 3, 4, 5 là các hạng mục; X = {2 5} là một tập hạng mục mức 2.Giao dịch tập các giao dịch xác định trên I là một ánh xạ → P(I). Tập T(k) là giao dịch thứ k của T.Các số 1,..,n là các định danh giao dịch dụ: giao dịch thứ 100, = {1 3 4}.Ký hiệu tập các giao dịch chứa tập hạng sử X = {1 3}, tập các định danh giao dịch có chứa X là {100, 300}, kí hiệu là =   left   { k   mid X T(k)  ight hỗ trợ hỗ trợ của một item set X ký hiệu là sup(X) xác định bởi công = T dụ: sup({1 3}) = 2/4 = phổ biến/ tập xuyên (Large itemset X gọi là phổ biến hay tập xuyên   geq đó 𝛿: minsup là một do dùng xác lại thì X gọi là tập không phổ biến (small kết luật kết hợp là một công thức có dạng X ⇒ Y, trong đó X, Y là hai itemset thoả X ∩ Y = ∅, X gọi là tiền đề và Y gọi là hệ quả của dụ: 2 ⇒ {3 kết hợp chỉ có ý nghĩa khi tần suất thể hiện mối tương quan giữa các tập thuộc tính là lớn hơn một nào hỗ trợ của luật kết hỗ trợ của một luật X ⇒ Y, ký hiệu sup(X ⇒ Y) là khả năng mà tập giao dịch T hỗ trợ cho các thuộc tính trong cả X và Y.sup(X Y) =   cup Y) T tin cậy của luật kết tin cậy của một luật X ⇒ Y, ký hiệu conf(X ⇒ Y) là xác suất có điều kiện P(Y   mid Y) =   cup dụ: xét luật kết hợp 2 ⇒ {3 5}, ta có sup(2 ⇒ {3 5}) = 2/4 = 1/2, conf(2 ⇒ {3 5}) = tính tính chất của large A B và A, B là các Itemset thì sup(A)   geq chất tập con của một tập phổ biến đều phổ biến, nghĩa Y X, sup(X)   geq minsup sup(Y)   geq tập mẹ của một tập không phổ biến đều không phổ biến, nghĩa là:% Y X, sup(X) < minsup sup(Y) < minsup vấn một tập giao dịch T, mục đích của bài toán phát hiện luật kết hợp là tìm ra tất cả các luật hỗ trợ ≥ minsup tin cậy ≥ toán gồm hai ra các tập mục phổ biến có độ hỗ trợ ≥ ra các luật kết mỗi tập mục phổ biến, sinh ra tất cả các luật có độ tin cậy cao (≥ luật là một phân tách nhị phân của một tập mục phổ sinh ra các tập mục phổ biến có độ phức tạp cao, chúng ta tập trung vào bài tập tìm tập mục phổ biến (các tập có độ hỗ trợ ≥ toán toán do Agrawal đề nghị năm 1994, ý của thuật toán dựa vào tính chất các ứng viên có độ hỗ trợ ≥ μ, phải sinh ra từ các có độ hỗ trợ ≥ μ.Xem lời giải dưới đây để hiểu cách triển khai thuật tiên tìm ra các itemset mức 1 và tính sup. của chúng dựa vào dữ liệu đã cho, đây gọi là tập ứng viên mức 1 - những itemset có sup. < 2, tập những itemset này kí hiệu là itemset mức 2 có sup. ≥ 2 chỉ có thể sinh ra từ những itemset mức 1 có sup. ≥ 2 (tức những itemset mức 1 vừa liệt kê), liệt kê các itemset mức 2 có thể (C2) và tính sup. tương 2}1{1 3}2{1 5}1{2 3}2{2 5}3{3 lại, loại những itemset có sup. < 2, ta 3}2{2 3}2{2 5}3{3 tập ứng viên tổ hợp: {1 2 3}, {1 2 5}, {2 3 {1 2 3}, vì {1 2 3} sinh ra {1 2} không thuộc {1 2 5}, vì {1 2 5} sinh ra {1 2} không thuộc {2 3 5}, vì các tập con của {2 3 5} là {2 3}, {2 5}, {3 5} đều thuộc tập ứng viên C3 và sup. tương 3 thấy L4 = L4 chỉ còn duy nhất 1 phần tử, thuật toán kết các tập mục có độ hỗ trợ ≥ 0.5 bao gồm {1}, {2}, {3}, {5}, {1 3}, {2 3}, {2 5}, {3 5}, {2 3 5}.Post h5 index ) hre=# >+ = index ) hre=# >++ = - 100;  get initial of the { assign scroll event =  get current >= {  apply fixed if scroll to that element or below ittop: else {  apply if you scroll above e ) ($( - 20)}, = () = = {  DON'T EDIT BELOW THIS LINEvar d = s = = +new || enable to view the powered by Disqus.",
          "relevence": "yes"
        },
        {
          "url": "http:doc.edu.vn/tai-lieu/tieu-luan-tim-hieu-luat-ket-hop-trong-khai-pha-du-lieu-6760/",
          "title": "Tiểu luận Tìm hiểu luật kết hợp trong khai phá dữ liệu - Tài liệu, ebook, giáo trình",
          "content": "g D, nghĩa là: hỗ trợ tối thiểu minsup là một giá trị cho bởi sử dụng. Nếu tập mục X có sup(X) ³ minsup thì ta nói X là một tập các mục phổ biến. Một tập phổ biến sử dụng như một tập đáng quan tâm trong các thuật toán, lại, những tập không phải tập phổ biến là những tập không đáng quan tâm. Các phần sau sẽ sử dụng những cụm từ khác như “X có độ hỗ trợ tối hay “X không có độ hỗ trợ tối cũng để nói lên rằng X thỏa mãn hay không thỏa mãn ³ khoản mục X gọi là nếu lực của X bằng k, tức là luật kết hợp có dạng R: X => Y, trong đó X, Y là tập các mục, X, Y Í I và X ÇY = Æ. X gọi là tiên đề và Y gọi là hệ quả của X => Y tồn tại một độ tin cậy c . Độ tin cậy c định nghĩa là khả năng giao dịch T hỗ trợ X thì cũng hỗ trợ Y. Ta có công thức tính độ tin cậy c như sau: nhiên, không phải bất cứ luật kết hợp nào có mặt trong tập các luật có thể sinh ra cũng đều có ý nghĩa trên thực tế. Mà các luật đều phải thoả mãn một hỗ trợ và tin cậy cụ thể. Thực vậy, cho một tập các giao dịch D, bài toán phát hiện luật kết hợp là sinh ra tất cả các luật kết hợp mà có độ tin cậy conf lớn hơn độ tin cậy tối thiểu minconf và độ hỗ trợ sup lớn hơn độ hỗ trợ tối thiểu minsup tương ứng do dùng xác Khai phá luật kết hợp phân thành hai bài toán toán 1: Tìm tất cả các tập mục mà có độ hỗ trợ lớn hơn độ hỗ trợ tối thiểu do dùng xác Các tập mục thoả mãn độ hỗ trợ tối thiểu gọi là các tập mục phổ toán 2: Dùng các tập mục phổ biến để sinh ra các luật mong muốn. Ý chung là nếu gọi ABCD và AB là các tập mục phổ biến, thì chúng ta có thể xác định luật nếu AB => CD giữ lại với tỷ lệ độ tin cậy: conf ≥ minconf thì luật giữ lại có tính bắc XY và YZ, chúng ta không thể suy ra dụ: giả sử T(X) Ì T(Y) Ì T(Z), ở đó T(X), T(Y), T(Z) tương ứng là các giao dịch chứa X,Y,Z, và độ tin cậy cực tiểu thế thì: < minconf vì minconf < 1, do đó luật XZ không đủ độ tin chất 4:Nếu A(L - A) không thoả mãn độ tin cậy cực tiểu thì luật B (L -B) cũng không thoả mãn, với các tập mục L,A,B và B Í A Ì LVì supp(B) ³ sup(A) (theo tính chất 1) và định nghĩa độ tin cậy, chúng ta nhận như vậy: Nếu có (L-C) C thì ta cũng có luật (L – D)D, với DÍC và vì DÍC nên (L - D) Ê (L - C), do đó sup(L - D) £ Þ³ tính chất này sẽ sử dụng trong thuật toán mô tả trong các Một số tiếp cận trong khai phá luật kết vực khai thác luật kết hợp cho đến nay đã nghiên cứu và phát triển theo nhiều khác nhau. Có những đề xuất nhằm cải tiến tốc độ thuật toán, có những đề xuất nhằm tìm kiếm luật có ý nghĩa hơn… và có một số chính như kết hợp nhị phân là nghiên cứu đầu tiên của luật kết hợp. Hầu hết các nghiên cứu ở thời kỳ đầu về luật kết hợp đều liên quan đến luật kết hợp nhị phân. Trong dạng luật kết hợp này, các mục, thuộc tính, chỉ quan tâm là có hay không xuất hiện trong giao tác của CSDL chứ không quan tâm về xuất hiện. Ví dụ: Trong hệ thống tính cước điện thoại thì việc gọi 10 cuộc điện thoại và một cuộc xem là giống nhau. Thuật toán tiêu biểu nhất khai phá dạng luật này là thuật toán Apriori và các biến thể của nó. Đây là dạng luật đơn giản và các luật khác cũng có thể về dạng luật này nhờ một số pháp như rời rạc hoá, mờ hoá, … Một ví dụ về dạng luật này: liên tỉnh= AND gọi di => gọi quốc tế= AND gọi dịch vụ 108 = với độ hỗ trợ 20% và độ tin cậy kết hợp có thuộc tính số và thuộc tính hạng mục: Các thuộc tính của các CSDL thực tế có kiểu rất đa dạng, như số nhị phân, giá trị định tính, định Để phát hiện luật kết hợp với các thuộc tính này, các nhà nghiên cứu đã đề xuất một số pháp rời rạc hoá nhằm dạng luật này về dạng nhị phân để có thể áp dụng các thuật toán đã có. Một ví dụ về dạng luật này thức gọi = ‘Tự AND giờ gọi IN AND Thời gian đàm thoại IN 300’] => gọi liên tỉnh = , với độ hỗ trợ là 23. 53% , và độ tin cậy là kết hợp tiếp cận theo tập thô: Tìm kiếm luật kết hợp dựa trên lý tập kết hợp nhiều mức: Cách tiếp cận theo luật này sẽ tìm kiếm thêm những luật có dạng “mua máy tính PC => mua hệ điều hành AND mua phần mềm tiện ích văn phòng, …” thay vì chỉ những luật quá cụ thể như “mua máy tính IBM PC => mua hệ điều hành Windows AND mua phần mềm tiện ích văn phòng Office, …”. Như vậy dạng luật đầu là dạng luật tổng quát hoá của dạng luật sau và tổng quát theo nhiều mức khác kết hợp mờ: Với những hạn chế còn gặp phải trong quá trình rời rạc hoá các thuộc tính số các nhà nghiên cứu đã đề xuất luật kết hợp mờ nhằm khắc phục các hạn chế trên và luật kết hợp về một dạng tự nhiên hơn, gần gũi hơn với sử dụng một ví dụ của dạng này là: bao tư nhân = AND thời gian đàm thoại lớn AND cước nội tỉnh = => cước không hợp lệ = với độ hỗ trợ 4% và độ tin cậy 85%”. Trong luật trên, điều kiện thời gian đàm thoại lớn ở vế trái của luật là một thuộc tính đã mờ kết hợp với thuộc tính đánh trọng số: Trong thực tế, các thuộc tính trong CSDL không phải lúc nào cũng có vai trò như nhau. Có một số thuộc tính chú trọng hơn và có mức độ quan trọng cao hơn các thuộc tính khác. Ví dụ khi khảo sát về doanh thu hàng tháng, thông tin về thời gian đàm vùng cước là quan trọng hơn nhiều so với thông tin về thức Trong quá trình tìm kiếm luật, chúng ta sẽ gán thời gian gọi, vùng cước các trọng số lớn hơn thuộc tính thức gọi. Đây là nghiên cứu rất thú vị và đã một số nhà nghiên cứu đề xuất cách giải quyết bài toán này. Với luật kết hợp có thuộc tính đánh trọng số, chúng ta sẽ khai thác những luật (tức là có độ hỗ trợ thấp, nhưng có ý nghĩa đặc biệt hoặc mang rất nhiều ý kết hợp song song: Bên cạnh khai thác luật kết hợp tuần tự, các nhà làm tin học cũng tập trung vào nghiên cứu các thuật giải song song cho quá trình phát hiện luật kết hợp. Nhu cầu song song hoá và xử lý phân tán là cần thiết bởi kích dữ liệu ngày càng lớn hơn nên đòi hỏi tốc độ xử lý cũng như dung bộ nhớ của hệ thống phải đảm bảo. Có rất nhiều thuật toán song song khác nhau đã đề xuất để có thể không phụ thuộc vào phần cạnh những nghiên cứu về các biến thể của luật kết hợp, các nhà nghiên cứu còn chú trọng đề xuất những thuật toán nhằm tăng tốc quá trình tìm kiếm tập phổ biến từ ra, còn có một số nghiên cứu khác về khai thác luật kết hợp như: khai thác luật kết hợp trực khai thác luật kết hợp kết nối trực tuyến đến các kho dữ liệu đa chiều thông qua công nghệ OLAP, MOLAP, ROLAP, Phát hiện luật kết hợp trên hệ thông tin nhị Các định nghĩa về hệ thông tin nhị thông tin nhị các tập O ={o1, o2, …, on} là một tập hữu hạn gồm n đối = {d1, d2, …, dm} là một tập hữu hạn gồm m chỉ báo,B = {0, 1}Hệ thông tin nhị phân định nghĩa là SB = (O, D, B, c) trong đó c là ánh xạ c:O x D → B, c(o,d) = 1 nếu đối o có chỉ báo d và c(o,d) = 0 nếu lại.Các ánh xạ thông tin nhị hệ thông tin nhị phân SB = (O, D, B, c). Cho P(O) là các tập con của O, P(D) là các tập con của D. Các ánh xạ thông tin nhị phân rB và lB định nghĩa như sau:rB: P(D)  P(O) với ý nghĩa: cho S Ì D, rB(S) = {o Î O|d Î S, c(o, d) = 1}lB: P(O)  P(D) với ý nhĩa: cho X Ì O, lB(X) = {d Î D|o Î X, c(o, d) = 1}Tập chỉ báo phổ biến nhị hệ thông tin nhị phân SB = (O, D, B, c) và một q Î (0, 1).Cho S Í D, S là tập chỉ báo phổ biến nhị phân với q nếu ≥ LB là một tập gồm tất cả các tập chỉ báo phổ biến nhị phân đã phát hiện từ SB, chúng có thuộc tính như sau: S Î LB, T Ì S thì T Î đó LB,h là tập con của LB nếu XÎLB,h thì (với h là số nguyên luật kết hợp phổ biến nhị phân và hệ số tin hệ thông tin nhị phân SB = (O, D, B, c) và một q Î (0, 1). Cho L là một phần tử của X và Y là hai tập con của L, trong đó:L = X È Y, X ≠ {}, Y ≠ {} và X Ç Y = ta xác định các luật kết hợp nhị phân giữa tập chỉ số X và tập chỉ số Y là một ánh xạ thông tin: X  Y. Hệ số tin cậy của luật này biểu diễn là: RB,b là tập tất cả các luật kết hợp phổ biến nhị phân phát hiện từ SB. Trong đó CFB(r) ≥ b,r Î vectơ chỉ báo nhị phân và các phép hệ thông tin nhị phân SB = (O, D, B, c) trong đó O ={o1, o2, …, on} là một tập hữu hạn gồm n đối D = {d1, d2, …, dm} là một tập hữu hạn gồm m chỉ chỉ báo nhị phân: vB(X) = {X1, X2, … , Xn} trong đó: X Ì D là một vectơ với n thành phần, mỗi thành phần chiếm một giá trị trong B. Cho VSB là tập tất cả các vectơ chỉ báo nhị phân của SB, nếu card(X) = 1 thì X là bộ chỉ báo của SB và Xj = c(o, X)Tích vectơ chỉ báo nhị phân: Cho X1, X2 Ì D, vB(X1) = (X11, X12, … , X1n), vB(X2) = (X21, X22, … , X2n) là các phần tử của VSB. Tích vectơ chỉ báo nhị phân vB(X1) và vB(X2) biểu hiện là vB(X3) = vB(X1) QB vB(X2). Trong = (X31, X32, … , X3n) với X3j = X2j), j = 1¸nX3 = X1 È X2 Î DTừ vectơ vB(X3), biết tất cả các đối hiện có trong tập chỉ báo X1 và X2. Chúng ta dùng vB(X1) để trình diễn rB(X1), vB(X2) để trình diễn rB(X2) và vB(X3) để trình diễn hỗ trợ các vectơ chỉ báo nhị phân Cho X1 Ì D, độ hỗ trợ của vB(X1) biểu diễn định nghĩa là: = {o Ì O| d Î X1, c(o, d) = 1} thấy rằng: = (lực của tập hợp): Cho S = {s1, s2, … , sk} là tập con của D. Trong đó sj là bộ chỉ báo của SB, j = 1 ¸ k. Mỗi sj tương ứng với vectơ chỉ báo nhị phân Các yếu tố của rB(S) tính bằng = QB QB … hiệu VSB, h là tập con của VSB chứa chỉ vectơ vB(X) trong đó X Ì D và card(X) = h (h là số nguyên dương cho Thuật toán phát hiện tập chỉ mục và luật kết hợp nhị toán phát triển từ thuật toán Để phát hiện các tập chỉ báo nhị phân phổ biến từ các luật kết hợp nhị phân từ hệ thông tin nhị phân. Thuật toán này làm việc với các bit trong bộ nhớ và không làm việc với CSDL trên đĩa, vì thế có thể cải tiến tốc độ quá trình phát hiện luật. Cho một CSDL và hai độ hỗ trợ tối thiểu minsup và độ tin cậy tối thiểu minconf của luật kết hợp. Thuật toán có hai pha:Pha 1: Phát hiện các tập chỉ báo phổ biến dựa trên minsup cho 2: Xây dựng các luật kết hợp dựa trên một minconf cho ma trận thông tin nhị phân SB = (O, D, B, c) và một q, b Î(0, 1). Trong đó q là minsup và b là tiết thuật toán như sau:Pha 1: Phát hiện tập chỉ mục phổ biến nhị phân1. TraLoi = Æ ;2. Sinh LB,1 từ SB theo thủ tục 1.a. dưới đây ;3. for (k = 2; LB,k {}; LB,k từ LB,k-1 theo thủ tục 2.a. dưới đây = Èk LB,k-1 ;6. }7. Return TraLoi ;1.a. Sinh LB,1 1. LB,1 = Æ ;2. for (i = 1; i <= m; > q * VSB,1) VSB,1)) ;6. }7. TraLoi = LB,1 ;8. Return TraLoi ;  Trong đó m = card(D) là lực của lập D.2.a. Sinh trên thuộc tính S Î LB, T Ì S thì T Î LB, chúng sinh ra LB,k từ LB,k-1. Kết quả như một ma trận có các dòng và cột là các thành phần của LB, k-11. LB,k = Æ ;2. for (Mỗi X Î LB,k-1 && XY)3. {T = X È Y > && card(T) LB,k) ;6. VSB,k)) ;7. }}9. TraLoi = ; 10. Return TraLoi ;Trong LB,k) là một hàm để ghi một tập chỉ báo phổ biến nhị phân T vào VSB,k)) là một hàm để lưu một vectơ chỉ báo phổ biến nhị phân vB(T) vào VSB,k. Dựa vào (1) và (2), ta có thể tính rất nhanh tại bước thứ k của vòng lặp ở trên, từ các phần tử của 2: Phát hiện các luật phổ biến nhị phân1. RB,b = Æ ;  Khởi tạo tập luật ban đầu là for (Mỗi L Î LB)3. X, Y Î L và XÇY ³ RB,b);  ghi luật X=>Y vào ³ RB,b);  ghi luật Y=>X vào }10. TraLoi = RB,b ;11. Return RB,b ;  Kết MỘT SỐ THUẬT TOÁN PHÁT HIỆN LUẬT KẾT HỢP1. Thuật toán Ý thuật toán là một thuật giải do Rakesh Tomasz Arun Swami đề xuất lần đầu vào năm 1993. Thuật toán tìm giao dịch t có độ hỗ trợ và độ tin cậy thoả mãn lớn hơn một giá trị nào đó. Thuật toán tỉa bớt những tập ứng cử viên có tập con không phổ biến khi tính độ hỗ toán Apriori tính tất cả các tập ứng cử của tập k trong một lần duyệt CSDL. Apriori dựa vào cấu trúc cây băm. Tìm kiếm đi xuống trên cấu trúc cây mỗi khi ta chạm lá, ta tìm một tập ứng cử viên có tiền tố chung bao gồm trong giao dịch. Sau đó các tập ứng cử này tìm trong giao dịch đã ánh xạ đó. Trong hợp tìm thấy biến đếm tăng lên 1.Ký hiệu: Giả sử các mục trong mỗi giao dịch lưu giữ theo trật tự từ Gọi số các mục trong một tập mục là kích của nó và gọi tập mục có kích k là tập k-mục (tập k mục). Các mục trong mỗi tập mục cũng giữ ở trật tự từ Ta sử dụng các ký hiệu sau:Lk: Tập các tập k-mục phổ biến (với độ hỗ trợ cực tiểu minsup nào đó)Ck : Tập các tập k-mục ứng cử (các tập mục phổ biến tiềm Thuật toán CSDL D, Tập các tập mục phổ L1 = {Các 1 - itemset phổ k=2; 3. While( Lk-1! =Æ )4. { Ck = các ứng cử mới theo trình con ở dưới dịch tÎ D)6. ứng cử viên chứa trong t7. for ( ứng cử c Î ++;10. }11. Lk={ c Î Ck÷ c.count ³ k++;13. }14. Return L= ÈkLk' ; sinh ứng cử viên mới minsup )1. { for ( itemset l1Î Lk-1)2. for ( itemset l2Î == L1(k-2) == == c= L1 kết nối L2;5. if( Lk-1)) delete add c to for ( sÎ Ï Lk-1) return return FALSE thích: Lần duyệt đầu tiên, sẽ tính số lần xuất hiện của mỗi mục để xác định các 1- itemset phổ biến. Lần duyệt thứ k (k ³ 2) sẽ bao gồm 2 giai phổ biến Lk-1 đã tìm thấy ở lần duyệt thứ k-1 sử dụng để sinh ra các tập ứng cử viên Ck bằng việc sử dụng hàm vào CSDL, tính độ hỗ trợ của các ứng của viên trong Ck. Các ứng cử viên trong Ck mà chứa trong giao dịch t có thể xác định một cách hiệu quả bằng việc sử dụng cây băm mô tả như giai đoạn 2 (giai đoạn sửa, tỉa): xoá bỏ các tập c Î Ck sao cho một vài (k-1) – tập con của c không nằm trong Lk-1. Thủ tục này là đầy đủ bởi đối với bất kì tập nào Lk với độ hỗ trợ tối thiểu thì các tập con kích cỡ (k-1) cũng có độ hỗ trợ tối do đó nếu ta mở rộng mỗi tập trong Lk-1 với tất cả các tập mục có thể và sau đó xoá tất cả các tập mà (k-1) – tập con của nó không nằm trong Lk-1, ta sẽ nhận tập các tập trong kết nối là tương với việc mở rộng Lk-1 với mỗi mục nằm trong CSDL và sau đó xoá bỏ các tập này mà đối với nó (k-1) nhận bằng việc xoá đi mục thứ (k-1) không nằm trong Lk-1. Ở giai đoạn này Ck Ê Lk . Với lập luận như vậy, giai đoạn tỉa là giai đoạn ta xoá khỏi Ck tất cả các tập mà các (k-1) tập con của nó không nằm trong Lk-1 , cũng không xoá bất kỳ một tập nào có thể nằm trong Lk.Hàm Subset: Các tập ứng cử viên Ck lưu trữ trong một cây băm. Một nút của cây này hoặc là chứa một danh sách của các tập (nút lá) hoặc bảng băm ( một nút trong). Trong mỗi một nút trong, mỗi cụm của bảng băm chỉ đến một nút khác. Gốc của cây băm xem ở độ sâu là 1. Một nút trong ở độ sâu d sẽ dẫn đến nút ở độ sâu d+1. Các tập lưu trữ trong các lá. Khi ta bổ sung thêm một tập c, ta bắt từ nút gốc và đi xuống cây cho đến khi ta chạm vào một lá. Tại một nút ở độ sâu d, ta quyết định sẽ đi theo cành nào bằng việc áp dụng hàm băm đối với mục thứ d của tập đó và theo con trỏ trong Bucket tương ứng. Tất cả các nút ban đầu tạo ra như là nút lá. Khi số các tập trong một nút lá vượt quá chọn, nút lá này thành một nút đầu từ nút gốc, hàm Subset tìm tất cả các ứng cử viên chứa trong giao dịch t như sau: Nếu ta bắt đầu tại một lá, ta tìm những tập trong nút lá này chứa trong giao dịch t và bổ sung các mối quan hệ với chúng đối với tập kết quả mong muốn. Nếu ta đang ở một nút trong và ta đến nó bằng việc băm mục i, ta băm trên mỗi mục đi sau i trong t và áp dụng một cách đệ quy thủ tục đó đối với nút này trong Bucket tương ứng. Đối với nút gốc, ta băm theo mỗi mục trong t.Để thấy tại sao hàm Subset trả lại tập các tham khảo mong muốn hãy để ý đến những gì sẽ xảy ra tại nút gốc. Đối với bất kỳ tập c nào chứa trong giao dịch t, mục đầu tiên cần phải có trong t. Tại nút gốc, việc băm mọi mục trong t đảm bảo rằng ta chỉ không biết các tập mà nó bắt đầu với một mục không nằm trong t. Những lí luận tương tự áp dụng cho các mức sâu hơn. Vì các mục trong bất kì tập nào cũng sắp thứ tự, nếu ta đến một nút hiện tại bằng việc băm mục i, ta chỉ cần quan tâm đến những mục trong t nó xuất hiện sau i. Bước tỉa: Xoá bớt tất cả các tập mục c Î Ck mà (k-1) tập con của c không phụ thuộc Lk-1.1. for ( tập mục c Î Ck)2. for ( (k-1) – tập con s của c)3. if(s Ï Lk-1)4. delete c khỏi xét: Thuật toán Apriori với n là độ dài lớn nhất của tập sinh ra. Vậy thì thuật toán sẽ thực hiện duyệt toàn bộ các giao tác n+1 lần. Như vậy, nếu bỏ qua thời gian so sánh tìm sự xuất hiện của một mẫu trong một giao tác thì độ phức tạp của thuật toán Apriori là O(A) > O(n*L) trong đó L là kích CSDL còn n là độ dài cần đạt của các ra, nếu độ hỗ trợ tối thiểu minsup bị thay đổi thì thuật toán sẽ phải thực hiện lại từ đầu, điều này sẽ rất mất thời gian. Thuật toán Apriori xây dựng nhằm phát hiện các luật kết hợp giữa các đối với độ hỗ trợ và độ tin cậy tối 1.3. Sinh các luật kết hợp từ tập mục phổ khi các tập mục phổ biến từ các tác vụ trong CSDL đã tìm thấy, nó có thể sinh ra các luật kết hợp mạnh, ở đó luật kết hợp mạnh (strong rule) là luật thoả mãn cả hai độ hỗ trợ cực tiểu và độ tin cậy cực tiểu. Điều đó có thể thực hiện bằng việc sử dụng tính độ tin cậy của luật, ta nhắc lại: độ tin cậy của luật X  Y là: conf (X  Y) = P(Y/X) = đó là độ hỗ trợ của XÈY và sup(X) là độ hỗ trợ của X.Có thể coi tỷ số trên là tỷ số giữa: số các tác vụ chứa XÈY và số các tác vụ chứa X. Dựa trên biểu thức tính toán đó, các luật kết hợp có thể sinh như mỗi tập mục phổ biến l, sinh ra tất cả các tập con không rỗng của lVới mỗi tập con không rỗng a của l, ta có luật a  (l-a) nếu ³ minconf ở đó minconf là độ tin cậy cực các luật sinh ra từ các tập mục phổ biến nên độ hỗ trợ của luật đã thoả mãn, tức là độ hỗ trợ của luật chính là sup(l). Thuật toán đơn ta cải tiến thủ tục xử lý bằng cách sinh ra các tập con của mục lớn theo kiểu đệ qui ưu tiên độ sâu. Ví dụ: với tập mục ABCD, đầu tiên chúng ta xét tập con ABC, sau đó đến đến, nếu tập con a của tập mục lớn l không sinh ra luật thì không cần xét đến các tập con của nó nữa. Chẳng hạn: nếu luật ABC D không đủ độ tin cậy thì ta không cần xét đến luật AB này có thể chứng minh đơn giản như luật a (l-a) không thoả mãn độ tin cậy, tức là: nhỏ hơn thế thì với bất kỳ tập con b nào của a ta có:Vì bÌ a nên do = là độ tin cậy của luật cũng nhỏ hơn toán đơn giản này có thể mô tả như toán 1.For all large lk , k³2 do call am: large am}; for all am-1Î A do again if (conf ³ then begin output the rule with and if (m-l > l) then call sinh ra các luật với tập con của am-1 là phần tiền toán nhanh trên ta đã chỉ ra rằng nếu một luật không thoả mãn với tập cha a thì cũng không thoả mãn với tập con của nó. Ví dụ như trên đã xét: nếu ABCD không đủ độ tin cậy thì luật ABCD cũng không đủ độ tin cậy. Điều đó cũng có thể áp dụng theo lại như sau: nếu xảy ra luật với tập con thì cũng xảy ra luật với tập cha. Ví dụ: nếu luật ABCD có đủ độ tin cậy thì luật ABCD cũng đủ độ tin toán 2.For all larger lk, k³ 2 doBegin phần kết luận của các luật nhận từ lk với l-mục ở kết Call Hm:set of m-item (k>m+1) For all do Begin Conf = If (conf then Output the với độ tin cậy là conf và độ hỗ trợ là support (lk) hm+1 from Hm+1 End Call toán nhanh hơn này sử dụng thủ tục mô tả ở phần thuật toán Apriori ở trên. Ta xem tại sao thuật toán 2 này nhanh hơn thuật toán 1 dụ, ta xét tập mục ABCDE: Giả sử rằng ADECB là các luật có l-mục ở phần kết luận thoả mãn độ hỗ trợ cực tiểu thuật toán đơn giản trên, gọi đệ quy ACD) sẽ kiểm tra các luật với 2-mục ở phần kết luận là: CDEAB và thứ nhất không xảy ra vì E Ì BE và ABCD E không thoả mãn độ tin cậy. Các luật thứ hai và thứ ba cũng không thoả mãn độ tin cậy với lý do tương có một luật với 2 - mục ở phần kết luận nhận là ở đó B và D là các kết luận của các luật kết hợp có 1- mục ở phần kết luận. Thuật toán nhanh hơn mô tả ở trên chỉ kiểm tra một luật này.2. Thuật toán Ý thuật toán kinh điển Apriori tìm tập mục phổ biến thực hiện tốt bởi rút gọn kích các tập ứng cử nhờ kỹ thuật tỉa. Tuy nhiên, trong tình huống mà số các mẫu mẫu dài hoặc độ hỗ trợ cực tiểu thấp, các thuật toán Apriori gặp phải 2 chi phí phí cho số khổng lồ các tập ứng cử. Ví dụ: nếu có 104 tập 1-mục phổ biến th",
          "relevence": "yes"
        },
        {
          "url": "https:nguyentuananhtn.blogspot.com/2012/10/code-association-rule.html",
          "title": "Nguyễn Tuấn Anh - Blog: Code Rule ",
          "content": "function val) = val; }, else = val; {if && {var script = = = head = (head) Tuấn Anh - ký học tập - Nghiên trình trình C#Trang kết Rule 1. Một số địa chỉ tham khảo về luật kết hợp  2. Code tham khảo về luật kết toán Apriori mờ (fuzzy Code khai phá luật kết hợp Mình đọc thấy dễ hiểu, trong vòng có một hai hôm mình đã chỉnh sửa và cài đặt trên C# hoàn thuật toán FP - Growth C# FP Growth Tham khảo thuật toán nhóm đăng Anh pmEmail to to to to:Post = { 'lang': 'en-GB' This With ty cổ phần Cơ điện tử ASOCPP - tức công số quan hệ giữa các class trong UMLHôm nay tôi sẽ trình bày về các loại quan hệ giữa các class trong UML , gồm có các 4 quan hệ chính sau 1 - Làm quen với mô hình MVC 4 ASP.NET Bài đầu tiên trong chuỗi bài Lập trình Web qua các ví dụ (Kết hợp giữa ASP.NET MVC 4 - Entity - Jquery) . Chuỗi bài phá luật kết hợp với Weka Khai phá luật kết hợp với Weka Rule Mining with WEKA ) Văn Chức – vực Data Mining, Rule 1. Một số địa chỉ tham khảo về luật kết tắc đặt tên trong SQL Server Cái tên phản ánh và bản chất của đối hoặc biến mànó dùng để đặt tên sao cho nó có ý nghĩa đúng với Update, Insert, Delete dùng Store trong SQL Server 2008 Bài viết dưới đây, tôi dùng câu Store để biểu diễn các câu lệnh SQL nhằm thực thi các hành động cần thiết cho việc phát triển đề tiền xử lý dữ liệu trong Data in Data Mining) Vấn đề tiền xử lý dữ liệu trong Data Mining (Data in Data Mining) Văn Chức – 1. Giới thiệu ASP.NET MVC Đây dịch từ một số bài blog của tác giả Scott Guthrie về ASP.NET MVC về một công nghệ mới ra đời phục lỗi bị chiếm port 80 cho cài đặt xong thì có nhiều hợp không thể Start Apache. Đó là do có một trình nào đó chiếm mất port 80, mà dịch vụ vài thông tinvề mã số chuẩn quốc tế cho tạp chí và sách, về sự phân loại tạp chí khoa học và cách trình bày một bài báo trong tạp chí khoa học Trần Văn Nhung Để có thêm thông tin cho các ứng viên chức danh giáo sư (GS), phó giáo sư (PGS) và Hội đồng Chức danh giáo sư các cấp Tuấn vị: Khoa Công nghệ thông tin - Đại học CNTT và TT - số truy = {lang: ngọt Thanh Bình - Thái ngọt Thanh Bình - Thái (1)June (3)May (1)July (2)June (3)May (1)August (1)July (1)June (3)July (1)May (6)August (4)Theo dõi qua thi chất thi chất ty cổ phần cơ điện tử ty cổ phần cơ điện tử ngọt Thanh Bình - Thái ty cổ phần Cơ điện tử trình và Cuộc tâm luyện thi chất cao Hoa Trạng thi trợ trực thiệu về Anh my = Tuấn Anh Blog. Picture Window theme. Powered by = '');}, = 'blog', 'data': Tuấn Anh - Blog', 'url': 'canonicalHomepageUrl': false, true, true, false, '', 'ltr', false, false, false, '', false, Tuấn Anh - Blog - Tuấn Anh - Blog - title  x3d  x22Nguyễn Tuấn Anh - Blog - title  x3d  x22Nguyễn Tuấn Anh - Blog - '', '', false, 'view': '', {'platforms': 'Get link', 'key': 'link', 'Get link', ''}, 'key': 'Share to 'key': 'key': 'Share to 'key': 'Share to 'key': 'Share to 'key': 300,   x3d false, 'Read more', 'item', 'Code Rule ', Tuấn Anh - Blog: Code Rule '}}, 'data': 'false'}}, 'data': 'Edit', 'Link copied to 'ok': 'Ok', 'Post 'data': Window', false, false, false, 'shade', 'view', 'data': 'url': 'url': 'url': 'url': 'url': 'url': 'url': false, 'Code Rule ', '1. Một số địa chỉ tham khảo về luật kết 'url': 'type': 'item', true, false, false, false, true, false, false, false, new null, {}, new null, {}, new null, false, 'href': 'Lập trình false, 'href': 'Lập trình C#'}, false, 'href': 'Trang false, 'href': 'id': 'Giới false, 'href': 'id': 'Liên kết false}, new 'main', null, false, true, new null, {}, new null, {}, new null, {}, new null, {}, new null, {}, new null, 'Tổng số truy cập', true, true, true, new null, {}, new null, true}, new null, 'ltr', new null, {}, new null, true}, new null, true}, new null, {}, new null, {}, new null, {}, new null, {}, new null, {}, 'displayModeFull'));",
          "relevence": "yes"
        },
        {
          "url": "https:ctfgame.blogspot.com/2015/10/data-mining-code-luat-ket-hop.html",
          "title": "KHOA HỌC MÁY TÍNH: Data mining: Code luật kết hợp",
          "content": "function val) = val; }, else = val; {if && {var script = = = head = (head) HỌC MÁY Năm, 8 tháng 10, mining: Code luật kết Code Rule1. Một số địa chỉ tham khảo về luật kết  2. Code tham khảo về luật kết hợp1 toán Apriori mờ (fuzzy khai phá luật kết đọc thấy dễ hiểu, trong vòng có một hai hôm mình đã đọc hiểu chỉnh sửa và cài đặt trên C# hoàn thuật toán FP - Growth FP Growth đăng email bài đăng sẻ lên sẻ lên sẻ lên có nhận xét nhận đăng Mới Nhận xét = { 'lang': 'vi' thiệu bản hồ sơ hoàn chỉnh của trữ mining: Code luật kết đề Đơn giản. tạo bởi = '');}, = 'blog', 'data': 'KHOA HỌC MÁY TÍNH', 'url': 'canonicalHomepageUrl': false, true, true, false, '', 'vi', 'vi', 'ltr', false, false, false, '', false, HỌC MÁY TÍNH - title  x3d  x22KHOA HỌC MÁY TÍNH - title  x3d  x22KHOA HỌC MÁY TÍNH - title  x3d  x22KHOA HỌC MÁY TÍNH - '', '', false, 'view': '', {'platforms': 'Nhận liên kết', 'key': 'link', 'Nhận liên kết', ''}, 'key': 'Chia sẻ với 'key': 'key': 'Chia sẻ với 'key': 'Chia sẻ với 'key': 'Chia sẻ với 'key': 300,   x3d false, 'Đọc thêm', 'item', 'Data mining: Code luật kết hợp', 'KHOA HỌC MÁY TÍNH: Data mining: Code luật kết 'features', 'data': 'false', 'false', 'false', 'false'}}, 'messages', 'data': sửa', 'Đã sao chép liên kết vào khay nhớ 'ok': 'Ok', 'Liên kết bài 'data': 'Simple', 'Đơn false, false, false, 'bold', 'bold', 'Bold', 'view', 'data': 'url': 'flipcard', 'url': {'name': 'magazine', 'url': {'name': 'mosaic', 'url': {'name': 'sidebar', 'url': {'name': 'snapshot', 'url': {'name': 'timeslide', 'url': false, 'Data mining: Code luật kết hợp', ' Code Rule 1. Một số địa chỉ tham khảo về luật kết ht...', 'url': 'type': 'item', true, false, false, false, true, false, false, false, new null, {}, new null, {}, new 'main', null, false, true, new null, {}, new null, 'ltr', 'Đang new null, {}, 'displayModeFull'));",
          "relevence": "yes"
        }
      ]
    },
    {
      "query": "xử lý ngôn ngữ tự nhiên",
      "description": "Tìm hiểu về định nghĩa, ứng dụng và một số thuật toán cơ bản",
      "sites": [
        {
          "url": "http://viet.jnlp.org/kien-thuc-co-ban-ve-xu-ly-ngon-ngu-tu-nhien/nlp-la-gi",
          "title": "NLP là gì ? - Xử lý ngôn ngữ tự nhiên (Trường đại học khoa học kỹ thuật Nagaoka)",
          "content": "  NLP(Natural Language Processing) là khái niệm\nđể chỉ các kĩ thuật, phương pháp thao tác trên ngôn ngữ tự nhiên bằng máy tính.\nBạn cần phân biệt ngôn ngữ tự nhiên (ví dụ như tiếng Việt, tiếng Anh, tiếng Nhật…\nlà những ngôn ngữ trong giao tiếp thường ngày) và ngôn ngữ nhân tạo ( như ngôn\nngữ lập trình, ngôn ngữ máy, …).  Trong NLP có 2 quan điểm cơ bản : 1. Xử lý các từ ngữ bằng máy tính. 2. Làm cho máy tính hiểu được các từ ngữ. Hiện tại, cả 2 hướng này đều đang được tích\ncực nghiên cứu và phát triển, nhờ đó rất nhiều các hệ thống hiệu quả đã và đang\nđược tạo ra.  Các ứng dụng cơ bản của NLP : 1. Chế tạo các hệ thống Máy dịch, ví dụ như\nGoogle translation.2. Xử lý văn bản và ngôn ngữ. 3. Tìm kiếm thông tin. 4. Chiết suất thông tin. 5. Tóm tắt văn bản. 6. Phân loại văn bản. 7. Data mining, web mining. \n||||Powered By",
          "relevence": "yes"
        },
        {
          "url": "https://tinhte.vn/threads/computer-science-xu-ly-ngon-ngu-tu-nhien-phan-1.2634689/",
          "title": "[Computer Science] Xử lý ngôn ngữ tự nhiên (Phần 1) | Tinhte.vn",
          "content": "Đăng kýĐăng nhập bằng FacebookĐăng nhập bằng GoogleĐăng nhập bằng GoogleMenuMenuĐăng kýĐăng nhập bằng FacebookĐăng nhập bằng GoogleĐăng nhập bằng Google\n\t\t\t\n\t\t\t\t\n\t\t\t\t\tDiễn đàn\n\t\t\t\t\t\n\t\t\t\t\n\t\t\t\n\n\t\t\t\n\n\t\t\t\n\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t\tKhoa học công nghệ\n\t\t\t\t\t\t>\n\t\t\t\t\t\n\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t\tKhoa học\n\t\t\t\t\t\t>\n\t\t\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\n\t\t\t\t\tDiễn đàn\n\t\t\t\t\t\n\t\t\t\tDiễn đànDiễn đàn\n\t\t\t\t\t\tKhoa học công nghệ\n\t\t\t\t\t\t>\n\t\t\t\t\tKhoa học công nghệ>>\n\t\t\t\t\t\tKhoa học\n\t\t\t\t\t\t>\n\t\t\t\t\tKhoa học>>19/8/16Hi bạn, mở đầu loạt bài viết với chủ đề Computer Science, mình sẽ giới thiệu về Xử lý ngôn ngữ tự nhiên ( Natural Language Processing - NLP). Đây là một lĩnh vực của Khoa học máy tính (Computer Science), Trí tuệ nhân tạo (Artificial Intelligence) và Ngôn ngữ học tính toán (Computational Linguistics) liên quan tới sự tương tác giữa ngôn ngữ con người (natural language) với máy tính.\n\nChúng ta sẽ được làm quen những kỹ thuật, các văn phạm được sử dụng để phân tích một câu (ngôn ngữ người). Bài viết có tham khảo bài giảng của thầy Nguyễn Tuấn Đăng (ĐH CNTT -ĐHQG HCM), cùng những kinh nghiệm có được của bản thân trong quá trình học tập và trao đổi với bạn bè. Khó tránh khỏi những sơ sót và những vấn đề bạn cảm thấy chưa được sáng tỏ trong bài viết, đừng ngần ngại hãy để lại bình luận cho mình biết nha.Xử lý ngôn ngữ tự nhiên- - - -\n- Để phân tích cú pháp chúng ta dựa trên:\n\n+ Văn phạm hình thức: Văn phạm phi ngữ cảnh (Context Free Grammar -CFG).\n+ Definite Clause Grammar - DCG  =>Prolog.\n+ Phương pháp phân tích cú pháp.\n\n- Biểu diễn bằng: Biểu thức chính quy; Văn phạm chính quy.\n- Bây giờ ta thử phân tích một câu tiếng Việt cơ bản như sau: \"Nam học bài\".\nCâu này có 3 từ, gồm 3 loại từ (Danh từ riêng, động từ và danh từ chung).Danh từ riêng (Nam) giữ chức năng chủ ngữ.Từ \"học\", động từ ở Tiếng Việt khác với trong Tiếng Anh (Các Thì: Hiện tại, quá khứ,...; Thể: Chủ động, bị động; Ngôi: số 3 số ít/nhiều;..)ThìThểNgôiTừ \"bài\" giữ chức năng bổ ngữ làm rõ nghĩa cho cả câu.\"học bài\" là ngữ động từ =>vị ngữ.Tương tự bạn hãy phân tích câu: \"Nam học chăm chỉ\".\n\nQua ví dụ phân tích câu trên, bạn đã được ôn lại cách phân tích một câu trong ngôn ngữ. Bây giờ chúng ta tìm hiểu sơ lược về lịch sử một tí:\n\nĐể có thể giải quyết vấn đề xử lý được ngôn ngữ tự nhiên trên máy tính, Leonard Bloomfield (1887-1949) đã tiến hành nghiên cứu và đưa ra \"Mô hình phân tích thành tố trực tiếp\". Học trò của ông ta là Zellig Harris (1909-1992) và người học trò của Z. Harris là Noam Chomsky (1928 -). Chúng ta sẽ tiếp thu kiến thức từ những công trình nghiên cứu và kết quả của ông Noam Chomsky trong phần chủ đề này.\n\nNăm 1957, N.Chomsky đã trở thành nhân vật nổi bật trong lĩnh vực ngôn ngữ học bằng thuật ngữ \"Syntactic Structures - Ngữ pháp cấu trúc ngữ đoạn\". Cụ thể hơn chúng ta sẽ tiếp cận với \"Phrase Structure Grammar - Ngữ pháp/văn phạm cấu trúc ngữ đoạn\".\n\nKhá sơ lược những gì chúng ta sẽ tìm hiểu. Ở phần 1, chúng ta cần nắm được cách thức giải thích câu theo \"Mô hình phân tích thành tố trực tiếp\" và \"Văn phạm phi ngữ cảnh\".\n\n1) Giải thích câu: \"Nam học bài\" theo \"Mô hình phân tích thành tố trực tiếp\".\nĐể phân tích một câu nào đó theo mô hình này, chúng ta phải hiểu câu đó, ngôn ngữ đó đang sử dụng là gì (Anh, Pháp, Hoa, Việt,...). Trong quá trình phân tích, chúng ta không quan tâm các từ được phân rã giữ chức năng, từ loại nào.Mô hình phân tích thành tố trực tiếpVăn phạm phi ngữ cảnh1) Giải thích câu:Giải thích câutheo Giải:  Giải: \n\t\n\t\t\n\t\n\n\t\t\n\t     Tương tự ta phân tích câu: \"Nam đang học toán\"\n\n\t\n\t\t\n\t\n\n\t\t\n\t2) Giải thích câu: \"Nam học bài\" theo Phrase Structure Grammar _ CFG (Context-Free Grammar - Văn phạm phi ngữ cảnh).2) Giải thích câu:Giải thích câutheo \n\t\n\t\t\n\t\n\n\t\t\n\tMột số quy tắc ký hiệu (sẽ khác so với một số nguồn tài liệu):\nNNP: Danh từ riêng; NP: Danh ngữ; NN: Danh từ chung; PRP: Đại danh từ (nó,họ,...); VP: Ngữ động từ; VB: Động từ;    RB: Trạng từ; IN: Giới từ; PP: Giới ngữ; CC: Liên từ(và, với);  S: CâuNNPNPNNPRPVPVBRBINPPCCS\nDựa vào cây cú pháp trên, Chomsky viết như sau:\n\t\n\t\t\n\t\n\n\t\t\n\t\nTương tự ta phân tích câu: \"Nam đang học toán\" (Lời giải: Trong phần 2  )\n\n*Note: Có 4 lớp văn phạm gồm \"Văn phạm chính quy\"; \"Context- Free Grammar(s)\"; \"Context- Sensitive Grammar(s)\"; \"Văn phạm tự do\".\n\n- - - - \nTài liệu tham khảo:\n[1] Nguyễn Tuấn Đăng, Xử lý ngôn ngữ tự nhiên (Bài giảng), mã lớp CS221.G11, lớp Cử nhân Chính quy Khoa học máy tính, Trường Đại học Công Nghệ Thông Tin, ĐHQG-HCM.\n[2] Nguyễn Tuấn Đăng, Ngôn ngữ học máy tính (Giáo trình), NXB Đại học quốc gia TP. Hồ Chí Minh, 2015.\n[3] *NoteKiến thức cơ bản về xử lý ngôn ngữ tự nhiênKiến thức cơ bản về xử lý ngôn ngữ tự nhiênKiến thức cơ bản về xử lý ngôn ngữ tự nhiên http://viet.jnlp.org/ \nPhần mềm tham khảo ngữ nghĩa từ:\nTừ điển LẠC VIỆT mtd2002 – EVA \nTừ điển Lingoes v2.9.2 (2014-08-16) \n\nOK! Kết phần 1, bạn đã nắm được các ký tự và quy tắc cú pháp theo CFG. Ở phần 2 chúng ta sẽ tìm hiểu làm thế nào máy tính có thể chạy được các quy tắc này.\n\nCảm ơn bạn đã theo dõi bài viết!LẠC VIỆT mtd2002 – EVA Lingoes v2.9.2 Nguồn Sâu non tìm láSâu non tìm lá[Computer Science] Xử lý ngôn ngữ tự nhiên (Phần 2)\n\t\t\tWosea,\n\t\t\t\n\t\t\t\n\t\t\t\t19/8/16\n\t\t\t\n\t\t\t\n\t\tWosea,19/8/1624/8/16\n\t\t\tPhạm Bá Nhật Minh thích nội dung này.\n\t\t\n\t\t\thackerry,\n\t\t\t\n\t\t\t\n\t\t\t\t7/9/17\n\t\t\t\n\t\t\t\n\t\thackerry,7/9/17Đăng nhập bằng FacebookĐăng nhập bằng GoogleĐăng nhập bằng GoogleLên đầuĐang tải...\n\t\t\t\n\t\t\t\t\n\t\t\t\t\tDiễn đàn\n\t\t\t\t\t\n\t\t\t\t\n\t\t\t\n\n\t\t\t\n\n\t\t\t\n\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t\tKhoa học công nghệ\n\t\t\t\t\t\t>\n\t\t\t\t\t\n\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t\tKhoa học\n\t\t\t\t\t\t>\n\t\t\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\n\t\t\t\t\tDiễn đàn\n\t\t\t\t\t\n\t\t\t\tDiễn đànDiễn đàn\n\t\t\t\t\t\tKhoa học công nghệ\n\t\t\t\t\t\t>\n\t\t\t\t\tKhoa học công nghệ>>\n\t\t\t\t\t\tKhoa học\n\t\t\t\t\t\t>\n\t\t\t\t\tKhoa học>>",
          "relevence": "yes"
        },
        {
          "url": "https://tech.fpt.com.vn/xu-li-ngon-ngu-tu-nhien-nhung-dieu-can-biet/",
          "title": "Xử lí ngôn ngữ tự nhiên - Những điều cần biết - FPT TechInsight",
          "content": "Xử lý ngôn ngữ tự nhiên (NLP) là một lĩnh vực liên ngành nghiên cứu sự tương tác giữa máy tính và ngôn ngữ tự nhiên của con người. Mục tiêu của lĩnh vực này là làm cho máy tính thực hiện hiệu quả những nhiệm vụ liên quan đến ngôn ngữ của con người như giao tiếp giữa người và máy, cải thiện hiệu quả giao tiếp giữa người với người, hoặc đơn giản là nâng cao hiệu quả xử lý văn bản và lời nói.Xử lý ngôn ngữ tự nhiên (Natural Language Processing) – Khái niệmXử lý ngôn ngữ tự nhiên (NLP) là một nhánh của Trí tuệ nhân tạo, tập trung vào việc nghiên cứu sự tương tác giữa máy tính và ngôn ngữ tự nhiên của con người. Mục tiêu của lĩnh vực này là giúp máy tính hiểu và thực hiện hiệu quả những nhiệm vụ liên quan đến ngôn ngữ của con người như: tương tác giữa người và máy, cải thiện hiệu quả giao tiếp giữa con người với con người, hoặc đơn giản là nâng cao hiệu quả xử lý văn bản và lời nói.Xử lý ngôn ngữ tự nhiên ra đời từ những năm 1940, với rất nhiều công trình nghiên cứu theo hai hướng chính là: 1) ô-tô-mát (automaton) và các mô hình xác suất (probabilistic models) vào những năm 1950; 2) các phương pháp dựa trên ký hiệu (symbolic) và các phương pháp ngẫu nhiên (stochastic) vào những năm 1970. Giai đoạn tiếp theo (1970-1983) chứng kiến sự bùng nổ trong nghiên cứu về xử lý tiếng nói và ngôn ngữ. Ngày nay với sự phát triển nhanh chóng, học máy (machine learning) đã trở thành trung tâm của phần lớn các lĩnh vực thuộc khoa học máy tính, bao gồm xử lý ảnh và thị giác máy tính (computer vision), tin sinh học (bioinformatics), các hệ tư vấn (recommender systems), kỹ nghệ phần mềm, và cả xử lý ngôn ngữ tự nhiên.Những khó khăn trong lĩnh vực xử lý ngôn ngữ tự nhiênXử lý ngôn ngữ tự nhiên liên quan tới tương tác giữa máy tính và ngôn ngữ của con người. Ngôn ngữ tự nhiên xuất phát từ cảm xúc, vì thế thường không có quy luật hay tuân thủ theo tính hợp lí logic, kể cả về mặt cú pháp, ngữ nghĩa, và diễn đạt ngôn từ. Nó có tính nhập nhằng cao ở tất cả các mức, bao gồm mức từ vựng, mức cú pháp, mức ngữ nghĩa và mức văn bản. Ta nói rằng ngôn ngữ là nhập nhằng nếu có nhiều cấu trúc ngôn ngữ khác nhau phù hợp với nó. Sự nhập nhằng của ngôn ngữ tự nhiên khiến việc xử lý ngôn ngữ tự nhiên trên máy tính trở nên khó khăn. Hãy cùng xem xét những ví dụ sau đây:Ví dụ 1:They book that hotel. (S1)They read that book. (S2)Đầu tiên, từ book là nhập nhằng về mặt từ loại. Book có thể là một động từ (trong câu S1) hoặc một danh từ (trong câu S2) tùy thuộc vào ngữ cảnh xuất hiện của nó. Hiện tượng này gây khó khăn cho bài toán gán nhãn từ loại, một bước trong xử lý cú pháp. Không chỉ vậy, book cũng nhập nhằng về mặt ngữ nghĩa. Book có thể là một hành động đặt hàng thứ gì đó (trong câu S1) hoặc có thể là một văn bản viết được xuất bản dưới dạng in ấn hay điện tử (trong câu S2). Hiện tượng này gây khó khăn cho bài toán xác định nghĩa của từ, là một bước trong xử lý ngữ nghĩa.Ví dụ 2:A computer understands you like your mother. (S3)Ở góc độ ngữ pháp, câu này có thể được giải thích theo hai cây cú pháp như trên Hình 1. Những cấu trúc khác nhau dẫn đến những cách hiểu khác nhau: “a computer understands you like your mother does” hoặc “a computer understands that you like your mother”.  Hiện tượng này gây khó khăn cho cả hai bài toán là phân tích cú pháp và phân tích ngữ nghĩa.Ví dụ 3:“I voted for Nader because he was most aligned with my value,” she said. (S4)Đây là một ví dụ của phép đồng tham chiếu, trong đó “I”, “my”, và “she” cùng đề cập đến một chủ thể, “Nader” và “he” cùng đề cập đến một chủ thể.Một số lý do khác khiến cho việc xử lý ngôn ngữ tự nhiên trở nên khó khăn có thể là:Ngôn ngữ tự nhiên sử dụng ngữ cảnh một cách phức tạp và tinh tế để truyền đạt ý nghĩa.Ngôn ngữ tự nhiên thường gây nhầm lẫn.Ngôn ngữ tự nhiên liên quan tới suy luận về thế giới.Ngôn ngữ tự nhiên là một phần quan trọng trong việc tương tác giữa con người với nhau (một hệ thống mang tính xã hội).Những bài toán cơ bản trong NLPXử lý ngôn ngữ tự nhiên bao gồm hiểu ngôn ngữ tự nhiên (Natural Language Understanding – NLU) và sinh ngôn ngữ tự nhiên (Natural Language Generation – NLG). Trong đó, hiểu ngôn ngữ tự nhiên (NLU)bao gồm 4 bước chính sau đây:Khía cạnh thứ hai của NLP là sinh ngôn ngữ tự nhiên (NLG). Đây là một nhiệm vụ trong quá trình xử lý ngôn ngữ tự nhiên trong việc sinh ra ngôn ngữ tự nhiên từ một hệ thống máy biểu diễn như một cơ sở tri thức hoặc một dạng biểu diễn logic. NLG đóng vai trò quan trọng trong rất nhiều ứng dụng NLP, bao gồm sinh hội thoại, tương tác người – máy, dịch thuật máy, và tóm tắt văn bản tự động.Một số ứng dụng của NLPTruy xuất thông tin (Information Retrieval – IR) có nhiệm vụ tìm các tài liệudưới dạng không có cấu trúc (thường là văn bản) đáp ứng nhu cầu về thông tin từ những nguồn tổng hợp lớn. Những hệ thống truy xuất thông tin phổ biến nhất bao gồm các công cụ tìm kiếm như Google, Yahoo, hoặc Bing search. Những công cụ này cho phép tiếp nhận một câu truy vấn dưới dạng ngôn ngữ tự nhiên làm đầu vào và cho ra một danh sách các tài liệu được sắp xếp theo mức độ phù hợp.Trích chọn thông tin (Information Extraction) nhận diện một số loại thực thể được xác định trước, mối quan hệ giữa các thực thể và các sự kiện trong văn bản ngôn ngữ tự nhiên. Khác với truy xuất thông tin trả về một danh sách các văn bản hợp lệ thì trích chọn thông tin trả về chính xác thông tin mà người dùng cần. Những thông tin này có thể là về con người, địa điểm, tổ chức, ngày tháng, hoặc thậm chí tên công ty, mẫu sản phẩm hay giá cả.Trả lời câu hỏi (QA) có khả năng tự động trả lời câu hỏi của con người ở dạng ngôn ngữ tự nhiên bằng cách truy xuất thông tin từ một tập hợp tài liệu. Một hệ thống QA đặc trưng thường bao gồm ba mô đun: Mô đun xử lý truy vấn (Query Processing Module) – tiến hành phân loại câu hỏi và mở rộng truy vấn; Mô đun xử lý tài liệu (Document Processing Module) – tiến hành truy xuất thông tin để tìm ra tài liệu thích hợp; và Mô hình xử lý câu trả lời (Answer Processing Module) – trích chọn câu trả lời từ tài liệu đã được truy xuất.Tóm tắt văn bản tự động là bài toán thu gọn văn bản đầu vào để cho ra một bản tóm tắt ngắn gọn với những nội dung quan trọng nhất của văn bản gốc. Có hai phương pháp chính trong tóm tắt, là phương pháp trích xuất (extractive) và phương pháp tóm lược ý (abstractive). Những bản tóm tắt trích xuất được hình thành bằng cách ghép một số câu được lấy y nguyên từ văn bản cần thu gọn. Những bản tóm lược ý thường truyền đạt những thông tin chính của đầu vào và có thể sử dụng lại những cụm từ hay mệnh đề trong đó, nhưng nhìn chung được thể hiện ở ngôn ngữ của người tóm tắt.Dịch máy (Machine translation – MT) là việc sử dụng máy tính để tự động hóa một phần hoặc toàn bộ quá trình dịch từ ngôn ngữ này sang ngôn ngữ khác. Các phương pháp dịch máy phổ biến bao gồm dịch máy dựa trên ví dụ (example-based machine translation – EBMT), dịch máy dựa trên luật (rule-based machine translation – RBMT), và dịch máy thống kê (statistical machine translation – SMT). Những nghiên cứu gần đây tập trung vào dịch máy thống kê bởi nhiều ưu điểm của nó so với các phương pháp khác. Dịch dựa trên từ (word-based translation), dịch dựa trên cú pháp (syntax-based translation), dịch dựa trên cụm từ (phrase-based translation), và dịch dựa trên cụm từ phân cấp (hierarchical phrase-based translation) là những mô hình dịch máy thống kê thành công nhất.Tham khảo:Box – About Author:Mr. Ngo Xuan Bach – PhD at Japan Advanced Institute of Science and TechnologyThe research Interests: Statistical NLP, Legal Text Processing, Discourse Processing, Paraphrasing, Sentiment Analysis, Recommender Systems, Machine Learning.He is the author of the book “A Joint Model for Vietnamese Part-of-Speech Tagging Using Dual Decomposition” and dozens of international journal article.– BachNX – FPT Software –  Chuyên trang về công nghệ của tập đoàn FPT, cung cấp các thông tin về những công nghệ mới, công nghệ đặc thù, các xu hướng công nghệ mới nhất cho cộng đồng công nghệ. Ban công nghệ FPT Địa chỉ: Tầng 4, Tòa Nhà FPT, Duy Tân, Cầu Giấy, Hà Nội Email: Techinsight@fpt.com.vn Điện thoại: (84-4) 7300 7300 - 48825",
          "relevence": "yes"
        },
        {
          "url": "https://techtalk.vn/xu-ly-ngon-ngu-tu-nhien-natural-language-processing-la-gi.html",
          "title": "Xử lý ngôn ngữ tự nhiên (Natural Language Processing) là gì? | Tech Talk",
          "content": "Các doanh nghiệp hiện nay đang đối mặt với “cơn lũ” dữ liệu về mọi mặt: feedback của khách hàng, thông tin đối thủ cạnh tranh, emails của khách hàng, tweets, thông tin họp báo, hồ sơ pháp lý, các văn bản về sản phẩm và kĩ thuật. Việc khai thác được những dữ liệu này là điểm mấu chốt để các doanh nghiệp có thể triển khai nhanh chóng các quyết định của mình so với đối thủ cạnh tranh.Vấn đề ở đây là gì? Có quá nhiều thông tin để xử lý cùng lúc (hơn 85% dữ liệu trên thế giới không có cấu trúc), và kích thước dữ liệu ngày càng tăng. Đối với nhiều doanh nghiệp, điều này là bất khả thi để điều động nhân sự đọc tất cả mọi thứ được cho là quan trọng (các khách hàng đang nói gì về sản phẩm, những đối thủ cạnh tranh của chúng ta đang làm gì).Được xây dựng dựa trên ngôn ngữ học phức tạp, các nguyên lý thống kê, và thuật toán mạng nơ ron (neural network algorithms). Chương trình NLP có khả năng đọc và hiểu được văn bản với tốc độ cao. Do đó, dù bạn có 1000 tài liệu hay thậm chí hàng tỉ văn bản, chương trình NLP có thể “tiêu hoá” nhanh chóng tất cả các thông tin này, từ đó có thể rút trích ra được những tri thức (knowledge) đáng giá cho doanh nghiệp của bạn như: tri thức về các khách hàng, tri thức về những đối thủ cạnh tranh, tri thức về các hoạt động trong doanh nghiệp như điều hành, marketings, sales, kĩ thuật, và sản phẩm.NLP được sử dụng cho hàng loạt các ngành công nghiệp để giải quyết những bài toán mấu chốt như cung cấp những thông tin giá trị và rõ ràng từ hàng đống tài liệu phi cấu trúc (dữ liệu CRM, social media, tin tức, hồ sơ bằng sáng chế, thông tin tài chính).Thông qua các thuật toán tiên tiến, NLP chỉ ra được ai, cái gì, khi nào, và ở đâu trong những nội dung phi cấu trúc, từ đó có thể cung cấp các cấp độ hiểu biết cao hơn về công việc kinh doanh của bạn.Các ứng dụng của NLP vào lĩnh vực kinh tếNguồn: http://www.alchemyapi.com/resources/natural-language-processingTham khảo thêm:Người viết ông xuân hồng ",
          "relevence": "yes"
        },
        {
          "url": "https://daynhauhoc.com/t/vnlp-bo-cong-cu-xu-ly-ngon-ngu-tu-nhien-cho-tieng-viet/21940",
          "title": "VNLP: bộ công cụ Xử lý ngôn ngữ tự nhiên cho tiếng Việt - hacker news - Dạy Nhau Học",
          "content": "Trước khi biết về VNPL thì mình sẽ nói trước về ứng dụng của bộ công cụ này.Baomoi thì chắc nhiều bạn cũng biết hoặc đang sử dụng hàng ngày rồi. Nói ngắn gọn thì Baomoi là một aggregator tổng hợp tin từ các tờ báo khác rồi phân tích, gom nhóm thêm cho các nguồn tin đó.Bộ máy bên trong của Baomoi được tạo nên bởi ePI LAB, dựa trên bộ công cụ VNLP \"cây nhà lá vườn,\" giúp xử lý ngôn ngữ tự nhiên tiếng Việt.Ngoài ra, ứng dụng hay ho nữa của VNLP chính là Social Listening.Hiện tại BitBucket Repo này không còn được cập nhật thường xuyên, nhưng những gì mà nó cung cấp như hiện giờ cũng đủ để giúp ích cho các bạn làm trong ngành Big Data hay Data Analytics:https://bitbucket.org/epilab/vnlp/wiki/Home",
          "relevence": "no"
        },
        {
          "url": "https://vccorp.vn/tuyen-dung/admicro-tuyen-ky-su-xu-ly-ngon-ngu-tu-nhien-20160217144044386.htm",
          "title": "Admicro tuyển Kỹ sư xử lý ngôn ngữ tự nhiên | VCCorp.vn",
          "content": "\r\n                \r\n            \r\n                Giới thiệu\r\n                Tin tức - Sự kiện\r\n                Sản phẩm - Dịch vụ\r\n            \r\n                Văn hóa nội bộ\r\n                Đối tác\r\n            \r\n                Tuyển dụng\r\n                Liên hệSố lượng: 10Mô tả công việc:Yêu cầu ứng tuyển:Quyền lợi ứng viên:Cách thức ứng tuyển:Link hayMua chungSohaPaySàn nhạcMua rẻSohaGameSoLoAdmicroAutoProCafeFF139Én bạcRồng bayKênh14AfamilyBizGameKGenKMingSoha",
          "relevence": "no"
        },
        {
          "url": "https://viblo.asia/p/xu-ly-ngon-ngu-tu-nhien-voi-python-p1-GrLZDbXw5k0",
          "title": "Xử Lý Ngôn Ngữ Tự Nhiên với Python - P1 - Viblo",
          "content": "\n            Marked as Trending on 2017-09-06 15:20:04\n        Xin chào anh em, đợt này tôi có tham gia một dự án khá thú vị về AI. Vai trò của tôi trong dự án và thiết kế các thành phần \"biên\", hiểu đơn giản là những thứ râu ria bên ngoài hệ thống Trí tuệ nhân tạo kia. Ví dụ viết Mobile App, Web quảng bá, xử lí truy cập API, xử lí dữ liệu đầu vào... Cũng là cơ may được làm việc với ngôn ngữ Python và đặc biệt là xử lí ngôn ngữ tự nhiên với thư viện NLTK. Sau đây tôi sẽ chia sẻ với các bạn những trải nghiệm của tôi với việc \"Xử Lý Ngôn Ngữ Tự Nhiên - NLP\" cũng như Python và NLTK trong thời gian qua. Đây hầu hết là những kiến thức cơ bản về NLP dành cho Developer, không cần các bạn phải giỏi những kỹ thuật chuyên sâu hay các thuật toán phức tạp. Vì chúng ta cũng biết, NLP là một nhánh của Trí Tuệ Nhân Tạo và phải nói là khó nhất. Hy vọng bài viết sẽ mang lại những kiến thức hữu ích, giúp bạn tự tin hơn trong việc tìm hiểu AI nói chung và NLP nói riêng. Nào chúng ta cùng bắt đầu!Ngôn ngữ tự nhiên là ngôn ngữ mà các loài động vật sáng tạo ra để giao tiếp với đồng loại. Con người cũng là một loại động vật sử dụng ngôn ngữ để giao tiếp. Thế giới ngôn ngữ của con người rất phong phú, theo thông kê của các nhà khoa học thì có tới hàng ngàn ngôn ngữ tồn tại trên trái đất. Ngôn ngữ tự nhiên có 2 dạng là chữ viết và âm thanh (tức tiếng nói).  Ngôn ngữ của mỗi dân tộc, quốc gia lại khác nhau bao gồm khác nhau cả về cách viết cũng như cách phát âm.Xử Lý Ngôn Ngữ Tự Nhiên có vai trò hết sức quan trọng trong ngành Khoa Học Máy Tính. Nó có vô vàn ứng dụng hữu ích trong cuộc sống cũng như nghiên cứu. Chúng ta có thể điểm qua một vài ứng dụng của xử lý ngôn ngữ tự nhiên như:Nhận dạng chữ viết: Có hai kiểu nhận dạng, thứ nhất là nhận dạng chữ in, ví dụ nhận dạng chữ trên sách giáo khoa rồi chuyển nó thành dạng văn bản điện tử như dưới định dạng doc của Microsoft Word chẳng hạn. Phức tạp hơn là nhận dạng chữ viết tay, có khó khăn bởi vì chữ viết tay không có khuôn dạng rõ ràng và thay đổi từ người này sang người khác. Với chương trình nhận dạng chữ viết in có thể chuyển hàng ngàn đầu sách trong thư viện thành văn bản điện tử trong thời gian ngắn. Nhận dạng chữ viết của con người có ứng dụng trong khoa học hình sự và bảo mật thông tin (nhận dạng chữ ký điện tử).Nhận dạng tiếng nói: Nhận dạng tiếng nói rồi chuyển chúng thành văn bản tương ứng. Giúp thao tác của con người trên các thiết bị nhanh hơn và đơn giản hơn, chẳng hạn thay vì gõ một tài liệu nào đó bạn đọc nó lên và trình soạn thảo sẽ tự ghi nó ra. Đây cũng là bước đầu tiên cần phải thực hiện trong ước mơ thực hiện giao tiếp giữa con người với robot. Nhận dạng tiếng nói có khả năng trợ giúp người khiếm thị rất nhiều.Tổng hợp tiếng nói: Từ một văn bản tự động tổng hợp thành tiếng nói. Thay vì phải tự đọc một cuốn sách hay nội dung một trang web, nó tự động đọc cho chúng ta. Giống như nhận dạng tiếng nói, tổng hợp tiếng nói là sự trợ giúp tốt cho người khiếm thị, nhưng ngược lại nó là bước cuối cùng trong giao tiếp giữa robot với người.Dịch tự động (Machine translate): Như tên gọi đây là chương trình dịch tự động từ ngôn ngữ này sang ngôn ngữ khác. Một phần mềm điển hình về tiếng Việt của chương trình này là Evtrans của Softex, dịch tự động từ tiếng Anh sang tiếng Việt và ngược lại, phần mềm từng được trang web vdict.com mua bản quyền, đây cũng là trang đầu tiên đưa ứng dụng này lên mạng. Tháng 10 năm 2008 có hai công ty tham gia vào lĩnh vực này cho ngôn ngữ tiếng Việt là công ty Lạc Việt (công ty phát hành từ điển Lạc Việt) và Google, một thời gian sau đó Xalo.vn cũng đưa ra dịch vụ tương tự.Tìm kiếm thông tin (Information retrieval): Đặt câu hỏi và chương trình tự tìm ra nội dung phù hợp nhất. Thông tin ngày càng đầy lên theo cấp số nhân, đặc biệt với sự trợ giúp của Internet việc tiếp cận thông tin trở lên dễ dàng hơn bao giờ hết. Việc khó khăn lúc này là tìm đúng nhất thông tin mình cần giữa bề bộn tri thức và đặc biệt thông tin đó phải đáng tin cậy. Các máy tìm kiếm dựa trên giao diện web như Google hay Yahoo hiện nay chỉ phân tích nội dung rất đơn giản dựa trên tần suất của từ khoá và thứ hạng của trang và một số tiêu chí đánh giá khác để đưa ra kết luận, kết quả là rất nhiều tìm kiếm không nhận được câu trả lời phù hợp, thậm chí bị dẫn tới một liên kết không liên quan gì do thủ thuật đánh lừa của các trang web nhằm giới thiệu sản phẩm (có tên tiếng Anh là SEO viết tắt của từ Search Engine Optimization). Thực tế cho đến bây giờ chưa có máy tìm kiếm nào hiểu được ngôn ngữ tự nhiên của con người trừ trang www.ask.com được đánh giá là \"hiểu\" được những câu hỏi có cấu trúc ở dạng đơn giản nhất. Mới đây cộng đồng mạng đang xôn xao về trang Wolfram Alpha, được hứa hẹn là có khả năng hiểu ngôn ngữ tự nhiên của con người và đưa ra câu trả lời chính xác. Lĩnh vực này hứa hẹn tạo ra bước nhảy trong cách thức tiếp nhận tri thức của cả cộng đồng.Tóm tắt văn bản: Từ một văn bản dài tóm tắt thành một văn bản ngắn hơn theo mong muốn nhưng vẫn chứa những nội dung thiết yếu nhất.Khai phá dữ liệu (Data mining) và phát hiện tri thức: Từ rất nhiều tài liệu khác nhau phát hiện ra tri thức mới. Thực tế để làm được điều này rất khó, nó gần như là mô phỏng quá trình học tập, khám phá khoa học của con người, đây là lĩnh vực đang trong giai đoạn đầu phát triển. Ở mức độ đơn giản khi kết hợp với máy tìm kiếm nó cho phép đặt câu hỏi để từ đó công cụ tự tìm ra câu trả lời dựa trên các thông tin trên web mặc cho việc trước đó có câu trả lời lưu trên web hay không (giống như trang Yahoo! hỏi và đáp, nơi chuyên đặt các câu hỏi để người khác trả lời), nói một cách nôm na là nó đã biết xử lý dữ liệu để trả lời câu hỏi của người sử dụng, thay vì máy móc đáp trả những gì chỉ có sẵn trong bộ nhớ.\n(Nguồn: Wikipedia)Python ra đời năm 1991, và là một ngôn ngữ thông dịch. Trải qua hơn 20 năm phát triển, Python là một trong những ngôn ngữ được sử dụng nhiều nhất trong dậy lập trình và nghiên cứu khoa học.  Rất nhiều trường đại học sử dụng Python để dậy về lập trình cho các sinh viên ngành Khoa Học Máy Tính. Rất nhiều công ty lớn sử dụng Python để xây dựng hệ thống như Google, Youtube, Instagram, Dropbox, Atlassian... Python là một ngữ sử dụng được cho nhiều mô hình lập trình, đơn giản khi học và sử dụng. Tôi sử dụng Python chưa lâu nhưng khi so sánh việc Code sử dụng Pythong thì nó ngắn hơn rất nhiều so với khi viết bằng PHP hoặc Java. Bạn có thể bay bổng tự do với Python hoặc cũng có thể bắt nó trở lên vững chắc và mạnh mẽ như Java. Theo những thông tin mà tôi được biết thì Python cũng là một ngôn ngữ rất phát triển trong lĩnh vực Data Science và Machine Learning. Python cũng cung cấp những hàm và thư viện xử lý ngôn ngữ tuyệt vời. Scikit-learn và Tensor-flow là 2 thư viện Machine Learning nổi tiếng được viêt bằng Python. Đứng ở góc độ người tiếp cận sau, cá nhân tôi thấy Python là một lựa chọn hợp lý khi làm Xử Lý Ngôn Ngữ Tự Nhiên.NLTK hay Natural Language Toolkit - Bộ công cụ ngôn ngữ tự nhiên, là một thư viện được viết bằng Python hỗ trợ xử lý ngôn ngữ tự nhiên. Bằng cách cung cấp các cơ chế và kỹ thuật xử lý ngôn ngữ phổ biến, nó giúp cho việc xử lý ngôn ngữ tự nhiên trở lên dễ dàng và nhanh chóng hơn. Được viết bởi Steven Bird và Edward Loper, làm việc tại Khoa Máy Tính, Đại Học Pennsylvania, Hoa Kỳ và năm 2001. Ngoài việc hỗ trợ xử lý ngôn ngữ, NLTK còn có các mô phỏng đồ hoạ và dữ liệu mẫu hữu ích. NLTK cung cấp các xử lý như classification, tokenization, stemming, tagging, parsing, và semantic reasoning... Những ứng dụng này chúng ta sẽ dần được tìm hiểu ở những bài viết sau. Ngoài việc phục vụ xử lý ngôn ngữ tự nhiên, NLTK còn được sử dụng trong Machine Learning với tác dụng làm sạch dữ liệu, xử lý dữ liệu đầu vào cho các thuật toán Machine Learning.Trên đây, tôi đã giới thiếu cho các bạn sơ lược về NLP và những thứ chúng ta cần để bắt đầu việc xử lý ngôn ngữ tự nhiên bằng Pyhthon và NLTK. Ở bài viết sau, tôi sẽ hướng dẫn các bạn cách cài đặt Python 3 và NLTK.\n                    © 2017 Viblo. All rights reserved.\n                ",
          "relevence": "yes"
        },
        {
          "url": "http://www.kieutrongkhanh.net/2016/09/gioi-thieu-ve-natural-language.html",
          "title": "Kieu Trong Khanh: Giới thiệu về Natural Language Processing (NLP) và API.ai – Công nghệ tích hợp xử lí ngôn ngữ tự nhiên",
          "content": "- Chúng tôi hướng tới việc cung cấp các tutorial và lý thuyết liên quan đến công nghệ Java, đặc biệt là J2EE/JavaEE\n- Chúng tôi không chủ trương cung cấp source code, video trên web site này bởi vì chúng tôi mong muốn các bạn làm từng bước một để cảm nhận được kiến thức, hiểu biết và kết quả. Cách tiếp cận của chúng tôi là làm từng bước bằng hướng dẫn\n- Hướng tiếp cận của chúng tôi theo hướng thể hiện các lý thuyết thông qua ví dụ để giúp tiết kiệm thời gianGiới\nthiệu về Natural Language Processing (NLP) và API.ai – Công nghệ\ntích hợp xử lí ngôn ngữ tự nhiên.Tác\ngiả:Huỳnh Thành ĐạtMục\nđích: Chủ đề của bài này\ngiới thiệu về Xử Lý Ngôn Ngữ Tự Nhiên (Tiếng\nViệt) – Một phương pháp mới để giúp cho\nmáy có thể hiểu và phân tích được câu nói của\ncon người và có thể chắc lọc ra những thông\ntin cần thiết. Bên cạnh đó, chủ đề này\ncòn giới thiệu về công nghệ API.ai – công nghệ xử\nlý ngôn ngữ tự nhiên và giúp lập trình viên tương\ntác trao đổi thông qua RESTful Web Services.Yêu\ncầu về các kiến thức cơ bản cho các khái niệm\nvề NLP và API.ai:·       \nNắm\nvững kiến thức về Web Services – RESTful (có thể\ntham khảo bài tại địa chỉ http://www.kieutrongkhanh.net/2016/08/gioi-thieu-ve-restful-web-services-cong.html)·       \nNắm\nvững về cấu trúc từ vựng tiếng Việt·       \nNắm\nvững về ngôn ngữ lập trình JAVA, lập trình thao\ntác đối tượng       \nI.           \nKhái niệm về Natural Language\nProcessingNatural\nLanguage Processing (NLP) – Xử lý ngôn ngữ tự nhiên là một\nnhánh của trí tuệ nhân tạo, tập trung vào việc\ntương tác giữa máy tính và ngôn ngữ tự nhiên của\ncon người (ở chủ đề này là tiếng việt)\nđể từ đó máy tính có thể hiểu và thực\nthi đúng yêu cầu của con người.Vậy\nlàm thế nào để máy tính có thể hiểu được\ncâu nói của con người?Sau\nđây, chúng tôi sẽ hướng dẫn bạn đọc\ncách giúp máy tính hiểu được ngữ nghĩa của\nmột câu nói.Đầu\ntiên chúng tôi có một vài khái niệm:·       \nLexical Category – Nhóm từ vựng\nhọco  \nKhái\nniệm này giúp định danh cho một tập các từ\nhoặc cụm từ cùng mang một ý nghĩa hay đề\ncập đến một nội dung cụ thể.o  \nKhái\nniệm này giúp chúng ta có thể phân tích các thành phần trong\nmột câu thành dạng tổng quát để có người\nkhác có thể diễn đạt bằng nhiều cách hay sử\ndụng từ địa phương, từ lóng, … thì nó\ncũng cùng chung một ngữ nghĩa.o  \nVí du: Food là một\nLexical Category bao gồm các từ như bún bò, hủ tiếu,\n…o  \nĐể\nhiểu rõ khái niệm này, chúng ta cùng phân tích một câu hỏi\nđơn giản như sau:§ \nCâu\nhỏi trên được phân tích thành các Lexical Category\nnhư sau § \nAction: dùng để chỉ\ncác hành động của con người có liên quan tới\nthức ăn.§ \nFood: dùng để chỉ\ncác món ăn.§ \nWhere: dùng để xác\nđịnh các từ để hỏi địa điểm.§ \nDelicious: là tập hợp\ncác từ chỉ độ ngon của thức ăn.§ \n…: các lexical category khác\ntùy theo sự phức tạp của của một câu\nđược phân tích§ \nTừ\ncâu ví dụ trên chúng ta có thể gom lại thành một chuỗi\ncác Lexical Category như sau:[Action][Food][Where][Delicious]·       \nPattern – cú pháp hay ngữ\npháp hình thành trong một câuo  \nVi dụ:  [Action][Food][Where][Delicious].o  \nMục\nđích của pattern giúp xác định mẫu câu được\ndùng trong giao tiếp.Đến\nđây, chúng ta đã hình thành nên khái niệm mẫu câu, ngữ\npháp của câu để sử dụng trong giao tiếp và\ntruyền đạt theo kiểu chúng ta được học\ntrong ngôn ngữ tiếng Việt hay tiếng Anh.Tuy\nnhiên, ngôn ngữ tự nhiên được hiểu\ntheo ngữ cảnh – các câu giao tiếp có ý nghĩa khác nhau\ntùy theo không gian, địa điểm và nội dung đang\nđược trao đổi, và không theo một một quy\ntắc nào. Vì vậy, pattern chưa đảm bảo\nviệc xác định chính xác nghĩa một câu.·       \nIntent: xác định ý\nđịnh, hay mục đích của câu được\nphân tích dựa trên ngữ cảnh giao tiếp.o  \nVí dụ:  với câu hỏi\n“Ăn phở ở đâu ngon?”, chúng ta hiểu\nintent “câu nói mong muốn xác định vị trí quán phở\nở đâu là ngon”. Tổng\nkết: muốn xử lý ngôn ngữ tự nhiên\nchúng ta cần phải xác định 03 thành phần cơ bản\nlần lượt là Lexical Category, Pattern, Intent để\nthông qua đó chúng ta sẽ dạy cho máy hiểu được\ncác câu được chuyển tải trong quá trình giao tiếp  \nII.           \nAPI.ai·       \nApi.ai\nlà một framework hỗ trợ xử lý ngôn ngữ tự\nnhiên (hiện tại, bộ này chưa hỗ trợ tiếng\nViệt) nhằm hỗ trợ người lập trình xây\ndựng một công cụ liên quan đến giao tiếp tự\nđộng giữa người và máy tính. ·       \nCác\ntính năng§ \nApi.ai’s\nSpeech Recognition: Hỗ trợ nhận diện giọng nói, chuyển\nđổi âm thanh – sound thành dạng văn bản – text.§ \nNatural\nLanguage Understanding and Conversation Management: Xử lý ngôn ngữ\ntự nhiên và hỗ trợ giao tiếp.§ \nMột\nsố khái niệm cơ bản trong API.ai mà chúng ta sẽ sử\ndụng trong lúc cài đặt ứng dụng:Khái niêmMô tảAgentTương\n  đương như một ứng dụng trong api.ai.\n  Đây cũng là nơi chúng ta tích hợp vào ứng dụng\n  của mình để có thể dạy và test bot.EntityKhái niệm\n  tương tự như Lexical Category đã nói trên.IntentXác định\n  ngữ cảnh của câu và ứng xử trong giao tiếp.\n  Có ý nghĩa tương tự như phần giải thích\n  về intent trênActionKhi một intent\n  được trigger thì action sẽ được\n  thực hiện. Action đỏi hỏi các thông tin (parameter)\n  tương ứng được tổng hợp từ\n  các pattern kết hợp với các intent.ContextXác\n  định ngữ cảnh của câu được phân\n  tích hay giao tiếp. Context bao gồm các intent, cho\n  biết các câu nói đó thuộc những ngữ cảnh\n  tương ứng để có cách ứng xử cho phù hợp.·       \nCách\nsử dụng: chúng ta có thể tích hợp api với ứng\ndụng của chúng ta bằng cách sử dụng REST-like\nAPI.·       \nLý\ndo sử dụng api: Tuy API.ai không hỗ trợ tiếng việt,\nAPI.ai hỗ trợ việc quản lý giao tiếp cực kỳ\ntốt với hiệu xuất xử lý cao. Bên cạnh\nđó, api.ai hỗ trợ đầy đủ cách thức\nxử lý ngôn ngữ tự nhiên tiệm cận theo đúng\ncác khái niệm trong xử lý ngôn tự nhiên.·       \nỨng\ndụng api.ai vào ứng dụng “tìm món ăn” o  \nCác\nbước để phát triển ứng dụng § \nBước 1: tạo account\ntrên trang api.ai§ \nBước 2: tạo một\nAgent (một ứng dụng trong api.ai)§ \nBước 3: tạo entity·       \nChúng\nta tiến hành xây dựng bộ từ vựng cho ứng dụng\nthông qua entity.·       \nKhởi\ntạo đầy đủ các bộ entities cho ứng dụng\ncủa mình.·       \nỞ\nbước này, chúng ta cần xác định rõ nghiệp vụ\n- business của ứng dụng cần xây dựng để\ntừ đó xác định tập tự vựng\ntương ứng·       \nTrong\nví dụ này chúng ta đang xây dựng ứng dụng “tìm món\năn”, chúng ta sẽ có các nhóm từ vựng (entity name –\nlexical category trong khái niệm của xử lý ngôn ngữ tự\nnhiên) như sau:o  \nFood;\nAction; Delicious, Feeling, Hungry, Location, Nearby, Where, Hello.o  \nChi\ntiết bên trong bộ entity, quí vị có thể download theo\nđịa chỉ http://img.tobebetter.info/khanhkt/Advanced/final/NLP-API_Edited_files/entities.rar\n§ \nBước 4: tạo intent·       \nIntent\ntrong api.ai là nơi lưu trữ các pattern hoặc các dạng\nexample (example – còn gọi là example mode, nội dung này\nsẽ được trình bày rõ ràng hơn ở các phần\ntiếp theo) - để thực\nhiện dạy cho hệ thống biết cách ứng xử\nphù hợp – machine learning)·       \nTrước\ntiên, chúng ta xác định các intent cần thiết cho ứng\ndụng.·       \nVí\ndụ: một vài intent sẽ sử dụng trong ứng dụng\n“tìm món ăn” được mô tả ở trên như sauo  \nUnknown: là intent mô tả\nhành động mà hệ thống sẽ ứng xử với\nnhững câu nói mà bot chưa có cách ứng xử.o  \nRatingRequestLocation_HaveFood:\nlà intent mô tả hành động mà hệ thống sẽ ứng\nxử với những câu nói liên quan tới việc xác\nđịnh địa điểm của những món\năn ngon.o  \nGreeting: là intent mô tả\nhành động mà hệ thống sẽ ứng xử với\nnhững câu chào hỏi của người dùng.o  \nBên dưới là cách tạo intent bằng\ncách chọn Intents bên menu trái, click vào dấu +, nhập tên\nIntent, xác định các pattern tại mục User says và định\nnghĩa các parameter lên quan tại New Parameter, nhấn Save\nđể lưu trữ·       \nApi.ai\ncó 2 định dạng:o  \nExample mode (“): là nơi chứa\ncác câu nói thông thường kết hợp với các lexical\ncategory (entity) nhằm huấn luyện hay dạy cho hệ\nthống cách thức hiểu và ứng xử à\nđây cũng là nơi machine learning được áp dụng.\nTuy nhiên, điều này chỉ có hiệu lực đối\nvới những ngôn ngữ mà api.ai hỗ trợ.§ \nỞ\nô vuông số (1) trên hình, chúng ta nhập một câu, ví dụ\nnhư “tôi muốn ăn mì”, Api sẽ tự động tô\nmàu và nhận biết được các parameter name như ở\nô vuông số (2). Điều này sẽ cực kì hay khi sử\ndụng machine learning ở cái công tắc phía trên hình. Tuy\nnhiên, khi sử dụng Tiếng Việt, chúng ta không nên sử\ndụng tính năng này vì api.ai không hỗ trợ dẫn\nđến giới hạn sự hiểu biết của hệ\nthống khi ứng xử với tính đa dạng khác nhau\ntrong câu nói trong quá trình giao tiếpo  \nTemplete mode (@): đây là\nnơi chứa pattern. Chúng ta sẽ quản lý tập\ncâu nói của người dùng dưới dạng pattern và lưu\ntrữ lại. § \nChúng\nta thực hiện tạo pattern bằng cách·       \nClick\nvào số (1). Sau đó ghi một pattern mình muốn vào dưới\ndạng: @[Entity_Name]:[Parameter_Name].·       \nTrong\nquá trình thực hiện, Api.ai sẽ hỗ trợ chúng ta\nauto complete các entity tương ứng.·       \nTương\ntự như vậy, các bạn tạo cho mình bộ pattern\ntrong các intent đã phân tích trên hoặc có thể download bộ\nintent tại địa chỉ http://img.tobebetter.info/khanhkt/Advanced/final/NLP-API_Edited_files/intents.rar\n·       \nChúng\nta đã tạo xong bộ xử lý ngôn ngữ tự nhiên kết\nhợp với api.ai·       \nBên\ncạnh đó, API.ai hỗ trợ chúng ta việc test ứng\ndụng như mô tả trong hình bên dưới ·       \nNgoài\nra, chúng ta có thể sử dụng RESTful để\ntương tác với api.·       \nTiếp\ntheo, chúng tôi sẽ hướng dẫn quí vị bổ sung\ncác từ, cụm từ vào entities và đồng thời bổ\nsung thêm pattern vào intent.o  \nĐầu\ntiên, chúng ta cần lấy access key tương tác với\napi. Quá trình được thực hiện như chú thích của\nhình bên dướio  \nChúng\nta sẽ sử dụng Restful để tương tác với\nAPI (Quí vị có thể tham khảo các khái niệm và cách thức\nsử dụng Restful tại đia chị http://www.kieutrongkhanh.net/2016/08/gioi-thieu-ve-restful-web-services-cong.html )o  \nTạo\njava class để tương tác với api như hình vẽ\nbên dướio  \nTạo\ntiếp một class thừa kế từ AbstractBuilder để\ntạo sự tương tác với entity của agent.o  \nSau\nkhi hoàn tất việc tương tác với entity, chúng ta thực\nhiện chạy thử bằng cách bổ sung vào Entity Delicious\ntừ khóa “ăn được”§ \napi.ai\ntrước khi thực hiện:§ \nTạo\nmột java main class để thực hiện việc bổ\nsung từ khóa “ăn được” vào entity Delicious§ \nKết\nquả trên api.ai sau khi chượng trình test chạyo  \nTương\ntự, chúng ta sẽ insert pattern cho câu “Quán phở ở\nđâu ngon”§ \nApi.ai\ntrước khi thực hiện bổ sung§ \nTạo\nclass IntentBuilders kế thừa thừa AbstractBuilder để\ntương tác với api §  Tạo\nJava main class để test thử việc insert một\npattern cho câu “Quán phở ở đâu ngon”§ \nTrong\ncác ví dụ trên, json của intent RatingRequestLocation_HaveFood\n được lấy về bởi vì toàn bộ pattern\nđược lưu trong chuỗi đó. Ngoài ra, một\nkhi chúng ta thêm pattern vào templetes thì chúng ta cũng cần\nxóa đi một số thành phần (userSays, auto, lastUpdate,..)\nđể đảm bảo thông tin trong chuỗi json sẽ\nđược cập nhật. Chúc\nmừng quí vị đã hoàn tất và nắm vững khái niệm\nsơ bộ về xử lý ngôn ngữ tự nhiên và cách sử\ndụng API.ai – Framework hỗ trợ xử lý ngôn ngữ tự\nnhiên. Quí\nvị có thể tìm hiểu thêm về API.ai tại địa\nchỉ https://docs.api.aiChúng\ntôi hy vọng nội dung của bài này giúp ích quí vị trong\nviệc thao thác với các ứng dụng có xử lý ngôn ngữ\ntự nhiên và có thể vận dụng api.ai linh hoạt\ntrong quá trình xử lý và hiện thực ứng dụng.Rất\nmong sự đóng góp của quí vị và hẹn gặp lại\nquí vị ở một chủ đề khác.\n\n\nĐăng nhận xét\n\n\n\n\n\n\n      BLOG_CMT_createIframe('https://www.blogger.com/rpc_relay.html');\n    \n\n\n\n                     1. Họ và tên: KIỀU TRỌNG KHÁNH    Trình độ chuyên môn:    Bachelor   of Engineer in Information Technology - Software E...\n",
          "relevence": "yes"
        },
        {
          "url": "https://nodejs.vn/topic/402/b%E1%BB%99-c%C3%B4ng-c%E1%BB%A5-x%E1%BB%AD-l%C3%BD-ng%C3%B4n-ng%E1%BB%AF-ti%E1%BA%BFng-vi%E1%BB%87t-b%E1%BA%B1ng-node-js",
          "title": "Bộ công cụ xử lý ngôn ngữ tiếng Việt bằng Node.js | A Community Node.js ",
          "content": "\n\t\t\t\tYour browser does not seem to support JavaScript. As a result, your viewing experience will be diminished, and you have been placed in read-only mode.\n\t\t\t\n\t\t\t\tPlease download a browser that supports JavaScript, or enable it if it's disabled (i.e. NoScript).\n\t\t\tChào các bạn,Hiện mình đang thực hiện một dự án mã mở, là tạo ra bộ công cụ xử lý ngôn ngữ tự nhiên dành cho tiếng Việt. Mục đích của toolkit này là hỗ trợ, giúp ta có thể tạo ra các sản phẩm có liên quan đến xử lý ngôn ngữ tự nhiên bằng Nodejs. Đặc biệt là tiếng Việt.Trước mắt là đi cóp nhặt các sản phẩm mã mở từ các bài báo đã công bố trong và ngoài nước để tích hợp vào công cụ này. Để giúp cho các dự án khác mà mình đang thực hiện, cũng như giúp các bạn khác (có thể là bạn?) cũng đang nghiên cứu trong lĩnh vực này, viết trên nền nodejs.Chắc chắn toolkit này còn rất nhiều lỗi. Vậy mong các bạn trong diễn đàn, cộng đồng Node.js mình có thể hỗ trợ cùng mình làm cho sản phẩm này ngày càng hoàn thiện hơn, tối ưu hơn. Xin cảm ơn các bạn!Các bạn có thể contribute cho dự án trên github tại link: https://github.com/vunb/vntk\nMong nhận được phản hồi từ các bạn :)\nCảm ơn các bạn đã xem bài viết này!Tech-nông\nTwitter: @nhubaovu\nProfile: about.me/vunb",
          "relevence": "no"
        },
        {
          "url": "http://seal.deha.vn/xu-ly-ngon-ngu-tu-nhien-voi-python-phan-1/",
          "title": "Xử Lý Ngôn Ngữ Tự Nhiên với Python – Phần 1 – DEHA's blog",
          "content": "\n\tXin chào anh em, đợt này mình có tham gia một dự án khá thú vị về AI. Vai trò của mình trong dự án và thiết kế các thành phần \"biên\", hiểu đơn giản là những thứ râu ria bên ngoài hệ thống Trí tuệ nhân tạo kia. Ví dụ viết Mobile App, Web quảng bá, xử lí truy cập API, xử lí dữ liệu đầu vào… Cũng là cơ may được làm việc với ngôn ngữ Python và đặc biệt là xử lí ngôn ngữ tự nhiên với thư viện NLTK. Sau đây mình sẽ chia sẻ với các bạn những trải nghiệm của mình với việc \"Xử Lý Ngôn Ngữ Tự Nhiên – NLP\" cũng như Python và NLTK trong thời gian qua. Đây hầu hết là những kiến thức cơ bản về NLP dành cho Developer, không cần các bạn phải giỏi những kỹ thuật chuyên sâu hay các thuật toán phức tạp. Vì chúng ta cũng biết, NLP là một nhánh của Trí Tuệ Nhân Tạo và phải nói là khó nhất. Hy vọng bài viết sẽ mang lại những kiến thức hữu ích, giúp bạn tự tin hơn trong việc tìm hiểu AI nói chung và NLP nói riêng. Nào chúng ta cùng bắt đầu!\n\n\tNgôn ngữ tự nhiên là ngôn ngữ mà các loài động vật sáng tạo ra để giao tiếp với đồng loại. Con người cũng là một loại động vật sử dụng ngôn ngữ để giao tiếp. Thế giới ngôn ngữ của con người rất phong phú, theo thông kê của các nhà khoa học thì có tới hàng ngàn ngôn ngữ tồn tại trên trái đất. Ngôn ngữ tự nhiên có 2 dạng là chữ viết và âm thanh (tức tiếng nói). Ngôn ngữ của mỗi dân tộc, quốc gia lại khác nhau bao gồm khác nhau cả về cách viết cũng như cách phát âm.\n\n\tXử Lý Ngôn Ngữ Tự Nhiên có vai trò hết sức quan trọng trong ngành Khoa Học Máy Tính. Nó có vô vàn ứng dụng hữu ích trong cuộc sống cũng như nghiên cứu. Chúng ta có thể điểm qua một vài ứng dụng của xử lý ngôn ngữ tự nhiên như:\n\n\t\t\tNhận dạng chữ viết: Có hai kiểu nhận dạng, thứ nhất là nhận dạng chữ in, ví dụ nhận dạng chữ trên sách giáo khoa rồi chuyển nó thành dạng văn bản điện tử như dưới định dạng doc của Microsoft Word chẳng hạn. Phức tạp hơn là nhận dạng chữ viết tay, có khó khăn bởi vì chữ viết tay không có khuôn dạng rõ ràng và thay đổi từ người này sang người khác. Với chương trình nhận dạng chữ viết in có thể chuyển hàng ngàn đầu sách trong thư viện thành văn bản điện tử trong thời gian ngắn. Nhận dạng chữ viết của con người có ứng dụng trong khoa học hình sự và bảo mật thông tin (nhận dạng chữ ký điện tử).\n\t\t\n\t\t\tNhận dạng tiếng nói: Nhận dạng tiếng nói rồi chuyển chúng thành văn bản tương ứng. Giúp thao tác của con người trên các thiết bị nhanh hơn và đơn giản hơn, chẳng hạn thay vì gõ một tài liệu nào đó bạn đọc nó lên và trình soạn thảo sẽ tự ghi nó ra. Đây cũng là bước đầu tiên cần phải thực hiện trong ước mơ thực hiện giao tiếp giữa con người với robot. Nhận dạng tiếng nói có khả năng trợ giúp người khiếm thị rất nhiều.\n\t\t\n\t\t\tTổng hợp tiếng nói: Từ một văn bản tự động tổng hợp thành tiếng nói. Thay vì phải tự đọc một cuốn sách hay nội dung một trang web, nó tự động đọc cho chúng ta. Giống như nhận dạng tiếng nói, tổng hợp tiếng nói là sự trợ giúp tốt cho người khiếm thị, nhưng ngược lại nó là bước cuối cùng trong giao tiếp giữa robot với người.\n\t\t\n\t\t\tDịch tự động (Machine translate): Như tên gọi đây là chương trình dịch tự động từ ngôn ngữ này sang ngôn ngữ khác. Một phần mềm điển hình về tiếng Việt của chương trình này là Evtrans của Softex, dịch tự động từ tiếng Anh sang tiếng Việt và ngược lại, phần mềm từng được trang web vdict.com mua bản quyền, đây cũng là trang đầu tiên đưa ứng dụng này lên mạng. Tháng 10 năm 2008 có hai công ty tham gia vào lĩnh vực này cho ngôn ngữ tiếng Việt là công ty Lạc Việt (công ty phát hành từ điển Lạc Việt) và Google, một thời gian sau đó Xalo.vn cũng đưa ra dịch vụ tương tự.\n\t\t\n\t\t\tTìm kiếm thông tin (Information retrieval): Đặt câu hỏi và chương trình tự tìm ra nội dung phù hợp nhất. Thông tin ngày càng đầy lên theo cấp số nhân, đặc biệt với sự trợ giúp của Internet việc tiếp cận thông tin trở lên dễ dàng hơn bao giờ hết. Việc khó khăn lúc này là tìm đúng nhất thông tin mình cần giữa bề bộn tri thức và đặc biệt thông tin đó phải đáng tin cậy. Các máy tìm kiếm dựa trên giao diện web như Google hay Yahoo hiện nay chỉ phân tích nội dung rất đơn giản dựa trên tần suất của từ khoá và thứ hạng của trang và một số tiêu chí đánh giá khác để đưa ra kết luận, kết quả là rất nhiều tìm kiếm không nhận được câu trả lời phù hợp, thậm chí bị dẫn tới một liên kết không liên quan gì do thủ thuật đánh lừa của các trang web nhằm giới thiệu sản phẩm (có tên tiếng Anh là SEO viết tắt của từ Search Engine Optimization). Thực tế cho đến bây giờ chưa có máy tìm kiếm nào hiểu được ngôn ngữ tự nhiên của con người trừ trang www.ask.com được đánh giá là \"hiểu\" được những câu hỏi có cấu trúc ở dạng đơn giản nhất. Mới đây cộng đồng mạng đang xôn xao về trang Wolfram Alpha, được hứa hẹn là có khả năng hiểu ngôn ngữ tự nhiên của con người và đưa ra câu trả lời chính xác. Lĩnh vực này hứa hẹn tạo ra bước nhảy trong cách thức tiếp nhận tri thức của cả cộng đồng.\n\t\t\n\t\t\tTóm tắt văn bản: Từ một văn bản dài tóm tắt thành một văn bản ngắn hơn theo mong muốn nhưng vẫn chứa những nội dung thiết yếu nhất.\n\t\t\n\t\t\tKhai phá dữ liệu (Data mining) và phát hiện tri thức: Từ rất nhiều tài liệu khác nhau phát hiện ra tri thức mới. Thực tế để làm được điều này rất khó, nó gần như là mô phỏng quá trình học tập, khám phá khoa học của con người, đây là lĩnh vực đang trong giai đoạn đầu phát triển. Ở mức độ đơn giản khi kết hợp với máy tìm kiếm nó cho phép đặt câu hỏi để từ đó công cụ tự tìm ra câu trả lời dựa trên các thông tin trên web mặc cho việc trước đó có câu trả lời lưu trên web hay không (giống như trang Yahoo! hỏi và đáp, nơi chuyên đặt các câu hỏi để người khác trả lời), nói một cách nôm na là nó đã biết xử lý dữ liệu để trả lời câu hỏi của người sử dụng, thay vì máy móc đáp trả những gì chỉ có sẵn trong bộ nhớ.\n\t\t\t(Nguồn: Wikipedia)\n\t\t\n\tPython ra đời năm 1991, và là một ngôn ngữ thông dịch. Trải qua hơn 20 năm phát triển, Python là một trong những ngôn ngữ được sử dụng nhiều nhất trong dậy lập trình và nghiên cứu khoa học. Rất nhiều trường đại học sử dụng Python để dậy về lập trình cho các sinh viên ngành Khoa Học Máy Tính. Rất nhiều công ty lớn sử dụng Python để xây dựng hệ thống như Google, Youtube, Instagram, Dropbox, Atlassian… Python là một ngữ sử dụng được cho nhiều mô hình lập trình, đơn giản khi học và sử dụng. Tôi sử dụng Python chưa lâu nhưng khi so sánh việc Code sử dụng Pythong thì nó ngắn hơn rất nhiều so với khi viết bằng PHP hoặc Java. Bạn có thể bay bổng tự do với Python hoặc cũng có thể bắt nó trở lên vững chắc và mạnh mẽ như Java. Theo những thông tin mà tôi được biết thì Python cũng là một ngôn ngữ rất phát triển trong lĩnh vực Data Science và Machine Learning. Python cũng cung cấp những hàm và thư viện xử lý ngôn ngữ tuyệt vời. Scikit-learn và Tensor-flow là 2 thư viện Machine Learning nổi tiếng được viêt bằng Python. Đứng ở góc độ người tiếp cận sau, cá nhân tôi thấy Python là một lựa chọn hợp lý khi làm Xử Lý Ngôn Ngữ Tự Nhiên.\n\n\tNLTK hay Natural Language Toolkit – Bộ công cụ ngôn ngữ tự nhiên, là một thư viện được viết bằng Python hỗ trợ xử lý ngôn ngữ tự nhiên. Bằng cách cung cấp các cơ chế và kỹ thuật xử lý ngôn ngữ phổ biến, nó giúp cho việc xử lý ngôn ngữ tự nhiên trở lên dễ dàng và nhanh chóng hơn. Được viết bởi Steven Bird và Edward Loper, làm việc tại Khoa Máy Tính, Đại Học Pennsylvania, Hoa Kỳ và năm 2001. Ngoài việc hỗ trợ xử lý ngôn ngữ, NLTK còn có các mô phỏng đồ hoạ và dữ liệu mẫu hữu ích. NLTK cung cấp các xử lý như classification, tokenization, stemming, tagging, parsing, và semantic reasoning… Những ứng dụng này chúng ta sẽ dần được tìm hiểu ở những bài viết sau. Ngoài việc phục vụ xử lý ngôn ngữ tự nhiên, NLTK còn được sử dụng trong Machine Learning với tác dụng làm sạch dữ liệu, xử lý dữ liệu đầu vào cho các thuật toán Machine Learning.\n\n\tTrên đây, tôi đã giới thiếu cho các bạn sơ lược về NLP và những thứ chúng ta cần để bắt đầu việc xử lý ngôn ngữ tự nhiên bằng Pyhthon và NLTK. Ở bài viết sau, tôi sẽ hướng dẫn các bạn cách cài đặt Python 3 và NLTK.Your email address will not be published. Required fields are marked *Comment Name * Email * Website  \n\nCurrently you have JavaScript disabled. In order to post comments, please make sure JavaScript and Cookies are enabled, and reload the page. Click here for instructions on how to enable JavaScript in your browser.",
          "relevence": "yes"
        },
        {
          "url": "http://chungta.vn/tin-tuc/cong-nghe/giai-ngo-ve-deep-learning-trong-xu-ly-ngon-ngu-tu-nhien-56981.html",
          "title": "'Giải ngố' về Deep learning trong xử lý ngôn ngữ tự nhiên - Chungta",
          "content": "\n\tVới chủ đề “Học sâu trong xử lý ngôn ngữ tự nhiên: tiếng nói và chatbot”, Solution Forum lần này sẽ giair đáp mọi vấn đề kỹ thuật liên quan tới thuật toán Deep Learning trong nhận diện giọng nói và xử lý ngôn ngữ tự nhiên cho chatbot.\n\t\t\t\t\tViễn cảnh của 10 năm nữa là trí tuệ nhân tạo, trong đó Deep learning là thuật toán \"hot\" nhất.\n\tHai diễn giả của sự kiện là anh Nguyễn Văn Huy (FPT Technology Research Institute) và anh Phạm Quang Nhật Minh (FPT Technology Research Institute), những tiến sĩ công nghệ hàng đầu FPT chuyên về các dự án xử lý ngôn ngữ tự nhiên và nhận dạng giọng nói.\n\tTham gia chương trình, thành viên sẽ được tìm hiểu sâu về Deep learning, thuật toán xử lý ngôn ngữ tự nhiên cho Chatbot cùng phương pháp tiếp cận và những bài toán thực tế khi triển khai các giải pháp trong việc điều khiển thiết bị nhà thông minh, xác định người dùng.\n\tCBNV quan tâm có thể đăng ký tham gia chương trình bằng cách gửi email về địa chỉ: Training.HN@fsoft.com.vn. Ngoài việc trao đổi thông tin, kết nối người yêu công nghệ, Solution Forum sẽ dành nhiều phần quà cho người tham gia.\n\tTiểu Thanh\r\n                                Botchat của FPT Telecom giành giải Vàng i Khiến   \r\n                             \r\n                                Liệu AI và IoT có giúp con người vượt qua khủng hoảng lương thực   \r\n                             \r\n                                iKhiến: Hệ thống đào tạo giúp phát triển phương thức học trực tuyến   \r\n                             \r\n                                iKhiến: Tool giúp FUNiX nâng cao hiệu suất tải video bài giảng   \r\n                             \r\n                                FPT Software phát triển tổng đài tự động trên nền tảng AI   \r\n                             \n\tChiều ngày 8/9, gần 2.000 người FPT khu vực miền Trung đã hội tụ về nhà thi đấu Cung thể thao Tiên Sơn, TP Đà Nẵng, để chào mừng sinh nhật tập đoàn lần thứ 29. Mang chủ đề \"The FPT Olympics 4.0\", người nhà họ F được trải nghiệm không gian văn hóa đặc sắc và tham gia các trò chơi thi đấu hấp dẫn.\n\tHội thao lần này có sự tham dự của nhiều lãnh đạo tập đoàn và công ty thành viên, những người vừa kết thúc 2 ngày họp Hội nghị chiến lược FPT 2017 cùng diễn ra tại Đà Nẵng. \n\tTrải qua nhiều giờ thi tài, với tinh thần quyết tâm và đoàn kết, FPT Telecom đã giành chiến thắng phần lớn nội dung thi đấu, trở thành Quán quân toàn đoàn lễ hội 13/9 FPT khu vực miền Trung.\n\tToàn cảnh lễ hội 13/9 miền Trung:\n\t\n\t>> Chủ tịch FPT 'gồng mình' kéo co\n\tXuân Phương\r\n                    \r\n                        Toàn cảnh lễ hội 13/9 miền Trung\r\n                \n\t\tNgày 13/8/2004, GĐ Trung tâm Phần mềm số 5 (G5) Bùi Thiện Cảnh (cựu TGĐ FPT City Đà Nẵng) đã khai trương Chi nhánh FPT tại Đà Nẵng và đặt quyết tâm xây dựng cơ sở của FPT Software tại đây. Đến giữa năm 2005, trụ sở đầu tiên của FPT Software Đà Nẵng đặt tại tầng 4 tòa nhà Ngân hàng Đông Á tại 51 Nguyễn Văn Linh với gần 50 nhân viên. Sau 3 năm, ngày 13/1/2008, FPT Software Đà Nẵng khai trương một văn phòng khác tại tầng 5-6 toà nhà Vĩnh Trung Plaza để đáp ứng nhu cầu tăng trưởng nguồn lực lên đến 235 người. \n\t\t\t\t\t\tBí thư Đà Nẵng Nguyễn Xuân Anh (thứ tư từ trái sang) cùng lãnh đạo Tập đoàn FPT dự lễ khánh thành khu phức hợp FPT Complex vào tháng 4/2016. Tòa nhà FPT Complex được ví như khu rừng hệ sinh thái với sự hòa hợp giữa khách hàng, đối tác và chính quyền thành phố... nhằm hướng đến môi trường phát triển bền vững, đưa công nghệ phần mềm của FPT vươn ra toàn cầu. Đây còn là biểu tượng của sự mạnh mẽ, trường tồn cũng như sự phát triển vượt bậc của FPT Software Đà Nẵng, mang lại một khí thế mới cho mảng xuất khấu phần mềm ở miền Trung - Tây Nguyên.\n\t\tHai năm sau đó, ngày 11/1/2010, FPT Software Đà Nẵng lại khai trương một văn phòng khác để đáp ứng nhu cầu tăng trưởng nóng tầm nhìn 2010-2015. Lần này, văn phòng có quy mô to lớn hơn rất nhiều - Toà nhà FPT Đà Nẵng đặt tại Khu Công nghiệp của thành phố. Tháng 4/2016, FPT Software Đà Nẵng tiếp tục đạt một cột mốc khác với công trình FPT Complex có sức chứa 10.000 người.\n\t\tTừ một văn phòng sản xuất phần mềm với quy mô 50 người, đến nay, đơn vị đã phát triển lên gần 2.500 người, mang lại việc làm cho hàng nghìn tri thức trẻ xuất thân miền Trung. Hằng năm, đơn vị đưa hàng trăm lượt nhân viên sang làm việc tại Nhật Bản, Mỹ, Singapore... Trong 3 năm gần đây, mảng xuất khẩu phần mềm của FPT Đà Nẵng luôn đạt con số tăng trưởng trung bình 50-60% mỗi năm, đội ngũ kỹ sư CNTT cũng tăng lên nhanh chóng. Nhiều dự án trị giá triệu USD với các đối tác lớn trên thế giới đã được triển khai tại FPT Đà Nẵng, đặc biệt là các dự án theo xu hướng công nghệ mới điện toán đám mây.\n\t\tHiện FPT Software hướng đến mục tiêu 1 tỷ USD doanh thu và 30.000 nhân sự vào năm 2020. Trong đó, FPT Software Đà Nẵng dự kiến đóng góp 25-30% vào mục tiêu nhân sự, tương đương khoảng 7.000-9.000 người vào năm 2020. Mục tiêu cụ thể nhất là đạt quân số hơn 5.000 người trong vòng ba năm tới. \n\t\tChặng đường 12 năm thành lập và phát triển của FPT Software Đà Nẵng:\n\t\t Việt Nguyễn\n\t\tVideo: BTC\r\n                    \r\n                        Hành trình 'ra biến lớn' của Phần mềm Đà Nẵng\r\n                \n\tRa đời từ 13/8/2007, FPT Telecom Hải Phòng là một trong những chi nhánh tỉnh đầu tiên của FPT Telecom khi đơn vị quyết định mở rộng quy mô ra toàn quốc. Những ngày đầu, Ban dự án với chưa đến 10 người, dưới sự dẫn dắt của anh Nguyễn Văn Khoa (nay là TGĐ FPT Telecom), đã từng bước xây dựng nền móng đầu tiên cho chi nhánh. \n\tSau một thập kỷ \"đóng quân\" tại đất Cảng, FPT Telecom đã có hạ tầng tại 11 quận huyện trên toàn thành phố với 5 Văn phòng giao dịch, cung cấp dịch vụ tới hàng chục nghìn khách hàng. Đồng thời, nhà \"Cáo\" Hải Phòng cũng luôn nằm trong Top các chi nhánh đứng đầu cả nước về hoạt động kinh doanh, ngoại trừ khu vực Hà Nội và TP HCM, với doanh thu tăng trưởng trung bình hằng năm 25%. Năm 2016, đây là đơn vị xuất sắc duy nhất toàn quốc đạt 8/8 chỉ tiêu kế hoạch được giao.\n\t\t\t\t\tChú Bùi Ngọc Long (giữa), phụ huynh anh Bùi Sơn Tùng, FPT Telecom Hải Phòng. \n\tLà một trong số nhiều phụ huynh có mặt tại buổi lễ sinh nhật 10 năm FPT Telecom Hải Phòng, chú Bùi Ngọc Long, bố anh Bùi Sơn Tùng, xúc động với chương trình từ khâu đón tiếp cho đến nghi lễ tri ân của lãnh đạo FPT Telecom. Phụ huynh FPT tâm sự, những ngày đầu anh Tùng làm việc tại Viễn thông FPT đất Cảng, cả gia đình lo lắng và nghi ngờ về thời gian thất thường của con trai. Có những hôm, đang ngồi ăn cơm cùng gia đình, anh Tùng nhận điện thoại và vội vã rời đi gặp khách hàng. Sau khi tìm hiểu và tâm sự với con, gia đình anh Tùng không chỉ hiểu và thông cảm với đặc thù công việc của anh mà còn khuyến khích, ủng hộ, sẻ chia giúp anh hoàn thành tốt công việc được giao. \n\tVideo lễ sinh nhật với chủ đề \"10 năm kết nối - Lan tỏa niềm tin\" của FPT Telecom Hải Phòng: \n\t\t Thanh Tùng\r\n                    \r\n                        'Tôi tự hào khi con làm việc tại FPT Telecom đất Cảng'\r\n                \r\n                    Tại Hà Nội: Hội thao FPT Olympics: Từ 7h30 sáng ngày 13/9, tại sân tập 1, đường Tân Mỹ, Khu Liên hợp thể thao quốc gia Mỹ Đình. Hội diễn STCo: Từ 19h30 tối cùng                 FPT IS xây dựng hệ thống dự phòng cho Trung tâm Internet Việt Nam   Là dự án có hàm lượng công nghệ cao nên khi hoàn thành, hệ thống do FPT IS xây dựng sẽ giảm tối đa các sự cố có thể xảy ra đối với dịch vụ Thuê bao Truyền hình tăng 142% trong ngày FPT Telecom vùng 1 ra quân   Truyền hình FPT vừa kết hợp với các Trung tâm Kinh doanh (TTKD) Hà Nội triển khai chương trình roadshow quảng bá sản phẩm và ra quân \"kích số\" trên toàn địa bàn Hà Nội 9 lý do nên mua iPhone 8 thay vì iPhone X   Theo Business Insider, nhiều tính năng trên hai mô hình này tương tự nhau thậm chí iPhone 8 còn có phần nổi trội hơn.iPhone 8 không 'sốt giá' ở Việt Nam   Dù chưa được bán ở Mỹ, bộ đôi iPhone 8 và 8 Plus đã có mặt ở nhiều cửa hàng Việt Nam với giá chỉ từ hơn 20 triệu đồng.\r\n          © 2011 Chúng ta, trang tin nội bộ của Tập đoàn FPT.\r\n          Chúng ta giữ bản quyền nội dung trên website này.\r\n       \r\n                    © 2011 Chúng ta, trang tin nội bộ của Tập đoàn FPT.\r\n                    Chúng ta giữ bản quyền nội dung trên website này.\r\n                ",
          "relevence": "no"
        },
        {
          "url": "https://vieclamit.careerbuilder.vn/advices/cac-thuat-ngu-trong-xu-ly-ngon-ngu-tu-nhien.35A4EA20.html",
          "title": "Các thuật ngữ trong Xử lý ngôn ngữ tự nhiên - ViecLamIT.CareerBuilder.vn",
          "content": "\r\n\t\t \r\n\t\tVai trò của Xử lý ngôn ngữ tự nhiên-XLNNTN (Natural Language Processing-NLP) trong khai thác Big Data là không thể phủ nhận trong bối cảnh phát triển của doanh nghiệp hiện nay. Đối với ngôn ngữ tiếng Anh, ta đã được kế thừa nhiều tri thức cũng như nhiều công cụ có sẵn để áp dụng ngay vào thực tiễn. Tuy nhiên, đối với ngôn ngữ tiếng Việt, ta vẫn còn gặp nhiều khó khăn (nhân sự có chuyên môn còn hạn chế, ngữ liệu để huấn luyện chưa đủ lớn) bên cạnh những cơ hội rất lớn (thị trường Việt Nam chưa được khai thác) cho những ai đam mê lĩnh vực này.\r\n\t\tVì vậy, trong bài viết này, tôi xin lập ra danh sách các thuật ngữ thường gặp trong NLP để tiện tham khảo cũng như giúp cho những bạn mới bắt đầu có thể nhanh chóng tra cứu sơ để tiến hành nghiên cứu ngay các tài liệu khoa học. Bài viết sẽ luôn được cập nhật. Nếu có các thuật ngữ chưa rõ, các bạn có thể comment để chúng ta tiếp tục mở rộng thêm danh sách này.\r\n\t\t \r\n\t\tNatural Language Processing (NLP) – Xử lý ngôn ngữ tự nhiên là lĩnh vực Khoa học máy tính kết hợp giữa Trí tuệ nhân tạo (Artificial Intelligence) và Ngôn ngữ học tính toán (Computational Linguistics) nhằm tập trung xử lý tương tác giữa con người và máy tính sao cho máy tính có thể hiểu hay bắt chước được ngôn ngữ của con người. Các ứng dụng thường thấy như hiện nay là Siri, Cortana và Google Now.\r\n\t\t\r\n\t\tSiri vs Google Now vs Cortana\r\n\t\tAmbiguity – nhập nhằng (ở nhiều cấp độ: lexical – từ vựng, morphological – hình vị, syntactic – cú pháp, semantic – ngữ nghĩa, domain – lĩnh vực). Ví dụ nhập nhằng từ “đậu” đại diện cho một hành động hay “đậu” đại diện cho một loài thực vật trong câu “Con ruồi đậu mâm xôi đậu”.\r\n\t\tPre-processing – tiền xử lý dữ liệu, xử lý sơ bộ văn bản: xóa bỏ những kí tự, những mã điều khiển, những vùng không cần thiết cho hệ thống gồm: tách đoạn/câu/từ (paragraph/sentence/word segmentation), làm sạch (cleaning), tích hợp (integreation), chuyển đổi (transformation), giảm số chiều (reduction).\r\n\t\t \r\n\t\tETL input output\r\n\t\t \r\n\t\tMorphological analysis (Phân tích hình thái)\r\n\t\tParser (Phân tích ngữ pháp)\r\n\t\t \r\n\t\tParse tree\r\n\t\t \r\n\t\tAnaphora – khử nhập nhằng thế đại từ. Ví dụ “The monkey ate the banana because it was hungry”. Đại từ “it” thay thế cho monkey hay banana.\r\n\t\tPragmatics – phân tích ngữ dụng: từ “sentence” trong phân tích văn phạm có nghĩa là câu, trong luật pháp có nghĩa là án tù. Do vậy, ta cần xem xét toàn bộ văn bản để đưa ra ý nghĩa chính xác.\r\n\t\tCorpus/Corpora – “ngữ liệu” là những “dữ liệu, cứ liệu của ngôn ngữ”, tức là những chứng cứ thực tế sử dụng ngôn ngữ, được dùng để kiểm chứng các quy luật của ngôn ngữ trong quá trình phân tích thông kê hay kiểm định giả thuyết thống kê của các mô hình dự đoán.\r\n\t\t \r\n\t\tCorpus\r\n\t\t \r\n\t\tInformation Extraction – là tiến trình rút trích ra các thông tin có cấu trúc một cách tự động từ các nguồn dữ liệu không cấu trúc hay bán cấu trúc (unstructured/semi-structure) ví dụ như các tài liệu văn bản hay các trang web.\r\n\t\tInformation Extraction\r\n\t\t \r\n\t\tNamed Entity Recognition (NER) – là tiến trình xác định và phân loại các phần tử trong văn bản vào các danh mục được định nghĩa trước như tên người, tên tổ chức, địa điểm, giá trị tiền tệ, tỷ lệ phần trăm,…\r\n\t\tNamed Entity Recognition\r\n\t\t \r\n\t\tSentiment Analysis -sử dụng các kĩ thuật NLP để rút trích thông tin chủ quan của người dùng từ một câu nói hay một văn bản. Đây cũng là kĩ thuật khai thác ý kiến người dùng xem họ đang có thái độ tích cực hay tiêu cực về sản phẩm của công ty.\r\n\t\tSentiment Analysis\r\n\t\t \r\n\t\tBag of Words -mô hình thường dùng trong các tác vụ phân lớp văn bản (Text Classification). Thông tin sẽ được biểu diễn thành tập các từ đi kèm với tần xuất xuất hiện của mỗi từ này trong văn bản. Bag of Words được dùng như feature để huấn luyện cho classifier.\r\n\t\t \r\n\t\t\r\n\t\tBag of Words\r\n\t\t \r\n\t\tExplicit Semantic Analysis (ESA) -là tiến trình giúp máy hiểu được ý nghĩa của văn bản, được sử dụng trong Information Retrieval, Document Classification, Semantic Relatedness calculation (độ tương tự về ý nghĩa giữa các từ hay văn bản)\r\n\t\tLatent Semantic Analysis (LSA) -tiến trình phân tích quan hệ giữa các văn bản và các từ. Đầu ra là mối liên quan giữa các khái niệm, văn bản, và các từ. LSA giả sử các từ gần nhau về mặt ý nghĩa sẽ xuất hiện trong các văn bản tương tự.\r\n\t\t\r\n\t\tLatent Semantic Analysis\r\n\t\t \r\n\t\tLatent Dirichlet Allocation (LDA) – kĩ thuật Topic Modeling thường dùng, ý tưởng của LDA dựa trên nguyên lý mỗi topic là phân bố của các từ, mỗi văn bản là sự trộn lẫn giữa nhiều topic, và mỗi từ phân bố vào một trong những topic này.\r\n\t\tLatent Dirichlet Allocation\r\n\tNguồn: TechtalkChức NăngCareerBuilder.vnKết nối với CareerBuilder.vnWebsite Đối TácLiên Hệ\nCông Ty Cổ Phần CareerBuilder Trụ̣ sở: 139 Pasteur, Phường 6, Quận 3, TP.HCMMST: 0303284985Ngày cấp: 25/04/2013 Nơi cấp: Sở Kế Hoạch Và Đầu Tư Thành Phố Hồ Chí MinhĐiện thoại: (84.28) 3822-6060  Email: contact@careerbuilder.vn",
          "relevence": "no"
        },
        {
          "url": "http://vforum.vn/diendan/showthread.php?69855-Tuyen-dung-ky-su-xu-ly-ngon-ngu-tu-nhien-tai-Ha-Noi",
          "title": " Tuyển dụng kỹ sư xử lý ngôn ngữ tự nhiên tại Hà Nội",
          "content": "\r\nFanpage: \r\n\r\n\r\n\r\nEmail: vforum.vn[a]gmail.comPowered by vBulletin® Version 4.x.x\r\nCopyright © 2014 - 2015 Vforum.vnMúi giờ GMT +7. Hiện tại là 08:46 PM",
          "relevence": "no"
        },
        {
          "url": "http://svbk.hust.edu.vn/nhom-nlp.html",
          "title": "Nhóm NLP",
          "content": "NHÓM XỬ LÝ NGÔN NGỮ TỰ NHIÊN - NLPThông tin chungTên nhóm: Xử lý ngôn ngữ tự nhiên - NLPTrực thuộc: Viện CNTT&TTEmail: nlp@soict.hust.edu.vnĐiện thoại:Trưởng nhóm nghiên cứuHọ và tên: TS. Nguyễn Thị Thu TrangChuyên môn: Tổng hợp tiếng nói, Phân tích văn bản.Hướng nghiên cứu: Tổng hợp tiếng nói, Chuẩn hoá văn bản, Xử lý văn bản, Dịch máy thống kê.Email: trangntt@soict.hust.edu.vnĐịnh hướng nghiên cứu của nhómĐịnh hướng chung:Nghiên cứu phát triển và ứng dụng xử lý ngôn ngữ tự nhiên trong các bài toán tóm tắt văn bản, hỏi-đáp, gợi ý, dịch máy, giao tiếp người máy, phân tích quan điểm, kiểm trùng tài liệu...Định hướng cụ thể trong thời gian tới:Kiểm trùng tài liệu khoa học (kiểm tra đạo văn): Trong đó cho phép tiến hành lưu trữ, đánh chỉ mục để thực hiện các tính năng truy vấn tài liệu (VD: Đồ án tốt nghiệp, luận văn cao học, luận án tiến sĩ, các bài báo khoá học…) để tìm ra các tài liệu liên quan, thực hiện kiểm tra đạo văn cho tài liệu khoa học về mặt câu chữ, đoạn văn hoặc ý tưởng; Dịch tiếng nói tự động (speech translation): Dịch tiếng nói từ ngôn ngữ nguồn sang tiếng nói của ngôn ngữ đích. Mục tiêu trong tương lai là có thể thực hiện việc dịch tự động cho tiếng Việt với một số ngôn ngữ phổ biến và có tiềm năng như: tiếng Anh, tiếng Nhật...; Phân tích quan điểm, sắc thái, tâm lý của văn bản: Phân tích tự động các sắc thái của văn bản cho các thực thể như cá nhân, tổ chức như các sắc thái tích cực, tiêu cực, thích, yêu, ghét...var sc_texthit_var = sc_texthit_var || [];sc_text_hit(1104647,\"\",\"000\");© 2015 Đoàn TNCS Hồ Chí Minh -  Hội Sinh viên Trường Đại học Bách khoa Hà Nội. Văn phòng: P208 - C2 | Điện thoại: 04 38692751 | Hotline: 0963135714 | Email: dtn@hust.edu.vn | Website: svbk.hust.edu.vn",
          "relevence": "no"
        },
        {
          "url": "http://congdongjava.com/forum/threads/tuy%E1%BB%83n-d%E1%BB%A5ng-k%E1%BB%B8-s%C6%AF-x%E1%BB%AC-l%C3%9D-ng%C3%94n-ng%E1%BB%AE-t%E1%BB%B0-nhi%C3%8An-lnp.21218/",
          "title": "Tuyển dụng KỸ SƯ XỬ LÝ NGÔN NGỮ TỰ NHIÊN (LNP) | Cộng đồng Java Việt Nam | Java Việt Nam | Java SE, Java ME, Java EE, Android",
          "content": "\n\tDiscussion in 'Tuyển dụng nhân sự CNTT - IT recruitment' started by Ms. Huong, 8/10/15.\nCác tên cách nhau bởi dấu phẩy(,)",
          "relevence": "no"
        },
        {
          "url": "https://22l5.com/xu-ly-ngon-ngu-tu-nhien/",
          "title": "Xử lý ngôn ngữ tự nhiên",
          "content": "Lĩnh vực này tiếng Anh gọi là Natural Language Processing. Đây là mục chính sẽ chỉ sang các bài con nói về các vấn đề có liên quan\n                  Published on\n                  January 13, 2017\n                ",
          "relevence": "no"
        },
        {
          "url": "http://www.tienphong.vn/cong-nghe-khoa-hoc/nguoi-chuyen-xu-ly-ngon-ngu-tu-nhien-trong-tin-hoc-109787.tpo",
          "title": "\r\n\tNgười chuyên 'xử lý ngôn ngữ tự nhiên' trong tin học - 13-02-2008 | Công nghệ | Báo điện tử Tiền Phong\r\n",
          "content": "\nTòa soạn\nQuảng cáo\nRSS\n\n\n\n\n\n\nKhoa học\n\n\nCông nghệ\n\nVi tính\n\nĐiện thoại\n\nXe\n\n\n13/02/2008 07:59\n\n\nTP - “Tôi đã làm ứng dụng xử lý ngôn ngữ tự nhiên cho nhiều thứ tiếng như Nhật, Hàn Quốc, Arập, Trung Quốc. Về nước lần này cũng vì muốn nghiên cứu ứng dụng cho ngôn ngữ nước mình”, TS Phạm Bảo Sơn tâm sự.\n\r\n\r\n\r\n\r\n\r\n\r\nPhạm Bảo SơnLà một trong 10 gương mặt thanh niên xuất sắc trong lĩnh vực công nghệ thông tin đạt giải Quả Cầu Vàng 2007 do T.Ư Đoàn TNCS Hồ Chí Minh và Bộ KHCN tổ chức từng có hai bằng sáng chế công nghệ thông tin, Tiến sĩ trẻ Phạm Bảo Sơn có mong muốn giản dị là có một công trình khoa học bằng tiếng Việt và truyền cho những sinh viên Việt ngọn lửa khám phá, sáng tạo từ những kiến thức mình được học và nghiên cứu về tin học nơi đất bạn. Đó cũng là lý do anh trở về làm giảng viên tại trường ĐH Công nghệ sau 10 năm học tập và nghiên cứu tại trường Đại học Tổng hợp New South Wales (Úc).Thành quả của 10 năm đam mê Đam mê ngôn ngữ tin học từ khi còn là học sinh phổ thông, suốt 10 năm học tập và nghiên cứu tại Úc, Phạm Bảo Sơn dồn toàn bộ thời gian và tâm huyết để nghiên cứu về Xử lý ngôn ngữ tự nhiên trong tin học. Tài năng và công sức của Sơn với thành quả của 10 năm là 2 bằng phát minh sáng chế công nghệ thông tin năm 2006 và bảo vệ xuất sắc luận án Tiến sĩ năm 2007. “Tôi có duyên trong nghiên cứu vấn đề này”.Làm thế nào để biết được các thông tin về tác giả của một email? Câu trả lời sẽ được tìm thấy trong sáng chế: “Xử lý ngôn ngữ email và thuộc tính người viết” và “Xử lý ngôn ngữ văn bản” mà anh Sơn và các bạn trong Công ty Appen (Công ty công nghệ ngôn ngữ của Úc) đã dày công nghiên cứu. \r\n\r\n\r\n\r\n\r\nNgôn ngữ tự nhiên chính là ngôn ngữ chúng ta vẫn dùng. Nó có thể được lưu trữ trên máy tính dưới dạng văn bản (email, blog, news...), hay ghi âm lại giọng nói. \r\nNghiên cứu “Xử lý ngôn ngữ tự nhiên” nhằm mục đích để máy tính có thể tự xử lý được ngôn ngữ tự nhiên, hiểu được ngôn ngữ tự nhiên…“Theo sáng chế này, bạn có thể xác định tên tuổi, tính cách, sở thích, và những thuộc tính khác của người gửi email chỉ qua cách viết của họ nhờ việc khai thác hoạt động tìm kiếm thông minh của máy tính”, anh Sơn chia sẻ. Ngôn ngữ tự nhiên chính là ngôn ngữ chúng ta vẫn dùng. Nó có thể được lưu trữ trên máy tính dưới dạng văn bản (email, blog, news...), hay ghi âm lại giọng nói. Nghiên cứu “Xử lý ngôn ngữ tự nhiên” nhằm mục đích để máy tính có thể tự xử lý được ngôn ngữ tự nhiên, hiểu được ngôn ngữ tự nhiên…Anh đã mất 1 năm để nghiên cứu email của hơn 100 người ở các nước sử dụng ngôn ngữ tiếng Anh và tiếng Ả rập, mỗi người 10 email, từ đó gửi những câu hỏi tâm lý học, phân tích thói quen, tính cách, sở thích, đặc điểm chung nhất của người viết.Còn đối với văn bản, có thể xác định được đoạn nào người gửi email viết đoạn nào của người khác. “Sáng chế này sẽ được áp dụng với mục đích an ninh như: tìm chủ nhân của thư lạc danh, thư khủng bố, chat, blog,... và hiện tại Cty Appen đã triển khai ứng dụng nó”, anh Sơn cho biết. Cùng với thành công trong những sáng chế, anh Sơn đã bảo vệ xuất sắc luận án tiến sĩ Luận văn nghiên cứu của anh là: “Incremental Knowledge Acquisition for Natural Language Processing” (“Thu thập tri thức từng bước cho xử lý ngôn ngữ tự nhiên”) sau 3,5 năm nghiên cứu. Đề tài của anh Sơn đề cập tới một vấn khó và không mới nhưng được đánh giá cao bởi đã đưa ra được những giải pháp. “Lập trình hệ thống luật để máy tính tự hỏi người sử dụng khi đặt lệnh xử lý, qua đó đưa trí thức của người sử dụng máy tính vào trong máy, để máy tính có thể tự đánh giá được vấn đề nào tốt hay xấu...”, anh Sơn chia sẻ. “Muốn khảo sát thị trường, bạn có thể đặt lệnh, máy tính sẽ tự khảo sát trên các trang web và đưa ra kết quả. Ví dụ, muốn tìm tên sản phẩm của Cty trên các web, máy tính sẽ tự động search khi được nhận lệnh và đưa ra kết quả thống kê chính xác từ các trang web”, anh Sơn nói về ứng dụng của đề tài này.Bảo vệ thành công luận án Tiến sĩ, Phạm Bảo Sơn là người duy nhất của Khoa học máy tính trường ĐH New South đạt Huy chương của trường dành cho sinh viên tốt nghiệp xuất sắc năm 2001.\r\n\r\n\r\n\r\n\r\n\r\nSơn cùng gia đình trong ngày bảo vệ luận án Tiến sĩ tại trường ĐH Tổng hợp New South Wales (Úc)Hai lần vô địch robot bóng đá thế giớiLà học sinh giỏi quốc tế (lớp 11 và 12 đạt Huy chương bạc quốc tế môn Tin học), được tuyển thẳng vào Đại học Công nghệ - ĐH Quốc gia Hà Nội. Sang Úc du học, Sơn tiếp tục theo đuổi môn Tin học.Năm thứ hai Đại học Sơn nhận được học bổng hè của trường dành cho những sinh viên có thành tích học tập tốt trong năm 2000. May mắn đã đến với anh khi người trao học bổng đó là vị giáo sư đầu ngành trong lĩnh vực trí tuệ nhân tạo. Ông ngỏ ý mời Sơn tham gia đội tuyển Robot bóng đá của trường.Sơn chia sẻ: “Tôi đồng ý trong tâm trạng vừa mừng, vừa lo. Mừng vì đây là cơ hội tốt cho tôi được cọ xát, nghiên cứu; lo vì  mình vừa sang được 1 năm sợ còn non kinh nghiệm”. Robot bóng đá quốc tế là cuộc thi thường niên dành cho các trường ĐH công nghệ thông tin trên thế giới. Với hình thức là các chú chó robot của các đội sẽ thi đá bóng với nhau trong một khoảng thời gian nhất định. “Mỗi con chó robot đều có phần cứng giống nhau, mỗi đội sẽ phải lập trình phần mềm cho con chó để thi đấu đạt kết quả cao nhất”- Sơn cho biết:Đội của Sơn (gồm 3 người) đã dành quyết tâm cao độ, miệt mài viết chương trình phần mềm trong cả 1 năm trời. Bước vào thi đấu đội robot của Sơn đi từ chiến thắng này đến chiến thắng khác. Đây là giải đấu bóng đá robot đầu tiên trường của Sơn có chiến thắng dòn giã như thế, bởi chiến thuật đã có sự khác biệt và nổi trội. Ngay cả trong trận gặp đội của trường CMU của Mỹ - trường hàng đầu thế giới về tin học, đội tuyển của Sơn cũng đã thắng với tỷ số 13 - 10, thắng đội của Trung tâm nghiên cứu robot của Paris tỷ số 10 - 0. Năm 2000, cũng là năm đầu tiên, trường của Sơn đoạt chức vô địch robot bóng đá thế giới. Đến năm 2001, Sơn làm huấn luyện cho đội tuyển robot bóng đá của trường. Truyền lại những kinh nghiệm, đồng thời cùng mọi người viết những chương trình mới, một lần nữa, Sơn đã đưa cúp vô địch robot bóng đá thế giới về cho trường New South.Hiện, Sơn về giảng dạy tại Khoa Công nghệ thông tin- Đại học Công nghệ Hà Nội với mong muốn rất giản dị là làm một công trình khoa học bằng tiếng Việt, điều mà 10 năm qua Sơn chưa làm được.“Tôi đã làm ứng dụng xử lý ngôn ngữ tự nhiên cho nhiều thứ tiếng như Nhật, Hàn Quốc, Arập, Trung Quốc. Về nước lần này cũng vì muốn nghiên cứu ứng dụng cho ngôn ngữ nước mình”, anh Sơn tâm sự.\nHải Yến\n\nin bài viết\nBình luận\n\n\nGiải tríThế giớiGiáo dụcVideoKinh tếGiới trẻGiáo dụcXã hộiThế giớiGiải tríKhoa họcPháp luậtThể thaoThể thaoXã hộiXã hộiĐịa ốcThế giới\nTòa soạn: 15 Hồ Xuân Hương, Hà Nội\nĐiện thoại: (84-4) 39431250\nFax: (84-4) 39430693\nEmail: online@baotienphong.com.vn\n",
          "relevence": "no"
        },
        {
          "url": "https://www.businesscard.vn/blog/tong-hop-du-an-viet-nam/",
          "title": "Dự án, hệ thống phần mềm mã nguồn mở về tiếng Việt, Việt Nam",
          "content": "Select PagePosted by Nguyễn Nhân | Th6 28, 2016 | Công nghệ, Đánh giá, Lập trình | 0  | \n\n\n\n\nNội dung chínhDự án này tổng hợp thông tin, các bộ thư viện, phần mềm, ứng ụng mã nguồn mở để phát triển các dự án liên quan đến xử lý ngôn ngữ tiếng Việt và các vấn đề liên quan đến đất nước con người Việt Nam (bản đồ, địa giới hành chính, vùng miền, …)Giới thiệu đến mọi lập trình viên để cùng nhau phát triển các ứng dụng liên quan tiếng Việt/Việt Nam nhằm giảm thiểu thời gian nghiên cứu, phát triển cũng như nâng cao chất lượng cộng đồng mã nguồn mở VN.Chú ý: Những thông tin dự án in đậm là còn thiếu hướng dẫn, hoặc có nhưng chưa đầy đủ, cần sự hỗ trợ thêm để hoàn thiện. Nếu bạn nào là chủ dự án này, vui lòng cập nhật thêm.#####　MobileShare:Rate:Xin chào, mình là kỹ sư công nghệ thông tin đang làm việc tại Tokyo, Nhật Bản. Đam mê lập trình web và mobile, mình đang cố gắng học Javascript để hoàn thiện bản thân để trở thành lập trình viên fullstack thực thụ. Mình yêu thích tối ưu tốc độ ứng dụng cũng như cải thiện quy trình làm việc để phát triển sản phẩm chất lượng và nhanh chóng. Sự động viên của bạn bằng các hình thức \"like\", \"share\", \"comment\" sẽ giúp cho mình có động lực để viết nhiều bài tốt hơn!12/31/201501/11/201605/07/201601/01/2016Thư điện tử của bạn sẽ không được hiển thị công khai. Các trường bắt buộc được đánh dấu *COMMENTTên * Thư điện tử * Trang web  \n\nCông nghệ đang thay đổi nhanh chóng, hãy cũng vươn ra thế giới với những bài viết chất lượng từ Fullstack Station bạn nhé.Cám ơn bạn!Có lỗi xảy ra, xin vui lòng thử lạiBạn sẽ nhận được email thông báo khi có bài viết mới, mình không spam vì mình cũng rất ghét :)Designed by Elegant Themes | Powered by WordPress",
          "relevence": "no"
        },
        {
          "url": "https://vlsp.hpda.vn/demo/?page=resources",
          "title": "VLSP :: Tài nguyên",
          "content": "",
          "relevence": "no"
        },
        {
          "url": "http://vienngonnguhoc.gov.vn/bai-viet/moi-du-thuyet-trinh-khoa-hoc-cua-ts-nguyen-phuong-thai-ung-dung-cong-nghe-thong-tin-de-xay-dung-treebank-trong-tieng-viet_540.aspx",
          "title": "\r\n\tMời dự thuyết trình khoa học của TS Nguyễn Phương Thái: \"Ứng dụng công nghệ thông tin để xây dựng TREEBANK trong tiếng Việt\" - Tin tức, thông báo, sự kiện\r\n",
          "content": "\r\n\t   Vào lúc 8h30 ngày 29/8/2013 sắp tới, TS Nguyễn Phương Thái, giảng viên trường Đại học Công nghệ, Đại học Quốc gia Hà Nội sẽ trình bày thuyết trình khoa học tại Viện Ngôn ngữ học. Nội dung chủ đề buổi thuyết trình \"Ứng dụng công nghệ thông tin để xây dựng TREEBANK trong tiếng Việt\".\r\n\tVIỆN NGÔN NGỮ HỌC\r\n\tĐịa chỉ: Số 9 Kim Mã Thượng, Ba Đình, Hà Nội\r\n\tĐiện thoại/Fax: 04. 37674572 - Email: info@vienngonnguhoc.gov.vn\r\n\tWebsite: www.vienngonnguhoc.gov.vn",
          "relevence": "no"
        },
        {
          "url": "http://timoday.edu.vn/bai-toan-tach-tu-tieng-viet/",
          "title": "Bài toán tách từ tiếng Việt | Tìm ở đây",
          "content": "Các khoá học về công nghệ thông tinbởi   \r\n     Phan Tiến\r\n    ·\r\n            02/12/2015\r\n     Xử lý ngôn ngữ tự nhiên bao gồm rất nhiều các bài toán như dịch tự động (machine translation), tóm tắt văn bản (text summarization), tìm kiếm thông tin (information retrieval), trích chọn thông tin (information extraction), v.v. Muốn giải quyết được các bài toán trên thì bài toán phân tách từ (word segmentation) là bài toán quan trọng nhất, nó quyết định thành công của các bài toán khác.Như chúng ta đã biết, văn bản tiếng Việt đặt dấu cách giữa các âm tiết chứ không phải giữa các từ. Một từ có thể có một, hai hoặc nhiều âm tiết nên có nhiều cách phân chia các âm tiết thành các từ, gây ra nhập nhằng. Việc phân giải nhập nhằng này gọi là bài toán tách từ.Tiêu chí quan trọng nhất trong bài toán tách từ đương nhiên là độ chính xác. Hiện tại người ta đã đạt được độ chính xác lên đến 97% tính theo từ. Tuy nhiên nếu tính theo câu (số câu được tách hoàn toàn đúng/tổng số câu) thì độ chính xác chỉ khoảng 50%. Đây là vấn đề nghiêm trọng đối với các bước xử lý sau như phân tích ngữ pháp, ngữ nghĩa vì một từ bị tách sai có ảnh hưởng toàn bộ đến cách phân tích cả câu.Ngoài ra tiêu chí độ chính xác tách từ mới cũng quan trọng với các ứng dụng thực tế. Tiếng Việt là một sinh ngữ – nó luôn luôn biến đổi. Các từ mới thuần Việt cũng như vay mượn được tạo ra hàng ngày. Nếu một ứng dụng không xử lý được những từ này thì hiệu năng của nó sẽ giảm dần theo thời gian.Hiện tại có một số cách tiếp cận bài toán tách từ như sau:Các thẻ: Xử lý ngôn ngữ\r\n  27/04/2016\r\n\r\n  01/12/2015\r\n\r\n  27/09/2015\r\nThư điện tử của bạn sẽ không được hiển thị công khai. Các trường bắt buộc được đánh dấu *Bình luận Please enable JavaScript to submit this form.Tên * Thư điện tử *  \n\nLiên kết:Thủ thuậtCode bị chèn thêm vào file functions trong WordPress24 Th9, 2017CSS / Khoá họcBài 10: CSS Height/ Width6 Th9, 2017CSS / Khoá họcBài 8: CSS Margin12 Th8, 2017CSS / Khoá họcBài 9: CSS Padding11 Th8, 2017CSS / Khoá họcBài 6: CSS Backgrounds25 Th6, 2017CSS / Khoá họcBài 5: Màu trong CSS5 Th6, 2017SEO / Tài liệu / Tổng hợpTài liệu tổng hợp về SEO28 Th5, 2017Tin tứcTài liệu khoá học về Khoa học dữ liệu25 Th5, 2017Tin tứcMười lập trình viên vĩ đại nhất thế giới9 Th5, 2017Khoá học / Lập trình CBài 3: Biến, Toán tử và Kiểu dữ liệu7 Th5, 2017.Net / Khoá họcXây dựng chương trình quản lý bán hàng bằng C#20 Th3, 2016.Net / Khoá họcPhần 1: Form và các Control thông dụng4 Th2, 2017.Net / Khoá họcTạo báo cáo với Crystal Report trong C#2 Th3, 2016.Net / Khoá họcTạo báo cáo dùng control ReportViewer với C#8 Th6, 2016.Net / Khoá họcLập trình Cơ sở dữ liệu với C#4 Th6, 2016.Net / Khoá họcPhần 4: Lập trình Cơ sở dữ liệu với C#26 Th2, 2017HTML / Khoá họcTrắc nghiệm HTML5 Th11, 2015.Net / Khoá họcPhần 5: Tạo báo cáo với C#7 Th3, 2017Khoá học / Tin học căn bản / Tin học đại cươngTạo trang web đơn giản và đưa lên Internet9 Th10, 2015.Net / Khoá họcXây dựng ứng dụng từ điển đơn giản trong C#21 Th2, 2016Đăng ký nhận bài\r\n\t\t\t\t\t\r\n\t\t\t\tTìm ở đây © 2015-2016. Được thiết kế và quản lý bởi FITA, VNUA.Sử dụng lại dữ liệu cần trích dẫn http://timoday.edu.vn",
          "relevence": "no"
        },
        {
          "url": "http://www.vietlex.com/xu-li-ngon-ngu/152-NHAN_DANG_THUC_THE_DINH_DANH_TRONG_VAN_BAN_TIENG_VIET",
          "title": "Vietlex :: Xu li ngon ngu",
          "content": "VŨ XUÂN LƯƠNG – Vietlex\r\n\t(Bài báo cập nhật trên cơ sở bài gửi tham gia Hội thảo khoa học Giữ gìn sự trong sáng của tiếng Việt trên các phương tiện thông tin đại chúng. 5-11-2016)\r\n\t\r\n\t \r\n\tNhận dạng thực thể định danh (Named Entity Recognition – NER), còn gọi là nhận dạng thực thể có tên, là nhiệm vụ nhận biết các từ xuất hiện trong văn bản là tên gọi của một đối tượng nào đó, như tên người (nhân danh), tên đất (địa danh, địa điểm), tên tổ chức, tên tác phẩm, tên sự kiện, thời gian, tiền tệ, v.v. NER đóng vai trò quan trọng trong các ứng dụng tự động trích xuất thông tin, khai phá dữ liệu, dịch máy, v.v.\r\n\tNER đã được quan tâm nghiên cứu trên thế giới từ đầu những năm 1990. Hiện nay các hệ thống NER cho tiếng Anh, tiếng Đức, tiếng Hà Lan, v.v. đã được xây dựng và được đánh giá cao. Điểm chung của các hệ thống NER này là tập trung đi vào nhận dạng 3 loại thực thể định danh: tên người, tên đất và tên tổ chức.\r\n\tĐối với tiếng Việt, có một số đơn vị thuộc lĩnh vực công nghệ thông tin đã xây dựng hệ thống NER, nhưng với quy mô công ti, có tính chất nội bộ và thường không phổ biến. Cho đến nay vẫn chưa có thông tin cụ thể về hệ thống NER trong văn bản tiếng Việt đã được xây dựng như thế nào. Vì lẽ đó, bài báo này đưa ra hướng nhận dạng 3 loại thực thể định danh điển hình là tên người, tên đất và tên tổ chức trong văn bản tiếng Việt, nhằm mục đích xây dựng nguồn tài nguyên huấn luyện cho máy học (machine learning). Theo đó, các ứng dụng xử lí ngôn ngữ tự nhiên (Natural Language Processing) biết cách tự động trích xuất thông tin thực thể định danh trong các văn bản tiếng Việt, phục vụ cho một số yêu cầu thiết yếu của đời sống.\r\n\t \r\n\tNhận dạng ba loại thực thể định danh trong tiếng Việt được xác định tương thích với các loại thực thể được mô tả trong hệ thống NER của tiếng Anh và tiếng Đức.\r\n\t○ Tên người (Person) gồm các loại tên sau:\r\n\t- Tên, tên đệm và họ của một người: Nguyễn Văn A.\r\n\t- Tên hiệu (biệt hiệu), bí danh (mật danh), biệt danh, v.v.\r\n\t- Tên các nhân vật hư cấu: Ngọc Hoàng, Thiên Lôi, Sa Tăng, v.v.\r\n\t○ Tên địa lí (Địa danh - Location) bao gồm các thực thể có toạ độ địa lí nhất định, ghi lại được trên bản đồ (trừ các địa danh tưởng tượng):\r\n\t- Tên gọi các hành tinh: Mặt Trăng, Mặt Trời, Trái Đất, v.v.\r\n\t- Tên gọi các thực thể mang yếu tố địa lí tự nhiên và địa lí lịch sử (quốc gia, vùng lãnh thổ, châu lục), các vùng quần cư (làng, thị trấn, thành phố, tỉnh, giáo khu, giáo xứ), v.v.\r\n\t- Tên gọi các thực thể tự nhiên: đèo, núi, dãy núi, rừng, sông, suối, hồ, biển, vịnh, vũng, eo biển, đại dương, thung lũng, cao nguyên, đồng bằng, bãi biển, khu bảo tồn thiên nhiên, khu sinh thái, v.v.\r\n\t- Tên gọi các thực thể là công trình xây dựng, công trình kiến trúc công cộng: cầu, đường, lâu đài, quảng trường, bảo tàng, trường học, nhà trẻ, thư viện, bệnh viện, nhà hát, nhà máy, v.v.\r\n\t- Tên gọi địa điểm, địa chỉ thương mại: nhà hàng, khách sạn, hiệu thuốc, quán bar, v.v.\r\n\t- Một số địa danh tưởng tượng khác: Vườn Địa Đàng, Sông Ngân, Cầu Ô Thước, v.v.\r\n\t○ Tên tổ chức (Organization) bao gồm các loại tên sau:\r\n\t- Các cơ quan chính phủ: các bộ ngành, uỷ ban nhân dân, hội đồng nhân dân, toà án, phòng ban, v.v.\r\n\t- Các tổ chức chính trị: cơ quan báo chí, đảng phái chính trị, đoàn thể chính trị, hội nghề nghiệp, v.v.\r\n\t- Kinh doanh, sản xuất: ngân hàng, thị trường chứng khoán, công ti, hãng phim, nhà sản xuất, hợp tác xã, v.v.\r\n\t- Các thương hiệu: Honda, Sony, Samsung, v.v.\r\n\t \r\n\t- Ngữ liệu trước khi được gán nhãn NER sẽ được tách từ và/hoặc gán nhãn từ loại một cách tự động. Mỗi từ được đặt trên một dòng riêng biệt. Mỗi dòng bao gồm năm cột:\r\n\t1. Đơn vị từ\r\n\t2. Nhãn từ loại của từ\r\n\t3. Nhãn cụm từ\r\n\t4. Nhãn thực thể (mức 1)\r\n\t5. Nhãn thực thể lồng  (mức 2)\r\n\t- Nhãn thực thể được gán theo cấu trúc BIO. Có 7 nhãn: B-PER và I-PER cho tên người, B-LOC và I-LOC cho địa danh, B-ORG và I-ORG cho tên tổ chức, và O cho các phần tử khác. Kí hiệu B: Begin, dùng cho từ đầu tiên của thực thể. I: Inside, dùng cho các từ tiếp theo trong cụm thực thể. O: Other, dùng cho từ không thuộc bất cứ thực thể nào. Các nhãn từ loại: N: noun. NPP: proper noun. V: verb. A: adjective, v.v. Các nhãn cụm từ: NP, VP, AP, v.v. Ví dụ:\r\n\t\r\n\t- Ở ví dụ trên, nhãn B-PER (Trịnh) chỉ ra đơn vị đầu tiên của tên người, nhãn I-PER là đơn vị tiếp theo và (cho đến khi) kết thúc của tên người (Xuân, Thanh). Nhãn B-ORG (Uỷ ban) chỉ ra đơn vị đầu tiên của tên tổ chức, nhãn I-ORG là đơn vị tiếp theo và kết thúc của tên tổ chức (Nhân dân, Tỉnh, Hậu Giang). Các đơn vị “Tỉnh, Hậu Giang”, về bản chất lại mang nhãn địa lí (location), chúng sẽ được miêu tả ở mức “Nhãn thực thể lồng” là: B-LOC (Tỉnh) - chỉ ra đơn vị đầu tiên của tên địa lí, và I-LOC (Hậu Giang) - chỉ ra đơn vị tiếp theo và kết thúc của tên địa lí.\r\n\t- Trong bài báo này, để dễ hình dung, chúng tôi chỉ trình bày định dạng ngữ liệu ở dạng có ba cột: 1. Đơn vị từ; 2. Nhãn thực thể; 3. Nhãn thực thể lồng .\r\n\t \r\n\t\r\n\t4.1. Tên người (Person)\r\n\t○ Tên người (nhân danh) được xem là tên riêng. Viết hoa tên riêng là để chỉ ra rằng người đó chỉ có một mà thôi, không giống với người khác.\r\n\t○ Thường đi trước tên người có các danh từ chung như ông, bà, anh, chị, chú, bác, thằng, chủ tịch, giám đốc, trưởng phòng, v.v. Các danh từ chung này được dùng để chỉ hoặc gọi người nào đó tuỳ theo mối quan hệ. Chẳng hạn, cùng một người tên Thanh, nhưng có người gọi anh Thanh, có người gọi ông Thanh, giám đốc Thanh, v.v. Chúng tôi cho rằng, các danh từ loại này không nằm trong cấu tạo tên người, vì chúng không có tính cố định.\r\n\t4.1.1. Tên người là tên riêng chỉ từng cá nhân. Dạng đầy đủ, tên người gồm 3 thành phần: họ + chữ đệm + tên. Không phân biệt họ, chữ đệm và tên vì coi chúng đều được riêng hoá, vì thế viết hoa chữ cái đầu của các âm tiết.\r\n\t\r\n\t4.1.2. Dạng rút gọn còn 2 thành phần: họ + tên, hoặc tên + họ (các ngôn ngữ Ấn-Âu), hoặc chữ đệm + tên\r\n\t\r\n\t4.1.3. Dạng rút còn 1 thành phần: tên gọi. Trường hợp có các danh từ chung là từ xưng hô đứng trước bộ phận tên (hoặc họ với các ngôn ngữ Ấn-Âu) thì các danh từ này không được coi là thuộc tên người. Trường hợp danh từ chung chỉ chức vụ, công việc, v.v. được dùng để gọi thay cho tên người đảm nhiệm chức vụ, công việc đó trong một không gian cụ thể (bối cảnh của câu chuyện) thì cũng được coi là tên người (có thể viết hoa theo phong cách: ông Hàn, cụ Bá, cụ Đề, anhLí, v.v.).\r\n\t\r\n\t4.1.4. Tên danh nhân, nhân vật lịch sử được cấu tạo bằng cách kết hợp giữa bộ phận là danh từ chung chỉ chức vụ, công việc hoặc từ tôn sùng (Y) với bộ phận là tên (CapWord - từ đỉnh) thì được coi là tên người (X). X = Y + CapWord. Chẳng hạn: Đề Thám, Đội Cấn, Lí Cường, Ông Đùng, Bà Đà, Thánh Gióng, Đức Phật Như Lai, v.v.\r\n\t\r\n\t4.1.5. Tên hiệu, tên tự, bí danh cũng được coi là tên người.\r\n\t\r\n\t4.2. Tên địa lí (Location)\r\n\t4.2.1. Tên các quốc gia thuộc lĩnh vực Địa chính trị (Geo-Political), mang tính tri nhận phổ quát. Viết hoa tất cả chữ cái đầu của các yếu tố cấu tạo nên tên địa lí, trừ chữ “và” (and) có trong cấu trúc.\r\n\t\r\n\t4.2.2. Tên địa phương được phân chia theo khu vực địa lí của một nước (như làng, xã, phường, huyện, quận, tỉnh, thành phố…), cũng là các đơn vị địa danh hành chính của một nước, thuộc tri thức nền mang tính cộng đồng cao. Chẳng hạn: New York, Paris, Canbera, Jakacta, Bangkok, Hà Nội, Hải Phòng, Hà Nam, Hoà Bình, Thanh Hoá, Quảng Nam, Kon Tum, Đắc Lắc, Lâm Đồng, Đồng Nai, Tiền Giang, Cà Mau, Cầu Dền, Cầu Giấy, Ba Vì, Gia Lâm, Củ Chi, Nhà Bè, Kiến An, Hải Châu, An Khê, Bát Xát, Mèo Vạc, Tân Trào, v.v.\r\n\t- Với các kiểu cấu tạo Tỉnh Nam Định, Thành phố Nam Định, Thành phố Hà Nội, Thủ đô Hà Nội, Thành phố Hồ Chí Minh, Thành phố New York (New York City), v.v. là bao hàm ý phân biệt vị thế, cấp độ của một địa danh. So sánh:\r\n\tTỉnh Nam Định: tỉnh gồm Thành phố Nam Định và 9 huyện.\r\n\tThành phố Nam Định: trung tâm của Tỉnh Nam Định, không bao gồm 9 huyện.\r\n\t- Với các cấu tạo Thành phố Hà Nội, Thủ đô Hà Nội, Hà Nội thì cả 3 kiểu đều chỉ chung một thực thể, do vậy chúng đồng nhất với nhau về tri nhận (Hà Nội là cách gọi rút gọn của Thành phố Hà Nội hoặcThủ đô Hà Nội). Trong khi, Thành phố Hồ Chí Minh và Hồ Chí Minh lại chỉ hai thực thể khác nhau: một chỉ địa danh, một chỉ nhân danh. Từ những lí do đó, chúng tôi cho rằng, yếu tố “thành phố, thủ đô, tỉnh, thị xã, thị trấn” là thành phần tham gia vào cấu tạo nên địa danh, và vì vậy coi cả khối Thành phố Nam Định, Thành phố Hồ Chí Minh là một đơn vị để phân biệt với Tỉnh Nam Định (bao hàm Thành phố Nam Định) và Hồ Chí Minh (chỉ người).\r\n\t- Xử lí tương tự với các trường hợp: Tỉnh Đồng Nai, Quận Cầu Giấy, Quận 3, Quận Hai Bà Trưng (phân biệt với Phố Hai Bà Trưng), Phường Minh Khai (phân biệt với Phố Minh Khai), Huyện Cầu Kè, Huyện Sông Cầu (phân biệt với Sông Cầu), Thị trấn Chợ Đồn, Thị xã Sông Công, v.v.\r\n\t\r\n\t4.2.3. Tên gọi chỉ thực thể địa lí tự nhiên (X) được cấu tạo giữa một danh từ chung chỉ loại của thực thể (Y: núi, rừng, sông, suối, hồ, biển, vịnh, vũng, châu, đại dương, đại lục, đồng bằng, cao nguyên, thiên thể, v.v.) luôn luôn xuất hiện với một tên gọi chính chỉ thực thể (CapWord) do nhu cầu phân biệt trong giao tiếp chi phối. Trong đó, CapWord thường là tên có lí do (theo một ý nghĩa, sự tích nào đó: Than Thở, Hoàn Kiếm, Kẻ Gỗ…; hoặc CapWord vốn là một tên gọi cụ thể khác).\r\n\tĐể đảm bảo tính khu biệt, tránh nhầm lẫn với các tên gọi có tính phổ biến khác, chúng tôi đề nghị xem X là tên địa lí, khi X = Y + CapWord (với tiếng nước ngoài thì thường X = CapWord + Y). Chẳng hạn: Hồ Tây, Hồ Gươm, Hồ Hoàn Kiếm, Hồ Ba Bể, Hồ Than Thở, Hồ Kẻ Gỗ, Sông Hồng, Sông Cầu, Sông Thái Bình, Sông Amazon (Amazon River), Sông Mississippi (Mississippi River), Sông Thêm, Sherwood Forest (một khu rừng ở Anh), Đảo Cô Tô, Quần đảo Hoàng Sa, Vịnh Hạ Long, Vịnh Cam Ranh, Núi Nùng, Núi Đọ, Núi Ba Vì, Đồng bằng Sông Hồng, Cao nguyên Lâm Viên, Châu Âu, Châu Phi, Thái Bình Dương, Lục địa Á-Âu, Mặt Trời, Mặt Trăng, Trái Đất, Sao Hoả (Hoả Tinh), Sao Mộc (Mộc Tinh), Sao Thổ (Thổ Tinh), Sao Thiên Vương (Thiên Vương Tinh), v.v.\r\n\t\r\n\t4.2.4. Tên địa lí (X) được cấu tạo giữa một hay hai danh từ chỉ phương hướng (Y: đông, tây, bắc, nam, phương, đông nam, đông bắc, v.v.) luôn luôn xuất hiện với một tên gọi chính chỉ một vùng, một miền (CapWord). Để đảm bảo tính khu biệt, chúng tôi đề nghị xem X là tên địa lí, khi X = Y + CapWord (với tiếng nước ngoài thì thường X = CapWord + Y.). Chẳng hạn: Đông Á, Tây Á, Đông Âu, Tây Âu, Trung Mỹ, Trung Phi, Đông Dương, Đông Nam Á, Đông Bắc Á, Bắc Bán Cầu, Nam Bán Cầu, West Texas, Northern California, Northern Ireland, v.v.\r\n\t\r\n\t4.2.5. Tên địa lí (X) được cấu tạo giữa một danh từ chung hoặc một (hay hai) danh từ chỉ hướng (Y: biển, miền, đông, tây, bắc, nam, phương, đông nam, nam trung, v.v.) với một danh từ chung hoặc một danh từ chỉ hướng (Z: hồ, bộ (chỉ đất liền), đông, tây, bắc, nam, phương, v.v.). Để đảm bảo tính khu biệt, chúng tôi đề nghị xem X là tên địa lí, khi X = Y + Z. Chằng hạn: Miền Bắc, Miền Nam, Phương Tây, Tây Phương, Bắc Bộ, Trung Bộ, Nam Bộ, Nam Trung Bộ, Đông Nam Bộ, Biển Đông, Biển Hồ, v.v.\r\n\t\r\n\t4.2.6. Tên các công trình xây dựng, kiến trúc (X) được cấu tạo giữa một danh từ chung chỉ loại của công trình (Y: cầu, đường, phố, đại lộ, cao tốc, chùa, tháp, v.v.) luôn luôn xuất hiện với một tên gọi chính chỉ công trình (CapWord). Trong đó, CapWord thường là tên có lí do (theo một ý nghĩa, sự tích nào đó). Để đảm bảo tính khu biệt, chúng tôi đề nghị xem X là tên công trình, khi X = Y + CapWord (với tiếng nước ngoài thì thường X = CapWord + Y). Chẳng hạn: Phố Huế, Phố Cầu Gỗ, Đường Phạm Văn Đồng, Cầu Long Biên, Cầu Tràng Tiền, Cầu Bố, Cầu Si, Chùa Keo, Tháp Bút, Tháp Chàm, Tháp Eiffel (Eiffel Tower, Tour Eiffel), Đại lộ Thăng Long, Đường cao tốc Pháp Vân - Cầu Giẽ, Đường vành đai 3 Hà Nội, v.v.\r\n\t\r\n\t\r\n\t4.3. Tên tổ chức (Organization)\r\n\t4.3.1. X là tên tổ chức, khi X được tạo bởi một danh từ chung (Y) với một danh từ chỉ tên gọi cụ thể (CapWord). Danh từ chung có thể đứng trước hoặc đứng sau danh từ chỉ tên gọi cụ thể, và được coi như hai từ. X = Y + CapWord. Chẳng hạn: Đạo Phật, Đạo Thiên Chúa, Đạo Cao Đài, Đạo Kitô, Phật Giáo, Công Giáo, Thiên Chúa Giáo, v.v.\r\n\t\r\n\t4.3.2. X là tên tổ chức, khi X được tạo bởi một một chuỗi từ, trong đó có cả danh từ chung và danh từ riêng. Căn cứ vào các thành tố tạo nên tên tổ chức, viết hoa chữ cái đầu của mỗi thành tố. Chẳng hạn: Bộ Ngoại giao, Bộ Giáo dục và Đào tạo, Trường Đại học Bách khoa Hà Nội, Trường Đại học Công nghệ, Trường Đại học Khoa học Tự nhiên Hà Nội, Nhà máy Thuốc lá Thăng Long, Công ti Xuất nhập khẩu Hải Hà, v.v.\r\n\t\r\n\t \r\n\t\r\n\t5.1. Nhập nhằng về cấu trúc\r\n\t\r\n\t\r\n\t5.2. Nhập nhằng về ngữ nghĩa\r\n\t5.2.1. Nhập nhằng giữa tên người và tên đường phố, tên thành phố\r\n\t\r\n\t5.2.2. Nhập nhằng giữa LOCATION và ORGANIZATION \r\n\t\r\n\t \r\n\t○ Chẳng có lí gì khi “phương” trong “phương Tây” thì viết thường, nhưng trong “Tây Phương” lại viết hoa; “châu” trong “châu Âu” thì viết thường, nhưng trong “Âu Châu” lại viết hoa; “sao” trong “sao Thổ” thì viết thường, nhưng “tinh” trong “Thổ Tinh” lại viết hoa, v.v. Có ý kiến cho rằng, sở dĩ viết “phương Tây, sao Thổ” là do chi phối của kết hợp theo trật tự ngữ pháp tiếng Việt, còn viết “Tây Phương, Thổ Tinh” là do chi phối của kết hợp theo trật tự ngữ pháp tiếng Hán. Trong khi “đạo” trong “đạo Phật” và “giáo” trong “Phật giáo” thì đều viết thường (?!) Theo chúng tôi, sự kết hợp của các yếu tố cấu tạo nên từ vựng theo trật tự ngữ pháp tiếng Việt hay theo trật tự ngữ pháp tiếng Hán phần lớn là tạo ra sự khác nhau về ý nghĩa từ vựng, hoặc khác nhau về sắc thái ngữ nghĩa mà thôi. Ví dụ: thanh thiên (trời xanh) và thiên thanh (có màu xanh da trời); bệ hạ (từ chỉ vua, dùng để nói với vua) và hạ bệ (lật đổ khỏi địa vị), đạo Phật (thông thường), Phật giáo (chuyên môn), v.v.\r\n\t○ Với những thành tựu trong việc gán nhãn ngữ nghĩa tự động cho ngữ liệu đang diễn ra hiện nay, vấn đề chưa thống nhất trong cách viết tên riêng tiếng Việt tuy là trở ngại, nhưng hoàn toàn có thể vượt qua. Bình diện ý nghĩa của đơn vị ngôn ngữ ngày càng được khai phá một cách toàn diện và có chiều sâu, báo hiệu một xu thế phát triển mới về khoa học dữ liệu trong tương lai gần.\r\n\t \r\n\tTÀI LIỆU THAM KHẢO\r\n\t[1] Bộ Nội vụ (2011), Hướng dẫn thể thức và kỹ thuật trình bày văn bản hành chính, Thông tư Số: 01/2011/TT-BNV.\r\n\t[2] https://sites.google.com/site/germeval2014ner/\r\n\t[3] http://www.cs.nyu.edu/cs/faculty/grishman/muc6.html\r\n\t[4] http://www.clips.uantwerpen.be/conll2002/ner/\r\n\t[5] http://www.cnts.ua.ac.be/conll2003/ner/\r\n\t[6] http://www.cs.nyu.edu/cs/faculty/grishman/NEtask20.book 7.html#HEADING18\r\n\t[7] NER Data tiếng Anh (eng.testa.txt), tiếng Đức (de-train.txt), tiếng Hà Lan (ned-train.txt).\r\n\t[8] Nguyễn Thị Minh Huyền, Vũ Xuân Lương (2016), Named Entity Recognition, IEEE-RIVF International Conference on Computing and Communication Technologies. Hanoi, Vietnam, Nov 7-9, 2016.\r\n\t[9] Nguyễn Thiện Giáp (2002), Vấn đề chuẩn hoá từ vựng tiếng Việt, Từ vựng học tiếng Việt. NXB Giáo dục, Hà Nội.\r\n\t[10] Nguyễn Trọng Báu (2005), Những quy cách viết hoa trong tiếng Việt, Từ điển chính tả tiếng Việt, Nxb Văn hoá Thông tin. ",
          "relevence": "no"
        },
        {
          "url": "http://nano.edu.vn/fpt-speech-synthesis/",
          "title": "FPT Speech Synthesis – Hệ thống tổng hợp tiếng nói tiếng Việt dành riêng cho người Việt | FPT Nanoversity",
          "content": "FPT Speech synthesis là bộ tổng hợp tiếng Việt mới của FPT, có khả năng đọc diễn cảm các câu văn, đoạn văn, thậm chí là cả một quyển sách với giọng đọc tự nhiên như người thật. Phần mềm sử dụng những thuật toán thông minh nhằm phân tích những âm thanh tiếng Việt được thu từ giọng người thật.Tổng hợp tiếng nói (Speech synthesis) là bài toán cơ bản trong bài toán lớn về giao tiếp giữa người và máy (Human-Machine Interface). Tuy là một bài toán có hàng nghìn năm tuổi, với những khởi đầu khá thô sơ, nhưng cho đến ngày nay vẫn là bài toán đang được tiếp tục giải với sự tham gia của rất nhiều “ông lớn” cùng những tham vọng khác nhau.\nVới tham vọng riêng của mình, Ban công nghệ FPT (FTI) đã nỗ lực nghiên cứu suốt gần 5 năm qua để cho ra mắt sản phẩm “FPT Speech Synthesis”. Sảm phẩm là sự kết hợp của :\n• Ngôn ngữ học (Linguistics): âm vị học (Phonology), hình thái học (Morphology), ngữ dụng học (Pragmatics);\n• Vật lý học: âm học;\n• Công nghệ: Xử lý ngôn ngữ tự nhiên (Natural Language Processing) , học máy (Machine Learning), xử lý tín hiệu số (Digital Signal Processing).\nĐược đánh giá là hệ thống tổng hợp tiếng nói tiếng Việt có chất lượng tốt nhất thị trường hiện nay, hệ thống tổng hợp tiếng Việt mới của FPT đang được mở trên Open FPT (http://openfpt.vn/). Các nhà phát triển có thể khai thác nguồn tài nguyên này để xây dựng ứng dụng của riêng mình trên các nền tảng khác nhau.\nCho đến thời điểm này, sản phẩm “Giao thông thông minh” do FPT IS phát triển đang sử dụng Speech Synthesis API của Open FPT và nhận được rất nhiều phản hồi tích cực từ người dùng.\nTham khảo thêm về công nghệ tổng hợp tiếng nói:\nKì 1: http://tech.fpt.com.vn/cong-nghe-trong-du-an/nghien-cuu-va-xu-ly-tieng-noi-ki-i-mot-so-khai-niem-can-ban-nd497952.html\nKì 2 : http://tech.fpt.com.vn/cong-nghe-trong-du-an/nghien-cuu-va-xu-ly-tieng-noi-ki-ii-cac-phuong-phap-tong-hop-tieng-noi-nd498233.html\nChi tiết về FPT Speech Synthesis API: http://doc.openfpt.vn/services/text2speech/documentation.html#/READMEVề FPT Nanoversity\nChương Trình Đào Tạo\nLiên hệ\nĐịa chỉ các trung tâm\nKhối Giáo dục FPT\n",
          "relevence": "no"
        },
        {
          "url": "https://stanford.com.vn/dao-tao/tin-chi-tiet/id/6773/cagId/27",
          "title": "\r\n            Những API mà các lập trình viên cần biết\r\n        ",
          "content": "\r\n            Các công cụ lập trình mới ngày càng được xuất hiện nhiều, các lập trình viên đôi khi phải khó khăn để lựa chọn công cụ phù hợp.\r\n        \r\n            Dưới đây là những API hay nhất được dựa trên nhiều năm nghiên cứu mà chắc chắn bạn sẽ cần phải biết.1. Google MapsCó lẽ không có một API nào có thể tương tác tốt và mở ra cho cộng đồng nhiều như Google Maps. API này hiện đã được tích hợp trên hàng triệu trang web trên khắp thế giới như một chuẩn mực map thời hiện đại. API của Goole Map luôn xuất hiện song hành cùng sự đi lên của các công cụ mà developer đang sử dụng trong kỷ nguyên đang lên của mobile.Một trong những ứng dụng thường dùng nhất chính là các marker trên bản đồ, bạn có thể thêm path vào cho nhiều điểm cũng lúc, phần front end code cũng được tương tác rất nhiều trên API này. 2. IBM WatsonCông nghệ của Watson được gọi là công nghệ xử lý ngôn ngữ tự nhiên (NLP), họ hỗ trợ lập trình viên rất nhiều trong việc truy cập vào kho thư viện nghiên cứu đã được tích trữ trong nhiều năm. Gần đây, phía công ty vừa đem về một công nghệ mới với tên gọi Alchemy API cho phép nhận biết và tính toán cả về nhận dạng hình ảnh.Các tính năng có trong công nghệ này cho phép lập trình viên có thể định dạng các từ ngữ, cũng như nhận diện khuôn mặt hay đồ vật trong các bức ảnh. Nếu bạn cần một số xử lý thông mình trong ứng dụng của mình thì hãy dùng công nghệ này thay vì tự xây dựng một hệ thống khác phức tạp và mất thời gian.3. FullContactFullcontact cung cấp thông tin về người dùng đằng sau email của họ. Từ dữ liệu đó bạn có thể xác định được cụ thể tên gọi, độ tuổi, vị trí, giới tính và cả tài khoản mạng xã hội của họ. Hệ thống cũng nhận input các tài khoản twitter, Facebook và cả số điện thoại được cập nhật trên mail. Bạn sẽ không phải khổ sở lấy chúng về từ từng email nhỏ nữa.Công cụ này rất hữu ích cho sale, bạn có thể tìm thấy Fullcontact ở các ứng dụng đọc business card hay các phần mềm dữ liệu công ty. Clearbit là một trong những đối thủ mới nổi lên những vẫn còn xa mới vượt qua được FullContact đã được hình thành hơn 4 năm.4. TwitterThật khó để cưỡng lại được sự tiện lợi của twitter, đây cũng là một API mà bất kỳ lập trình viên nào cũng muốn sử dụng. Tuỳ thuộc vào tập người dùng của bạn mà twitter sẽ phát huy tính hiệu quả của mình. Dĩ nhiên, Twitter luôn được coi là một công cụ cực tốt giúp nhà phát triển chia sẻ ứng dụng của mình tốt hơn trên twitter.5. FacebookFacebook hiện đang chiếm lượng người dùng đông đảo nhất trên thế giới. Với 1 tỷ người dùng, bạn có thể có rất nhiều lợi thế khi sử dụng cho social login (dùng Stormpath), điều này giúp việc chia sẻ và kiếm tiền nhanh hơn.Facebook hiện đang hạn chế loại data được xuất hiện trên ứng dụng, nhưng vẫn còn rất nhiều đất cho các nhà lâp trình phát triển tiếp. Hơn nữa, Facebook cũng có hệ thống định dạng ngôn ngữ riêng, mobile backed, Parse và nhiều thứ khác nữa đang chờ bạn tìm hiểu.6. StormpathNếu bạn đang tìm bất kỳ dạng đăng nhập (login) nào cho trang của bạn thì đừng làm gì khác ngoài việc nghĩ đến Stormpath. Stormpath là một API giúp quản lý user được xậy dụng để hỗ trợ cho Facebook login và một số thứ khác nữa.Nếu bạn chỉ dùng chức năng nhận diện bằng mạng xã hội thì nên nghĩ nhiều tới việc xây dựng ứng dụng trên Stormpath hơn là tiếp tục việc thay đổi hoặc tạo ra các API khác. Nó sẽ giúp duy trì, quản lý và lưu lượng người dùng cho phép giải quyết luôn các vấn đề như quên password,…Stormpath hỗ trợ Facebook, Google, LinkedIn, Twitter và cả GitHub7. TwilioLà một trong những API cũng được các lập trình viên rất yêu thích. Các tập đoàn viễn thông đã đưa ra một giải pháp API rất đơn giản về voice và text. Gửi và nhận tin nhắn cuộc gọi, mms hình ảnh. Bạn có thể xây dựng Twilio như một cấu phần căn bản hoặc một giá trị thêm vào cho ứng dụng của bạn.8. SendGridTransactional email hoàn toàn khác với marketing email ở chỗ nó chỉ gửi đến một người ngay tại thời điểm tác vụ. Tin gửi đi cực kỳ tập trung vào đối tượng. Chúng ta có thể liên tưởng đến những thứ như receipts, reset password, xác nhận tài khoản và notification từ mạng xã hội.Sendgrid là một trong những đơn vị mang transactional email đến cho các nhà phát triển. Nó cho phép nhân rộng hệ thống email delivery ra hơn nhiều lần kể cả real-time data (thông qua webhook), cho phép người dùng phản ứng và tương tác trực tiếp. Về cơ bản, bất cứ ứng dụng nào có tài khoản user hoặc thương mại điện tử đều cần email giao dịch này. Mặc dù có rất nhiều đối thủ ở phân khúc này, nhưng SendGrid thật sự tạo được khác biệt, một trong những đối thủ lớn nhất của nó hiện nay không ai khác chính là Mail Chimp.Hy vọng rằng những chia sẻ trên sẽ giúp các bạn lập trình viên tiết kiệm thời gian khi giải quyết các vấn đề của mình.Còn nếu bạn muốn trở thành những lập trình viên chuyên nghiệp thì Stanford – dạy kinh nghiệm lập trình là lựa chọn số 1 sẽ giúp bạn thành công.Hiện tại, Stanford liên tục tuyển sinh các khóa học lập trình. Quyết định ngay, học luôn, đi làm ngay sau khi hoàn thành khóa học. Gọi ngay tới hotline: 0936.172.315 - 0963.723.236; 04.6275 2212 - 04.6662 3355 hoặc truy cập www.stanford.com.vn để biết thêm thông tin về khóa học và đăng ký ngay bạn nhé!Sưu tầmNhật Lệ (Stanford - Nâng tầm tri thức)\r\n            Tags:\r\n            \r\n        \r\n            Tầng 2 số 20 ngõ 678 Đường Láng (Hoặc cuối ngõ 100 Nguyễn Chí Thanh), Đống Đa, Hà Nội.\r\n        P.504 Tầng 5 - CT6 Khu đô thị mới Tứ Hiệp, Thanh Trì, Hà Nội(024) 6275 2212 | (024) 6662 3355Chuyên môn: (0869) 188 2550936.172.315 | 0963.723.236daotao@stanford.com.vn\r\n                                \r\n                                    Chọn Bộ phận hỗ trợ\r\n                                \r\n                            \r\n                                Chọn Bộ phận hỗ trợ\r\n                            \r\n                                \r\n                            \r\n                                Hãy điền tên của bạn\r\n                            \r\n                                \r\n                            \r\n                                Vui lòng cung cấp địa chỉ email\r\n                            \r\n                                \r\n                            \r\n                                Vui lòng điền số điện thoại của bạn\r\n                            \r\n                                Đăng nhập bằng\r\n                                \r\n                                \r\n                            \r\n                                \r\n                            \r\n                                Bắt đầu trò chuyện\r\n                            \r\n                                Đội hỗ trợ\r\n                            \r\n                                Hãy hỏi chúng tôi\r\n                            \r\n                                \r\n                                    Tốt\r\n                                \r\n                                \r\n                                    \r\n                                \r\n                            \r\n                                        Bạn có câu hỏi? Hãy chat với chúng tôi!\r\n                                    \r\n                                \r\n                            Hỗ trợ trực tuyến\r\n                            \r\n                            ĐÀO TẠO: 024.6275.2212 024.6662.3355\r\n                        \r\n                            \r\n                            CHUYÊN MÔN: 0869.188.255\r\n                        ",
          "relevence": "no"
        },
        {
          "url": "http://hvazone.com/tai-sao-chatbots-that-bai-dich-tu-website-chatbotfail.1227.html",
          "title": "Tại sao chatbots thất bại? - Dịch từ website chatbot.fail\n - HVA Zone",
          "content": "Did you joined on our Facebook Group ?\n                   Unknown User                                    Unknown User Acc LyndaEm share tài liệu Windows System Programming 4th edition!Áp dụng học máy và dữ liệu lớn trong an toàn thông tinTại sao chatbots thất bại - Một câu chuyện ngắn về lý do tại sao chatbot thất bại và bạn có thể làm gì về nó. Thực hiện bởi uxdesign.cc.Chatbots là một chủ đề nóng bỏng trong ngành dịch vụ truyền thông ngay bây giờ. Vào thời điểm này, chúng ta đã nghe nói về các chương trình, hoặc thậm chí là tạo mẫu và đưa ra một vài trong số bots cho khách hàng và công ty kinh doanh hoạt động.Một số người cho rằng chatbot là các trang web mới, chúng sẽ giết 99% ứng dụng trên mạng - dự đoán rằng các giao diện đàm thoại sẽ sớm thay thế các mẫu thiết kế trung tâm điểm ảnh mà chúng tôi đã sử dụng trong nhiều thập kỶ trong công việc của chúng tôi.  Tại sao chatbots không thành công? Ý tưởng tự động hóa và mở rộng các cuộc hội thoại one-to-one bằng cách sử dụng công nghệ thu hút rất nhiều thương hiệu và dịch vụ trên mạng. Trong quá trình này, các nhà thiết kế đóng một vai trò quan trọng trong việc xác định cách mỗi cuộc trò chuyện được kịch bản và hành vi người dùng có thể mong đợi khi tương tác với chatbots.Tuy nhiên, đôi khi các chatbot cung cấp trải nghiệm người dùng không liên tục, không thú vị và kém hiệu quả như chúng tôi đã hình dung ra.Dưới đây là một vài lý do phổ biến cho việc chatbot thất bại:  Trí tuệ nhân tạo (AI)\nVẫn không thể truy cập được Phần lớn chatbot không thực sự thông minh. Chúng được xây dựng dựa trên logic, nơi mà phản ứng của bot phụ thuộc vào các từ khóa cụ thể được xác định trong đầu vào của người dùng. NẾU người dùng nhập chứa 'cửa hàng' hoặc 'mua';\nTHÌ gửi tin nhắn với danh sách sản phẩm Điều này có nghĩa là ứng dụng thông minh như khả năng của nhà thiết kế / lập trình viên đã dự đoán các trường hợp sử dụng và đầu vào của người sử dụng tiềm năng.Bots với khả năng học ngôn ngữ và ngôn ngữ tự nhiên vẫn còn khá hiếm.   Nắm bắt các trường hợp từ người dùng không mạnh  Điều này xảy ra với mọi công nghệ mới được đưa ra trên thế giới, vì thế, các nhà thiết kế và các lập trình viên thực sự vui mừng về nó.Những gì chúng ta đang thấy bây giờ là một cơn sốt vàng của các công ty đang cố gắng trở thành người đầu tiên trong nhóm của họ để khai thác thành công bot. Trong quá trình đó, chúng ta sẽ thấy rất nhiều bots đang giải quyết các trường hợp không liên quan hoặc cung cấp những trải nghiệm rất nghèo nàn.Đó là một phần của chu kỳ! Ngành nghề này cần phải học hỏi từ những thất bại của nó trước khi có thể triển khai các chương trình thông minh, hữu dụng.   Một số chương trình thiếu minh bạch Các chương trình thành công nhất rõ ràng bắt đầu từ kinh nghiệm: Người dùng đang trò chuyện với một con robot, chứ không phải với người khác. Thiết lập đúng kỳ vọng sẽ làm cho người dùng tha thứ hơn về những sai lầm nhất định mà bot có thể gây ra. Bạn chắc chắn muốn bot của bạn cảm thấy như con người có thể, nhưng nói dối người dùng của bạn và giả vờ là cái gì đó không phải là nó có thể dẫn đến mất lòng tin tưởng không thể đảo ngược được.  Họ không hiểu ngữ cảnh Con người thực sự giỏi nói chuyện. Chúng ta hiểu sự mỉa mai, và chúng ta liên tục tận dụng thông tin theo ngữ cảnh khi chúng ta trả lời ai đó. Khi tôi sắp xếp các kế hoạch ăn tối với ai đó qua điện thoại và tôi hỏi nếu tôi mang chiếc ô của tôi, người đó biết chúng tôi sẽ đi đâu, thời gian nào trong ngày chúng tôi họp, và đó là địa điểm trong nhà hay ngoài trời. Họ thậm chí còn biết cách thận trọng, và xem xét kỹ lưỡng những điều đó khi tôi trả lời. Bots thì không. Trừ trường hợp các chương trình được cung cấp bởi công nghệ xử lý ngôn ngữ tự nhiên, họ không thể giữ thông tin theo ngữ cảnh lâu hơn một vài bong bóng trò chuyện và sẽ mất đi theo dõi những gì người dùng đã nói trước khi đặt câu hỏi.  Họ không liên lạc với các hệ thống kinh doanh hiện tại Một cám dỗ phổ biến khác khi xây dựng một chatbot đang cố gắng tạo lại các chức năng từ đầu.Giả sử bạn đang tạo một bot để sắp đặt các cuộc hẹn trong spa. Nếu chatbot của bạn không liên lạc được với hệ thống quản lý cuộc hẹn hiện tại của spa, điều đó có nghĩa là làm việc thêm cho chủ doanh nghiệp để xử lý các yêu cầu đến qua kênh mới này - và cuối cùng là sự thiếu nhất quán cho người dùng.Bots là một phần của một hệ sinh thái lớn hơn, được hình thành bởi nhiều điểm tiếp xúc giữa khách hàng và thương hiệu. Tạo một chatbot trong một xilô có thể khá nguy hiểm cho cả doanh nghiệp và khách hàng. Họ cố gắng để xử lý\nquá nhiều thứ cùng một lúc Các nhà thiết kế và nhà phát triển có khuynh hướng hứng thú với tất cả các nhiệm vụ mà một bot có thể trợ giúp, nhưng quên thu hẹp phạm vi trọng tâm của nó. Đừng cố giải quyết các vấn đề vượt quá phạm vi của bạn.Bots làm một điều tốt hơn là hữu ích mà các chương trình mà làm nhiều điều không tốt. Bạn thực sự có thể thay thế \"chương trình\" trong câu ở trên bằng \"ứng dụng\", \"trang web\" và thậm chí cả \"người\"Các giao thức leo thang của con người Khi công nghệ thất bại, người dùng vẫn muốn có thể dựa vào con người để giúp họ giải quyết vấn đề của họ. Tuy nhiên, rất ít chatbot có một quy trình công việc leo thang để cho phép con người tiếp nhận cuộc trò chuyện khi bot không thể giúp đỡ.Kết quả? Bots để lại người dùng treo - đôi khi thậm chí còn thất vọng hơn khi họ bắt đầu cuộc trò chuyện với thương hiệu.   Bots để lại người dùng treo - đôi khi thậm chí còn thất vọng hơn khi họ bắt đầu cuộc trò chuyện với thương hiệu. Vậy tôi bắt đầu ở đâu? Vâng, đây là nơi bạn bè của bạn tại uxdesign.cc xuất hiện. Trong vài tháng qua chúng tôi đã chuẩn bị một bộ sưu tập các bài báo đặc biệt để giúp bạn trong quá trình xây dựng những trải nghiệm chatbot tốt hơn, có liên quan và thú vị hơn. Bộ sưu tập bao gồm các hướng dẫn, tài liệu tham khảo, ví dụ và câu chuyện từ lĩnh vực do nhóm chuyên gia trong ngành của chúng tôi viết, những người đã học được từ những thành công và thất bại của chính họ. Sẳn sàng? https://goo.gl/V102eZ",
          "relevence": "no"
        },
        {
          "url": "http://nhipsongso.tuoitre.vn/ung-dung-nhan-dang-giong-noi-co-hon-500-trieu-nguoi-dung-20170918095548951.htm",
          "title": "Ứng dụng nhận dạng giọng nói có hơn 500 triệu người dùng - Tuổi Trẻ Online",
          "content": "Trụ sở công ty iFlytek tại Hợp Phì, Trung Quốc - Ảnh: TECHNOLOGYREVIEWMột số người sử dụng ứng dụng này để gửi tin nhắn qua câu thoại lệnh trong lúc lái xe, hoặc để giao tiếp với một người sử dụng các ngôn ngữ địa phương khác tại Trung Quốc.Câu chuyện của ông Gang Xu, 46 tuổi ở Bắc Kinh là một ví dụ. Khi ông Gang cần trao đổi với một người thuê nhà của ông là người Canada về việc thanh toán phí thuê nhà và hóa đơn tiền điện, ông mở ứng dụng iFlytek Input trên smartphone và nhấp vào biểu tượng microphone và bắt đầu nói.Phần mềm iFlytek Input lập tức biến những lời nói của ông Gang Xu bằng tiếng Trung Quốc thành tin nhắn tiếng Anh rồi gửi tới người thuê nhà.Và cũng ứng dụng này sẽ lại biến những câu trả lời bằng tiếng Anh của người Canada kia thành tin nhắn Trung Quốc hiển  thị trên màn hình smartphone của ông Gang.Cứ thế hai người có thể trao đổi với nhau về những vấn đề liên quan một cách nhẹ nhàng, thoải mái, không cần tới sự trợ giúp của bất cứ thông dịch viên nào.TTO - Mạng xã hội lớn nhất thế giới thông báo từ nay trở đi, khoảng 4,5 tỉ nội dung dịch tự động mỗi ngày trên nền tảng này sẽ do  trí tuệ nhân tạo (AI) xử lý.TTO - Cuộc đua cho vị trí \"cường quốc\" về trí tuệ nhân tạo đang diễn ra giữa hai cường quốc kinh tế hàng đầu thế giới. Bên nào sẽ trở thành siêu cường trước tiên?TTO - Chính phủ Trung Quốc mới đây đặt mục tiêu tham vọng trở thành cường quốc đứng đầu về trí tuệ nhân tạo trong một thập kỷ tới với doanh thu ngành này đạt 59 tỉ USD năm 2025, AP đưa tin ngày 21-7.Ứng dụng iFlytek Input do công ty trí tuệ nhân tạo iFlytek phát triển. Đây là công ty chuyên áp dụng công nghệ deep learning trong một loạt các lĩnh vực như nhận dạng giọng nói, xử lý ngôn ngữ giao tiếp tự nhiên, dịch tự động và khai phá dữ liệu.Mặc dù đã đạt được một số tiến bộ ấn tượng trong công nghệ nhận dạng giọng nói và dịch tự động trực tiếp, tuy nhiên công ty iFlytek thừa nhận vẫn còn một số thách thức đáng kể cần giải quyết để hoàn thiện ứng dụng của mình, đặc biệt những thông tin liên quan tới ngữ cảnh giao tiếp đời thường để ứng dụng có thể truyền đạt chính xác nhất thông tin muốn trao đổi.Để làm được điều đó, chắc chắc iFlytek sẽ phải thu thập càng nhiều dữ liệu về các giao tiếp tương tác đời thường của mọi người càng tốt. Hiện ứng dụng của công ty này được cung cấp miễn phí và vẫn đang tiếp tục thu thập dữ liệu kể từ khi đưa vào hoạt động năm 2010.Nền tảng phát triển cho ứng dụng của iFlytek là iFlytek Open Platform cung cấp công nghệ AI dựa trên giọng nói cho hơn 400.000 nhà phát triển ở nhiều lĩnh vực ngành nghề khác nhau như nhà thông minh hay Internet di động.iFlytek hiện là công ty được định giá 80 tỉ nhân dân tệ (12 tỉ USD). Công ty này đang có những tham vọng vươn ra quốc tế và dự định mở rộng ra các ngôn ngữ khác ngoài tiếng Trung Quốc.  Trong quá trình đó, công ty này cũng đang làm thay đổi cách thức hoạt động của nhiều lĩnh vực ở Trung Quốc như lái xe, chăm sóc sức khỏe và giáo dục.Vui lòng nhập EmailEmail Không đúng định dạngVui lòng nhập Họ & Tên.\r\n            © Copyright 2017 TUOITRE.VN, All rights reserved\r\n            \r\n            ® Tuổi Trẻ Online giữ bản quyền nội dung trên website này.\r\n        \r\n            Thông tin Toà soạn -\r\n            Thông tin Thành Đoàn -\r\n            Liên hệ Quảng cáo \r\n            Điện thoại liên hệ: 0918.033.133\r\n        ",
          "relevence": "no"
        }
      ]
    },
    {
      "query": "thuật toán svm",
      "description": "tìm hiểu về thuật toán svm, nền tảng và kỹ thuật kernel",
      "sites": [
        {
          "url": "https://vi.wikipedia.org/wiki/M%C3%A1y_vect%C6%A1_h%E1%BB%97_tr%E1%BB%A3",
          "title": "Máy vectơ hỗ trợ – Wikipedia tiếng Việt",
          "content": "Máy vectơ hỗ trợ (SVM - viết tắt tên tiếng Anh support vector machine) là một khái niệm trong thống kê và khoa học máy tính cho một tập hợp các phương pháp học có giám sát liên quan đến nhau để phân loại và phân tích hồi quy. SVM dạng chuẩn nhận dữ liệu vào và phân loại chúng vào hai lớp khác nhau. Do đó SVM là một thuật toán phân loại nhị phân. Với một bộ các ví dụ luyện tập thuộc hai thể loại cho trước, thuật toán luyện tập SVM xây dựng một mô hình SVM để phân loại các ví dụ khác vào hai thể loại đó. Một mô hình SVM là một cách biểu diễn các điểm trong không gian và lựa chọn ranh giới giữa hai thể loại sao cho khoảng cách từ các ví dụ luyện tập tới ranh giới là xa nhất có thể. Các ví dụ mới cũng được biểu diễn trong cùng một không gian và được thuật toán dự đoán thuộc một trong hai thể loại tùy vào ví dụ đó nằm ở phía nào của ranh giới.Một máy vectơ hỗ trợ xây dựng một siêu phẳng hoặc một tập hợp các siêu phẳng trong một không gian nhiều chiều hoặc vô hạn chiều, có thể được sử dụng cho phân loại, hồi quy, hoặc các nhiệm vụ khác. Một cách trực giác, để phân loại tốt nhất thì các siêu phẳng nằm ở càng xa các điểm dữ liệu của tất cả các lớp (gọi là hàm lề) càng tốt, vì nói chung lề càng lớn thì sai số tổng quát hóa của thuật toán phân loại càng bé.Trong nhiều trường hợp, không thể phân chia các lớp dữ liệu một cách tuyến tính trong một không gian ban đầu được dùng để mô tả một vấn đề. Vì vậy, nhiều khi cần phải ánh xạ các điểm dữ liệu trong không gian ban đầu vào một không gian mới nhiều chiều hơn, để việc phân tách chúng trở nên dễ dàng hơn trong không gian mới. Để việc tính toán được hiệu quả, ánh xạ sử dụng trong thuật toán SVM chỉ đòi hỏi tích vô hướng của các vectơ dữ liệu trong không gian mới có thể được tính dễ dàng từ các tọa độ trong không gian cũ. Tích vô hướng này được xác định bằng một hàm hạt nhân K(x,y) phù hợp.[1] Một siêu phẳng trong không gian mới được định nghĩa là tập hợp các điểm có tích vô hướng với một vectơ cố định trong không gian đó là một hằng số. Vectơ xác định một siêu phẳng sử dụng trong SVM là một tổ hợp tuyến tính của các vectơ dữ liệu luyện tập trong không gian mới với các hệ số αi. Với siêu phẳng lựa chọn như trên, các điểm x trong không gian đặc trưng được ánh xạ vào một siêu mặt phẳng là các điểm thỏa mãn:Ghi chú rằng nếu K(x,y) nhận giá trị ngày càng nhỏ khi y xa dần khỏi x thì mỗi số hạng của tổng trên được dùng để đo độ tương tự giữa x với điểm xi tương ứng trong dữ liệu luyện tập. Như vậy, tác dụng của tổng trên chính là so sánh khoảng cách giữa điểm cần dự đoán với các điểm dữ liệu đã biết. Lưu ý là tập hợp các điểm x được ánh xạ vào một siêu phẳng có thể có độ phức tạp tùy ý trong không gian ban đầu, nên có thể phân tách các tập hợp thậm chí không lồi trong không gian ban đầu.Thuật toán SVM ban đầu được tìm ra bởi Vladimir N. Vapnik và dạng chuẩn hiện nay sử dụng lề mềm được tìm ra bởi Vapnik và Corinna Cortes năm 1995.[2]Phân loại thống kê là một nhiệm vụ phổ biến trong học máy. Trong mô hình học có giám sát, thuật toán được cho trước một số điểm dữ liệu cùng với nhãn của chúng thuộc một trong hai lớp cho trước. Mục tiêu của thuật toán là xác định xem một điểm dữ liệu mới sẽ được thuộc về lớp nào. Mỗi điểm dữ liệu được biểu diễn dưới dạng một vector p-chiều, và ta muốn biết liệu có thể chia tách hai lớp dữ liệu bằng một siêu phẳng p − 1 chiều. Đây gọi là phân loại tuyến tính. Có nhiều siêu phẳng có thể phân loại được dữ liệu. Một lựa chọn hợp lý trong chúng là siêu phẳng có lề lớn nhất giữa hai lớp.Ta có một tập huấn luyện \n  \n    \n      \n        \n          \n            D\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}}\n  \n gồm n điểm có dạngvới yi mang giá trị 1 hoặc −1, xác định lớp của điểm \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n  \n. Mỗi \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n  \n là một vectơ thực p-chiều. Ta cần tìm siêu phẳng có lề lớn nhất chia tách các điểm có \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle y_{i}=1}\n  \n và các điểm có \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        −\n        1\n      \n    \n    {\\displaystyle y_{i}=-1}\n  \n. Mỗi siêu phẳng đều có thể được viết dưới dạng một tập hợp các điểm \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n  \n thỏa mãnvới \n  \n    \n      \n        ⋅\n      \n    \n    {\\displaystyle \\cdot }\n  \n ký hiệu cho tích vô hướng và \n  \n    \n      \n        \n          \n            w\n          \n        \n      \n    \n    {\\displaystyle {\\mathbf {w} }}\n  \n là một vectơ pháp tuyến của siêu phẳng. Tham số \n  \n    \n      \n        \n          \n            \n              b\n              \n                ∥\n                \n                  w\n                \n                ∥\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {b}{\\|\\mathbf {w} \\|}}}\n  \n xác định khoảng cách giữa gốc tọa độ và siêu phẳng theo hướng vectơ pháp tuyến \n  \n    \n      \n        \n          \n            w\n          \n        \n      \n    \n    {\\displaystyle {\\mathbf {w} }}\n  \n.Chúng ta cần chọn \n  \n    \n      \n        \n          \n            w\n          \n        \n      \n    \n    {\\displaystyle {\\mathbf {w} }}\n  \n và \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n để cực đại hóa lề, hay khoảng cách giữa hai siêu mặt song song ở xa nhau nhất có thể trong khi vẫn phân chia được dữ liệu. Các siêu mặt ấy được xác định bằngvàĐể ý rằng nếu dữ liệu huấn luyện có thể được chia tách một cách tuyến tính, thì ta có thể chọn hai siêu phẳng của lề sao cho không có điểm nào ở giữa chúng và sau đó tăng khoảng cách giữa chúng đến tối đa có thể. Bằng hình học, ta tìm được khoảng cách giữa hai siêu phẳng là \n  \n    \n      \n        \n          \n            \n              2\n              \n                ∥\n                \n                  w\n                \n                ∥\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {2}{\\|\\mathbf {w} \\|}}}\n  \n. Vì vậy ta muốn cực tiểu hóa giá trị \n  \n    \n      \n        ∥\n        \n          w\n        \n        ∥\n      \n    \n    {\\displaystyle \\|\\mathbf {w} \\|}\n  \n. Để đảm bảo không có điểm dữ liệu nào trong lề, ta thêm vào các điều kiện sau:Với mỗi \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n ta cóhoặcCó thể viết gọn lại như sau với mọi \n  \n    \n      \n        1\n        ≤\n        i\n        ≤\n        n\n      \n    \n    {\\displaystyle 1\\leq i\\leq n}\n  \n:Tóm lại, ta có bài toán tối ưu hóa sau:Cực tiểu hóa (theo \n  \n    \n      \n        \n          \n            w\n          \n          ,\n          b\n        \n      \n    \n    {\\displaystyle {\\mathbf {w} ,b}}\n  \n)với điều kiện (với mọi \n  \n    \n      \n        i\n        =\n        1\n        ,\n        …\n        ,\n        n\n      \n    \n    {\\displaystyle i=1,\\dots ,n}\n  \n)Bài toán tối ưu ở mục trên tương đối khó giải vì hàm mục tiêu phụ thuộc vào ||w||, là một hàm có khai căn. Tuy nhiên có thể thay ||w|| bằng hàm mục tiêu \n  \n    \n      \n        \n          \n            \n              1\n              2\n            \n          \n        \n        ∥\n        \n          w\n        \n        \n          ∥\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {1}{2}}\\|\\mathbf {w} \\|^{2}}\n  \n (hệ số 1/2 để tiện cho các biến đổi toán học sau này) mà không làm thay đổi lời giải (lời giải của bài toán mới và bài toán ban đầu có cùng w và b). Đây là một bài toán quy hoạch toàn phương. Cụ thể hơn:Cực tiểu hóa (theo \n  \n    \n      \n        \n          \n            w\n          \n          ,\n          b\n        \n      \n    \n    {\\displaystyle {\\mathbf {w} ,b}}\n  \n)với điều kiện (với mọi \n  \n    \n      \n        i\n        =\n        1\n        ,\n        …\n        ,\n        n\n      \n    \n    {\\displaystyle i=1,\\dots ,n}\n  \n)Bằng cách thêm các nhân tử Lagrange \n  \n    \n      \n        \n          α\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\alpha }}}\n  \n, bài toán trên trở thànhnghĩa là ta cần tìm một điểm yên ngựa. Khi đó, tất cả các điểm không nằm trên lề, nghĩa là \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        (\n        \n          w\n        \n        ⋅\n        \n          \n            x\n            \n              i\n            \n          \n        \n        −\n        b\n        )\n        −\n        1\n        >\n        0\n      \n    \n    {\\displaystyle y_{i}(\\mathbf {w} \\cdot \\mathbf {x_{i}} -b)-1>0}\n  \n đều không ảnh hưởng đến giá trị hàm mục tiêu vì ta có thể chọn \n  \n    \n      \n        \n          α\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{i}}\n  \n bằng không.Có thể giải bài toán này bằng các kĩ thuật thông thường cho quy hoạch toàn phương. Theo điều kiện Karush–Kuhn–Tucker, lời giải có thể được viết dưới dạng tổ hợp tuyến tính của các vectơ luyện tậpChỉ có một vài \n  \n    \n      \n        \n          α\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{i}}\n  \n nhận giá trị lớn hơn 0. Các điểm \n  \n    \n      \n        \n          \n            x\n            \n              i\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x_{i}} }\n  \n tương ứng là các vectơ hỗ trợ nằm trên lề và thỏa mãn \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        (\n        \n          w\n        \n        ⋅\n        \n          \n            x\n            \n              i\n            \n          \n        \n        −\n        b\n        )\n        =\n        1\n      \n    \n    {\\displaystyle y_{i}(\\mathbf {w} \\cdot \\mathbf {x_{i}} -b)=1}\n  \n. Từ điều kiện này, ta nhận thấytừ đó ta suy ra được giá trị \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n. Trên thực tế, một cách thức tốt hơn để tính \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n là tính giá trị trung bình từ tất cả \n  \n    \n      \n        \n          N\n          \n            S\n            V\n          \n        \n      \n    \n    {\\displaystyle N_{SV}}\n  \n vectơ hỗ trợ:Nếu viết điều kiện phân loại dưới dạng đối ngẫu không điều kiện thì sẽ dễ dàng nhận thấy siêu phẳng với lề lớn nhất, và do đó nhiệm vụ phân loại, chỉ phụ thuộc vào các điểm luyện tập nằm trên lề, còn gọi là các vectơ hỗ trợ.Vì \n  \n    \n      \n        ∥\n        \n          w\n        \n        \n          ∥\n          \n            2\n          \n        \n        =\n        w\n        ⋅\n        w\n      \n    \n    {\\displaystyle \\|\\mathbf {w} \\|^{2}=w\\cdot w}\n  \n và \n  \n    \n      \n        \n          w\n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \n            α\n            \n              i\n            \n          \n          \n            y\n            \n              i\n            \n          \n          \n            \n              x\n              \n                i\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {w} =\\sum _{i=1}^{n}{\\alpha _{i}y_{i}\\mathbf {x_{i}} }}\n  \n, ta nhận thấy bài toán đối ngẫu của SVM là chính là bài toán tối ưu hóa sau:Cực đại hóa (theo \n  \n    \n      \n        \n          α\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{i}}\n  \n)với điều kiện (với mọi \n  \n    \n      \n        i\n        =\n        1\n        ,\n        …\n        ,\n        n\n      \n    \n    {\\displaystyle i=1,\\dots ,n}\n  \n)và điều kiện sau ứng với việc cực tiểu hóa theo \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \nỞ đây hàm hạt nhân được định nghĩa là \n  \n    \n      \n        k\n        (\n        \n          \n            x\n          \n          \n            i\n          \n        \n        ,\n        \n          \n            x\n          \n          \n            j\n          \n        \n        )\n        =\n        \n          \n            x\n          \n          \n            i\n          \n        \n        ⋅\n        \n          \n            x\n          \n          \n            j\n          \n        \n      \n    \n    {\\displaystyle k(\\mathbf {x} _{i},\\mathbf {x} _{j})=\\mathbf {x} _{i}\\cdot \\mathbf {x} _{j}}\n  \n.Sau khi giải xong, có thể tính \n  \n    \n      \n        \n          w\n        \n      \n    \n    {\\displaystyle \\mathbf {w} }\n  \n từ các giá trị \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n tìm được như sau:Năm 1995, Corinna Cortes và Vladimir N. Vapnik đề xuất một ý tưởng mới cho phép thuật toán gán nhãn sai cho một số ví dụ luyện tập.[2] Nếu không tồn tại siêu phẳng nào phân tách được hai lớp dữ liệu, thì thuật toán lề mềm sẽ chọn một siêu phẳng phân tách các ví dụ luyện tập tốt nhất có thể, và đồng thời cực đại hóa khoảng cách giữa siêu phẳng với các ví dụ được gán đúng nhãn. Phương pháp này sử dụng các biến bù \n  \n    \n      \n        \n          ξ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\xi _{i}}\n  \n, dùng để đo độ sai lệch của ví dụ \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n  \nHàm mục tiêu có thêm một số hạng mới để phạt thuật toán khi \n  \n    \n      \n        \n          ξ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\xi _{i}}\n  \n khác không, và bài toán tối ưu hóa trở thành việc trao đổi giữa lề lớn và mức phạt nhỏ. Nếu hàm phạt là tuyến tính thì bài toán trở thành:với điều kiện (với mọi \n  \n    \n      \n        i\n        =\n        1\n        ,\n        …\n        n\n      \n    \n    {\\displaystyle i=1,\\dots n}\n  \n)Có thể giải bài toán trên bằng nhân tử Lagrange tương tự như trường hợp cơ bản ở trên. Bài toán cần giải trở thành:với \n  \n    \n      \n        \n          α\n          \n            i\n          \n        \n        ,\n        \n          β\n          \n            i\n          \n        \n        ≥\n        0\n      \n    \n    {\\displaystyle \\alpha _{i},\\beta _{i}\\geq 0}\n  \n.Cực đại hóa (theo \n  \n    \n      \n        \n          α\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{i}}\n  \n)với điều kiện (với mọi \n  \n    \n      \n        i\n        =\n        1\n        ,\n        …\n        ,\n        n\n      \n    \n    {\\displaystyle i=1,\\dots ,n}\n  \n)vàƯu điểm của việc dùng hàm phạt tuyến tính là các biến bù biến mất khỏi bài toán đối ngẫu, và hằng số C chỉ xuất hiện dưới dạng một chặn trên cho các nhân tử Lagrange. Cách đặt vấn đề trên đã mang lại nhiều thành quả trong thực tiễn, và Cortes và Vapnik đã nhận được giải Paris Kanellakis của ACM năm 2008 cho đóng góp này.[3] Các hàm phạt phi tuyến cũng được sử dụng, đặc biệt là để giảm ảnh hưởng của các trường hợp ngoại lệ, tuy nhiên nếu không lựa chọn hàm phạt cẩn thận thì bài toán trở thành không lồi, và việc tìm lời giải tối ưu toàn cục thường là rất khó.",
          "relevence": "yes"
        },
        {
          "url": "https://viblo.asia/p/mot-chut-ve-thuat-toan-svm-support-vector-machine-algorithm-OeVKBgGAZkW",
          "title": "Một chút về thuật toán SVM (Support Vector Machine algorithm) - Viblo",
          "content": "Việc nắm vững về các thuật toán máy tính không phải là khủng khiếp với tất cả mọi người. Đa số những người mới bắt đâu sẽ học về đệ quy. Nó đơn giản để học và sử dụng, nhưng điều đó có giải quyết được  mục tiêu của bạn. Tất nhiên là không, bởi vì bạn có thể làm được nhiều hơn chứ không chỉ là hồi quy dữ liệu nào đấy.\nCác thuật toán máy tính (machine learning algorithms), xem chúng như một kho vũ khí với rất nhiều chủng loại rìu, cưa, cuốc, xẻng, súng, lựu đạn..v.v, bạn có nhiều công cụ khác nhau, nhưng cần phải học cách sử dụng chúng vào đúng thời điểm. Như là một phép loại suy, suy nghĩ về việc \"đệ quy\" như là một thanh kiếm có thể cắt và chia dữ liệu một cách hiệu quả, nhưng nó không có khả năng xử lí với các dữ liệu rất phức tạp. Ngược lại Support Vector Machines (SVM) như là một con dao nhỏ sắc nhọn- nó hoạt động trên các tập dữ liệu nhỏ. nhưng trên đó nó có khả năng xử lí mạnh mẽ hơn trong việc xây dựng các mô hình.SVM là một thuật toán giám sát, nó có thể sử dụng cho cả việc phân loại hoặc đệ quy. Tuy nhiên nó được sử dụng chủ yếu cho việc phân loại. Trong thuật toán này, chúng ta vẽ đồi thị dữ liệu là các điểm trong n  chiều ( ở đây n là số lượng các tính năng bạn có) với giá trị của mỗi tính năng sẽ là một phần liên kết. Sau đó chúng ta thực hiện tìm \"đường bay\" phân chia các lớp. Đường bay - nó chỉ hiểu đơn giản là 1 đường thằng có thể phân chia các lớp ra thành hai phần riêng biệt.\n\nSupport Vectors hiểu một cách đơn giản là các đối tượng trên đồ thị tọa độ quan sát,  Support Vector Machine là một biên giới để chia hai lớp tốt nhất.Ở trên, chúng ta đã thấy được việc chia hyper-plane. Bấy giờ  làm thế nào chúng ta có thể xác định \"Làm sao để vẽ-xác định đúng hyper-plane\".\nChúng ta sẽ theo các tiêu chí sau:Bạn cần nhớ quy tắc số một để chọn 1 hyper-lane, chọn một hyper-plane để phân chia hai lớp tốt nhất. Trong ví dụ này chính là đường BỞ đây chúng ta cũng có 3 đường hyper-plane (A,B và C), theo quy tắc số 1, chúng đều thỏa mãn\n\nQuy tắc thứ hai chính là xác định khoảng cách lớn nhất từ điểu gần nhất của một lớp nào đó đến đường hyper-plane. Khoảng cách này được gọi là \"Margin\", Hãy nhìn hình bên dưới, trong đấy bạn có thể nhìn thấy khoảng cách margin lớn nhất đấy là đường C. Ở đây bạn nhớ nếu chọn lầm hyper-lane có margin thấp hơn thì sau này khi dữ liệu tăng lên thì sẽ sinh ra nguy cơ cao về việc xác định nhầm lớp cho dữ liệuBạn hãy sử dụng các nguyên tắc đã nêu trên để chọn ra hyper-plane cho trường hợp sau\n\nCó thể có một vài bạn sẽ chọn đường B bởi vì nó có margin cao hơn đường A, nhưng đấy sẽ không đúng bởi vì nguyên tắt đầu tiên sẽ là nguyên tắc số 1., chúng ta cần chọn hyper-plane để phân chia các lớp thành riêng biệt. Vì vậy đường A mới là lựa chọn chính xác.Tiếp the hãy xem hình bên dưới, mình không thể chia  thành hai lớp riêng biệt với 1 đường thẳng, để tạo 1 phần chỉ có các ngôi sao và một vùng chỉ chứa các điểm tròn.\n\nỞ đây chúng ta sẽ chấp nhận, một ngôi sao ở bên ngoài cuối được em như một ngôi sao phía ngoài hơn, SVM có tính năng cho phép bỏ qua các ngoại lệ và tìm ra hyper-plane có biên giới tối đa . Do đó chúng chúng ta có thể nói, SVM có khả năng mạnh trong việc chấp nhận ngoại lệ.\nTrong trường hợp dưới đây, chúng ta khong thể tìm ra 1 đường hyper-plane tương đối để chia các lớp, vậy làm thế nào để SVM phân tách dữ liệu thành hai lớp riêng biệt? Cho đến bây giờ chúng ta chỉ nhìn vào các đường tuyến tính hyper-plane\n\nSVM có thể giải quyết vấn đề này, Khá đơn giản, nó sẽ được giải quyết bằng việc thêm một tính năng, Ở đây chúng ta sẽ thêm tính năng z = x^2+ y^2. Bây giờ  dữ liện sẽ được biến đổi theo trục x và z như sau\n\nTrong sơ đồ trên, các điểm cần xem xét là:Trong SVM, rất dễ dàng để có một siêu phẳng tuyến tính (linear hyper-plane) để chia thành hai lớp, Nhưng một câu hỏi sẽ nảy sinh đấy là,  chúng ta có cần phải thêm một tính năng phân chia này bằng tay hay không. Không, bởi vì SVM có một kỹ thuật được gọi là kernel trick ( kỹ thuật hạt nhân), đây là tính năng có không gian đầu vào có chiều sâu thấm và biến đổi nó thành không gian có chiều cao hơn, tức là nó không phân chia các vấn đề thành các vấn đề riêng biệt, các tính năng này được gọi là kernel. Nói một cách đơn giản nó thực hiện một số biết đổi dữ liệu phức tạp, sau đó tìm ra quá trình tách dữ liệu dựa trên các nhãn hoặc đầu ra mà chúng ra đã xác định trước.Mình sẽ tìm hiểu và demo về SVM trong mục tiếp theo với python. Thân mọi người đã đọc\n                    © 2017 Viblo. All rights reserved.\n                ",
          "relevence": "yes"
        },
        {
          "url": "https://viblo.asia/p/ung-dung-support-vector-machine-trong-bai-toan-phan-loai-hoa-PdbGnLXBkyA",
          "title": "Ứng dụng Support Vector Machine trong bài toán phân loại hoa - Viblo",
          "content": "Xin chào các bạn, mình lại trở lại rồi đây. Tiếp tục với loạt bài viết về Machine Learning trong bài trước mình đã giới thiệu với các bạn một cách tổng quan nhất về Support Vector Machine - một phương pháp vô cùng hiệu quả trong bài toán phân lớp dữ liệu. Tuy nhiên nếu đọc lý thuyết nhiều quá hẳn sẽ rất khó khăn cho những bạn mới bắt đầu (mình cũng mới bắt đầu tìm hiểu về Machine Learning mà). Chính vì thế, ngày hôm nay chúng ta sẽ đến với một bài toán khá cụ thể đó là sử dụng SVM trong bài toán phân loại hoa. Hiểu nôm na đó là chúng ta sẽ xây dựng một mô hình (model) sao cho khi input vào là một bông hoa và ouput trả ra kết quả nó là loài hoa gì. Thú vị phải không các bạn . OK chúng ta bắt đầu thôi nào.Cái gì cũng phải có cơ sở một chút phải không nào. Ví dụ trong bài toán của ta đang quan tâm đến đối tượng là hoa chẳng hạn, vậy có một câu hỏi đặt ra là \"Làm thế nào để phân biệt các loại hoa với nhau ???\".Con người chúng ta qua quá trình lớn lên đã tự tích lũy cho mình những đặc điểm rất riêng của mỗi loài hoa. Ví dụ hoa loa kèn có hình giống cái kèn, hoa huệ màu trắng có mùi thơm đặc trưng, hoa hồng màu đỏ và có gai... Vậy tức là trong đầu chúng ta đã có một tập dữ liệu về các loài hoa và nó chính là căn cứ để chúng ta nhận dạng một loài hoa khi chúng ta gặp phải. Nguyên tắc học của máy tính cũng như vậy thôi. Chúng ta cần phải cung cấp cho nó một tập dữ liệu và huấn luyện cho nó băng một cách nào đó để nó có thể căn cứ vào đó mà đoán được một bông hoa mới thuộc vào loài hoa nào. OK, quan trọng nhất vẫn là phải có một tập dữ liệu huấn luyện phải không nào? Trong bài viết này chúng ta sẽ sử dụng một tập dữ liệu về hoa vô cùng nổi tiếng đó là tập dữ liệu Iris. Chúng ta cùng nhau tìm hiểu sâu hơn về tập dữ liệu này nhé.Tập dữ liệu này còn có tên gọi khác là **Fisher's Iris ** vì nó do Ronald Fisher thu thập và tổng hợp. Tập dữ liệu này gồm 50 mẫu về 3 loài hoa khác nhau của họ Iris là (Iris setosa, Iris virginica và Iris versicolor). Cho cái ảnh cho các bạn dễ hình dungVới mỗi một mẫu hoa này hắn thu thập bốn thuộc tính là chiều dài và chiều rộng của đài hoa và cánh hoa với đơn vị centimet. Để có thể sử dụng tập dữ liệu này chúng ta sẽ  sử dụng thư viện datasets trong sklearn.Sau khi đã có dữ liệu rồi, chúng ta hoàn toàn có thể chạy thuật toán SVM ngay để tiến hành phân loại. Tuy nhiên, mình nghĩ nên giúp các bạn có một cái nhìn trực quan hơn về tập dữ liệu này. Mà để hiển thị trực quan nhất thì không gì bằng biểu đồ với thư viện matplotlib thần thánh. Chúng ta cùng thử biểu diễn trên đồ thị xem tập dữ liệu mà chúng ta nhét vào máy tính nó là cái gì nhé.Chúng ta tưởng tượng tập dữ liệu của ta là một tập hợp của 150 điểm dữ liệu tương ứng với 150 bông hoa.. Chúng ta sẽ lấy chúng ta từ datasets bằng hàm sau:Lúc này đã có một tập hợp các điểm dữ liệu. Mỗi điểm dữ liệu bao gồm 4 thuộc tính như đã nói ở trên. Tuy nhiên để biểu diễn trong đồ thị hai chiều chúng ta cần phải giảm bớt số thuộc tính biểu diễn. Ở đây giả sử chọn hai thuộc tính đầu tiên là độ rộng và chiều cao của đài hoa. Chúng ta có hàm xử lý vẽ đồ thị 2D như sau:Và đây là thành quảChúng ta có thể thấy được các điểm dữ liệu với hai thuộc tính trên đồ thị hai chiều. Các điểm này được phân biệt bằng mắt thường với 3 màu khác nhau. Tuy nhiên muốn máy tính phân biệt được như chúng ta lại là một câu chuyện hoàn toàn khác và bạn sẵn sàng cùng tôi đi đến cuối cùng của câu chuyện này chứ. OK chúng ta tiếp tục thôiVới tập dữ liệu Iris chúng ta cần phân loại các bông hoa thành 3 lớp dữ liệu. Sử dụng SVM với các phương pháp khác nhau sẽ cho hiệu quả phân lớp khác nhau. Cũng tương tự như trên, chúng ta chỉ xem xét đến 2 thuộc tính đầu tiên của tập dữ liệu, tức là phân lớp trong không gian 2 chiều. Chúng ta sử dụng các Kernel khác nhau bao gồm:Đầu tiên chúng ta cần huấn luyện dữ liệuTiếp theo là việc đẩy các mô hình thu được vào đồ thịBây giờ sau khi vẽ các đồ thị trên là chúng ta có thể so sánh hiệu quả phân lớp của các Kernel khác nhau rồi đấy. Kết quả như sau:Bằng việc phân lớp chúng ta có thể chia mặt phẳng tọa độ thành các phần khác nhau. Khi có một điểm dữ liệu mới chúng ta có thể dựa vào tọa độ của chúng để phán đoán xem nó thuộc lớp nào.Chúng ta hoàn toàn có thể  tự custom một Kernel riêng của mình. Điều đó làm cho SVM trở nên linh hoạt hơn.Mình rất hi vọng được trình bày với các bạn sâu hơn về Custom Kernel của mình để việc phân lớp cụ thể là phân lớp ảnh được tốt hơn. Nhưng trong phạm vi bài viết này mình sẽ dừng lại ở các Kernel tuyến tính và phi tuyến thôi đã. Kẻo nói nhiều quá chúng ta khó năm bắt được hết vấn đề.Các bạn có thể tham khảo mã nguồn mình viết trong bài này tại đâySVM Kernel Scikit LearnSVM Scikit LearnIris Flower Dataset\n                    © 2017 Viblo. All rights reserved.\n                ",
          "relevence": "no"
        },
        {
          "url": "https://www.stdio.vn/articles/read/436/gioi-thieu-ve-mo-hinh-svm",
          "title": "Giới Thiệu Về Mô Hình SVM :: Bài viết :: STDIO",
          "content": "SVM là mô hình được sử dụng trong nhiều ngành, là một mô hình máy học giám sát được dùng để phân tích, phân lớp dữ liệu. Có thể những điều ở đây khá trừ tượng. Trong bài viết này, tôi sẽ giới thiệu một cách tổng quan về mô hình SVM và ví dụ về SVM trong OPenCV.Bài viết nằm trong loạt bài viết chương trình Tự Học OpenCV Qua Các Ví Dụ Thực Tế của STDIO. Kiến thức trong bài viết này rất quan trọng trong việc hoàn thành ứng dụng nhận diện biển số xe và nhận diện chữ viết tay,...Bài viết hướng tới các bạn đọc mới tìm hiểu về xử lý ảnh, đặc biệt là với OpenCV và chưa có kiến thức tương đương.Trong bài viết này tôi sử dụng Windows 10 (64-bit), Visual Stdio 2013 Ultimate và OpenCV 3.0.SVM (Support Vector Machine) là một khái niệm trong thống kê và khoa học máy tính cho một tập hợp các phương pháp học có giám sát liên quan đến nhau để phân loại và phân tích hồi quy.SVM là một thuật toán phân loại nhị phân, SVM nhận dữ liệu vào và phân loại chúng vào hai lớp khác nhau. Với một bộ các ví dụ luyện tập thuộc hai thể loại cho trước, thuật toán luyện tập SVM xây dựng một mô hình SVM để phân loại các ví dụ khác vào hai thể loại đó.Tôi có một khôn gian có nhiều điểm và các kí hiệu như sau:Phân tíchvàTổng quanVới các điểm tổng quan ở trên thì nhiệm vụ chính là phân loại thống kế.Dữ liệu được huấn luyện được tạo thành bởi một tập điểm có giá trị 2D. Ở đoạn code trên, tôi có:Chú ý: Số lượng phần tử của ma trận labels bằng với số cột của ma trận của trainingData.Chú ý: Hàm CvSVM::train() sử dụng kiểu dữ liệu truyền vào là kiểu Mat.Phân tíchChú ý: Như ở phần Đối tượng hướng đến trong bài viết. Tôi sử dụng OpenCV 3.0. Với các phiên bản khác của OpenCV, để thiết lập thông số cho SVM sẽ có cách hiện thực khác.Để xây dựng mô hình huấn luyện SVM, tôi sử dụng phương thức CvSVM::train().Như vậy tôi đã huấn luyện xong cho mô hình SVM của ví dụ ở trên.Phương pháp CvSVM::predict() được sử dụng để phân loại một mẫu đầu vào bằng cách sử dụng một SVM được huấn luyện. Như bạn thấy ở trên, đây chính là kết quả của đoạn code trên.Trong OpenCV 3.0, để có thể lấy được các điểm Support Vector thì sử dụng phương thức SVM::getSupportVectors(). Kiểu trả về ở đây là một Mat, với số hàng (Rows) chính là số lượng của các điểm Support VectorTrong bài viết này, tôi đã giới thiệu sơ qua về mô hình SVM cũng như ví dụ về hoạt động của SVM trong OpenCV. Ứng dụng của SVM trong các lĩnh vực nói chung là rất nhiều. Ở trong xử lý ảnh, SVM cũng tác dụng to lớn nhiều trong các ứng dụng như trong ứng dụng nhận dạng ký tự viết tay, nhận dạng biển số xe,...Mọi thắc mắc có thể bình luận tại bài viết hoặc liên hệ với  Trương Đạt.http://docs.opencv.org/https://en.wikipedia.org/wiki/Support_vector_machine",
          "relevence": "yes"
        },
        {
          "url": "https://machinelearningcoban.com/2017/04/22/kernelsmv/",
          "title": "Machine Learning cơ bản",
          "content": "Bạn đọc được khuyến khích đọc Bài 19 và Bài 20 trước khi đọc bài này.\nCó một sự tương ứng thú vị giữa hai nhóm thuật toán phân lớp phổ biến nhất: Neural Network và Support Vector Machine. Chúng đều bắt đầu từ bài toán phân lớp với 2 linearly separable classes, tiếp theo đến 2 almost linear separable classes, đến bài toán có nhiều classes rồi các bài toán với biên không tuyến tính. Sự tương ứng được cho trong bảng dưới đây:Trong Bài 21 này, tôi sẽ viết về Kernel SVM, tức việc áp dụng SVM lên bài toán mà dữ liệu giữa hai classes là hoàn toàn không linear separable (tôi tạm dịch là không phân biệt tuyến tính). Bài toán phân biệt nhiều classes sẽ được tôi trình bày trong Bài 22: Multiclass SVM.Ý tưởng cơ bản của Kernel SVM và các phương pháp kernel nói chung là tìm một phép biến đổi sao cho dữ liệu ban đầu là không phân biệt tuyến tính được biến sang không gian mới. Ở không gian mới này, dữ liệu trở nên phân biệt tuyến tính.Xét ví dụ dưới đây với việc biến dữ liệu không phân biệt tuyến tính trong không gian hai chiều thành phân biệt tuyến tính trong không gian ba chiều bằng cách giới thiệu thêm một chiều mới:Để xem ví dụ này một cách sinh động hơn, bạn có thể xem clip nhỏ dưới đây:Nói một cách ngắn gọn, Kernel SVM là việc đi tìm một hàm số biến đổi dữ liệu \\(\\mathbf{x}\\) từ không gian feature ban đầu thành dữ liệu trong một không gian mới bằng hàm số \\(\\Phi(\\mathbf{x})\\). Trong ví dụ này, hàm \\(\\Phi()\\) đơn giản là giới thiệu thêm một chiều dữ liệu mới (một feature mới) là một hàm số của các features đã biết. Hàm số này cần thỏa mãn mục đích của chúng ta: trong không gian mới, dữ liệu giữa hai classes là phân biệt tuyến tính hoặc gần như phần biệt tuyến tính. Khi đó, ta có thể dùng các bộ phân lớp tuyến tính thông thường như PLA, Logistic Regression, hay Hard/Soft Margin SVM.Nếu phải so sánh, ta có thể thấy rằng hàm biến đổi \\(\\Phi()\\) tương tự như activation functions trong Neural Networks. Tuy nhiên, có một điểm khác biệt ở đây là: trong khi nhiệm vụ của activation function là phá vỡ tính tuyến tính của mô hình, hàm biến đổi \\(\\Phi()\\) đi biến dữ liệu không phân biệt tuyến tính thành phân biệt tuyến tính. Như vậy là để đạt được mục đích chung, ta có hai cách nhìn khác nhau về cách giải quyết.Các hàm \\(\\Phi()\\) thường tạo ra dữ liệu mới có số chiều cao hơn số chiều của dữ liệu ban đầu, thậm chí là vô hạn chiều. Nếu tính toán các hàm này trực tiếp, chắc chắn chúng ta sẽ gặp các vấn đề về bộ nhớ và hiệu năng tính toán. Có một cách tiếp cận là sử dụng các kernel functions mô tả quan hệ giữa hai điểm dữ liệu bất kỳ trong không gian mới, thay vì đi tính toán trực tiếp từng điểm dữ liệu trong không gian mới. Kỹ thuật này được xây dựng dựa trên quan sát về bài toán đối ngẫu của SVM.Trong Mục 2 dưới đây, chúng ta cùng tìm hiểu cơ sở toán học của Kernel SVM và Mục 3 sẽ giới thiệu một số hàm Kernel thường được sử dụng.Tôi xin nhắc lại bài toán đối ngẫu trong Soft Margin SVM cho dữ liệu gần phân biệt tuyến tính:\\begin{eqnarray}\n     \\lambda &=& \\arg \\max_{\\lambda} \\sum_{n=1}^N \\lambda_n - \\frac{1}{2} \\sum_{n=1}^N\\sum_{m=1}^N \\lambda_n \\lambda_m y_n y_m \\mathbf{x}_n^T \\mathbf{x}_m &&\\\\\n     \\text{subject to:}~ && \\sum_{n=1}^N \\lambda_ny_n = 0 &&\\quad\\quad\\quad\\quad(1)\\\\\n     && 0 \\leq \\lambda_n \\leq C, ~\\forall n= 1, 2, \\dots, N \n \\end{eqnarray}Trong đó:\\(N\\): số cặp điểm dữ liệu trong tập training.\\(\\mathbf{x}_n\\): feature vector của dữ liệu thứ \\(n\\) trong tập training.\\(y_n\\): nhãn của dữ liệu thứ \\(n\\), bằng 1 hoặc -1.\\(\\lambda_n\\): nhân tử Lagrange ứng với điểm dữ liệu thứ \\(n\\).\\(C\\): hằng số dương giúp cân đối độ lớn của margin và sự hy sinh của các điểm nằm trong vùng không an toàn. Khi \\(C = \\infty\\) hoặc rất lớn, Soft Margin SVM trở thành Hard Margin SVM.Sau khi giải được \\(\\lambda\\) cho bài toán \\((1)\\), nhãn của một điểm dữ liệu mới sẽ được xác định bởi dấu của biểu thức: \n\\[\n\\sum_{m \\in \\mathcal{S}} \\lambda_m y_m \\mathbf{x}_m^T \\mathbf{x} + \\frac{1}{N_{\\mathcal{M}}} \\sum_{n \\in \\mathcal{M}} \\left(y_n - \\sum_{m \\in \\mathcal{S}} \\lambda_m y_m \\mathbf{x}_m^T\\mathbf{x}_n\\right)~~~~~~~~~ (2)\n\\]Trong đó:\\(\\mathcal{M} = \\{n: 0 < \\lambda_n < C\\}\\) là tập hợp những điểm nằm trên margin.\\(\\mathcal{S} = \\{n: 0 < \\lambda_n\\}\\) là tập hợp các điểm support.\\(N_{\\mathcal{M}}\\) là số phần tử của \\(\\mathcal{M}\\).Với dữ liệu thực tế, rất khó để có dữ liệu gần phân biệt tuyến tính, vì vậy nghiệm của bài toán \\((1)\\) có thể không thực sự tạo ra một bộ phân lớp tốt. Giả sử rằng ta có thể tìm được hàm số \\(\\Phi()\\) sao cho sau khi được biến đổi sang không gian mới, mỗi điểm dữ liệu \\(\\mathbf{x}\\) trở thành \\(\\Phi(\\mathbf{x})\\), và trong không gian mới này, dữ liệu trở nên gần phân biệt tuyến tính. Lúc này, hy vọng rằng nghiệm của bài toán Soft Margin SVM sẽ cho chúng ta một bộ phân lớp tốt hơn.Trong không gian mới, bài toán \\((1)\\) trở thành: \n \\begin{eqnarray}\n     \\lambda &=& \\arg \\max_{\\lambda} \\sum_{n=1}^N \\lambda_n - \\frac{1}{2} \\sum_{n=1}^N\\sum_{m=1}^N \\lambda_n \\lambda_m y_n y_m \\Phi(\\mathbf{x}_n)^T \\Phi(\\mathbf{x}_m) &&\\\\\n     \\text{subject to:}~ && \\sum_{n=1}^N \\lambda_ny_n = 0 &&\\quad\\quad\\quad\\quad(3)\\\\\n     && 0 \\leq \\lambda_n \\leq C, ~\\forall n= 1, 2, \\dots, N \n \\end{eqnarray}và nhãn của một điểm dữ liệu mới được xác định bởi dấu của biểu thức:\\[\n\\mathbf{w}^T\\Phi(\\mathbf{x}) + b = \\sum_{m \\in \\mathcal{S}} \\lambda_m y_m \\Phi(\\mathbf{x}_m)^T \\Phi(\\mathbf{x}) + \\frac{1}{N_{\\mathcal{M}}} \\sum_{n \\in \\mathcal{M}} \\left(y_n - \\sum_{m \\in \\mathcal{S}} \\lambda_m y_m \\Phi(\\mathbf{x}_m)^T\\Phi(\\mathbf{x}_n)\\right)~~~~~~~~~ (4)\n\\]Như đã nói ở trên, việc tính toán trực tiếp \\(\\Phi(\\mathbf{x})\\) cho mỗi điểm dữ liệu có thể sẽ tốn rất nhiều bộ nhớ và thời gian vì số chiều của \\(\\Phi(\\mathbf{x})\\) thường là rất lớn, có thể là vô hạn! Thêm nữa, để tìm nhãn của một điểm dữ liệu mới \\(\\mathbf{x}\\), ta lại phải tìm biến đổi của nó \\(\\Phi(\\mathbf{x})\\) trong không gian mới rồi lấy tích vô hướng của nó với tất cả các \\(\\Phi(\\mathbf{x}_m)\\) với \\(m\\) trong tập hợp support. Để tránh việc này, ta quan sát thấy một điều thú vị sau đây.Trong bài toán \\((3)\\) và biểu thức \\((4)\\), chúng ta không cần tính trực tiếp \\(\\Phi(\\mathbf{x})\\) cho mọi điểm dữ liệu. Chúng ta chỉ cần tính được \\(\\Phi(\\mathbf{x})^T\\Phi(\\mathbf{z})\\) dựa trên hai điểm dữ liệu \\(\\mathbf{x}, \\mathbf{z}\\) bất kỳ! Kỹ thuật này còn được gọi là kernel trick. Những phương pháp dựa trên kỹ thuật này, tức thay vì trực tiếp tính tọa độ của một điểm trong không gian mới, ta đi tính tích vô hướng giữa hai điểm trong không gian mới, được gọi chung là kerrnel method.Lúc này, bằng cách định nghĩa hàm kernel \\(k(\\mathbf{x}, \\mathbf{z}) = \\Phi(\\mathbf{x})^T\\Phi(\\mathbf{z}) \\), ta có thể viết lại bài toán \\((3)\\) và biểu thức \\((4)\\) như sau:\\begin{eqnarray}\n    \\lambda &=& \\arg \\max_{\\lambda} \\sum_{n=1}^N \\lambda_n - \\frac{1}{2} \\sum_{n=1}^N\\sum_{m=1}^N \\lambda_n \\lambda_m y_n y_m k(\\mathbf{x}_n,\\mathbf{x}_m) &&\\\\\n    \\text{subject to:}~ && \\sum_{n=1}^N \\lambda_ny_n = 0 &&\\quad\\quad\\quad\\quad(5)\\\\\n    && 0 \\leq \\lambda_n \\leq C, ~\\forall n= 1, 2, \\dots, N &&\n\\end{eqnarray}\nvà:\\[\n\\sum_{m \\in \\mathcal{S}} \\lambda_m y_m k(\\mathbf{x}_m, \\mathbf{x}) + \\frac{1}{N_{\\mathcal{M}}} \\sum_{n \\in \\mathcal{M}} \\left(y_n - \\sum_{m \\in \\mathcal{S}} \\lambda_m y_m k(\\mathbf{x}_m, \\mathbf{x}_n)\\right)~~~~~~~~~ (6)\n\\]Ví dụ: Xét phép biến đổi 1 điểm dữ liệu trong không gian hai chiều \\(\\mathbf{x} = [x_1, x_2]^T\\) thành một điểm trong không gian 5 chiều \\(\\Phi(\\mathbf{x}) = [1, \\sqrt{2} x_1, \\sqrt{2} x_2, x_1^2, \\sqrt{2} x_1x_2, x_2^2]^T\\). Ta có:\\begin{eqnarray}\n\\Phi(\\mathbf{x})^T\\Phi(\\mathbf{z}) &=& [1, \\sqrt{2} x_1, \\sqrt{2} x_2, x_1^2, \\sqrt{2} x_1x_2, x_2^2] [1, \\sqrt{2} z_1, \\sqrt{2} z_2, z_1^2, \\sqrt{2} z_1z_2, z_2^2]^T \\\\\n&=& 1 + 2x_1z_1 + 2x_2z_2 + x_1^2x_2^2 + 2x_1z_1x_2z_2 + x_2^2z_2^2 \\\\\n&=& (1 + x_1z_1 + x_2z_2)^2 = (1 + \\mathbf{x}^T\\mathbf{z})^2 = k(\\mathbf{x}, \\mathbf{z})\n\\end{eqnarray}Trong ví dụ này, rõ ràng rằng việc tính toán hàm kernel \\(k()\\) cho hai điểm dữ liệu dễ dàng hơn việc tính từng \\(\\Phi()\\) rồi nhân chúng với nhau.Vậy những hàm số kernel cần có những tính chất gì, và những hàm như thế nào được sử dụng trong thực tế?\nKhông phải hàm \\(k()\\) bất kỳ nào cũng được sử dụng. Các hàm kerrnel cần có các tính chất:Đối xứng: \\(k(\\mathbf{x}, \\mathbf{z}) = k(\\mathbf{z}, \\mathbf{x}) \\). Điều này dễ nhận ra vì tích vô hướng của hai vector có tính đối xứng.Về lý thuyết, hàm kerrnel cần thỏa mãn điều kiện Mercer: \n\\[\n\\sum_{n=1}^N \\sum_{m=1}^N k(\\mathbf{x}_m, \\mathbf{x}_n) c_nc_m \\geq 0, ~~ \\forall c_i \\in \\mathbb{R}, i = 1, 2, \\dots, N \\quad \\quad (7)\n\\]\nTính chất này để đảm bảo cho việc hàm mục tiêu của bài toán đối ngẫu \\((5)\\) là lồi.Trong thực hành, có một vài hàm số \\(k()\\) không thỏa mãn điều kiện Merrcer nhưng vẫn cho kết quả chấp nhận được. Những hàm số này vẫn được gọi là kernel. Trong bài viết này, tôi chỉ tập trung vào các hàm kernel thông dụng và có sẵn trong các thư viện.Nếu một hàm kerrnel thỏa mãn điều kiện \\((7)\\), xét \\(c_n = y_n \\lambda_n\\), ta sẽ có: \n\\[\n\\lambda^T \\mathbf{K} \\lambda = \\sum_{n=1}^N \\sum_{m=1}^N k(\\mathbf{x}_m, \\mathbf{x}_n) y_ny_m \\lambda_n \\lambda_m \\geq 0, ~\\forall \\lambda_n \\quad\\quad (8)\n\\]\nvới \\(\\mathbf{K}\\) là một ma trận đối xứng mà phần tử ở hàng thứ \\(n\\) cột thứ \\(m\\) của nó được định nghĩa bởi: \n\\(\nk_{nm} = y_ny_m k(\\mathbf{x}_n, \\mathbf{x}_m)\n\\)Từ \\((8)\\) ta suy ra \\(\\mathbf{K}\\) là một ma trận nửa xác định dương. Vì vậy, bài toán tối ưu \\((5)\\) có ràng buộc là lồi và hàm mục tiêu là một hàm lồi (một quadratic form). Vì vậy chúng ta có thể giải quyết bài toán này một cách hiệu quả.Trong bài viết này, tôi sẽ không đi sâu vào việc giải quyết bài toán \\((5)\\) vì nó hoàn toàn tương tự như bài toán đối ngẫu của Soft Margin SVM. Thay vào đó, tôi sẽ trình bày các hàm kernel thông dụng và hiệu năng của chúng trong các bài toán thực tế. Việc này sẽ được thực hiện thông qua các ví dụ và cách sử dụng thư viện sklearn.Đây là trường hợp đơn giản với kernel chính tích vô hướng của hai vector: \n\\[\nk(\\mathbf{x}, \\mathbf{z}) = \\mathbf{x}^T\\mathbf{z}\n\\]Hàm số này, như đã chứng minh trong Bài 19, thỏa mãn điều kiện \\((7)\\).Khi sử dụng hàm sklearn.svm.SVC, kernel này được chọn bằng cách đặt kernel = 'linear'\\[\nk(\\mathbf{x}, \\mathbf{z}) = (r + \\gamma \\mathbf{x}^T\\mathbf{z})^d\n\\]Với \\(d\\) là một số dương để chỉ bậc của đa thức. \\(d\\) có thể không là số tự nhiên vì mục đích chính của ta không phải là bậc của đa thức mà là cách tính kernel. Polynomial kernel có thể dùng để mô tả hầu hết các đa thức có bậc không vượt quá \\(d\\) nếu \\(d\\) là một số tự nhiên.Phần kiểm tra liệu hàm này có thỏa mãn điều kiện \\((7)\\) hay không xin được bỏ qua.Khi sử dụng thư viện sklearn, kerrnel này được chọn bằng cách đặt kernel = 'poly'. Thông tin cụ thể về cách sử dụng có thể xem tại đây.Radial Basic Function (RBF) kernel hay Gaussian kernel được sử dụng nhiều nhất trong thực tế, và là lựa chọn mặc định trong sklearn. Nó được định nghĩa bởi:\n\\[\nk(\\mathbf{x}, \\mathbf{z}) = \\exp(-\\gamma ||\\mathbf{x} - \\mathbf{z}||_2^2), ~~ \\gamma > 0\n\\]Trong sklearn, kernel = 'rbf'.Sigmoid function cũng được sử dụng làm kernel:\n\\[\nk(\\mathbf{x}, \\mathbf{z}) = \\text{tanh}(\\gamma \\mathbf{x}^T\\mathbf{z} + r)\n\\]kernel = 'sigmoid'Dưới đây là bảng tóm tắt các kernel thông dụng và cách sử dụng trong sklearn.Nếu bạn muốn sử dụng các thư viện cho C/C++, các bạn có thể tham khảo LIBSVM và LIBLINEARNgoài các hàm kernel thông dụng như trên, chúng ta cũng có thể tự định nghĩa các kernel của mình như trong hướng dẫn này. \nChúng ta cùng quay lại với bài toán XOR. Chúng ta biết rằng bài toán XOR không thể giải quyết nếu chỉ dùng một bộ phân lớp tuyến tính. Neurrel Network cần 2 layers để giải quyết bài toán này. Với SVM, chúng ta có cách để chỉ cần sử dụng một bộ phân lớp. Dưới đây là ví dụ:Kết quả được cho trong Hình 2 dưới đây:Ta có các nhận xét đối với mỗi kernel như sau:sigmoid: nghiệm tìm được không thật tốt vì có 3 trong 4 điểm nằm chính xác trên đường phân chia. Nói cách khác, nghiệm này rất nhạy cảm với nhiễu.poly: Nghiệm này có tốt hơn nghiệm của sigmoid nhưng kết quả có phần giống với overfitting.rbf: Dữ liệu được tạo ra một cách đối xứng, đường phân lớp tìm được cũng tạo ra các vùng đối xứng với mỗi class. Nghiệm này được cho là hợp lý hơn. Trên thực tế, các rbf kernel được sử dụng nhiều nhất và cũng là lựa chọn mặc định trong hàm sklearn.svm.SVC.Xét một ví dụ khác với dữ liệu giữa hai classes là gần phân biệt tuyến tính như HÌnh 3 dưới đây:Trong ví dụ này, kernel = 'poly' cho kết quả tốt hơn kernel = 'rbf' vì trực quan cho ta thấy rằng nửa bên phải của mặt phẳng nên hoàn thoàn thuộc vào class xanh. sigmoid kernel cho kết quả không thực sự tốt và ít được sử dụng.Bài toán này đã được đề cập ở Bài 12 với dữ liệu đầu vào là các ảnh khuôn mặt. Vì tôi không được phép phân phối cơ sở dữ liệu gốc này, tôi sẽ chia sẻ cho các bạn về dữ liệu đã qua xử lý, được lưu trong file myARgender.mat, có thể được download tại đây. Dưới đây là ví dụ về cách sử dụng thư viện sklearn.svm.SVC để giải quyết bài toán:Kết quả không tệ! Các bạn thử thay các kernel và thiết lập các tham số khác xem kết quả thay đổi như thế nào. Vì dữ liệu giữa hai classes là gần phân biệt tuyến tính nên không có sự khác nhau nhiều giữa các kernel.Nếu dữ liệu của hai lớp là không phân biệt tuyến tính, chúng ta có thể tìm cách biến đổi dữ liệu sang một không gian mới sao cho trong không gian mới ấy, dữ liệu của hai lớp là phân biệt tuyến tính hoặc gần phân biệt tuyến tính.Việc tính toán trực tiếp hàm \\(\\Phi()\\) đôi khi phức tạp và tốn nhiều bộ nhớ. Thay vào đó, ta có thể sử dụng kernel trick. Trong cách tiếp cận này, ta chỉ cần tính tích vô hướng của hai vector bất kỳ trong không gian mới: \\(k(\\mathbf{x}, \\mathbf{z}) = \\Phi(\\mathbf{x})^T\\Phi(\\mathbf{z})\\).Thông thường, các hàm \\(k()\\) thỏa mãn điều kiện Merrcer, và được gọi là kernel. Cách giải bài toán SVM với kernel hoàn toàn giống với cách giải bài toán Soft Margin SVM.Có 4 loại kernel thông dụng: linear, poly, rbf, sigmoid. Trong đó, rbf được sử dụng nhiều nhất và là lựa chọn mặc định trong các thư viện SVM.Với dữ liệu gần phân biệt tuyến tính, linear và poly kernels cho kết quả tốt hơn.Source code.[1] Bishop, Christopher M. “Pattern recognition and Machine Learning.”, Springer  (2006). (book)[2] Duda, Richard O., Peter E. Hart, and David G. Stork. Pattern classification. John Wiley & Sons, 2012.[3] sklearn.svm.SVC[4] LIBSVM – A Library for Support Vector Machines[5] Bennett, K. P. (1992). “Robust linear programming discrimination of two linearly separable sets”. Optimization Methods and Software 1, 23–34.[6] Sch¨olkopf, B., A. Smola, R. C.Williamson, and P. L. Bartlett (2000). “New support vector algorithms”. Neural Computation 12(5), 1207–1245[7]  Rosasco, L.; De Vito, E. D.; Caponnetto, A.; Piana, M.; Verri, A. (2004). “Are Loss Functions All the Same?”. Neural Computation. 16 (5): 1063–1076[8] slearn Kernel functions[9] Kernel method[10] http://www.support-vector-machines.org/",
          "relevence": "yes"
        },
        {
          "url": "https://techtalk.vn/ung-dung-support-vector-machine-trong-bai-toan-phan-loai-hoa.html",
          "title": "Ứng dụng Support Vector Machine trong bài toán phân loại hoa | Tech Talk",
          "content": "Xin chào các bạn, mình lại trở lại rồi đây. Tiếp tục với loạt bài viết về Machine Learning trong bài trước mình đã giới thiệu với các bạn một cách tổng quan nhất về Support Vector Machine – một phương pháp vô cùng hiệu quả trong bài toán phân lớp dữ liệu. Tuy nhiên nếu đọc lý thuyết nhiều quá hẳn sẽ rất khó khăn cho những bạn mới bắt đầu (mình cũng mới bắt đầu tìm hiểu về Machine Learning mà). Chính vì thế, ngày hôm nay chúng ta sẽ đến với một bài toán khá cụ thể đó là sử dụng SVM trong bài toán phân loại hoa. Hiểu nôm na đó là chúng ta sẽ xây dựng một mô hình (model) sao cho khi input vào là một bông hoa và ouput trả ra kết quả nó là loài hoa gì. Thú vị phải không các bạn . OK chúng ta bắt đầu thôi nào.Cái gì cũng phải có cơ sở một chút phải không nào. Ví dụ trong bài toán của ta đang quan tâm đến đối tượng là hoa chẳng hạn, vậy có một câu hỏi đặt ra là “Làm thế nào để phân biệt các loại hoa với nhau ???”Con người chúng ta qua quá trình lớn lên đã tự tích lũy cho mình những đặc điểm rất riêng của mỗi loài hoa. Ví dụ hoa loa kèn có hình giống cái kèn, hoa huệ màu trắng có mùi thơm đặc trưng, hoa hồng màu đỏ và có gai… Vậy tức là trong đầu chúng ta đã có một tập dữ liệu về các loài hoa và nó chính là căn cứ để chúng ta nhận dạng một loài hoa khi chúng ta gặp phải. Nguyên tắc học của máy tính cũng như vậy thôi. Chúng ta cần phải cung cấp cho nó một tập dữ liệu và huấn luyện cho nó băng một cách nào đó để nó có thể căn cứ vào đó mà đoán được một bông hoa mới thuộc vào loài hoa nào. OK, quan trọng nhất vẫn là phải có một tập dữ liệu huấn luyện phải không nào? Trong bài viết này chúng ta sẽ sử dụng một tập dữ liệu về hoa vô cùng nổi tiếng đó là tập dữ liệu Iris. Chúng ta cùng nhau tìm hiểu sâu hơn về tập dữ liệu này nhé.Tập dữ liệu này còn có tên gọi khác là Fisher’s Iris vì nó do Ronald Fisher thu thập và tổng hợp. Tập dữ liệu này gồm 50 mẫu về 3 loài hoa khác nhau của họ Iris là (Iris setosa, Iris virginica và Iris versicolor). Cho cái ảnh cho các bạn dễ hình dung Với mỗi một mẫu hoa này hắn thu thập bốn thuộc tính là chiều dài và chiều rộng của đài hoa và cánh hoa với đơn vị centimet. Để có thể sử dụng tập dữ liệu này chúng ta sẽ sử dụng thư viện datasets trong sklearn.Sau khi đã có dữ liệu rồi, chúng ta hoàn toàn có thể chạy thuật toán SVM ngay để tiến hành phân loại. Tuy nhiên, mình nghĩ nên giúp các bạn có một cái nhìn trực quan hơn về tập dữ liệu này. Mà để hiển thị trực quan nhất thì không gì bằng biểu đồ với thư viện matplotlib thần thánh. Chúng ta cùng thử biểu diễn trên đồ thị xem tập dữ liệu mà chúng ta nhét vào máy tính nó là cái gì nhé.Chúng ta tưởng tượng tập dữ liệu của ta là một tập hợp của 150 điểm dữ liệu tương ứng với 150 bông hoa.. Chúng ta sẽ lấy chúng ta từ datasets bằng hàm sau:Lúc này đã có một tập hợp các điểm dữ liệu. Mỗi điểm dữ liệu bao gồm 4 thuộc tính như đã nói ở trên. Tuy nhiên để biểu diễn trong đồ thị hai chiều chúng ta cần phải giảm bớt số thuộc tính biểu diễn. Ở đây giả sử chọn hai thuộc tính đầu tiên là độ rộng và chiều cao của đài hoa. Chúng ta có hàm xử lý vẽ đồ thị 2D như sau:Và đây là thành quảChúng ta có thể thấy được các điểm dữ liệu với hai thuộc tính trên đồ thị hai chiều. Các điểm này được phân biệt bằng mắt thường với 3 màu khác nhau. Tuy nhiên muốn máy tính phân biệt được như chúng ta lại là một câu chuyện hoàn toàn khác và bạn sẵn sàng cùng tôi đi đến cuối cùng của câu chuyện này chứ. OK chúng ta tiếp tục thôiPhân lớp sử dụng SVM với các Kernel khác nhauVới tập dữ liệu Iris chúng ta cần phân loại các bông hoa thành 3 lớp dữ liệu. Sử dụng SVM với các phương pháp khác nhau sẽ cho hiệu quả phân lớp khác nhau. Cũng tương tự như trên, chúng ta chỉ xem xét đến 2 thuộc tính đầu tiên của tập dữ liệu, tức là phân lớp trong không gian 2 chiều. Chúng ta sử dụng các Kernel khác nhau bao gồm:Đầu tiên chúng ta cần huấn luyện dữ liệuTiếp theo là việc đẩy các mô hình thu được vào đồ thịBây giờ sau khi vẽ các đồ thị trên là chúng ta có thể so sánh hiệu quả phân lớp của các Kernel khác nhau rồi đấy. Kết quả như sau:Bằng việc phân lớp chúng ta có thể chia mặt phẳng tọa độ thành các phần khác nhau. Khi có một điểm dữ liệu mới chúng ta có thể dựa vào tọa độ của chúng để phán đoán xem nó thuộc lớp nào.Chúng ta hoàn toàn có thể tự custom một Kernel riêng của mình. Điều đó làm cho SVM trở nên linh hoạt hơn.Mình rất hi vọng được trình bày với các bạn sâu hơn về Custom Kernel của mình để việc phân lớp cụ thể là phân lớp ảnh được tốt hơn. Nhưng trong phạm vi bài viết này mình sẽ dừng lại ở các Kernel tuyến tính và phi tuyến thôi đã. Kẻo nói nhiều quá chúng ta khó năm bắt được hết vấn đề.Source code trong bài viếtCác bạn có thể tham khảo mã nguồn mình viết trong bài này tại đâyTham khảoSVM Kernel Scikit LearnSVM Scikit LearnIris Flower DatasetNgười viết Pham Van ToanTechtalk via Viblo",
          "relevence": "no"
        },
        {
          "url": "http://viet.jnlp.org/kien-thuc-co-ban-ve-xu-ly-ngon-ngu-tu-nhien/machine-learning-trong-nlp/thuat-toan-may-ho-tro-vector",
          "title": "Thuật toán máy hỗ trợ vector - Xử lý ngôn ngữ tự nhiên (Trường đại học khoa học kỹ thuật Nagaoka)",
          "content": "\n\nTìm kiếm trang web này\n\n\n\n\n\n\n\n\nTrang chủLY NAM PHONGLưu Tuấn AnhNguyễn Văn HảiCác công cụ xử lýTrích lọc tiếng Việt từ HTMLDongDuDownload dữ liệuGiới thiệu về các nghiên cứu mới[Máy học]Learning Combination Features with L1 Regularization[phân loại]Text Categorization with All Substring Features  [in Japanese]Automatic Tree and String Based Wrapper Generation for Semi-structured DocumentsExtracting Structured Data from Web PagesOnline Feature Selection using GraftingKiến thức cơ bản về xử lý ngôn ngữ tự nhiênKhởi đầu NLP với PythonLiblinear-thư viện học máyLựa chọn đặc trưng (Feature selection)Machine Learning trong NLPMô hình ngôn ngữNLP là gì ?Phân nhóm dữ liệu (Clustering)Thuật toán tách từ (Tokenizer)Xử lý tiếng Việt bằng Python (1)Ứng dụng Pointwise để tách từNghiên cứu của tác giảBài toán thêm dấu cho tiếng ViệtViệt hoá MecabNhập môn LinuxSHELL là gìSHELL mạnh nhất : zshTài nguyên ngôn ngữ tiếng ViệtKhái yếu về corpusKhái yếu về từ điểnKế hoạch xây dựng tự động corpus từ nguồn WebĐặc trưng của tiếng ViệtTạp đàmseminar là gìSơ đồ trang webHoạt động gần đây của trang web\nTác giả tranganhtháng hai 7, 2012\n\n\n\n \n\nKiến thức cơ bản về xử lý ngôn ngữ tự nhiên‎ > ‎Machine Learning trong NLP‎ > ‎\n  \n\nThuật toán máy hỗ trợ vector\n\n\n\nThuật toán máy vector hỗ trợ (Support Vector Machines - SVM) được Corters và Vapnik giới thiệu vào năm 1995. SVM rất hiệu quả để giải quyết các bài toán với dữ liệu có số chiều lớn như các vector biểu diễn văn bản. Thuật toán SVM ban đầu chỉ được thiết kế để giải quyết bài toán phân lớp nhị phân tức là số lớp hạn chế là hai lớp. Hiện nay, SVM được đánh giá là bộ phân lớp chính xác nhất cho bài toán phânlớp văn bản, bởi vì đó là bộ phân lớp tốc độ rất nhanh và hiệu quả đối với bài toán phân lớp văn bản.Vì có khá nhiều công thức mà giới hạn của trang web này không thể biểu diễn hết được, nên tôi sẽ không nói về lý thuyết của SVM, bạn có thể tham khảo tại Wikipedia(tiếng việt) http://vi.wikipedia.org/wiki/Support_Vector_MachinesPhương pháp SVM được coi là phương pháp hiệu quả để giải quyết bài toán phân lớp với dữ liệu có số chiều lớn như các vector biểu diễn văn bản. Về mặt lý thuyết, thuật toán phân lớp nhị phân này cũng có thể sử dụng cho bài toán phân lớp đa lớp bằng cách chuyển bài toán đa lớp thành bài toán nhị phân. Tuy nhiên, đối với bài toán phân lớp văn bản sử dụng phương pháp SVM thì việc lựa chọn thuộc tính cho từng phân lớp lại là vấn đề cực kỳ quan trọng, nó quyết định đến hiệu quả của phân lớp.\n \n \n\n \n\n\n\nComments\n\n \n \n\nThuật toán máy vector hỗ trợ (Support Vector Machines - SVM) được Corters và Vapnik giới thiệu vào năm 1995. SVM rất hiệu quả để giải quyết các bài toán với dữ liệu có số chiều lớn như các vector biểu diễn văn bản. Thuật toán SVM ban đầu chỉ được thiết kế để giải quyết bài toán phân lớp nhị phân tức là số lớp hạn chế là hai lớp. Hiện nay, SVM được đánh giá là bộ phân lớp chính xác nhất cho bài toán phânlớp văn bản, bởi vì đó là bộ phân lớp tốc độ rất nhanh và hiệu quả đối với bài toán phân lớp văn bản.Vì có khá nhiều công thức mà giới hạn của trang web này không thể biểu diễn hết được, nên tôi sẽ không nói về lý thuyết của SVM, bạn có thể tham khảo tại Wikipedia(tiếng việt) http://vi.wikipedia.org/wiki/Support_Vector_MachinesPhương pháp SVM được coi là phương pháp hiệu quả để giải quyết bài toán phân lớp với dữ liệu có số chiều lớn như các vector biểu diễn văn bản. Về mặt lý thuyết, thuật toán phân lớp nhị phân này cũng có thể sử dụng cho bài toán phân lớp đa lớp bằng cách chuyển bài toán đa lớp thành bài toán nhị phân. Tuy nhiên, đối với bài toán phân lớp văn bản sử dụng phương pháp SVM thì việc lựa chọn thuộc tính cho từng phân lớp lại là vấn đề cực kỳ quan trọng, nó quyết định đến hiệu quả của phân lớp.",
          "relevence": "no"
        },
        {
          "url": "http://scv.udn.vn/nguyenduchien/BBao/8630",
          "title": "\r\n\tScience curiculum vitae personally - University of Da Nang\r\n",
          "content": " ỨNG DỤNG MÔ HÌNH MÁY HỌC VÉC-TƠ TỰA (SVM) TRONG PHÂN TÍCH DỮ LIỆU ĐIỂM SINH VIÊN Tác giả hoặc Nhóm tác giả: Nguyễn Đức Hiển Nơi đăng: Tạp chí Khoa học và Công nghệ - Đại học Đà Nẵng; Số: 12(73).2013;Từ->đến trang: 33-37;Năm:  2013Lĩnh vực:  Công nghệ thông tin; Loại: Bài báo khoa học; Thể loại: Trong nướcTÓM TẮTBài báo này đề xuất ứng dụng mô \r\nhình kết hợp máy học véc-tơ tựa và hệ thống mờ trong việc trích xuất \r\nluật mờ từ dữ liệu điểm sinh viên. Máy học Véc-tơ tựa (SVMs) và hệ thống\r\n luật mờ có sự tương đương nhau với một số điều kiện nhất định. Trên cơ \r\nsở phân tích sự tương đương giữa mô hình máy học Véc-tơ tựa với mô hình \r\nmờ (Fuzzy model), chúng tôi đề xuất một mô hình tích hợp SVMs và Fuzzy \r\nmodel để trích xuất luật mờ từ kết quả huấn luyện SVMs. Thuật toán f-SVM\r\n cho phép sản xuất được các luật mờ từ dữ liệu huấn luyện. Dữ liệu điểm \r\nthực tế của sinh viên được sử dụng để kiểm tra khả năng thực hiện của mô\r\n hình đề xuất. Tập luật mờ trích xuất được từ tập dữ liệu huấn luyện \r\nbằng thuật toán f-SVM, sẽ được sử dụng để suy luận trên tập dữ liệu thử \r\nnghiệm.Từ khóa: Máy học véc-tơ tựa; mô hình mờ; khai phá luật kết hợp; khai phá dữ liệu; luật mờwalgreens pharmacy coupon walgreen online coupons promo codes walgreensABSTRACTThis paper proposed an \r\napplication of a combining model based on the support vector machine and\r\n fuzzy system to extract fuzzy rules from the student score data. The \r\nsupport vector machines and fuzzy rule systems are functionally \r\nequivalent under some conditions. Based on the discussions about the \r\nequivalence between SVMs and the Fuzzy model, we proposed a combining \r\nmodel for extracting fuzzy rules from the trained SVMs. The f-SVM \r\nalgorithm allows us to extract the fuzzy rules from the training data \r\nset. The real data sets of student scores were used to examine the \r\nperformance of the proposed model. The fuzzy rules obtained from the \r\ntraining data using f-SVM algorithm is tested on the testing data.Key words: support vector machine; Fuzzy model; Association Rule Discovery / Association Rule mining; data mining; fuzzy rules",
          "relevence": "no"
        },
        {
          "url": "http://openlab.forumvi.com/t16-topic",
          "title": "Support Vector Machine",
          "content": "Máy vectơ hỗ trợ (SVM - viết tắt tên tiếng Anh support vector machine) là một khái niệm trong thống kê và khoa học máy tính cho một tập hợp các phương pháp học có giám sát liên quan đến nhau để phân loại và phân tích hồi quy. SVM dạng chuẩn nhận dữ liệu vào và phân loại chúng vào hai lớp khác nhau. Do đó SVM là một thuật toán phân loại nhị phân. Với một bộ các ví dụ luyện tập thuộc hai thể loại cho trước, thuật toán luyện tập SVM xây dựng một mô hình SVM để phân loại các ví dụ khác vào hai thể loại đó. Một mô hình SVM là một cách biểu diễn các điểm trong không gian và lựa chọn ranh giới giữa hai thể loại sao cho khoảng cách từ các ví dụ luyện tập tới ranh giới là xa nhất có thể. Các ví dụ mới cũng được biểu diễn trong cùng một không gian và được thuật toán dự đoán thuộc một trong hai thể loại tùy vào ví dụ đó nằm ở phía nào của ranh giới.Tổng quan về máy vectơ hỗ trợ[sửa | sửa mã nguồn]Một máy vectơ hỗ trợ xây dựng một siêu phẳng hoặc một tập hợp các siêu phẳng trong một không gian nhiều chiều hoặc vô hạn chiều, có thể được sử dụng cho phân loại, hồi quy, hoặc các nhiệm vụ khác. Một cách trực giác, để phân loại tốt nhất thì các siêu phẳng nằm ở càng xa các điểm dữ liệu của tất cả các lớp (gọi là hàm lề) càng tốt, vì nói chung lề càng lớn thì sai số tổng quát hóa của thuật toán phân loại càng bé.Trong nhiều trường hợp, không thể phân chia các lớp dữ liệu một cách tuyến tính trong một không gian ban đầu được dùng để mô tả một vấn đề. Vì vậy, nhiều khi cần phải ánh xạ các điểm dữ liệu trong không gian ban đầu vào một không gian mới nhiều chiều hơn, để việc phân tách chúng trở nên dễ dàng hơn trong không gian mới. Để việc tính toán được hiệu quả, ánh xạ sử dụng trong thuật toán SVM chỉ đòi hỏi tích vô hướng của các vectơ dữ liệu trong không gian mới có thể được tính dễ dàng từ các tọa độ trong không gian cũ. Tích vô hướng này được xác định bằng một hàm hạt nhân K(x,y) phù hợp.[1] Một siêu phẳng trong không gian mới được định nghĩa là tập hợp các điểm có tích vô hướng với một vectơ cố định trong không gian đó là một hằng số. Vectơ xác định một siêu phẳng sử dụng trong SVM là một tổ hợp tuyến tính của các vectơ dữ liệu luyện tập trong không gian mới với các hệ số αi. Với siêu phẳng lựa chọn như trên, các điểm x trong không gian đặc trưng được ánh xạ vào một siêu mặt phẳng là các điểm thỏa mãn:Σi αi K(xi,x) = hằng số.Ghi chú rằng nếu K(x,y) nhận giá trị ngày càng nhỏ khi y xa dần khỏi x thì mỗi số hạng của tổng trên được dùng để đo độ tương tự giữa x với điểm xi tương ứng trong dữ liệu luyện tập. Như vậy, tác dụng của tổng trên chính là so sánh khoảng cách giữa điểm cần dự đoán với các điểm dữ liệu đã biết. Lưu ý là tập hợp các điểm x được ánh xạ vào một siêu phẳng có thể có độ phức tạp tùy ý trong không gian ban đầu, nên có thể phân tách các tập hợp thậm chí không lồi trong không gian ban đầu.Lịch sử[sửa | sửa mã nguồn]Thuật toán SVM ban đầu được tìm ra bởi Vladimir N. Vapnik và dạng chuẩn hiện nay sử dụng lề mềm được tìm ra bởi Vapnik và Corinna Cortes năm 1995Video cực kỳ dễ hiểu về Suport Vector Machine. Minh sẽ đưa lên cơ sở lý thuyết trong nhận dạng sau:_________________Em gọi ta khi mùa trăng đã dứt Nắng nhạt phai, còn thanh xuân qua rồi. Dĩ vãng êm đềm xin trôi, trôi mãi Để ta lớn lên, bước về trời xa Nếu một mai quay về còn gặp lại Nửa đời thương nhớ, nửa đời vấn vương Hoa kia xin cài vào miền quá khứ Để nồng nàn góc phố ta gặp nhau.» Tỷ lệ vàng» Happy birthday Rán Towua :)~» [Showroom]Momi aka Lacie aka Abyss» Mẫu Giấy khen» Offshore Support Vessels: A Practical Guide (tài liệu cho Tàu dịch vụ ngoài khơi)",
          "relevence": "yes"
        },
        {
          "url": "https://daynhauhoc.com/t/thuat-toan-phan-loai-van-ban-svm-su-dung-tf-idf/45964",
          "title": "Thuật toán phân loại văn bản SVM sử dụng TF IDF - programming - Dạy Nhau Học",
          "content": "Chào mọi người. Em đang sử dụng thuật toán SVM để phân loại văn bản. Em sử dụng TF IDF để tính trọng số của một từ trong một câu. Khi chuyển tập traning sang dạng vector thì không có vấn đề gì. Nhưng khi chuyển tập test sang thì em đang thắc mắc chỗ này. IDF của một từ trong tập test sẽ được tính như thế nào:     - Sử dụng luôn giá trị IDF của từ đấy trong tập training (IDF này chỉ tính dựa trên tập training).    - Hay tính lại giá trị IDF của từ đấy (kết hợp cả tập training, testing để tính).Và 1 ý nữa là có những từ chưa xuất hiện trong tập training thì phải làm sao ạ.Em cảm ơn",
          "relevence": "no"
        },
        {
          "url": "http://www.kienthuc2plus.com/2017/03/data-mining-thuat-toan-support-vector.html",
          "title": "[Data Mining] Thuật toán Support Vector Machine (SVM) - Kiến Thức ++ | Phát triển bản thân",
          "content": "\nXin chào các bạn !!! Mình tên là Thành hiện tại mình đang học CNTT. Mình lập ra blog này để chia sẻ, tổng hợp lại những bài học, kinh nghiệm, trải nghiệm mình có trong quá trình nâng cao trình đô bản thân để giúp các bạn một phần nào đó để có thể đi nhanh hơn mình trong quá trình mày mò, nghiên cứu.      \nhttps://drive.google.com/drive/folders/0B4I0iSeUWMotY09lWXZMODVWWkk\n\n\n\n\n\n\n\n\n\n      BLOG_CMT_createIframe('https://www.blogger.com/rpc_relay.html', '03622159840755489255');\n    \n\n\n Click Liên Kết Admin Hiếu\n Đăng Khải BlogChính Trực Blog Blog Thủ Thuật Hưng Star - IT",
          "relevence": "no"
        },
        {
          "url": "http://seowebsite.site/blog/giai-thuat-svm-co-the-ap-dung-tao-phan-mem-seo-website/",
          "title": "Giải thuật SVM có thể áp dụng tạo phần mềm SEO WEBSITE",
          "content": "Bài toán phân loại là một trong những bài toán kinh điển trong lĩnh vực xử lý văn bản. Đã có nhiều công trình nghiên cứu về vấn đề này. Tuy nhiên việc ứng dụng đối với các văn bản Tiếng Việt còn rất hạn chế.Một trong những khó khăn trong việc áp dụng những thuật toán phân loại văn bản vào tiếng Việt là việc phân tách một câu thành các từ một cách chính xác.Một trong những phương pháp được nghiên cứu và áp dụng trong thực tế là phương pháp sử dụng bộ phân loại vector SVM.2.1.  Tổng quan: SVM là tập hợp các phương pháp học có giám sát bao gồm phân tích dữ liệu và nhận dạng mẫu.Ý tưởng của phương pháp này là cho trước tập huấn luyện được biểu diễn trong không gian vector, trong đó mỗi văn bản được xem như một điểm trong không gian này.Phương pháp này sẽ tìm ra một siêu phẳng tốt nhất chia các điểm trong không gian thành hai lớp riêng biệt tương ứng. Chất lượng của siêu phẳng được quyết định bởi khoảng cách\n(gọi là biên) của điểm dữ liệu gần nhất của mỗi lớp đến mặt phẳng này. Tức là khoảng cách biên càng lớn thì kết quả phân loại đạt được càng cao. Mục tiêu của thuật toán SVM là tìm được khoảng cách biên lớn nhất để đạt kết quả phân loại cao.Việc tính toán để tìm ra siêu phẳng tối ưu rất phức tạp và khó khăn. Hiện nay đã có những bộ thư viện hỗ trợ việc tính toán trên như: LIBSVM, SVMlight, jSVM,…2.2.  Phân loại văn bản và SVM:Phân loại văn bản là tiến trình đưa văn bản chưa biết chủ đề vào các lớp văn bản đã biết. Mỗi lĩnh vực được xác định bởi một số tài liệu mẫu của lĩnh vực đó.Trong quá trình phân loại, các văn bản được biểu diễn dưới dạng vector với các thành phần (chiều) của vector này là trọng số của các từ. Một số phương pháp xác định trọng số từ:–          Tần suất từ (TF): trọng số từ là tần suất xuất hiện của từ đó trong tài liệu. Tức một từ là quan trọng trong tài liệu đó khi nó xuất hiện nhiều lần.–          TFIDF: trọng số từ là tích của tần suất từ TF và tần suất nghịch đảo của từ đó và được xác định bằng công thức:IDF = log(N/DF) + 1Trong đó: N là kích thước tập huấn luyện, DF là số tài liệu mà một từ xuất hiện trong đó.Để tiến hành phân loại văn bản nói chung, chúng ta cần thực hiện các bước sau:–          Bước 1: Rút trích đặc trưng văn bản và biểu diễn văn bản bằng mô hình vector. Để rút trích đặc trưng văn bản, cần thực hiện thao tác tách từ trong văn bản, xác định từ loại của từ và sau đó tiến hành biểu diễn các văn bản bằng mô hình vector.–          Bước 2: Áp dụng thuật toán phân loại. Trong bước này chúng ta sử dụng SVM.3.1.  Giới thiệu:LIBSVM là bộ phần mềm tích hợp được xây dựng để hiện thực hóa thuật toán SVM. Mục tiêu giúp người dùng có thể sử dụng SVM một cách dễ dàng. LBSVM cung cấp một giao diện đơn giản mà người dùng có thể dễ dàng liên kết nó với chương trình do họ tạo ra. Một số tính năng của LIBSVM:–          Tích hợp các công thức tính toán trong SVM–          Khả năng phân loại đa lớp–          Cho phép lựa chọn mô hình phân loại–          Ước lượng xác suất–          Các kernel–          Hỗ trợ hầu hết các ngôn ngữ phổ biến hiện nay như: C++, Java, C#.Net, Python…–          Tự động lựa chọn mô hình nhằm tạo ra độ chính xác nhất định.3.2.  Sử dụng LibSVM cho .NET:SVM.Net là phiên bản được chuyển đổi từ phiên bản libSVM dành cho Java. Mang đầy đủ tính năng và hiệu quả giống như phiên bản gốc nhưng cấu trúc được thay đổi cho phù hợp với nền tảng .NET. Việc sử dụng thư viện này khá đơn giản, bạn cần có bộ dữ liệu mẫu và thực hiện các bước sau đây:Các bước căn bản để training và test data với libSVM.//Đầu tiên, đọc file training dữ liệuProblem train = Problem.Read(“a1a.train”);Problem test = Problem.Read(“a1a.test”);//Sử dụng một số tham số mặc định.Parameter parameters = new Parameter();double C;double Gamma;//Tối ưu hóa để tìm được các giá trị tham số thích hợp nhất//các tham số này được lưu trữ trong C và Gamma.ParameterSelection.Grid(train, parameters, “params.txt”, out C, out Gamma);parameters.C = C;parameters.Gamma = Gamma;//Train model sử dụng các tham số tối ưu.Model model = Training.Train(train, parameters);//Thực hiện phân loại với dữ liệu test,đưa kết quả ra //results.txtPrediction.Predict(test, “results.txt”, model, false);3.2.1.      Các thành phần trong lớp Problem. Chứa tập hợp các vector được phân loại.Các thuộc tính:–          Count: số lượng vector–          Y: nhãn của lớp vector–          X: dữ liệu dạng vector–          maxIndex: chỉ số lớn nhất trong 1 vector.Các phương thức:–          Phương thức Read(): đọc tập dữ liệu được biểu diễn dưới dạng vector.–          Phương thức Write(): ghi tập dữ liệu dưới dạng vector.3.2.2.      Các thành phần trong lớp Parameter: bao gồm các tham số khác nhau có thể ảnh hưởng đến SVM. Có thể sử dụng các giá trị mặc định đã được cung cấp.–          C_SVC (C-SVC)–          NU_SVC (nu-SVC)–          ONE_CLASS (one-class SVM)–          EPSILON_SVR (epsilon-SVR)–          NU_SVR (nu-SVR)Các loại kernel mà thư viện SVM sử dụng. Có thể chọn lựa các kernel phù hợp. Mặc định là Polynomial.–          LINEAR (Linear: u’*v): được minh họa như sau–          POLY (Polynomial: (gamma*u’*v + coef0)^degree)–          RBF (Radial basis function: exp(-gamma*|u-v|^2))–          SIGMOID (Sigmoid: tanh(gamma*u’*v + coef0))–          PRECOMPUTED (Precomputed kernel)–          _degree: giá trị degree bậc của kernel (mặc định 3)–          _gamma: sử dụng trong kernel (mặc định 1/k)–          _coef0: sử dụng trong kernel (mặc định 0)–          _cacheSize: kích thước bộ nhớ đệm tính theo MB (mặc định 100)–          _C: tham số trong C-SVC, epsilon-SVR, và nu-SVR (mặc định 1). Giới hạn lỗi ràng buộc.–          _eps: mặc định 0.001. Dung sai của tiêu chí dừng.–          _weights: mặc định là 1–          _nu: tham số trong nu-SVC, one-class SVM, và nu-SVR (mặc định 5)–          _p: giá trị epsilon trong hàm loss của epsilon-SVR (mặc định 0.1)–          _shrinking: sử dụng công nghệ tự động thu hẹp (mặc định là true)–          _probability: ước tính xác suất cho SVC và SVR (mặc định false).3.2.3.      Lớp ParameterSelection:–          Phương thức Grid: thực hiện việc chọn lựa các giá trị tham số tối ưu3.2.4.      Lớp Training:–          Phương thức Train: thực hiện việc training dữ liệu với các tham số đã được tối ưu hóa.3.2.5.      Lớp Model:–          Chứa các thông tin được trả sau khi training dữ liệu3.2.6.      Lớp Prediction:–          Phương thức Predict: thực hiện phân loại dữ liệu dựa vào việc so khớp giữa dữ liệu test và dữ liệu lấy từ lớp Model.Phân loại văn bản với SVM yêu cầu văn bản phải được biểu diễn dưới dạng vector đặc trưng. Vì vậy để có kết quả phân loại tốt cần:–          Dữ liệu huấn luyện chuẩn và đủ lớn.–          Phương pháp tách từ trong văn bản đóng vai trò quan trọng trong quá trình biểu diễn văn bản bằng vector.Lên trang đầuHotline: 0908 176 020Email: cuongnd.ecvn@gmail.com",
          "relevence": "no"
        },
        {
          "url": "http://thigiacmaytinh.com/su-dung-svm-trong-opencv-de-nhan-dien-ky-tu/",
          "title": "Sử dụng SVM trong OpenCV để nhận diện ký tự | Thị giác máy tính",
          "content": "SVM là một kỹ thuật hay, đơn giản nhưng hiệu quả cao. Các bạn có thể đọc bài LIBSVM – giới thiệu tổng quan và ví dụ để có thể hiểu khái quát SVM là gì. Bài này sẽ hướng dẫn các bạn dùng SVM có sẵn trong lib OpenCV để nhận diện ký tự. Bài này khá đơn giản với mục đích cho các bạn thấy tính ứng dụng thực tế của SVM. Sau đó có thể đọc các bài viết nâng cao để hiểu sâu và nâng cao độ chính xác bằng các tham số phù hợpMình đã chuẩn bị sẵn cho các bạn, bao gồm 2 bộ dữ liệu:Mình vẽ ký tự màu đỏ vì màu đen – trắng khá là khó nhìn, còn thực tế là sẽ làm với ảnh xám – gray scale. Link download ở cuối bài.Thuật toán như sau: với mỗi ảnh 2D chuyển thành ảnh 1D. Sau đó ghép các ảnh 1D thành ảnh 2D dùng để huấn luyện.\nẢnh 1D là ảnh có height = 1, có thể xem nó là mảng 1 chiều. Còn ảnh 2D có height > 1 và cũng xem như là mảng 2 chiều.\nThí dụ: ảnh 2D có kích thước 10 x 10 pixels chuyển thành ảnh 1D có kích thước là 100 x 1 pixels.\nẢnh ký tự 2D và chuyển sang 1D (mình tăng height cho các bạn dễ nhìn)\n\nSau khi ghép các ký tự từ 0 – 9 được hình như sau:\n\nTiếp theo là tạo 1 danh sách các nhãn của ảnh train, danh sách nhãn cũng là ảnh 1D theo thứ tự. Do đó danh sách nhãn là 0123456789Code để train nằm ở cuối bài. Sau khi train thì được 1 file text.Lấy 1 ảnh bất kỳ chưa được train để dự đoán kết quảSource bên dưới chứa sẵn 2 bộ dữ liệu Train set và Test set. Đầu tiên các bạn gọi hàm Train(), sau đó gọi hàm Predict() để lấy kết quả. Chúc các bạn thành công.SvmOpenCV.zip (32 MB)",
          "relevence": "no"
        },
        {
          "url": "http://www.vjol.info/index.php/JSTD/article/viewArticle/16646",
          "title": "Thuật toán PR-SVM cho nhận dạng hoạt động gõ tay sử dụng quang phổ cận hồng ngoại chức năng | Hải | Tạp chí Phát triển Khoa học và Công nghệ",
          "content": "Trong những thập kỷ gần đây, các nghiên cứu về giao tiếp não-máy tính phục vụ cho mục đích chẩn đoán và phục hồi chức năng không ngừng phát triển. Oxy trên vỏ não và lưu lượng máu trên các vùng của não người có thể đo bằng phương pháp không xâm nhập – quang phổ cận hồng ngoại chức năng fNIRS (functional Near InfraRed Spectroscopy). Trong bài báo này, người thực hiện xây dựng thuật toán để nhận dạng một người đang gõ tay trái hay tay phải dựa trên tín hiệu não đo được. Dữ liệu còn nhiễu thu thập được từ nhiều kênh sẽ đi qua bộ tiền xử lý dùng bộ lọc Savitzky-Golay để có được tín hiệu phẳng hơn. Một phương pháp mới lạ máy vector hỗ trợ hồi quy đa thức (PR-SVM) cho nhận dạng gõ tay được áp dụng. Cụ thể, đặc tính của tín hiệu sau lọc trong quá trình gõ tay trái và phải được trích ra thông qua hồi quy đa thức (PR). Hệ số hồi quy tương ứng với lượng tập trung oxy- hemoglobin (oxy-Hb) sẽ được dùng cho việc nhận dạng. Sau cùng, máy vector hỗ trợ (SVM) được áp dụng để huấn luyện và nhận dạng tay trái hay tay phải đang được gõ. Các kết quả thí nghiệm trên ba người với nhiều lần gõ tay đã cho thấy độ tin cậy của giải thuật đã đề xuất.",
          "relevence": "no"
        },
        {
          "url": "http://blog.duyet.net/2016/06/topic-modeling-tim-chu-de-cho-tap-van-ban-bai-viet.html#.WckL1dpSDIU",
          "title": "Topic Modeling: Tìm chủ đề cho tập văn bản bài viết",
          "content": "\nMột công ty A hoạt động trong lĩnh vực nghiên cứu thị trường đã tiến hành thu thập dữ liệu từ các trang báo điện tử Việt Nam để khảo sát xem thị hiếu của người dân về các chủ đề xã hội và đời sống như thế nào. Từ đó hỗ trợ cho các công ty bán hàng làm chiến lược marketing hiệu quả hơn. Dữ liệu được lấy về, lưu trên một cơ sở dữ liệu dưới định dạng file văn bản (.txt) mà chưa qua bất kỳ khâu xử lý nào. Do trong quá trình lấy dữ liệu, các kỹ thuật viên của công ty A đã sơ suất quên ghi nhớ chủ đề cho từng bài viết khi được tải về. Những gì công ty A hiện có là một thư mục chứa hơn 28,000 file văn bản (text), mỗi file văn bản là nội dung một bài viết trên một trang báo nào đó.\n\nCâu hỏi: Với số lượng bài viết lớn như vậy (hơn 28,000 bài viết), bạn hãy tìm cách nào đó để nhóm các bài viết theo những chủ đề khác nhau. Bạn hãy đề xuất một phương pháp để có thể đặt tên cho từng chủ đề một cách hợp lý nhất. Kết quả công ty A mong đợi sẽ là một file dạng csv gồm 2 cột: cột 1 là tên bài báo, cột 2 là tên chủ đề tương ứng.\n\nDownload file dữ liệu 28,000 bài viết.\n----------------------------------- \n\nVới lượng dữ liệu lớn như vậy, có 2 mục tiêu cần thực hiện:\n\nNhóm các bài viết thành các chủ đề khác nhau\nĐặt tên chủ đề cho từng nhóm bài viết ấy. \n\nViệc nhóm các văn bản thành các chủ đề khác nhau có thể sử dụng các thuật toán phân cụm như K-means, khai phá chủ đề LDA (Latent Dirichlet Allocation), ... Nhưng với LDA và cả K-means đều yêu cầu phải biết trước giá trị k - số cụm để phân chia.\n\nSố cụm của đề bài không thể xác định chính xác. Quan sát dữ liệu, ta thấy được đa số các bài báo được lấy từ các trang tin lớn như VnExpress, Tuổi trẻ Online.\n\n1. Phân tích về tên chủ đề bài viết trên báo điện tử\nPhân tích một chút về danh mục chủ đề trên các trang báo này. Các mục tại VnExpress được phân thành từng mục chính, mỗi mục chính lại có các chuyên mục nhỏ.\n\n\n\n\n\nMột số trang báo có các chuyên mục riêng: như mục Video (http://video.vnexpress.net), iOne (http://ione.vnexpress.net), ... là một mục nhỏ của VnExpress, nhưng lại có các chủ đề con cùng tên với tất cả các mục lớn của VnExpress. \nKhi thu thập bài viết từ nhiều nguồn, sẽ có trường hợp các bài viết trùng nhau (hoặc gần giống nhau), nhưng 2 báo lại có cách đặt tên chủ đề khác nhau. \nMột số trang tin tổng hợp (Zing.vn, Báo mới, ...) sẽ tổng hợp các tin từ các trang chính thống, và sẽ sắp xếp các bài viết vào các mục tương đương. \n\n\n2. Phân tích dữ liệu thô\nDữ liệu thu được ở dạng text, mỗi file là một bài viết. Mỗi bài viết đều bị nhiễu (do thu thập dư các liên kết bài viết liên quan, các bài xem nhiều nhất, ...).\n\n\n\n\n\n\nDữ liệu cũng thu thập các bài ở dạng bài viết ảnh (chỉ thu thập được Caption của ảnh), hoặc bài viết Video (thu thập được số giây của video, ...).\n\n\n\n\nCác dữ liệu này khá nhiễu, khó rút trích đặc trưng, cần phải trải qua trá trình tiền xử lý để xử lý các bài viết này. \n\n3. Phương pháp thực hiện\n\n3.1. Đề xuất phương pháp \nDo sự khó khăn trong dữ liệu, và không xác định được số chủ đề của bài viết, cách đặt tên chủ đề, ... có nhiều phương pháp khác nhau để tiến hành nhóm các bài viết. \nTa thống nhất sẽ chỉ chia các bài viết thành các chủ đề chính (không chia thành các chủ đề phân cấp nhỏ hơn). Tổng quát lại chúng ta sẽ có các cách sau để tiến hành nhóm các bài viết cùng chủ đề lại với nhau:\n\nSử dụng thuật toán DBSCAN (Density-based spatial clustering of applications with noise): đây là thuật toán được đề xuất để phát hiện các cụm trong tập dữ liệu (chấp nhận dữ liệu nhiễu), với DBSCAN ta không cần biết trước số cụm. Nhược điểm của DBSCAN là độ phức tạp cao, chạy chậm. \n\n\n\n\n\n\nSử dụng thuật toán K-means: K-means sẽ phân cụm dữ liệu bài viết vào số cụm k xác định, số cụm có thể ước lượng từ các phân tích tên chủ đề ở trên. Công ty A thu thập dữ liệu từ các trang báo thuộc chủ đề xã hội và đời sống, tổng hợp lại ta sẽ có các chủ đề: Thời sự, Thế giới, Kinh doanh, Giải trí, Thể thao, Pháp luật, Giáo dục, Sức khỏe, Gia đình, Du lịch, Khoa học, Số hóa, Xe, Cộng đồng và mục khác. Số cụm ước lượng sẽ từ 13 ~ 15 cụm. Nếu các bài viết ở mục \"Khác\" chênh lệch lớn thì tiến hành điều chỉnh tham số k cho phù hợp. \nSử dụng kỹ thuật phân lớp văn bản: kỹ thuật này có thể tốn thời gian nhưng hiệu quả và giải quyết được cả vấn đề gom nhóm và đặt tên chủ đề. Tiến hành thu thập rút trích lại một số bài viết từ tất cả các chủ đề trên báo điện tử, dữ liệu này thu thập sẽ bao gồm bài viết và nhãn (chủ đề) của bài viết đó. Sử dụng các phương pháp/công cụ thống kê hoặc máy học (Machine Learning) để tiến hành tạo ra mô hình, sử dụng mô hình để phân lớp cho 28.000 văn bản của công ty A. \n\nĐặt tên chủ đề: Với phương pháp 1 và 2, việc làm sau khi phân cụm được các bài viết là tìm cách đặt trên cho các chủ đề này. Từ mỗi nhóm bài viết, ta có thể tiến hành rút trích từ khóa đặc trưng sử dụng mô hình túi từ, tính tần số, chọn ra các từ khóa đặc trưng. Từ các từ khóa đặc trưng này có ta thể suy luận ra được chủ đề, bằng phương pháp thủ công hoặc tự động. Để có các kết quả chính xác thì trong tập bài viết các từ stopwords, các ký hiệu đặc biệt, ... phải được lọc bỏ. \n\n\n\n\n3.2. Thực nghiệm\nTrong giới hạn, mình không thể thực nghiệm hết được tất cả các phương pháp đã nêu, mà chỉ chọn một phương pháp truyền thống: sử dụng phương pháp thu thập lại và phân lớp chủ đề sử dụng SVM. Phương pháp gồm 4 bước cơ bản: \n\n\n\nBước 1: Thu thập, rút trích Tiến hành thu thập, rút trích lại một số bài viết từ VnExpress.net và các chủ đề tương ứng đi kèm cho mỗi bài viết. Các chủ đề này sẽ là các chủ đề chuẩn cho 28.000 bài viết của công ty A.\nBước 2: Tiền xử lý dữ liệuTập văn bản (bao gồm thu thập được và dữ liệu thô có sẵn) sẽ được xử lý tách câu, tách từ, loại bỏ các dấu câu và các stopword. Sau bước này, mỗi văn bản sẽ là tập hợp của các từ đã được sàng lọc trong văn bản đó. Quá trình tách câu tách từ trong tiếng Việt được sử dụng công cụ vnTokenizer với độ chính xác được tác giả công bố 96% - 98%. Stopwords sẽ được xóa bỏ khỏi kết quả bằng cách sử dụng bộ từ điển stopwords tiếng Việt. \nBước 3: Vector hóa văn bảnTập từ thu được từ bước tiền xử lý đang ở dạng không cấu trúc, do đó để xử lý phân lớp bằng các phương pháp máy học cần vector hóa chúng. Mô hình túi từ được áp dụng, theo mô hình này, dữ liệu văn bản không có cấu trúc (độ dài khác nhau) được biểu diễn thành dạng vector tần số xuất hiện của từ trong văn bản. Từ tần số của từ, vector của từng văn bản sẽ được tính bằng công thức TF*IDF (tham khảo). Đây là công thức giúp đánh giá mức độ quan trọng của một từ đối với văn bản trong bối cảnh của tập ngữ liệu. \n\nTF (term frequency) là tần số xuất hiện của một từ trong một văn bản.\nIDF (inverse document frequency) là tần số nghịch của 1 từ trong tập ngữ liệu.\n\nKết quả của bước này là vector phân bố xác suất của tập từ biểu diễn chủ đề của từng văn bản. Các từ có tần số TF*IDF dưới 1 ngưỡng quy định sẽ bị lọc bỏ. Việc lọc này nhằm lựa ra những từ đủ tính chất đặc trưng cho chủ đề, loại bỏ những từ quá hiếm xuất hiện hoặc xuất hiện quá phổ biến.\nBước 4: Phân lớp văn bảnTiến hành phân lớp sử dụng phương pháp học máy SVM.Tập văn bản đầu vào sau khi trải qua các bước xử lý sẽ được đại diện bằng tập các vector Chúng sẽ là đầu vào của giải thuật SVM truyền thống. SVM là thuật toán phân lớp nhị phân, do đó ta phải tổ chức sử dụng các kết hợp các mô hình One-vs-All hoặc All-vs-All Classiﬁcation Sau quá trình phân lớp sẽ cho ta kết quả gãn nhãn chủ đề cho từng văn bản dựa trên văn bản đã thu thập được.\n\nCó thể tổng quá quá trình thực hiện như sau\n\n\n\n\n\nTổng kết\nQua quá trình xử lý, ta được tập kết quả của 28,000 văn bản cùng với chủ đề. Với lượng dữ liệu lớn, mô hình xử lý đơn lẻ truyền thống không thể phân tích nhanh chóng và có hiệu quả. Có thể kết hợp thêm kỹ thuật phân tán hóa dữ liệu và song song các tác vụ để nâng cao tốc độ thực thi, cụ thể có thể sử dụng mô hình MapReduce của các framework Apache Hadoop hoặc Apache Spark\n\n",
          "relevence": "no"
        },
        {
          "url": "https://phamhuudanh.com/2017/02/18/mot-so-thuat-toan-trong-machine-learning/",
          "title": "Một số thuật toán trong Machine Learning – Phạm Hữu Danh",
          "content": "Phạm Hữu DanhLet's share the passion for technology!Đây là bài post mình dùng đe ôn tập và chia sẻ kiến thức đã học. Mình rất vui khi nhận được chia sẻ, góp ý nội dung từ các bạn. – Hữu DanhTheo mình, điểm khác nhau giữa người và máy tính chính là người có thể học tập kiến thức mới từ những thứ trong cuộc sống.\nTrước khi biết đến Machine Learning, mình chỉ nghĩ lập trình là xây dựng thuật toán để giải quyết một vấn đề cụ thể.\nCòn đối với Machine Learning, thì chúng ta lập trình để máy tính học và và giải quyết các yêu cầu từ “kiến thức đã có”. Vốn kiến thức này chính là dữ liệu (dữ liệu dùng để dạy cho máy tính học trước khi suy ra kết quả gọi là training data).Trước khi chúng ta học giải Phương trình, Vi phân, Toán cao cấp, … thì chúng ta phải học những phép tính cơ bản trước như: cộng, trừ, nhân, chia, lấy căn, …\nĐối với Machine Learning cũng vậy, khi mình tìm hiểu về nó qua các khóa học, mình được ho những thuật toán cơ bản nhất, để từ đó kết hợp và giải quyết các vấn đề phức tạp hơn.Tổng hợp lại thì ta có:\n– Decision Trees\n– Naive Bayes\n– Gradient Descent\n– Linear Regression\n– Logistic Regression\n– Neural Networks\n– ClusteringTất nhiên còn rất nhiều thuật toán khác, nếu bạn thấy thuật toán nào hay muốn chia sẻ, hãy comment phía dưới nhé!Decision Trees là một thuật toán mà trong đó chúng ta xét từng thuộc tính của đối tượng so với điều kiện mà chúng ta đặt ra trước để phân loại đối tượng vào các nhóm. Xem hình phía dưới, bạn sẽ hiểu tại sao nó được gọi là Decision Trees (cây quyết định).ví dụ kinh điển: tính khả năng sống sót của một người trên tàu TitanicTheo mình học được thì thuật toán này có các điểm mạnh sau:Hạn chế thì có:Naive Bayes là thuật toán dùng để phân nhóm dựa vào tính xác suất có điều kiện. Nếu ai đã học xác xuất thống kế thì có lẽ đã nghe đến công thức này.Ví dụ kinh điển: detecting spam emailsDựa vào training data đã phân loại, ta sẽ xác định các thuộc tính của chúng, ví dụ ở đây là từ “cheap”, ta sẽ tính tỉ lệ email là spam khi có tư “cheap”. Sau đó ta xác định với các thuộc tính khác.\nKhi có email mới sẽ xác định tỉ lệ email mới có phải spam không dựa theo việc xác định theo thuộc tính, thường là lấy tích các thuộc tính. Xem thêm về công thức Naive Bayes để hiểu cách tính.Ưu điểm mình nhận thấy là: tạo ra các bộ lọc khác nhau cho các trường hợp khác nhau, ví dụ email bị cho là spam khác nhau với mỗi người.Nhươc điểm là:Đây là một dạng thuật toán tìm thành phần ưu tiên. Thuật toán này tương tự cách chúng ta đi xuống đồi, chúng sẽ tìm nơi có độ cao thấp nhất và đi về hướng đó. Thuật toán này sẽ tìm biến để cho biểu thức xác định đạt giá trị nhỏ nhất.Thuật toán này chủ yếu được ứng dụng để xây dựng các thuật toán khác.Với mình thì các thuật toán dạng Regression giố với các bài phương trình hàm: với biểu thức y = a x + b mà ta có rất nhiều cặp số (x, y) và nhiệm vụ là tìm ngược lại a và bLinear Regression đúng như tên gọi của nó, ta cố gắng vẽ một đường thẳng sao cho tổng khoảng cách từ các điểm đến nó là ngắn nhất.\nKhi có một dữ liệu mới, ta sẽ dựa vào đường thẳng này để dự đoán.\nVí dụ, giá nhà sẽ tăng khi diện tích căn nhà tăng và ta sẽ ước tính sự liên quan này qua một đường thẳng trên biểu đồ.Tất nhiên, trong thực tế thì sẽ ít khi xuất hiện đường thẳng. Lúc đó ta cần đến Logistic Regression.Lúc này chúng ta sẽ cố gắng dự đoán bằng cách phân chia hai miền giá trị bằng một hàm đặc biệt.Regression đều ứng dụng Gradient Descent để tìm được hàm chính xác.Vấn đề đặt ra là khi mà dữ liệu mẫu phân nhóm, không bao phủ thì độ chính xác của Regression không cao nữa. Ví dụ như hình sau:Bạn thấy đấy, các đường kẻ chia cắt 2 nhóm đều đúng, vậy thì đường nào là đúng nhất?\nLúc này thì ta áp dụng Support Vector Machines (SVM). Với SVM thì ta sẽ cố gắng mở rộng mỗi miền giá trị nhất có thể.\nCó thể hiểu thuật toán này qua hình trên, chúng ta cần tìm đường thẳng chia ở giữa làm sao cho khoảng cách ngắn nhất của các điểm đến đường thẳng là dài nhất.I will update later\n\t\t\tA young and passionate student who loves learning new technologies.\t\t\t\n\t\t\t\tView all posts by Phạm Hữu Danh\t\t\t\n\t\tMachine LearningFill in your details below or click an icon to log in: You are commenting using your WordPress.com account. ( Log Out / Change ) You are commenting using your Twitter account. ( Log Out / Change ) You are commenting using your Facebook account. ( Log Out / Change ) You are commenting using your Google+ account. ( Log Out / Change )Connecting to %s Notify me of new comments via email. \n\n\n\t\t\t\t\n\t\t\t\tCreate a website or blog at WordPress.com\n\t\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\tUp ↑\n\t\t\t",
          "relevence": "no"
        },
        {
          "url": "http://www.bmthicong.com.vn/research.html",
          "title": "Bộ Môn Công Nghệ Và Quản Lý Xây Dựng - Nghiên cứu",
          "content": "\n\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\t\n\n\t\t\t\t08 Tháng 12 2016\n\t\t\t\n\n\t\t\t\t\t\n\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\t\tPosted in \n\t\t\t\n\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\t\n\t\t\t\t\tNghiên cứu\n\t\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\n\t\n\n\t\t\t\n\n\n\t\tBộ môn Công nghệ & Quản lý xây dựng tham gia “Hội thảo Toàn quốc lần thứ 29 về Kết cấu và Công nghệ Xây dựng”                Sáng 30/11 tại trường Đại học Xây dựng (ĐHXD) đã diễn ra “Hội thảo Toàn quốc lần thứ 29 về Kết cấu và Công nghệ Xây dựng” do Hội Kết cấu và Công nghệ Xây dựng Việt Nam (VASECT) phối hợp cùng Bộ Xây dựng, Bộ Giao thông vận tải và Bộ Nông nghiệp & Phát triển Nông thôn tổ chức.             Thứ trưởng Bộ Xây dựng Đỗ Đức Duy; Chủ tịch VASECT, GS.TSKH Nguyễn Văn Liên; Hiệu trưởng trường ĐHXD PGS.TS Phạm Duy Hòa cùng đại diện Bộ Giao thông Vận tải, Tổng Hội Xây dựng Việt Nam tham dự hội thảo. PGS.TS Hồ Ngọc Khoa – Trưởng Bộ môn Công nghệ & Quản lý xây dựng cũng đã đại diện Bộ môn tham gia Hội thảo. Chủ tịch VASECT, GS.TSKH Nguyễn Văn Liên phát biểu khai mạc hội thảo.                Phát biểu khai mạc hội thảo, GS.TSKH Nguyễn Văn Liên cho biết trong 32 năm hoạt động và phát triển, Hội Kết cấu và Công nghệ Xây dựng Việt Nam tập trung vào sinh hoạt học thuật với trọng tâm phát triển công nghệ xây dựng. Hội thảo lần này tập hợp các kết quả nghiên cứu mới nhất mang tính thời sự trong 3 lĩnh vực xây dựng gồm xây dựng dân dụng công nghiệp, xây dựng công trình giao thông và xây dựng công trình thủy lợi. Các báo cáo gắn chặt thực tế của các vấn đề chính như rủi ro trong xây dựng và thị trường xây dựng Việt Nam hiện nay.Thứ trưởng Bộ Xây dựng Đỗ Đức Duy phát biểu tại hội thảo.                Thứ trưởng Bộ Xây dựng Đỗ Đức Duy đánh giá cao hoạt động của Hội Kết cấu và Công nghệ Xây dựng Việt Nam trong những năm qua. Thứ trưởng mong muốn hội thảo sẽ là nơi mà các chuyên gia, nhà khoa học, báo cáo viên trao đổi, chia sẻ kinh nghiệm chuyên ngành.PGS.TS Phạm Duy Hòa – Hiệu trưởng trường Đại học Xây dựng phát biểu tại hội thảo.                    Phát biểu tại hội thảo, PGS.TS Phạm Duy Hòa cho biết nhiều năm qua trường ĐHXD luôn có mối quan hệ mật thiết với các hội chuyên ngành, đặc biệt là Hội Kết cấu và Công nghệ Xây dựng Việt Nam. VASECT và nhà trường đã có nhiều chương trình hợp tác hiệu quả đóng góp vào sự phát triển của trường và của Hội.Quang cảnh hội thảo.              Tại hội thảo, các chuyên gia đầu ngành trong lĩnh vực xây dựng đã báo cáo 16 tham luận về kết cấu và công nghệ xây dựng trên thế giới có thể ứng dụng tại Việt Nam. Đại diện giảng viên Bộ môn – PGS.TS Hồ Ngọc Khoa đã tham gia Hội thảo với tham luận: “Công nghệ xây dựng nhà siêu cao tầng: Xu hướng phát triển và ứng dụng ở Việt Nam”. Bài tham luận trình bày kết quả phân tích, tổng hợp các công nghệ xây dựng mới, tiên tiến được áp dụng trên thế giới và trong quá trình thi công các công trình siêu cao tầng ở Việt Nam. Qua đó đưa ra một số nhận định, đánh giá về xu hướng phát triển và tiềm năng ứng dụng phù hợp với điều kiện nước ta. PGS.TS Hồ Ngọc Khoa – Trưởng Bộ môn Công nghệ & Quản lý xây dựng báo cáo tại Hội thảo.  \n\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\t\n\n\t\t\t\t17 Tháng 11 2016\n\t\t\t\n\n\t\t\t\t\t\n\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\t\tPosted in \n\t\t\t\n\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\t\n\t\t\t\t\tNghiên cứu\n\t\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\n\t\n\n\t\t\t\n\n\n\t\tBỘ MÔN CHỦ TRÌ THÀNH CÔNG PHIÊN THẢO LUẬN TIỂU BAN CÔNG NGHỆ VÀ QUẢN LÝ XÂY DỰNG - HỘI THẢO QUỐC TẾ VỀ PHÁT TRIỂN BỀN VỮNG TRONG XÂY DỰNG.               Ngày 15 tháng 11 năm 2016, tại trường Đại học Xây dựng đã diễn ra Hội thảo Quốc tế về Phát triển Bền vững trong Xây dựng do Khoa Xây dựng Dân dụng và Công nghiệp chủ trì. Tham dự hội thảo có các học giả, nhà nghiên cứu đến từ các trường đại học, các viện nghiên cứu, các công ty xây dựng trong và ngoài nước như: Đại học Melbourne (Australia), Công ty JPE (Nhật Bản), Công ty Xây dựng Turner, Công ty Xây dựng Delta, Công ty Xây dựng Conteccon, Công ty NUCETECH… Tại phiên toàn thể diễn ra vào buổi sáng, các học giả trong nước và quốc tế đã trình bày các báo cáo khoa học về các kết quả nghiên cứu, ứng dụng trong lĩnh vực phát triển bền vững mà đơn vị mình đã nghiên cứu, thực hiện. Buổi chiều cùng ngày diễn ra các phiên báo cáo tại tiểu ban, Bộ môn Công nghệ và Quản lý xây dựng chủ trì phiên họp tại tiểu ban Công nghệ và Quản lý Xây dựng.             Trong phiên thảo luận tại tiểu ban Công nghệ và Quản lý Xây dựng, các học giả đến từ các trường Đại học Bách khoa thành phố Hồ Chí Minh, Đại học Kiên trúc Hà Nội, Đại học Xây dựng đã trình bày 13 báo cáo khoa học về các đề tài về ứng dụng công nghệ mới trong xây dựng cũng như trong quản lý. Tại hội thảo, các chuyên gia và khách mời cũng đã có những thảo luận, trao đổi về mặt học thuật cho các báo cáo viên. Ảnh: PGS. TS. Hồ Ngọc Khoa phát biểu chủ trì phiên họp tiểu ban Công nghệ và Quản lý Xây dựng.Ảnh: ThS. Vương Đỗ Tuấn Cường trình bày nghiên cứu “A sustainable approach in soft soil treatment in Vietnam” tại hội thảo.Ảnh: PGS. TS. Lương Đức Long – ĐH Bách khoa TP HCM đang trình bày báo cáo “Fuzzy programming model in optimal usage of multi-skilled labors under fuzziness in construction projects”.Ảnh: NCS Chu Thị Hải Ninh đang trình bày báo cáo “Manufacture of lightweight fireproof-insulating concrete using Hoang Thach PCB 30 portland cement and Pha Lai fly ash”.Ảnh: PGS. TS. Hồ Ngọc Khoa phát biểu góp ý cho một báo cáo tại hội thảo.Ảnh: Các chuyên gia thảo luận cùng các báo cáo viên.Ảnh: Các báo cáo viên chụp ảnh lưu niệm tại hội thảo. KẾT QUẢ BẢO VỆ CƠ SỞ CÁC ĐỀ TÀI NGHIÊN CỨU KHOA HỌC CỦA BỘ MÔNBộ môn CN & QLXD chủ trì 02 đề tài KHCN trong Chương trình KHCN cấp Bộ GD&ĐT “Thiết kế và thi công nhà siêu cao tầng” của trường ĐHXD.  Đó là các đề tài: “Nghiên cứu các giải pháp bảo trì và quản lý nhà siêu cao tầng ở Việt Nam” do PGS.TS. Hồ Ngọc Khoa chủ trì và đề tài “Nghiên cứu công nghệ xây dựng nhà siêu cao tầng ở Việt Nam” do TS. Trần Hồng Hải chủ trì.Chiều ngày 23/12/2015 đề tài “Nghiên cứu các giải pháp bảo trì và quản lý nhà siêu cao tầng ở Việt Nam” và chiều 28/12/2015 đề tài “Nghiên cứu công nghệ xây dựng nhà siêu cao tầng ở Việt Nam” đã bảo vệ thành công cấp cơ sở.Hai Hội đồng cơ sở do GS.TS. Phan Quang Minh làm Chủ tịch cùng với các thành viên: PGS.TS. Trần Văn Liên, PGS.TS. Ngô Văn Quỳ, TS. Đinh Tuấn Hải, PGS.TS. Trịnh Quốc Thắng, TS. Mỵ Duy Thành, TS. Đoàn Dương Hải, PGS.TS. Trần Chủng, TS. Nguyễn Ngọc Linh, TS. Nguyễn Đại Minh, PGS.TS. Nguyễn Ngọc Phương đánh giá cao kết quả nghiên cứu đạt được; nhận xét, góp ý để các nhóm thực hiện đề tài chỉnh sửa, hoàn thiện, chuẩn bị bảo vệ chính thức tại Hội đồng nghiệm thu cấp Bộ.Chủ trì và các thành viên hiện đề tài bao gồm PGS.TS. Hồ Ngọc Khoa, TS. Trần Hồng Hải, TS. Nguyễn Mạnh Tuấn, ThS. Vương Đỗ Tuấn Cường, ThS. Phạm Tiến Tới, ThS. Nguyễn Hùng Cường, ThS. Cao Tuấn Anh, ThS. Phạm Nguyễn Vân Phương, ThS. Lê Đình Tiến, ThS. Lê Thái Hòa cảm ơn Hội đồng, tiếp thu ý kiến đóng góp để hoàn thiện đế tài, để bảo vệ chính thức thành công, đúng tiến độ.Một vài hình ảnh của các buổi bảo vệ đề tài:PGS. TS. Hồ Ngọc Khoa trình bày các kết quả nghiên cứu đề tài bảo trìCác thành viên hội đồng tham gia đóng góp ý kiến cho đề tài bảo trìTS. Nguyễn Mạnh Tuấn trình bày kết quả nghiên cứu đề tài công nghệ thi côngPGS. TS. Trần Chủng phát biểu ý kiến phản biện đề tài công nghệ thi công \n\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\t\n\n\t\t\t\t29 Tháng 12 2014\n\t\t\t\n\n\t\t\t\t\t\n\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\t\tPosted in \n\t\t\t\n\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\t\n\t\t\t\t\tNghiên cứu\n\t\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\n\t\n\n\t\t\t\n\n\n\t\tDỰ BÁO KẾT QUẢ CỦA QUÁ TRÌNH PHUN VỮA XI MĂNG BẰNG TRÍ THÔNG MINH NHÂN TẠOAN ARTIFICIAL INTELLIGENCE APPROACH FOR GROUTABILITY ESTIMATION BASED ON AUTOTUNING SUPPORT VECTOR MACHINETS. Trần Hồng Hải, TS. Hoàng Nhật Đức        Phương pháp bơm vữa gia cố nền kiểu xâm nhập (permeation grouting) thương được sử dụng trong kỹ thuật xây dựng. Như vậy, việc dự đoán kết quả của quá trình phun vữa là một công tác rất quan trọng cần được chú trọng từ giai đoạn lập kế hoạch. Trong bài nghiên cứu này, một phương pháp mới sử dụng trí thông minh nhân tạo – máy véc tơ hỗ trợ tự động điều chỉnh (autotuning support vector machine - SVM) được đưa ra nhằm dự báo kết quả quá trình phun vữa xi măng sử dụng vữa xi măng hạt mịn. Ở phương pháp mới này, thuật toán của máy SVM được sử dụng để phân loại quá trình phun vữa bao gồm: thành công và thất bại. Trong đó, thuật toán vi phân (DE) tối ưu hóa được dùng để nhận dạng các thông số điều chỉnh tối ưu của thuật toán từ máy SVM, gồm có các thông số bù trừ và thông số chức năng cốt lõi. Sự kết hợp các thuật toán SVM và DE cho phép phương pháp dự đoán mới này có thể vận hành tự động mà không cần đến các tác động của con người, bỏ qua quá trình điều chỉnh các thông số lặp đi lặp lại. Một thí nghiệm có sử dụng các mẫu thử tại chỗ cũng chỉ ra rằng phương pháp dự báo mới này có thể đưa ra các kết quả dự đoán chính xác.Bạn đọc và các bạn sinh viên quan tâm có thể theo dõi ở đường link sau:  DỰ BÁO KẾT QUẢ CỦA QUÁ TRÌNH PHUN VỮA XI MĂNG BẰNG TRÍ THÔNG MINH NHÂN TẠO\n\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\t\n\n\t\t\t\t29 Tháng 12 2014\n\t\t\t\n\n\t\t\t\t\t\n\n\t\t\n\n\t\t\t\n\t\t\n\n\t\t\t\n\t\t\t\tPosted in \n\t\t\t\n\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\t\n\t\t\t\t\tNghiên cứu\n\t\t\t\t\t\n\t\t\t\t\n\n\t\t\t\t\n\t\n\n\t\t\t\n\n\n\t\tĐIỀU HÒA TÀI NGUYÊN CHO DỰ ÁN XÂY DỰNG BẰNG THUẬT TOÁN TIẾN HÓA VI PHÂNA NOVEL RESOURCE-LEVELLING APPROACH FOR CONSTRUCTION PROJECT BASED ON DIFFERENTIAL EVOLUTIONTS. Trần Hồng Hải, TS. Hoàng Nhật Đức        Trong kỹ thuật xây dựng, thông thường phương pháp đường Găng (CPM - Critical Path Method) được sử dụng để lập tiến độ của các dự án. Tuy nhiên, cách lập tiến độ này thường xuất hiện các biến động đáng kể trong danh mục tài nguyên, điều này không chỉ phi thực tế mà còn gây tốn kém cho nhà thầu khi thực hiện theo. Vì vậy, để điều hòa các danh mục tài nguyên đó, nhà quản lý xây dựng cần thực hiện quá trình cân bằng các hạng mục tài nguyên. Bài nghiên cứu này đề xuất ra phương pháp điều hòa tài nguyên dựa trên các thuật toán tiến hóa vi phân (viết tắt tiếng Anh: RLDE – resource levelling based on differential evolution). Cách thể hiện của RLDE so với phần mềm Microsoft Project khác biệt ở việc sử dụng các thuật toán di truyền (the genetic algorithm) và các thuật toán tối ưu hóa tổ hợp phần tử (the particle swarm optimization algorithm). Nhiều thực nghiệm đã chỉ ra rằng phương pháp tính toán mới này có thể mang lại kết quả tối ưu trong việc thực hiện điều hòa tài nguyên. Như vậy, có thể kết luận phương pháp RLDE là một phương pháp hiệu quả, có thể được sử dụng như một công cụ hữu ích cho người làm công tác quản lý, lên kế hoạch trong lĩnh vực quản lý dự án.Bạn đọc và các bạn sinh viên quan tâm có thể theo dõi ở đường link sau:  ĐIỀU HÒA TÀI NGUYÊN CHO DỰ ÁN XÂY DỰNG BẰNG THUẬT TOÁN TIẾN HÓA VI PHÂN\r\n\t\t\t\tTrang 1 / 4\t\t\t1. Sinh viên hỏi:câu hỏi từ bạn: congthien.nuce54@gmail.com Thưa thầy cô, hiện tại em đang làm đồ án thi công 1, và ở phần chọn máy thi công em đang băn khoăn chưa chọn được máy cẩu tháp nào phù hợp bởi vì khối lượng bê tông quá lớn mà các máy hiện có trong các sổ tay máy cũ không đạt yêu cầu.Cho nên hiện e đang cần catalog của một số loại cẩu tháp chạy trên ray và máy trộn bê tông những loại mới nhất hiện nay. Em xin cảm ơn ạ!Bộ môn Trả lời:Em hãy lập biện pháp kỹ thuật và tổ chức thi công theo các loại cần trục tháp và máy trộn bê tông mà em có thể biết hiện có ở Việt Nam. 2. Sinh viên hỏi:Câu hỏi từ bạn: hùng <hanhieuvi0112@gmail.com\">hanhieuvi0112@gmail.com>kính thưa bộ môn,nhóm làm đồ án tốt nghiệp 53xd3 được nhà trường phân thầy Lê Thế Thái hướng dãn phần thi công.hiện tại bọn em đã xong phần kết cấu.liên hệ với thầy thì thầy bảo tất cả các nhóm tự liên hệ với bạn Tuấn Anh.thực sự bọn em đã lên hỏi bộ môn,phòng đào tạo,tìm ai tên là tuấn anh nhưng k có thông tin gì cả.tất cả đều là tự liên hệ với thầy để nhận đề tài.kính mong bộ môn hồi âm để bọn em được nhận sự hướng dẫn của thầy ạBộ môn Trả lời: Em hãy liên hệ với thầy Lê Thế Thái và đề nghị với thầy bố trí làm việc với các em. 3. Sinh viên hỏi: Câu hỏi từ bạn: Đặng Thành Luân <thanhluanxd7@gmail.com\">thanhluanxd7@gmail.com>Em xin hỏi thày cô, khi hạ mực nước ngầm có sử dụng được bằng phương pháp bấc thấm được ko ạ? Và nếu sử dụng được thì nguyên lý tính toán và trình tự tính thế nào ạ? Mong thày cô có thể cho tiêu đề các tài liệu, hoặc cho em xin tài liệu về phương pháp bấc thấm ( Nếu có công trình đã sử dụng thì rất tốt ạ)? Chúc thày cô mạnh khỏe - Hạnh phúc - Thành đạt!Bộ môn Trả lời: Bấc thấm là biện pháp để gia cố nền chứ không phải để hạ mực nước ngầm, em nên tìm hiểu kỹ lại nguyên lý của phương pháp này.  4. Sinh viên hỏi:Câu hỏi từ bạn:lưu bá vũ <luubavudexauxa@gmail.com\">luubavudexauxa@gmail.com>các thầy cô cho em hỏi.định mức giờ công trong thống kê lắp đặt ván khuôn lấy như thế nào? Bộ môn Trả lời:Em có thể tìm hiểu tất cả các định mức lao động mà Nhà nước ViệtNam đã ban hành.Nếu muốn tìm hiểu chi tiết hơn thì em xem định mức 726Chúc các em sức khỏe và học tập tốt.Ban chủ nhiệm Bộ môn CN&QLXD Sinh viên hỏi:nguyenquyenxd113@gmail.comem chào thầy! thầy có thể cho em hỏi là: thực trạng về cơ sở pháp lý của công tác giám sát thi công công trình xây dựng không ạ?em cám ơn thầy nhiều ạ!!Bộ môn trả lời:Giám sát thi công là do các cơ quan tư vấn giám sát thực hiện. Muốn làm công việc đó ( giám sát tư vấn ) yêu cầu phải có chứng chỉ tư vấn giám sát và giấy phép hành nghề tư vấn giám sátChúc bạn sức khỏe - học tập tốt!Ban chủ nhiệm Bộ mônCâu hỏi:Em chào thầy!Em muốn nhờ thầy giúp đở em về học tập.Vậy em có thể gặp thầy Nguyễn Đình Thám vào thời gian nào trong tuần ạ!Em xin cảm ơn thầy!Bộ môn trả lời:Trả lời câu hỏi của bạn Phan Văn Hoàng - lớp 52KSCT.Bộ môn rất hoan nghênh tinh thần học tập của em.Em có thể liên hệ trực tiếp với thầy Nguyễn Đình Thám , cũng như các thầy khác trong Bộ môn qua số điện thoại của các thầy đã đăng trên websiteChúc em học tập tốt!Ban chủ nhiệm Bộ môn Công nghệ và Quản lý Xây dựng.Bản quyền thuộc Bộ môn Công nghệ và Quản lý Xây dựng - Khoa Xây dựng Dân dụng & Công nghiệp - Trường Đại học Xây Dựng Địa chỉ liên hệ: Phòng 307 - Nhà A1 - Trường Đại học Xây dựng Số 55 - Đường Giải Phóng - Quận Hai Bà Trưng - Hà NộiĐiện thoại: (+84)4.3869.7349  Email: bm.thicong@gmail.com Chủ biên: TS. Trần Hồng Hải; Th.S Cao Thế Trực",
          "relevence": "no"
        },
        {
          "url": "http://vinaai.com/chuyen-nghe/10-thuat-toan-machine-learning-quan-trong-ma-engineers-nen-biet",
          "title": "10 thuật toán machine learning quan trọng mà Engineers nên biết - Vina AI",
          "content": " by duonghau | Apr 3, 2017 | Chuyện nghề | 1 commentĐây là bài viết đầu tiên trên blog vinaai.com. Bản thân tôi xuất phát điểm từ dân lập trình (chính xác hơn là kỹ thuật phần mềm), nhìn thế giới đang phát triển ào ào các công nghệ mới có ứng dụng lớn, tôi tự hỏi chẳng lẽ mình cứ đi code thuê mãi? Tôi không có ý nói code thuê là xấu, tôi chỉ muốn tham gia vào một nhóm việc khó hơn, dùng nhiều chất xám hơn và được trả cao hơn. Tôi quyết định chuyển hướng sang học về AI từ năm 2014. Đây là bài viết tôi dịch lại trên kdnuggets.com (đối với tôi đây là website hàng đầu về data mining và machine learning), ngoài ra tôi cũng có đưa thêm những quan điểm cá nhân. Hi vọng giúp ích được cho các bạn đang tìm hướng đi cho riêng mình.Machine learning / trí tuệ nhân tạo đang là xu hướng mạnh trên thế giới cũng như bắt đầu xuất hiện ở Việt Nam. Một nhánh nhỏ của nó là Big data đang là một xu hướng cực hot được các ông trùm về công nghệ như Microsoft, Google, Facebook, Amazon đầu tư vào. AI hiện hữu khắp nơi: từ hệ thống gợi ý phim của Netflix, gợi ý video của Youtube, tag ảnh tự động của Facebook, gợi ý sản phẩm của Amazon…. Đấy là các ông lớn, thế còn các ứng dụng ở quy mô nhỏ thì sao? AI giúp nhận dạng biển số xe tự động, giúp sửa lỗi chính tả, giúp dự đoán giá cả, dự đoán tình trạng bệnh …(danh sách còn dài, nếu có thời gian tôi sẽ liệt kê khoảng 100 ý tưởng doanh nghiệp nhỏ hưởng lợi từ AI).AI đang phát triển và còn phát triển mạnh trong tương lai. Tôi là lập trình viên, muốn tiếp cận nó thì phải bắt đầu từ đâu? Machine learning có thể được chia làm 3 nhánh chính: supervised learning (học giám sát), unsupervised learning (học không giám sát), and reinforcement learning (học tăng cường, tôi sẽ giải thích sau). Học có giám sát được hiểu là chúng ta đã có kết quả đầu ra (label, outcome) trong dữ liệu training (một bộ x, y), nhiệm vụ của chúng ta là xây dựng mô hình dựa trên dữ liệu này để dự đoán những bản ghi mới mà chúng ta chưa biết kết quả (new x). Học không giám sát thì ngược lại, chúng ta không có outcome, chúng ta cố gắng gom nhóm các đối tượng dựa trên mối quan hệ giữa chúng (ví dụ góm nhóm dựa trên khoảng cách). Học tăng cường (tôi dịch từ này cũng chưa sát lắm, nhưng chưa tìm được từ tốt hơn, ta tạm chấp nhận từ này) nằm ở giữa 2 dạng trên. 10 thuật toán dưới đây rơi vào 2 nhánh đầu, hi vọng một ngày không xa tôi cập nhật thuật toán reinforcement learning bằng một bài độc lập.Tôi sẽ trình bày 10 thuật toán phổ biến, tôi sẽ cố gắng lý giải bằng ngôn ngữ hàng ngày, tránh các công thức toán lằng nhằng đằng sau. Trong trường hợp bất đắc dĩ phải đưa công thức, tôi sẽ lý giải giản lược (thực tế tôi cũng không thích các công thức toán cho lắm 🙂 )Như đúng cái tên của nó, cây quyết định là mô hình hỗ trợ ra quyết định dựa trên đồ thị các điều kiện. Tại mỗi nút, ta sẽ đối chiếu các điều kiện thực tế để quyết định rẽ nhánh nào. Nút lá là quyết định cuối cùng.Dưới đây là ví dụ kinh điển về cây quyết định. Cây này cho ta gợi ý về việc có đi chơi Tennis hay không. Ví dụ quang cảnh có nắng, độ ẩm Trung bình thì tôi sẽ đi chơi Tennis. Ngược lại nếu trời mưa, gió mạnh tôi sẽ ở nhà xem phim.Cây quyết định tuy là mô hình khá cũ, khá đơn giản những vẫn còn được ứng dụng khá nhiều và rất hiệu quả. Đứng dưới góc nhìn thự tế, cây quyết định là một danh sách tối thiểu các câu hỏi dạng yes/no nhằm đưa ra quyết định đúng đắn. Sẽ có một bài khác nói về mô hình Random forest (một mở rộng của cây quyết định).Phân loại Bayes là phương pháp phân loại dựa trên xác suất sảy ra biến cố dựa trên định lý Bayes giữa 2 biến độc lập.Phương pháp phân loại Bayes được ứng dụng trong:Qua một người thầy của tôi, tôi biết rằng phương pháp này còn được ứng dụng khá nhiều trong y học. Ví dụ để dự đoán bệnh nhân mắc hay không mắc bệnh A khi có triệu chứng B.Hồi quy tuyến tính được những người làm thống kê rất chuộng vì tính đơn giản, hiệu quả và quan trọng nhất nó không phải là black box như mạng neuron. Người ta có thể lấy ra các tham số của mô hình hồi quy tuyến tính để hiểu tác động của từng biến lên kết quả (outcome). Hiểu đơn giản, mô hình hồi quy tuyến tính cố gắng tìm một đường thẳng (tuyến tính) qua một tập các điểm sao cho sai số (khoảng cách từ điểm dữ liệu đến đường thẳng) là nhỏ nhất (mean square error).Có một lưu ý là outcome của mô hình hồi quy tuyến tính phải là biến liên tục (numeric).Một ví dụ đơn giản và cũng kinh điển là người ta có thể dùng mô hình này để dự đoán giá cả (nhà đất, chứng khoán), điểm số …Mô hình hồi quy tuyến tính dùng cho dữ liệu có outcome là biến liên tục. Nếu outcome là biến phân loại (binomial) thì chúng ta có thể dùng mô hình hồi quy logistic. Thầy Phan Tuấn có series video trên youtube lý giải rất kỹ lý thuyết của mô hình này, thế nên tôi sẽ không đề cập sâu.Mô hình hồi quy logistics được ứng dụng trong:SVM là phương pháp phân loại nhị phân. Cho một  tập dữ liệu gồm 2 nhóm trong môi trường n-chiều, SVM cố gắng tìm ra n-1 mặt phẳng nhằm phân tách tập dữ liệu trên thành 2 nhóm.Ví dụ, cho tập gồm 2 nhóm điểm như hình dưới, SVM sẽ tìm ra 1 đường thẳng nhằm phân cách 2 nhóm sao cho khoảng cách giữa đường thẳng và các điểm xa nhất có thể.Đây là phương pháp khá mạnh. Thực sự thì sau khi phương pháp này trở nên phổ biến, người ta suýt đã quên đi mạng neuron. Trong các vấn đề như phân loại ảnh, phân loại text, SVM cho kết quả tốt hơn mạng neuron rất nhiều. Mãi đến gần đây, khi deep learning lên ngôi thì mạng neuron mới tìm được chỗ đứng trở lại.SVM có khả năng xử lý các bài toán quy mô lớn: hiển thị quảng cáo ngữ cảnh, nhận dạng ảnh, nhận dạng giới tính dựa vào ảnh…Phương pháp này dựa trên kết hợp một vài phương pháp kể trên để dự đoán kết quả, sau đó sẽ đưa ra kết quả cuối cùng dựa vào trọng số của từng phương pháp.Phương pháp này hoạt động nhờ:Clustering nhằm gom nhóm một tập đối tượng vào các nhóm sao cho các đối tượng trong cùng 1 nhóm có độ tương đồng lớn hơn so với đối tượng của nhóm khác.Clustering lại có nhiều phương pháp khác nhau, đây là một vài trong số chúng:Phương pháp này được sử dụng nhiều trong việc giảm chiều dữ liệu mà hạn chế tối đa mất mát thông tin. Khi dữ liệu lên tới hàng nghìn chiều, hàng triệu chiều thì việc giảm chiều là cần thiết cho các thuật toán như SVM, regression….Ý tưởng chính của phương pháp này là chuyển dữ liệu từ không gian ban đầu sang không gian mới sao cho các biến độc lập với nhau (gọi là các thành phần chính).Dân làm machine learning khá thích PCA, chỉ cần cho dữ liệu 1000 chiều vào, chạy vù cái thu được dữ liệu 100 chiều, cho vào model dự đoán chạy thấy tăng độ chính xác. Đối với anh em làm AI như thế là vui rồi. Tuy nhiên mình để ý dân làm statistic họ không thích PCA vì nó gây mất ý nghĩa các cột dữ liệu. Ví dụ, nếu dùng PCA, có thể mô hình dự đoán rất tốt bệnh nhân mắc hay không mắc bệnh, tuy nhiên người làm nghiên cứu không có chút thông tin nào về việc gene nào đóng góp vào việc mang bệnh (do dữ liệu đã được chuyển sang không gian khác).Tôi chưa có kinh nghiệm làm việc với phương pháp này. Hẹn một ngày gần đây tôi sẽ bổ sung vào mục này.Đây là phương pháp có gốc từ statistic, nó nhằm tìm ra nhân tố ẩn nằm trong một tập các biến, độ đo hay tín hiệu. Trong các nghiên cứu xã hội người ta dùng phương pháp này tương đối nhiều. Kết thúc bài này ở đây, tác giả bài viết James Le và tôi chỉ mong muốn giúp các bạn có một cái nhìn chung (overview) về các thuật toán phổ biến trong AI. Nó giống như đưa cho bạn một miếng Bít tết thay vì bắt bạn ngốn cả một con bò. Cần thời gian để tìm hiểu, từ tìm hiểu dẫn đến thích, từ thích dẫn đến đào sâu, tôi sẽ đi sâu vào từng thuật toán trong các bài viết khác. Hi vọng bạn nhận được cái gì đó trong bài viết này.Bài viết gốc: http://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.htmlCảm ơn anh vì bài viết .\nHi vọng anh sẽ có những bài viết tiếp theo về mạng neural network & deep learningYour email address will not be published. Required fields are marked *Comment Name * Email * Website Please input characters displayed above. \n\nDesigned by Elegant Themes | Powered by WordPress",
          "relevence": "yes"
        },
        {
          "url": "http://bis.net.vn/forums/t/619.aspx",
          "title": "\r\n\tCác phương pháp học máy (Machine Learning) - BIS\r\n",
          "content": "Trong lĩnh vực Học máy có các phương\r\npháp học sau:1) Học có giám sát (supervised\r\nlearning)2) Học không có giám sát\r\n(unsupervised learning)3) Học bán giám sát\r\n(semi-supervised learning)4) Học tăng cường (reinforcement\r\nlearning)Không phải ngẫu nhiên người ta lại\r\nphân chia và đặt tên cho các phương pháp học như vậy.Sau một thời gian tìm đọc tài liệu\r\nvề học máy, cố gắng hiểu được bản chất và phân biệt được sự giống và khác nhau\r\ncủa các phương pháp học này, tôi đưa ra một bài toán ví dụ sau:Cho một công-ten-nơ chứa đầy hoa\r\nquả. Nhiệm vụ phải chia số quả này thành các nhóm đúng với loại quả đó. (xoài\r\nra xoài, cam ra cam, táo ra táo,…)1) Với Học có giám sát\r\n(supervised learning)Là kỹ thuật học sử dụng cho các bài\r\ntoán phân lớp (Classification)Để thực hiện được bài toán trên,\r\ntrước tiên cần phải có 2 điều kiện:Điều kiện 1: phải biết trước số\r\nnhãn lớp cần phân loại, tức là phải biết trong công-ten-nơ đó có nhưng loại quả\r\ngì. Giả sử trong công-ten-nơ đó có 5 loại quả là xoài, cam, táo, ổi, đào (đây\r\nchính là 5 loại nhãn lớp).Điều kiện 2: phải có tập đặc trưng\r\ncủa mỗi loại quả, ví dụ các đặc trưng là: hình dáng, màu sắc, trọng lượng, độ cứng\r\nmềm, v.v… Tập đặc trưng này có được thông qua học một tập dữ liệu huấn luyện\r\n(chính là các công-ten-nơ của các chuyến hàng trước đó)Khi thực hiện phân loại các loại\r\nquả trong công-ten-nơ đang xét, dựa vào đặc trưng của các loại quả (điều kiện\r\n2), quả sẽ được đưa vào 1 trong 5 nhóm đã biết (điều kiện 1).2) Học không có giám sát\r\n(unsupervised learning)Là kỹ thuật học sử dụng cho các bài\r\ntoán phân cụm, gom cụm (Clustering) Để thực hiện được bài toán trên,\r\ncần phải có tập đặc trưng của mỗi loại quả. Tập đặc trưng này có được cũng thông\r\nqua học một tập dữ liệu huấn luyện (như điều kiện 2 của Học có giám sát). Điểm khác của Học không giám sát\r\nso với Học có giám sát là: trước khi phân cụm, không biết trong công-ten-nơ đang\r\nxét có bao nhiêu loại quả và đó là những loại quả gì.Khi thực hiện phân cụm, dựa vào đặc\r\ntrưng của mỗi loại quả, sẽ đưa quả đang xét vào nhóm (cụm) có đặc trưng tương đồng\r\nvới nó nhất. Khi đó, 2 quả bất kỳ ở cùng cụm sẽ tương đồng nhau, 2 quả khác cụm\r\nsẽ khác biệt nhau.* Nhận xétGiống nhau: Cả\r\nhai phương pháp học 1) và 2) đều cần phải có một tập huấn luyện (training data\r\nset) để hệ thống có thể “học” và rút ra được các đặc trưng dùng cho việc gán nhãn.Khác nhau: Phương\r\npháp 1) cần biết trước đầu ra chính là số nhãn lớp. Phương pháp 2) không cần biết\r\ntrước đầu ra  (là số cụm và nhãn) để phân\r\ncụm. Trên đây là cách hiểu của tôi về\r\nphương pháp Học có giám sát và Học không giám sát. Nhân đây tôi xin đưa một thắc\r\nmắc để các thành viên và những ai quan tâm đưa ra ý kiến để cùng thảo luận.Thắc mắc: Học bán giám sát là gì?\r\nHọc tăng cường là gì? Phân biệt sự khác nhau và giống nhau giữa các phương pháp\r\nhọc bán giám sát và học tăng cường. Cho ví dụ minh hoạ?Đúng k means là thuật toán học không giám sát vì nó được dùng để phân cụm cho các dữ liệu chưa biết nhãn. Để dễ phân biệt bạn chú ý là việc biết (cho) trước k cụm không liên quan gì đến dữ liệu đầu vào (hay dữ liệu huấn luyện - training data set). Với học có giám sát dữ liệu đầu vào đều đã được gán nhãn trước, phương pháp học này dùng giải quyết bài toán phân lớp (classification). Với học không giám sát, không biết trước nhãn của dữ liệu (hay dữ liệu đầu vào đều chưa được gán nhãn), phương pháp này dùng để giải quyết bài toán phân cụm (clustering). Còn với học nửa giám sát, sử dụng tập dữ liệu đầu vào gồm cả dữ liệu đã gán nhãn và dữ liệu chưa gán nhãn, trong đó dữ liệu gán nhãn thường (rất) ít và dữ liệu chưa gán nhãn thường (rất) nhiều. Hai kỹ thuật tiêu biểu cho học nửa giám sát là Self-training và Co-training, bạn có thể tìm kiếm trên internet.Chúc vui vẻ!Có nhiều bạn gửi mail hỏi mình về Machine Learning, các bạn thông cảm vì đi làm bận nên không trả lời riêng các bạn, và cũng để mọi người có thể cùng trao đổi mình sẽ post lên đây để mọi người chia sẻ những kiến thức về ML. Trước tiên mình nêu một số định nghĩa rất quan trọng trong ML. Học máy (hay máy học – Machine learning) là một thành phần quan trọng của trí tuệ nhân tạo nhằm nghiên cứu và phát triển các phương pháp, kỹ thuật giúp cho các hệ thống hay máy tính có khả năng học (Tiến Phong).Thuật ngữ over-fitting ra đời dùng để chỉ một hiện tượng xuất hiện trong quá trình khai phá dữ liệu sử dụng phương pháp học máy. Hiện tượng này gây khó khăn đáng kể cho việc thiết kế, xây dựng hệ thống ngay từ bước chuẩn bị dữ liệu cho đến bước kiểm thử hệ thống. Khi hiện tượng over-fitting xảy ra sẽ làm cho hiệu quả của hệ thống giảm xuống và kết quả thu được từ hệ thống không còn độ tin cậy cao. Có thể định nghĩa hiện tượng over-fitting như sau:Một hàm mục tiêu hay một giả thiết học được h, sẽ được gọi là over-fitting (quá vừa dữ liệu) với một tập dữ liệu huấn luyện nếu tồn tại một hàm mục tiêu khác là h’ sao cho:h’ kém phù hợp hơn, đạt độ chính xác kém hơn so với h trên tập dữ liệu huấn luyện, nhưng h’ lại đạt độ chính xác cao hơn h đối với toàn bộ tập dữ liệu (bao gồm cả tập dữ liệu liệu huấn luyện và tập dữ liệu kiểm tra)Giả sử gọi D là tập toàn bộ các dữ liệu có thể có, Training_D là tập các dữ liệu huấn luyệnGiả sử Err_D(h) là mức lỗi mà giả thiết h sinh ra đối với tập D, và Err_Training_D(h) là mức lỗi mà giả thiết h sinh ra đối với tập Training_D.Nếu tồn tại một giả thiết khác là h’ sao cho:Err_Training_D(h) < Err_Training_D(h’) vàErr_D(h) > Err_D(h’)Vấn đề over-fitting thường do các nguyên nhân:- Lỗi (nhiễu) trong tập huấn luyện phát sinh trong quá trình thu thập, xây dựng tập dữ liệu.- Số lượng dữ liệu của tập huấn luyện quá nhỏ, không đại diện cho toàn bộ tập dữ liệu có thể có hay toàn bộ phân bố dữ liệu của bài toán.Có thể mô hình hoá một vấn đề máy học như sau:Cho một dãy l quan sát: (x1, y1), (x2, y2), … , (xl, yl). Trong đó:- x1, x2, …, xl là các mẫu, xi Rn. Các mẫu xi được phát sinh ngẫu nhiên theo một hàm phân bố xác suất F(x) nào đó mà ta không biết.- yi là các kết quả học tương ứng với mẫu xi, yi R. Thường thì y là kết quả của một hàm f(x) nào đó – đơn trị. Tuy nhiên trong trường hợp tổng quát thì y không đơn trị. Do đó y được xác định theo một hàm phân bố điều kiện F(y|x) mà ta cũng không biết.- Bây giờ cho một mẫu x*, vấn đề của máy học là xác định một hàm f0(x) mà có thể ước lượng tốt nhất giá trị y* tương ứng. Như vậy theo lý thuyết tương quan trong thống kê thì f0(x) tốt nhất theo lý thuyết phải là kỳ vọng của y theo x theo phân bố F(y|x).f0(x) còn được gọi là phương trình hồi quy.Với x tuân theo phân bố F(x), y tuân theo phân bố có điều kiện F(y|x) thì hàm phân bố của cặp (x, y) là F(x, y) = F(x)F(y|x). Có thể thấy xác suất để có dãy (x1, y1), (x2, y2), … , (xl, yl) là tích F(x1, y1)F(x2, y2)…F(xl, yl).Tuy nhiên, ở đây ta không biết F(x) lẫn F(y|x) nên không thể xác định chính xác kỳ vọng này. Tất cả dữ liệu mà ta biết chỉ là dãy hữu hạn các mẫu quan sát (x1, y1), (x2, y2), … , (xl, yl). Nhiệm vụ của máy học là xác định chính xác nhất có thể được hàm f0(x) dựa trên các dữ liệu hữu hạn này.Trong trường hợp khi y  R, tức đây là vấn đề hồi quy (regression). Trong trường hợp bài toán phân lớp (classification) thì y {-1, 1} là trường hợp nhận dạng hai lớp, nếu yi = -1 thì xi thuộc lớp thứ nhất (không được quan tâm), còn yi = 1 thì xi thuộc lớp thứ 2 (lớp được quan tâm)Trong lĩnh vực học máy có nhiều phương pháp học khác nhau, trong phần này đề cập đến 3 phương pháp học được sử dụng phổ biến nhất, gồm có: học không giám sát, học bán/ nửa giám sát và học có giám sát.* Khái niệm học không giám sátHọc không giám sát là học với tập dữ liệu huấn luyện ban đầu hoàn toàn chưa được gán nhãn.Học không giám sát là phương pháp học sử dụng cho lớp bài toán gom cụm, phân cụm (Clustering).- Để thực hiện phân cụm, trước tiên cần một tập dữ liệu huấn luyện (training dataset) – là một tập các ví dụ học (training examples/instances). Trong đó, mỗi ví dụ học chỉ chứa thông tin biểu diễn (ví dụ: một vector các giá trị thuộc tính), mà không có bất kỳ thông tin gì về nhãn lớp hoặc giá trị đầu ra mong muốn (expected output).- Áp dụng một thuật toán học không có giám sát (ví dụ k-Means) để học hàm/mô hình mục tiêu (trong trường hợp này là hàm phân cụm ứng với thuật toán được chọn).- Sử dụng một phương pháp thử nghiệm (có thể kết hợp với một tập dữ liệu có gán nhãn) để đánh giá hiệu năng/chất lượng của hàm mục tiêu học được.Có rất nhiều thuật toán học không giám sát được ra đời và phát triển nhằm giải quyết bài toán phân cụm phục vụ khai thác hiệu quả nguồn dữ liệu chưa gán nhãn nhiều và rất đa dạng. Việc lựa chọn sử dụng thuật toán nào tuỳ thuộc vào dữ liệu và mục đích của từng bài toán. Trong đó các thuật toán thường được sử dụng như: k-means, HAC (Hierarchical Agglomerative Clustering), SOM (Self-Organizing Map), DBSCAN, FCM,... (chi tiết các thuật toán này có thể tìm kiếm trên Internet)Trong thực tế, để có được một tập dữ liệu có chất lượng và đã được gán nhãn của một lĩnh vực, thường được thực hiện thủ công bằng tay bởi người có nhiều kinh nghiệm về lĩnh vực đó. Vì vậy, dữ liệu đã được gán nhãn thường ít và đắt. Trong khi đó, dữ liệu chưa được gán nhãn lại rất nhiều và phong phú. Phương pháp học bán giám sát (hay học nửa giám sát) được đặt ra để tận dụng cả hai nguồn dữ liệu này.Học bán giám sát là học với tập dữ liệu huấn luyện gồm cả dữ liệu đã được gán nhãn và dữ liệu chưa được gán nhãn.Tuỳ vào từng mục đích cụ thể, học bán giám sát có thể được áp dụng cho bài toán phân lớp hoặc phân cụm.- Nội dung chính của học bán giám sát là hệ thống sử dụng một tập học (training set) gồm 2 phần: các ví dụ học có nhãn, thường với số lượng (rất) ít, và các ví dụ học không có nhãn, thường với số lượng (rất) nhiều. Thực tế cho thấy khi sử dụng kết hợp dữ liệu không có nhãn với một lượng nhất định dữ liệu có nhãn có thể tăng độ chính xác đáng kể.- Một thuật toán học bán giám sát được sử dụng (ví dụ Self-training) sẽ học các ví dụ có nhãn, sau đó tiến hành gán nhãn cho một số (có lựa chọn) các ví dụ không có nhãn - một cách hợp lý, có đánh giá chất lượng công việc hay độ chính xác. Tiếp theo, chọn các ví dụ vừa được gán nhãn có độ tin cậy cao (vượt trên một ngưỡng chọn trước) đưa vào kết hợp với tập dữ liệu có nhãn, tạo thành một tập dữ liệu huấn luyện mới.- Áp dụng một phương pháp kiểm thử (có thể kết hợp với một tập dữ liệu đã biết trước nhãn) để đánh giá hiệu năng/độ chính xác của mô hình.Một số thuật toán thường được sử dụng gồm có: thuật toán Cực đại kỳ vọng (EM - Expectation Maximization), SVM truyền dẫn (TSVM - Transductive Support Vector Machine), Self-training, Co-training và các phương pháp dựa trên đồ thị (graph-based).Việc lựa chọn thuật toán nào dựa trên một số định hướng: nếu các lớp dữ liệu có tính phân cụm cao thì nên dùng EM với mô hình hỗn hợp sinh; nếu đã sử dụng SVM thì mở rộng thành TSVM; khi khó nâng cấp mô hình học có giám sát đã có, thì nên dùng self-training; nếu các đặc trưng của dữ liệu phân chia tự nhiên thành hai phần riêng rẽ thì nên dùng Co-training; còn nếu hai mẫu dữ liệu có đặc trưng tương tự nhau hướng tới một lớp thì sử dụng phương pháp dựa trên đồ thị. Trong số các thuật toán học bán giám sát thông dụng, có 2 thuật toán tiêu biểu là Self-training và Co-training.Self-training là kỹ thuật học bán giám sát được sử dụng khá phổ biến do tận dụng được nguồn dữ liệu chưa gán nhãn lớn và ban đầu chỉ cần lượng nhỏ dữ liệu đã gán nhãn. Nội dung chính của Self-training là lặp nhiều lần phương pháp học có giám sát.Gọi    D: là tập các dữ liệu đã được gán nhãn.          C : là tập các dữ liệu chưa gán nhãn.Thuật toán Self-training thực hiện như sau:Lặp (cho đến khi C = Æ):i. Huấn luyện bộ phân lớp có giám sát h trên tập Dii. Sử dụng h để phân lớp dữ liệu trong tập Ciii. Tìm tập con C’ Í C có độ tin cậy cao nhất:D + C’ Þ D ; C – C’ Þ C.Ban đầu huấn luyện bộ phân lớp bằng cách cho bộ phân lớp học một tập dữ liệu huấn luyện đã được gán nhãn (tập này thường nhỏ so với tập dữ liệu chưa gán nhãn). Dùng bộ phân lớp đã được huấn luyện, phân lớp cho các dữ liệu chưa được gán nhãn. Trong số dữ liệu mới được gán nhãn, chọn các dữ liệu có độ tin cậy cao (lớn hơn một ngưỡng nào đó) kèm với nhãn vừa gán, đem bổ sung vào tập dữ liệu huấn luyện ban đầu. Sau đó, bộ phân lớp được học lại trên tập huấn luyện mới (gồm dữ liệu đã gán nhãn ban đầu và dữ liệu do bộ phân lớp mới gán nhãn) và thuật toán được lặp lại. Sau mỗi vòng lặp, bộ phân lớp sẽ bổ sung một số mẫu dữ liệu có độ tin cậy cao nhất cùng với dự đoán phân lớp của chúng vào tập dữ liệu huấn luyện. Tên gọi Self-training xuất phát từ việc sử dụng dự đoán của nó để huấn luyện chính nó.Thuật toán Co-training dựa trên giả thuyết rằng các đặc trưng của tập dữ liệu huấn luyện có thể được phân chia thành 2 tập con (trường hợp lý tưởng là hai tập con này thoả mãn điều kiện độc lập nhau - conditional independent). Nội dung chính của thuật toán như sau:+ Dùng 2 bộ phân lớp phù hợp để học 2 tập con tương ứng (mỗi tập con huấn luyện một bộ phân lớp).+ Mỗi bộ phân lớp thực hiện phân lớp cho các dữ liệu chưa gán nhãn, thu được kết quả là tập dữ liệu chưa gán nhãn kèm theo nhãn dự đoán của chúng. Trong tập kết quả của bộ phân lớp 1, chọn ra những mẫu dữ liệu (kèm nhãn đã dự đoán) có độ tin cậy cao nhất bổ sung vào tập huấn luyện của bộ phân lớp 2 và ngược lại.+ Mỗi bộ phân lớp được học lại tập dữ liệu huấn luyện (gồm dữ liệu gán nhãn ban đầu và dữ liệu gán nhãn mới bổ sung từ kết quả của bộ phân lớp kia). Quá trình được lặp lại cho đến khi tập dữ liệu chưa gán nhãn rỗng hoặc số vòng lặp đạt tới một ngưỡng được xác định trước. Thuật toán Co-training:(1). Huấn luyện hai bộ phân lớp:   f (1) từ (Xl (1), Yl), f (2) từ (Xl (2), Yl). (2). Phân lớp các mẫu dữ liệu chưa gán nhãn Xu với f (1) và f (2) tách biệt nhau. (U là tập các mẫu dữ liệu chưa gán nhãn)(3).  Chèn thêm vào f (1) k-most-confident (x, f (1)(x)) tới các dữ liệu đã gán nhãn của f (2).(4). Chèn thêm vào f (2) k-most-confident (x, f (2) (x)) tới các dữ liệu đã gán nhãn của f (1).(5). Lặp lại các quá trình trên. Thuật toán Co-training trên có thể viết như sau:L: là tập các mẫu dữ liệu đã gán nhãnU: là tập các mẫu dữ liệu chưa gán nhãn(1). L có thể phân chia thành hai tập con L1 và L2 (trường hợp lý tưởng thì L1 và L2 độc lập nhau).(2). Cho bộ phân lớp h1 học L1 (hay L1 huấn luyện bộ phân lớp h1)Cho bộ phân lớp h2 học L2 (hay dùng L2 huấn luyện bộ phân lớp h2)(3). Dùng h1 phân lớp cho U thu được tập U1’ kèm nhãn dự đoán của chúng. Dùng h2 phân lớp cho U thu được tập U2’ kèm nhãn dự đoán của chúng.(4). Từ U1’ chọn ra u1 mẫu dữ liệu kèm theo nhãn của nó, có độ tin cậy cao nhất. Bổ sung u1 vào L2. Khi đó, L2 + u1 => L2.Từ U2’ chọn ra u2 mẫu dữ liệu kèm theo nhãn của nó, có độ tin cậy cao nhất. Bổ sung u2 vào L1. Khi đó, L1 + u2 => L1.(5). Dùng L1 mới huấn luyện bộ phân lớp h1 (hay h1 học L1)Dùng L2 mới huấn luyện bộ phân lớp h2 (hay h2 học L2)(6). Lặp lại từ bước (3). cho đến khi tập U rỗng hoặc số vòng lặp đạt đến ngưỡng xác định trước. Có thể viết rút gọn bằng cách bỏ bước (5). ở trên. Bước (6). đổi thành bước (5): Lặp lại từ bước (2). cho đến khi tập U rỗng hoặc số vòng lặp đạt đến ngưỡng xác định trước. Học có giám sát là học với tập dữ liệu huấn luyện ban đầu hoàn toàn được gán nhãn từ trước.Học có giám sát là phương pháp học sử dụng cho lớp bài toán phân lớp, phân loại (Classification).- Để thực hiện phân lớp, trước tiên phải chuẩn bị một tập dữ liệu huấn luyện (trainning data set), để có tập dữ liệu huấn luyện phải thực hiện gán nhãn cho dữ liệu ban đầu, đây được gọi là quá trình thu thập tập huấn luyện. - Lựa chọn một thuật toán phân lớp (ví dụ SVM) xây dựng bộ phân lớp để học tập dữ liệu huấn luyện. Hay nói cách khác, dùng tập dữ liệu huấn luyện để huấn luyện bộ phân lớp. Thuật ngữ học có giám sát được hiểu là học tập dữ liệu đã được gán nhãn trước (các dữ liệu kèm theo nhãn tương ứng này coi như đã được giám sát bởi người thực hiện gán nhãn).- Sử dụng một tập dữ liệu kiểm tra (test data set) đã được gán nhãn trước, để kiểm tra tính đúng đắn của bộ phân lớp. Sau đó, có thể dùng bộ phân lớp để phân lớp cho các dữ liệu mới.Một số thuật toán thường được lựa chọn khi xây dựng bộ phân lớp gồm có: máy vector hỗ trợ (Support Vector Machine – SVM); k láng giềng gần nhất (K Nearest Neighbours – KNN); tiếp cận xác suất thống kê (Naïve Bayes – NB); Cây quyết định (Decision Tree – DT); sử dụng mạng nơron (Neural Network – Nnet); dựa trên vector trọng tâm (Centroid–base vector); hay tuyến tính bình phương nhỏ nhất (Linear Least Square Fit – LLSF). (Chi tiết các thuật toán này có thể tham khảo trên Internet).Trong phần “Bản chất của học máy dưới góc nhìn của xác suất thống kê” - x1, x2, …, xl là các mẫu, xi thuộc Rn. Các mẫu xi được phát sinh ngẫu nhiên theo một hàm phân bố xác suất F(x) nào đó mà ta không biết.- yi là các kết quả học tương ứng với mẫu xi, yi thuộc R. Thường thì y là kết quả của một hàm f(x) nào đó – đơn trị. Tuy nhiên trong trường hợp tổng quát thì y không đơn trị. Do đó y được xác định theo một hàm phân bố điều kiện F(y|x) mà ta cũng không biết.Trong trường hợp khi y thuộc R, tức đây là vấn đề hồi quy (regression). Trong trường hợp bài toán phân lớp (classification) thì y thuộc{-1, 1} là trường hợp nhận dạng hai lớp, nếu yi = -1 thì xi thuộc lớp thứ nhất (không được quan tâm), còn yi = 1 thì xi thuộc lớp thứ 2 (lớp được quan tâm).",
          "relevence": "yes"
        },
        {
          "url": "https://sites.google.com/a/wru.vn/cse445fall2016/lecture-materials",
          "title": "Bài giảng, tài liệu - Học máy CSE445, K55",
          "content": "Học máy (Machine Learning) CSE 445Bài giảng, tài liệuBài tậpTài liệu tham khảoPiazzaslides: Giới thiệu về Học máy  (31/8, 2.14 MB) (Outline)Sách: ISL Chương 2Sử dụng R (Nguyễn Chí Dũng)Học máy (HXH): Chương 1 & 2.Các ứng dụng của Học máyXe không người lái (video)Nhân tài Việt Nam tại Google làm về Học máy (Lê Việt Quốc- BBC video)\"How Credit Card Companies Spot Fraud Before You Do\" (article, U.S. News)\"Mining Electronic Records for Revealing Health Data\" (article, New York Times)\"The Algorithm That's Hunting Ebola\" (article, IEEE Spectrum)tùy chọn Slide giới thiệu ngôn ngữ R của nhóm sinh viên ĐH Cần Thơ (tại đây)Bài giảng học máy của Nguyễn Nhật Quang (slides)Giới thiệu ngôn ngữ R của PGS. Đỗ Thanh Nghị (tại đây).http://www.listendata.com/2016/10/datatable-tutorial.htmlhttp://www.r2d3.us/visual-intro-to-machine-learning-part-1/ https://github.com/ZuzooVn/machine-learning-for-software-engineershttps://jalammar.github.io/visual-interactive-guide-basics-neural-networks/?utm_content=buffer79526&utm_medium=social&utm_source=facebook.com&utm_campaign=buffer ML cơ bản của Nguyễn Xuân KhánhML cơ bản của Vũ Hữu TiệpBài giảng R của UCLA Department of Statistics: Basic_R_course, Intermediate_RTài liệu ngôn ngữ R của GS. Nguyễn Văn Tuấn (tài liệu, Video)tutorial codeDữ liệu: Auto.data, Auto.csv10 thuật toán Học máy cần biết; R: Best Programming Language for Machine Learningslides: Học máy có giám sát và Hồi quy tuyến tính (2MB)Chú ý:  Chi tiết về hồi quy logic sẽ trình bày ở phía cuối khóa họcISL: Chương 3 (linear regression)tùy chọn   Nhắc lại về véc-tơ riêng/giá trị riêng (xem tại đây) (video, 10 phút)slides: Kiểm tra chéo, Hiệu chỉnh mô hình và mô hình thưa (1MB)ISL: Mục 5.1 (cross-validation)ISL: Mục 6.2 (lasso and ridge regression)slides: Hồi quy Logistic và Máy véc-tơ hỗ trợ (1,473 MB, 17/9)ISL: Mục 4.3 (logistic regression), video giới thiệu mô hình hồi quy Logistic của GS. TuấnISL: Chương 9 (support vector machines)Textbook về SVM của TS. Nguyễn Đức Dũng (tại đây)Logistic của Vũ Hữu TiệpAndrew Ng's CS 229 course notes on SVMs available here for those interested in the mathematics behind SVMs. slides:  Cây phân loại và hồi quy (0.77 MB)ISL: Mục 8.1, 8.3.1, and 8.3.2 (CART sử dụng \"tree\")Why do Decision Trees Work?Không bắt buộc  10-phút R video minh họa cây CART:  lệnh \"rpart\"Mã nguồn R về CART and Dữ liệu thử nghiệmslides: Phương pháp Bootstrap và các phương pháp Học máy kết hợp (0.77 MB, Boosting, Bagging, and Random Forests)Đọc trướcISL: Mục 5.2 and 5.3.4 (The Bootstrap)ISL: Mục 8.2.1, 8.2.3, and 8.3.4 (Boosting and Bagging)ISL: Mục 8.2.2 and 8.3.3 (Random Forests)http://mlwave.com/kaggle-ensembling-guide/ slides: Kỹ thuật phân cụm và Giảm chiều dữ liệu (3 MB)ISL: Chương 10 (Phương pháp phân cụm K-means 10.3.1)ISL: Chương 10 (Phương pháp phân cụm phân cấp 10.3.2)Học máy (HXH): Chương 6.ISL: Chương 10 (PCA 10.2, clustering 10.3)ISL: Mục 10.4 \"Lab 1: Principal Component Analysis\"Minh họa K-means của tác giả Ông Xuân HồngK-means của Vũ Hữu Tiệpcác mã nguồn ví dụ (thảo luận trong bài giảng)Mã nguồn PCA trong R và dataset tương ứng (chemical levels in olive oils from different regions)",
          "relevence": "no"
        },
        {
          "url": "https://toaiquangton.blogspot.com/2014/04/r-support-vector-machine.html",
          "title": "toai's blog: R - Support Vector Machine",
          "content": "Một instance dữ liệu (hay 1 quan sát - observation) gồm nhiều thuộc tính và được xem là một điểm trong không gian thuộc tính dữ liệuVector là 1 điểm dữ liệuVector hỗ trợ là 1 hay nhiều vector dùng để tạo mặt phân cách dữ liệu (siêu phẳng - hyperland)Tác giả của SVM: Vladimir N VapnikĐề xuất chuẩn hiện nay: Vapnik và Corinna CortesNăm: 1995Tìm tham số tối ưu của mô hình: tune.svm(…)Huấn luyện (Training): svm(…) Tiên lượng (Predicting/Testing): predict(…)Trực quan hóa (Visualizing): dữ liệu, support vector, biên quyết định: plot(…)\nData science and Machine learning\n(11)\n\nEnglish\n(2)\n\nKỹ năng học tập\n(4)\n\nLập trình và Thuật toán\n(10)\n\nPhần mềm\n(2)\n\nTâm\n(10)\n\n\n\n\n        ► \n      \n\n\n\n2017\n\n(14)\n\n\n\n\n\n        ► \n      \n\n\n\ntháng năm\n\n(2)\n\n\n\n\n\n\n\n        ► \n      \n\n\n\ntháng tư\n\n(2)\n\n\n\n\n\n\n\n        ► \n      \n\n\n\ntháng ba\n\n(8)\n\n\n\n\n\n\n\n        ► \n      \n\n\n\ntháng một\n\n(2)\n\n\n\n\n\n\n        ► \n      \n\n\n\ntháng năm\n\n(2)\n\n\n\n\n        ► \n      \n\n\n\ntháng tư\n\n(2)\n\n\n\n\n        ► \n      \n\n\n\ntháng ba\n\n(8)\n\n\n\n\n        ► \n      \n\n\n\ntháng một\n\n(2)\n\n\n\n\n        ► \n      \n\n\n\n2015\n\n(1)\n\n\n\n\n\n        ► \n      \n\n\n\ntháng ba\n\n(1)\n\n\n\n\n\n\n        ► \n      \n\n\n\ntháng ba\n\n(1)\n\n\n\n\n        ▼ \n      \n\n\n\n2014\n\n(17)\n\n\n\n\n\n        ► \n      \n\n\n\ntháng sáu\n\n(1)\n\n\n\n\n\n\n\n        ► \n      \n\n\n\ntháng năm\n\n(5)\n\n\n\n\n\n\n\n        ▼ \n      \n\n\n\ntháng tư\n\n(10)\n\nTài liệu tham khảo Hidden Markov Model\nTài liệu tham khảo mạng Neuron\nNhập công thức toán vào blogger\nExcel - Trendline\nBài tập lập trình\nR - Neural network\nTổng 2 số lớn\nR - Support Vector Machine\nNhớ\nMở file trong Code::Blocks\n\n\n\n\n\n\n\n\n        ► \n      \n\n\n\ntháng ba\n\n(1)\n\n\n\n\n\n\n        ► \n      \n\n\n\ntháng sáu\n\n(1)\n\n\n\n\n        ► \n      \n\n\n\ntháng năm\n\n(5)\n\n\n\n\n        ▼ \n      \n\n\n\ntháng tư\n\n(10)\n\nTài liệu tham khảo Hidden Markov Model\nTài liệu tham khảo mạng Neuron\nNhập công thức toán vào blogger\nExcel - Trendline\nBài tập lập trình\nR - Neural network\nTổng 2 số lớn\nR - Support Vector Machine\nNhớ\nMở file trong Code::Blocks\n\nTài liệu tham khảo Hidden Markov ModelTài liệu tham khảo mạng NeuronNhập công thức toán vào bloggerExcel - TrendlineBài tập lập trìnhR - Neural networkTổng 2 số lớnR - Support Vector MachineNhớMở file trong Code::Blocks\n\n\n\n        ► \n      \n\n\n\ntháng ba\n\n(1)\n\n\n\n\n        ► \n      \n\n\n\n2013\n\n(9)\n\n\n\n\n\n        ► \n      \n\n\n\ntháng mười\n\n(2)\n\n\n\n\n\n\n\n        ► \n      \n\n\n\ntháng chín\n\n(7)\n\n\n\n\n\n\n        ► \n      \n\n\n\ntháng mười\n\n(2)\n\n\n\n\n        ► \n      \n\n\n\ntháng chín\n\n(7)\n",
          "relevence": "no"
        },
        {
          "url": "https://phvu.net/2011/10/05/pca-principal-component-analysis/",
          "title": "PCA – Principal Component Analysis | mlcvGru",
          "content": "Phân tích thành phần chính (Principal Component Analysis – PCA) là một trong những phương pháp phân tích dữ liệu nhiều biến đơn giản nhất. Phần 1 của bài này sẽ nói về động lực và ý tưởng chính của  PCA, sau đó trình bày từng bước trong thuật  toán PCA. Như thường lệ, để hiểu thuật toán thì cài đặt là cách tốt nhất, do đó một đoạn mã MATLAB minh họa từng bước trong PCA cũng được trình bày. Phần cuối cùng là chi tiết các khai triển Toán học trong  PCA. Do mục tiêu ứng dụng nên các phương trình cụ thể được để dành đến phần này, người đọc nếu thật sự quan tâm nguyên lí của PCA, tại sao PCA lại sử dụng những công thức như thế v.v… thì có thể tham khảo phần này để biết thêm chi tiết.Như đã nói trong bài trước, trong thống kê, thông thường cần phải “nghiên cứu” dữ liệu trước khi xây dựng các mô hình suy diễn dựa trên dữ liệu đó. Tuy nhiên đôi khi dữ liệu có số chiều lớn, không thể visualize (không biết dịch là gì) trong không gian 2 hay 3 chiều, do đó cần phải tìm cách đưa dữ liệu về không gian có số chiều nhỏ hơn.PCA là một trong những phương pháp như thế, nhưng hơn thế, PCA còn giống như một Swiss knife với nhiều đặc tính tốt:Nói một cách ngắn gọn, mục tiêu của  PCA là tìm một không gian mới (với số chiều nhỏ hơn không gian cũ). Các trục tọa độ trong không gian mới được xây dựng sao cho trên mỗi trục, độ biến thiên của dữ liệu trên đó là lớn nhất có thể. Tiếng  Việt thì dài dòng, nhưng tiếng Anh thì mục tiêu này gọi là maximize the variability. Ba chữ này gói gọn ý tưởng chính của  PCA.Minh họa PCA: phép chiếu lên các trục tọa độ khác nhau có thể cho cách nhìn rất khác nhau về cùng một dữ liệu.Một ví dụ kinh điển là hình ảnh về con lạc đà. Cùng là một con lạc đà nhưng nếu nhìn từ bên hông thì ta có được đầy đủ thông tin nhất, trong khi nhìn từ phía trước thì thật khó để nói nó là lạc đà.Một ví dụ thuyết phục hơn được minh họa trong hình sauMinh họa PCA: tìm các trục tọa độ mới sao cho dữ liệu có độ biến thiên cao nhấtGiả sử tập dữ liệu ban đầu (tập điểm màu xanh) được quan sát trong không gian 3 chiều (trục màu đen) như hình bên trái. Rõ ràng 3 trục này không biểu diễn được tốt nhất mức độ biến thiên của dữ liệu. PCA do đó sẽ tìm hệ trục tọa độ mới (là hệ trục màu đỏ trong hình bên trái). Sau khi tìm được không gian mới, dữ liệu sẽ được chuyển sang không gian này để được biểu diễn như trong hình bên phải. Rõ ràng hình bên phải chỉ cần 2 trục tọa độ nhưng biểu diễn tốt hơn độ biến thiên của dữ liệu so với hệ trục 3 chiều ban đầu.Một điểm rất đẹp nữa của  PCA là các trục tọa độ trong không gian mới luôn đảm bảo trực giao đôi một với nhau, mặc dù trong không gian ban đầu, các trục có thể không trực giao.Dài dòng như vậy là đủ, ta sẽ trình bày từng bước thuật toán PCA trong phần tới. Chi tiết về ý tưởng và khai triển toán học được dành lại để trình bày ở cuối bài.Cho ma trận . Các bước của PCA lần lượt như sau:1. Tiền xử líDữ liệu ban đầu có thể có giá trị thay đổi bất thường. Ví dụ trên feature 1 (cột 1 của  ) giá trị thay đổi trong khoảng (0, 1), trên feature 2 lại biến thiên trong đoạn (-100, 100). Rõ ràng cần phải có một bước tiền xử lí để chuẩn hóa giá trị trên các cột của ma trận X. Có 2 cách tiền xử lí thường được dùng cho PCA là Centered PCA và  Normed PCA.Centered PCA mang tất cả các feature (các cột của X) về cùng một gốc tọa độ:,,             (1a).Trong đó n là số dòng của  X,  là mean của cột thứ j của  X, được tính như trên.Normed PCA mang tất cả các feature về cùng một gốc tọa độ, đồng thời chuẩn hóa về cùng một quãng standard-deviation bằng 1:,.             (1b)Trong đó  là độ lệch chuẩn  (standard deviation) của cột thứ j trong X.Thông thường Normed PCA hay được dùng. Sau bước tiền xử lí, ma trận  sẽ là đầu vào cho bước tiếp theo.2. Xây dựng không gian mớiTính ma trận hiệp phương sai (covariance) của các feature trong :               (2)Do là tích của ma trận  với chuyển vị của nó nên  là ma trận positive semidefinite kích thước . Hơn nữa  có p trị riêng .Tiếp theo, PCA tìm trị riêng và vector riêng tương ứng của , sắp xếp theo thứ tự giảm dần của trị riêng. Giả sử p trị riêng của V là,              (3)và p vector riêng tương ứng là.               (4)Khi đó các trục của không gian mới chính là các vector riêng  ở trên, đương nhiên các vector riêng hoàn toàn độc lập tuyến tính (nghĩa là trực giao đôi một).Có thể nói trong PCA, trị riêng và vector riêng có vị trí rất đẹp, thỏa mãn tất cả các yêu cầu của PCA. Bản thân tôi khi đọc đến phần này cũng thấy bất ngờ vì lời giải cho PCA không gì khác lại hoàn toàn trọn vẹn trong trị riêng và vector riêng. Tuy nhiên tại thời điềm này, ta sẽ chấp nhận như vậy.  Phần cơ sở Toán học ở cuối bài sẽ giải thích tại sao trị riêng và vector riêng lại xuất hiện (có phần bất ngờ) trong  PCA như vậy.3. Chuyển dữ liệu từ không gian ban đầu vào không gian mớiThông thường không gian mới không được xây dựng bằng tất cả p vector riêng trong (4), mà thông thường chỉ từ k vector riêng đầu tiên, với k < p. Tại sao là các vector đầu tiên, và chọn k bao nhiêu thì tốt, ta sẽ bàn trong phần cuối.Như vậy gọi.Khi đó tọa độ các điểm trong hệ tọa độ mới là             (5)Xong. Ta đã kết thúc giải thuật PCA, không thể đơn giản hơn.Trong phần này ta chỉ thực hiện phân tích các sample trong không gian tạo bởi các feature. Ta còn có thể thực hiện khảo sát các feature trong không gian tạo bởi các sample. Đương nhiên có thể chuyển vị ma trận  X rồi thực hiện tương tự, nhưng đó là cách võ biền, cày bừa. Thực sự không cần phải như vậy, mà nhờ vào những đặc tínhđẹp của trị riêng và vector riêng, ta có thể tính ngay không gian mới của các sample (để biểu diễn các feature) từ các vector riêng trong (4). Trị riêng còn đẹp đến nỗi được dùng trong các tiêu chuẩn để chọn k. Ta sẽ trở lại với 2 vấn đề này trong phần cuối của bài.Ngoài ra để đánh giá chất lượng của không gian mới tạo bởi PCA, ta dùng 2 độ đo là contribution và squared cosine. Ta cũng dành 2 phần này cho phần cuối bài.Sau đây là đoạn mã MATLAB minh họa các bước của thuật toán PCA, thực hiện phân tích các observation trong không gian các feature, sau cùng tính các độ đo contribution và squared cosine. Download here.Xem tại đây.Một số hạn chế của PCA:Ngoai PCA thi con co ICA cung la 1 pp thong dung de xu ly data.Rất mong tác giả có thể trình bày tiếp phần “Cơ sở Toán học của  PCA”.Chào anh V, cho em hỏi là PCA và Karhunen Loeve khác nhau như thế nàoHi Khoa,\nTrong trường hợp rời rạc và áp dụng lên sample của biến ngẫu nhiên, thì PCA và Kerhunen-Loeve Transforms là một.Thanks for your replyTheo em hiểu thì Karhunen là ma trận A dùng để biến Rx (covariance của X) thành Ry (là ma trận chéo, có các phần tử trên đường chéo là các giá trị riêng của Rx)\nNhưng khi áp dụng PCA, em thấy từ Rx là mình có thể suy ra giá trị riêng, vector riêng luôn rồi ?\nTại sao lại cần ma trận chéo ?Hi em,\nEm tham khảo pp KLT ở đâu thế? Theo anh đọc thì trong trường hợp rời rạc, KLT cũng chỉ dùng ma trận tạo bởi các vector riêng để thực hiện biến đổi các vector ban đầu, giống hệt như PCA.Em đọc trong bài giảng của thầy L.Q.Ngọc\nVậy mình có thể hiểu PCA là ý tưởng cơ sở, còn Karhunen Loeve là cài đặt cụ thể của biến đổi vector không anh ?\nKhi dùng PCA để chuyển X thành Y, thì các tài liệu thường chứng minh covariance matrix của Y là ma trận chéo, có các giá trị trên đường chéo là các giá trị riêng của covariance matrix của X. Anh có thể nói rõ hơn chỗ này được ko ?Em cảm ơn.1. Anh không nghĩ thế. Anh thấy Karhunen Loeve tổng quát hơn PCA.\n2. Anh không thấy lí do tại sao covariance của Y lại phải là ma trận đường chéo. Em có thể nói rõ nội dung chứng minh thế nào ko?Thank vì bài viết 😉ps: “visualize” có thể dịch là “quan sát (một cách) trực quan” (?), tức là quan sát bằng mắt thường.đúng nghĩa tiếng Anh-Việt visualize = “hình dung” các bác ạDoc xong da hieu duoc rat nhieu thu thac mac, cam on anh nhieu!chao anhHien nay e dang lam de tai ve xu ly anh nhan dang mat nguoi su dung thuat toan pcanhung e co mot van de ko hieu ve khai niem truc giao – va truc giao de lam gie cam on anh nhieuRat hy vong nhan duoc su hoi am cua anhChao anh Em hien dang lam de tai lien quan den xu ly anh nhan dang mat nguoi ( face recognition) Qua tim hieu em co gap mot van de ma khong giai thich duocDo la truc giao – Vay truc giao la gi- y nghia cua no de lam giEm cam on anh nhieuChuc anh suc khoe va that nhieu niem vuiPS: neu co tai lieu lien quan den PCA anh gui qua mail giup e nhevie.hanguyen89@gmail.comHi Hà,\n“Trực giao” hiểu nôm na nghĩa là “vuông góc”. Trong đại số tuyến tính, ta nói 2 vector n chiều là trực giao khi và chỉ khi tích vô hướng của chúng bằng 0.ThânAnh Vu ah em co mot thac mac mong anh giup do nhe\nTheo mot tai lieu em doc duoc ve thuat toan PCA gom co 5 buoc co ban:\n1 – lay du lieu\n2 – Tru di tri trung binh ( mean) cua moi chieu ( dimension)\n3- Tinh toan ma tran hiep psai\n4 – Thiet lap vecto dac trung ( feature vector)\n5 – chuyen du lieu ban dau ve khong gian moiBuoc thu 2 em ko hieu lam\n Em nghi no la buoc tien xu ly du lieu phai khong ah\nmong anh giup em giai thich nheem cam on anh nhieu!Hi Ha,\nĐúng vậy, trong bước đó người ta muốn chuẩn hóa dữ liệu sao cho giá trị trung bình (mean) của nó bằng 0.\nVí dụ X là ma trận sau:\ntức là ta có 3 mẫu dữ liệu trong không gian 2 chiều. Nếu thực hiện centered PCA (là cách em mô tả ở trên), thì với mỗi cột, ta trừ đi giá trị trung bình của cột đó. Cụ thể trong ma trận trên thì giá trị trung bình của cột 1 là , và của cột 2 là . Như vậy sau khi biến đổi thì ma trận mới là:\n\nNhư vậy sau khi chuyển đổi, ta thấy giá trị trung bình của mỗi cột trong ma trận mới đều bằng 0, đây chính là ý nghĩa của centered PCA.Nếu ta chia thêm cho phương sai của mỗi cột, thì các cột trong ma trận cuối cùng sẽ có phương sai bằng 1, và cách làm đó gọi là Normed PCA.Thực ra  đây không phải là 1 bước bắt buộc của PCA, tuy nhiên vì PCA hoạt động dựa trên trị riêng của ma trận covariance, mà ma trận covariance khá nhạy với “variance” của các biến, nên thực tế cho thấy normed và centered PCA cho kết quả tốt hơn PCA trên ma  trận gốc ban đầu.PS: Em thuc su muon giai quyet bai toan nhan dang mat nguoi theo huong xay dung di tu: Hinh dung van de cot loi (hieu duoc muc tieu cuoi cung) – Xay dung tren co so toan hoc ( tai sao lai su dung nhung cong thuc va ham toan hoc nhu vay) va cai dat thuat toan tren matlabNeu A co tai lieu lien quan thi gui giup em nhe anh ( neu duoc thi em mong nhan duoc tai lieu thong qua gmail) hoac neu anh ban ^^ thi co the giup e ten cac tai lieu day du ve no cung duoc )\nEm cam on anh!\nPS: Chuc anh ngay le vui ve !\nAnd: Em van phai lam viec de hoan thanh final project theo dun tien do( Em rat an tuong voi cach giai quyet bai toan cua anh – Dieu ma it giao vien hien nay tiep can va huong dan students xay dung phuong huong giai quyet van de)\nAnd: Dac biet la hinh con lac daNhận dạng mặt người cũng có khá nhiều phương pháp, không biết em đang làm theo phương pháp nào?Em cam on anh nhieu! (^-^) ( Em ko nghi la duoc reply nhanh nhu vay)\nHien em dang lam final Project su dung phuong phap PCA\nThuc su la em chua qua hieu mo hinh toan hoc cua PP nen Em muon tim hieu no that ky truoc khi xay dung thuat toan cung nhu can xay dung nhung chu y (uu va nhuoc diem cua no) de khac phucneu con thoi gian em se ket hop voi mot so pp khac de cai thien do chinh xac cua chuong trinh anh ahHien gio e su dung matlab de viet chuong trinh ( GUI cho Thiet lap Camera va GUi cho giao tiep phan cung em da hoan thanh)\nmot so code mau tu tren matlab nguoi ta thuc hien cung cuc ky tuyet voi nhung em van muon tu minh viet code duoi goc do am hieu thuat toan anh ah^^ PS: Nhan duoc hoi am cua anh Em rat vui ( quen het met moi lun) ^0^Chào anh Vũ,\nTrong học máy, ví dụ dùng SVM để luyện, cho tập training set.\nMục tiêu của mình là dùng PCA để giảm số chiều của các quan sát (giảm số các feature) từ không gian trạng thái ban đầu thành không gian mới. Lấy ví dụ, tập các samples, mỗi sample có 10.000 feature (hiểu nôm na là 10.000 chiều).\n1. Trong trường hợp này cần tìm trị riêng của X^TX, XX^T.\n2. Ở bước phân loại, làm sao biến đổi tập dữ liệu cần phân loại trong không gian cũ sang không gian đã được xây dựng bởi CPA ở trên.\nXin cảm ơn anh!Hi em,\n1. Anh không hiểu câu này. Thông thường em có thể chạy PCA bằng các cài đặt có sẵn (OpenCV hay MATLAB) để tìm ma trận U. Sau đó thì chiếu toàn bộ dữ liệu ban đầu lên không gian của U. Việc này hình như cũng được làm sẵn luôn rồi.\n2. Trong trường hợp PCA là input cho SVM (tổng quát là các thuật toán supervised), thì em nên làm như sau:\n  – Chạy PCA trên training set để tìm ma trận U của các vector riêng. Lưu lại ma trận U này.\n  – Chiếu training set vào không gian mới: X’ = X*U. Dùng X’ để huấn luyện SVM.\n  – Trong quá trình test, vì các vector trong tập test ở trong không gian ban đầu nên cần phải chiếu vào không gian của U, tức là tính Y’ = Y*U, trong đó Y là tập test ban đầu, Y’ là tập test trong không gian PCA.\n  – Dùng Y’ để test mô hình đã huấn luyện.Thân.Cảm ơn anh Vũ.\nAnh giải thích đúng ý muốn hỏi rồi đó.\nChúc anh khỏe!Anh Vu oi! A co tai lieu (tieng viet) về thuật toán 2D-PCA không?\nE dang nghien cuu ma khong hieu gie het A.Hi em, anh không có tài liệu tiếng Việt nào về 2DPCA cả 😀Anh vu oi! A co biet gi thuat toan 2D-PCA khong?\nA có gợi ý gì cho e với.\nCám ơn Anh thật nhiềuHy dduuyy,\nCách làm của 2DPCA khá đơn giản. Giả sử cho tập ảnh huấn luyện  thì 2DPCA tìm ma trận sau:\ntrong đó  là trung bình cộng của tất cả các ảnh.Sau đó 2DPCA tìm trị riêng và vector riêng của ma trận G, các vector riêng ứng với các trị riêng lớn nhất sẽ là cơ sở cho không gian mới. Trong PCA thì ta dùng ma trận covariance, còn 2DPCA thì dùng ma trận G. Còn lại hoàn toàn tương tự.\nKhông hiểu bạn không rõ chỗ nào.Cam on ban tra loi minh!\nCai minh khong hieu la giua pca va 2dpca, thi cai nao tot hon!\nva muon xin ban thuat toan cua 2dpca.\nMinh lap trinh hoai ma khong duoc, chac tai thuat toan minh bi saiTheo mình hiểu thì 2DPCA có chi phí thấp hơn PCA truyền thống vì ma trận dùng để tính trị riêng của 2DPCA nhỏ hơn nhiều so với ma trận trong PCA. Về hiệu quả thì hình như là tương đương, mình không chắc lắm, bạn có thể xem thêm trong paper của 2DPCA.\nBạn có thể download toolbox này, trong đó có chứa các hàm liên quan đến 2DPCA:\nhttp://www.mathworks.com/matlabcentral/fileexchange/12333-statistical-learning-toolboxCảm ơn A. Vũ nhiều lắm!Một số hạn chế của PCA:Chỉ làm việc với dữ liệu numeric,\nNhạy cảm với các điểm outlier/extreme,\nKhông phù hợp với các mô hình phi tuyến, do PCA hoàn toàn dựa trên các biến đổi tuyến tính.Nhờ anh Vu giải thích dùm em 3 ý trên với  , em còn mơ hồ quá . Cảm ơn anhHi Minh,\n1/ Chỉ làm việc với dữ liệu numeric: PCA không thể sử dụng được cho dữ liệu categorical. Ý là dữ liệu trong ma trận X phải là số thực (liên tục), chứ không phải là các categorical variable.2/ Nhạy cảm với các điểm outlier/extreme: nếu có vài điểm outlier (ngoại lệ) trong dữ liệu ban đầu thì “chất lượng” của PCA có thể sẽ không cao. Chất lượng không cao theo nghĩa là dữ liệu sau khi biến đổi PCA sẽ có thể không giữ được variance cao như ban đầu.3/ Không phù hợp với các mô hình phi tuyến, do PCA hoàn toàn dựa trên các biến đổi tuyến tính: bản chất PCA là thực hiện một biến đổi tuyến tính từ không gian ban đầu sang không gian mới. Theo nghĩa đó thì PCA không “mạnh” bằng các phép biến đổi phi tuyến như RBF v.v…Em cảm ơn anh Vu nhiều lắm .\n^_^anh Vu ơi cho em hỏi chỗ này tí :\nkhi em test bên cửa sổ command matlab Xhat’ thì xuất ra được ma trận nghịch đảo nhưng khi vào code trong hàm thì báo lỗi ngay dòng này V= Xhat’ * Xhat em dọc đủ kiểu mà cũng ko tìm được ma trận nghịc đảo\nChi tiết lỗi :\n??? Error using ==> ctranspose\nTranspose on ND array is not defined.Error in ==> mypca at 18\n  V = Xhat’ * Xhat;Cảm ơn anh đã đọcHi em,\nAnh vừa thử lại nhưng không thấy có lỗi gì cả. Có thể ma trận input X của em có nhiều hơn 2 chiều nên mới có lỗi trên. Trước khi gọi mypca(), em thử gọi size(X) xem kết quả là gì nhé.\nEm có thể download file mypca.m ở đây: https://www.box.com/s/l7jgipl8eyqdwzlxnr2o\nMatlab đã cài đặt sẵn PCA trong hàm princomp (http://www.mathworks.fr/fr/help/stats/princomp.html). Nếu em dùng PCA trong chương trình của em thì nên dùng hàm của Matlab. Cài đặt của anh chỉ để minh họa cho thuật toán thôi.ah hình của em 3 chiều lun RGB hèn chi .anh Vu có code nào trong matlab để nhận dạng khuôn mặt bằng thuật toán PCA có sử dụng Yale Face Database hok. Thuật toán PCA thì em đọc của anh đã nắm rõ . Nhưng khi áp dụng bộ dữ liệu Yale Face Database thì em chưa bít . Nếu anh có thì gửi mail cho em với (minhchi_a4@yahoo.com) . Cảm ơn Anh Vu nhiềuĐây em: http://www.mathworks.fr/matlabcentral/fileexchange/17032-pca-based-face-recognition-systemem test đc rầu cảm ơn anh Vu đã chia sẽanh Vu ơi giải thích kỹ dùm em vài câu hỏi mà em còn thắc mắc với nha :\ncâu 1 : Tại sao PCA lại dùng vector riêng và trị riêng ? Dùng nó có ích lợi gì ?\ncâu 2 : cách tìm K trị riêng tương ứng với vector riêng của anh viết ở trên  và  cái link anh chia sẽ cho em ở trên nó tìm k = cách lấy những trị riêng tương ứng với vector riêng > 1 thì cách chọn k nào tốt hơn anh Vu … em test thử cách theo cái link này thì có hình trả về đúng , có hình trả về kết quả sai  http://www.mathworks.fr/matlabcentral/fileexchange/17032-pca-based-face-recognition-systemcâu 3: PCA dùng để nén dữ liệu thì mình lưu lại cái gì để mình có thể giải nén chính xác như ban đầu ?Thanks anh đã đọcHi mọi người, e đang làm đề tài về nhận dạng một số thao tác cơ bản của tay người, trước mắt e định làm trên các ảnh tĩnh, sau đó nếu ngon nghẻ thì sẽ làm trên webcome. Hiên e mới biết ngâm cứu thuật toán PCA, bác nào có quyển face recognition using eigenfaces and neural network không, share cho e với.\nMail của e là: tieudoan208@gmail.com  Thanks mọi người rất nhiềuHi anh, anh có thể giải thích em thắc mắc này không ạ.\nEm có một tập dataset có 58 samples, mỗi samples có 166200 features. Em dùng PCA trong matlab đề giảm số lượng features xuống . Sau khi giảm thì số features cao nhất mà em có thể đạt được là 57. Em không hiểu tại sao lại như vậy.hi em,Em dùng hàm nào trong matlab để tính PCA vậy?Hi anh,\nCảm ơn anh đã trả lời câu hỏi của em. Em dùng hàm pca ở trong thư viện cua prtools anh ạ.hi em,\ncó thể là ma trận dữ liệu của e không đúng nên prtools hiểu là e có 166200 samples và 58 features. Em thử transpose ma trận đó xem.Ngoài ra e có thể dùng hàm princomp (http://www.mathworks.fr/fr/help/stats/princomp.html) của matlab để kiểm tra kết quả.Minh Khởi\nChào bạn!\n– kết quả của output_args  là gì sao trong code không thấy bạn trả về giá trị cho nó?\n– Nếu mình cho pca cho 1 ảnh thì kết quả mình được gì? lợi ít gì? có cón nhìn thấy ảnh với hàm imshow được không?Chào a Vũ\nPCA có thể áp dụng để nhận dạng ra người và không phải người dựa vào silhouette đươc không a?…\nAnh có thể hướng dẫn em cách áp dụng đươc không ạ?..\nAnh có code C trong opencv về vấn đề này không a, em đang rất muốn tham khảo để hiểu về nó.\nCảm ơn a.a Vũ ơi, em thấy trong opencv đã định nghĩa sẵn lớp PCA rồi, vậy không biết nó có khác gì so với những nội dung anh đề cập đến ở đây không anh? nếu em muốn sử dụng PCA trong opencv thì có cần định nghĩa lại không anh?…Hi em,Em có thể xem ví dụ ở đây nhé: http://www.bytefish.de/blog/pca_in_opencv/Chào anh. Bài viết rất bổ ích mà em đang cần..\nEm đang làm 1 project nhận dạng ra những đối tượng đã lưu sẵn trong Tranning Set.Em định nhận dạng đối tượng đó  luôn bằng cách đo  khoảng các Eclid và tìm MIN nó trong trong tập\nTranning Set đa được chuẩn hóa theo PCA.Theo anh thì nó có khả thi và hiệu quả cao không ạ. ? Thanks !Hi em,\nTùy vào dữ liệu của em như thế nào. Nói chung dùng PCA thì feature thu được có vẻ hơi “low-level”, nên độ chính xác có thể không cao. Em có thể xem xét dùng Histogram of Gradients (HoG) để rút đặc trưng (thay vì PCA), và dùng SVM để phân lớp (thay vì khoảng cách Euclide, tương ứng kNN với k=1).Tuy nhiên còn tùy vào dữ liệu của e nữa.Nếu sử dụng Normed PCA mà độ lệch chuẩn sigma(i)=0 thì giải quyết thế nào hả anh? Liệu có còn sử dụng phương pháp này được nữa ko?Trong trường hợp đó anh thấy người ta hay dùng smoothing bằng cách cộng 0.01 vào variance:trong đó  là hàm tính variance của các cột trong ma trận X (giống hàm var() trong matlab).Cách này sẽ đảm bảo  và em có thể chia bình thường. Nhớ là nếu dùng smoothing thì em phải cộng 0.01 vào variance của tất cả các cột, chứ không phải chỉ cộng vào những cột có .Đương nhiên em có thể dùng giá trị khác thay vì 0.01, nhưng không nên chọn giá trị nhỏ quá hoặc lớn quá.Em cảm ơn anh rất nhiều vì vừa có cách giải quyết thỏa đáng lại vừa nhanh, đây là vấn đề em đang gặp thực tế ạ.chào anh. bài viet cua a thuc su rat bo ich.\nem muốn hỏi anh 1 vấn đề ạ: em dùng thuật toán PCA để phát hiện khuôn mặt thì cái ngưỡng thường dùng để quyêt định xem đó có phải là khuôn mặt hay không thường được chọn như thế nào ạ?? e ko hiểu chỗ đó.mong a giải đáps<α thì H là bức ảnh khuôn mặt ( do H đủ gần với không gian mặt). cai nguong a o day lấy giá trị như thế nào là hợp lí ??Hi em,\nCâu hỏi của em thuần tuý phụ thuộc vào mô hình máy học em đang dùng. Cụ thể anh không biết “s” được tính thế nào nên không thể trả lời chính xác được.Tuy nhiên anh đoán là em sử dụng thuật toán nearest neighborhood (s là khoảng cách Euclid hoặc đại loại vậy). Trong trường hợp đó thì em cần chọn  sao cho tỉ lệ nhận dạng trên tập validation set là cao nhất.Tuy nhiên em nên sử dụng các thuật toán mạnh hơn, chẳng hạn SVM.Em chào a ạ.\nEm nghe cô giáo em nói thì PCA làm giảm số chiều theo kiểu % chính xác. vậy em muốn kiểu như pca(X, %) thì em phải cài như thế nào ạ. em cảm ơn a ạNhư anh nói ở phần 2 bài này: https://phvu.net/2011/11/15/pca-intuition-maths/\nEm dựa vào “% chính xác” để chọn giá trị cho , trong đó  là số trị riêng lớn nhất còn giữ lại sau PCA.Chào anh.\nEm đang tìm hiểu về PCA. Anh cho em hỏi từ trị riêng và vector riêng có mối liên hệ nào với input (X) không? và nhận biết được thành phần nào quan trọng trong X?Chào Anh!\nEm cũng đang nghiên cứu về PCA, em có ví dụ cụ thể: các nhân tố: GPD, dân số, giới, tỷ lệ hộ nghèo, số trạm y tế, cơ sở hạ tầng,… trong các nhân tố đó Em có thể dùng phương pháp PCA để xác định nhân tố nào bị ảnh hưởng nhiều khi lũ về không Anh?\nEm rất vui khi nhận được hồi âm của Anh ^^. Chúc Anh thành công!%eigenvectors\n  eivec2 = zeros(size(eivec));\n  for i=1:size(eivec, 2)\n    eivec2(:, i) = eivec(:, size(eivec, 2) – i + 1);\n  end\n  eivec = eivec2;\n  display(‘Full eigenvectors:’);\n  display(eivec);anh cho em hỏi dòng lệnh trên có phải là sắp xếp eivec2 tăng dần không ạ? em đọc mà không hiểu lắm, nếu vậy em có thể thay bằng dòng eivec2 = sort(eivec,2); không anh?Gửi anh Vũ,Em là sinh viên năm cuối và đang làm đồ án về tìm hiểu phương pháp PCA để nhận biết nguồn gốc dầu. Từ hôm nhận đồ án em đã tìm kiếm tài liệu về PCA mà không tìm được đúng. Đến khi đọc được bài viết này của anh, em thực sự có động lực để làm tiếp đồ án cũng như em đã có cái nhìn tổng quát PCA là gì. Anh Vũ có thể cho em xin thêm tài liệu về PCA được không ạ? Em muốn tìm hiểu thêm về các bước giải quyết bài toán bằng PCA, từ tập hợp số liệu cho đến khi đưa vào không gian có số chiều ít hơn (làm thế nào để chuyển đổi số liệu từ không gian nhiều chiều đưa vào không gian có số chiều ít hơn) và đánh giá kết quả. Em rất mong nhận được hồi âm của anh! Chân thành cảm ơn anh!Anh có thể chỉ em cách làm PCA trên MiniTab 17 Không ạ 🙂em xem cái này chưa: http://support.minitab.com/en-us/minitab/17/topic-library/modeling-statistics/multivariate/principal-components-and-factor-analysis/perform-pca-with-varimax-rotation/#ad cho e hỏi dòng acc_eival = 0; hiểu là gì vậynó là accumulated eigen values.vâng ạ, a giải thích giúp em vòng lặp for ở dòng 30 được ko, em bối rối quáAnh cho em hỏi trong bước 2 xây dựng không gian mới,  ma trận hiệp phương sai cov(X,Y) sẽ được theo cách khác chứ không phải theo công thức (2), vì (2) thực chất là ma trận tự tương quan của các đặc trưng.\n– Trong phần code của anh, có đoạn\n% correlation matrix\n  V = Xhat’ * Xhat;\n  fprintf(1, ‘Correlation between features (columns) of normed data:’);\n  V\nnhư vậy tên bước với tính toán có vẻ không thống nhất a..\nRất mong anh giải đáp thắc mắc của em. Thank anh!Ah tại vì đối với các biến ngẫu nhiên đã chuẩn hoá (phương sai bằng 1) thì ma trận correlation chính là covariance.\nTrong bước 1 ta chuẩn hoá các đặc trưng bằng phương sai, nên correlation hay covariance cũng như nhau.Dạ vâng, em cảm ơn anh ạ. Chúc anh khỏeAnh ơi cho em hỏi 1 tí. Theo lý thuyết thì cái không gian mới là xắp xếp các vecto riêng theo trị riêng có độ lớn giảm dần. Và theo dòng code thì từ dòng 27 -> 46 là anh đang đảo ngược thứ tự các vecto riêng. Tức là dòng 23 khi xuất các vecto rieng nó đã theo thứ tứ tăng dần của các trị riêng sẵn, nên chỉ cần đảo ngược lại sẽ được giảm dần theo giống lý thuyết . Vậy cho em hỏi là vì sao khi xuất các trị riêng như trên, nó đã được sắp xếp theo tăng dần. Em test thử với 1 ma trận bất kỳ, thì em thấy trị riêng xuất ra không phải tăng dần. Nhưng nếu là trị riêng của tích (ma trận  và chuyển vị của nó) giống như là V= (X * chuyển vị X) như đoạn code ở trên, thì các trị riêng sẽ xuất hiện theo thứ tự tăng dần. Cho em hỏi là tại sao lại như v ạ, và các trị riêng khi tính = matlab thì sẽ xuất hiện theo thứ tự như thế nào ạCái này phụ thuộc vào cài đặt của matlab thôi em. Trong documentation của matlab không nói chắc chắn là kết quả của eig() đã được sort. Thông thường thứ tự của các eigenvalues trả về theo thứ tự tính được trong thuật toán.Nếu em muốn chắn chắn là các eigenvalues có thứ tự thì có thể dùng thêm hàm sort:dạ em cảm ơn anh. Vậy mình có thể thay đoạn code từ  dòng 27->36 = 3 dòng code trên của anh với [eival,I] = sort (diag(eival),’descend’); thì cũng được kết quả tương tự phải không anh. Tại em thấy đoạn for đó nó rối quáKhông, đoạn code từ 27->36 là để tính eigenInfo, chứa vài thông tin lặt vặt về các trị riêng để debug (và vì mục đích giáo dục 🙂 ) thôi, em không nhất thiết phải viết y như vây.dạ em cảm ơn anh, em tưởng anh đang xây dựng lại các vecto riêng theo thứ tự giảm dần trị riêng ^^. Em hiểu r ạ, cảm ơn vì những chia sẽ và sự nhiệt tình của anh, nó thật sự giúp em rất nhiều ^^Fill in your details below or click an icon to log in: You are commenting using your WordPress.com account. ( Log Out / Change ) You are commenting using your Twitter account. ( Log Out / Change ) You are commenting using your Facebook account. ( Log Out / Change ) You are commenting using your Google+ account. ( Log Out / Change )Connecting to %s Notify me of new comments via email. \n\nEnter your email address to subscribe to this blog and receive notifications of new posts by email.Join 144 other followers\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t",
          "relevence": "no"
        },
        {
          "url": "https://forum.uit.edu.vn/threads/55314-Help-Ve-LibSVM-tren-C-",
          "title": " [Help] Về LibSVM trên C#",
          "content": "Qui định về quản lý SPAM, vui lòng đọc kĩ trước khi post bài\r\n\r\nMọi thắc mắc về công tác sinh viên, học vụ (lịch thi, điểm thi, đăng ký môn, tổ chức lớp, thông tin liên lạc lớp trưởng/giảng viên v.v...) sinh viên sử dụng tài khoản là MSSV để trao đổi tại các box phù hợp trong Mục CỘNG ĐỒNG UIT \r\n\t\t\t\tXem đám mây từ khóa (Tổng hợp tât cả từ khóa)\r\n\t\t\tNội quy diễn đàn",
          "relevence": "no"
        },
        {
          "url": "https://apusvietnam.com/toi-da-den-voi-machine-learning-nhu-the-nao/",
          "title": "Tôi đã đến với Machine Learning như thế nào? | Apus Vietnam",
          "content": "Tôi đã đến với Machine Learning như thế nào, và một phần nguyên do tôi viết blog này.Tôi vốn có xuất thân từ Toán với 5 năm liền thi toán Quốc gia (cả cho học sinh cấp 3 và sinh viên đại học). Đầu năm thứ ba đại học (2009), thấy các bạn khác đã bắt đầu xin vào các lab trong khoa để làm nghiên cứu, tôi và mấy cậu bạn thân đi hỏi các thầy cô trong khoa và đều nhận được câu trả lời là: Nếu học tốt toán thì nên theo Xử lý ảnh. Chúng tôi tìm và được một thầy giao cho đề tài Nhận dạng biển số xe bằng SVM. Không biết một chút gì về tối ưu và Machine Learning, chúng tôi đã bỏ cuộc sau gần một năm vật lộn để hiểu thuật toán SVM (mà cuối cùng vẫn không hiểu, và sử dụng K-nearest neighbors với độ chính xác rất thấp).Sau đó, chúng tôi mỗi người một hướng khác nhau, tôi được một thầy làm về Hệ thống nhúng và Tính toán khả cấu hình (Embedded System and Reconfigurable Computing) nhận vào lab cuối năm 2010. Thầy rất tốt, hướng dẫn sinh viên rất tận tình (tôi khó có thể được đi học tiếp nếu không có sự giúp đỡ của thầy). Tôi chuyển hướng sang làm FPGA và cố gắng hướng đến các ứng dụng có Xử lý ảnh, video – mặc dù vẫn không biết tại sao mình thích xử lý ảnh sau bao nhiêu lần thất bại ở project kia. Đề tài tốt nghiệp đại học năm 2012 của tôi là thiết kế một IP Camera trên FPGA. Tôi và một cậu bạn vất vả mấy tháng trời mà mới có kết quả tạm chấp nhận được lúc tốt nghiệp. Chúng tôi thường tới thư viện Tạ Quang Bửu (lab tôi ở đó) lúc 8h sáng và về nhà lúc 10h tối. Mùa hè năm ấy rất nóng và KTX thường xuyên mất nước. Hai ba ngày không tắm rửa là chuyện thường. Nhưng đó là chuyện nhỏ, chuyện lớn và tôi làm cháy vài cái mạch, khi sắp đến ngày bảo vệ mà vẫn chưa có kết quả gì, trong khi bạn cùng lớp đã viết xong luận văn cách đó mấy tuần.Cuối cùng, tất nhiên chúng tôi thành công (thì tôi mới tốt nghiệp được). Tôi có làm thêm một vài đề tài khác với thầy về FPGA, cũng thành công nhưng tôi gần như không rút ra được nhiều kinh nghiệm trong những đề tài đó. Mỗi khi bắt đầu một đề tài mới, tôi lại gặp khó khăn y như những lần trước.Tôi nhận ra rằng tôi không khéo léo và không có duyên với những thứ tỉ mỉ như thiết kế mạch điện.Cả 5 năm đại học tôi cố gắng tìm lại chính mình như hồi cấp ba. Tôi không làm được việc đó.Tôi không sợ khó, không sợ khổ, chỉ sợ nhất phải làm những thứ không đúng đam mê. Tôi đã làm nhiều thứ, và cũng thành công ở nhiều thứ. Nhưng tôi chỉ hứng thú với những thứ khi làm nhiều thì kinh nghiệm của mình tăng lên, mình có thể tự tin hơn khi nói chuyện với những người trong nghề.Tôi cũng nhận ra rằng tôi chỉ đam mê với những thứ tôi biết rõ nguyên do!Lúc đó thứ duy nhất tôi có thể nghĩ ra là Xử lý ảnh, ít nhất là vì nó có nhiều toán. Tôi cố gắng apply đi học về ngành Xử lý ảnh, lúc đấy vẫn chưa thật rõ về Machine Learning. Nếu ở lại Việt Nam và đi làm ngay, tôi biết chắc chắn rằng tôi sẽ khó thành công! Thật may mắn, tôi được thầy hiện giờ nhận (thầy duy nhất trả lời tích cực trong rất nhiều thầy tôi gửi email, và Penn State cũng là trường duy nhất nhận tôi).Trong kỳ đầu ở đây, cuối năm 2013, tôi phải học ba lớp: Linear Algebra, Probabilities and Random Processes, và 1 lớp tiếng Anh. Tôi cũng phải tập trung ôn tập cho kỳ thi quan trọng đầu tiên của PhD, Candidacy exam, vào đầu kỳ thứ hai. Hai môn học chuyên ngành kia là hai môn quan trọng nhất trong cả PhD của tôi. Trong thời gian này, tôi cũng thường xuyên được nghe đến Convex Optimization và Machine Learning từ các anh trong lab. Ai ai cũng nói về những thứ đó mà tôi không biết gì. Vậy là sau khi qua kỳ Candidacy và trở thành PhD candidate, tôi tự học hai khoá miễn phí trên mạng: Machine Learning của Andrew Ng và Convex Optimization của Stephen Boyd. Tôi học được rất nhiều thứ từ hai khoá này. Cùng kỳ đó, tôi cũng được học môn Pattern Recognition từ giáo sư yêu thích nhất của tôi trong khoa. Chỗ này hơi khoe một tí, tôi đạt điểm cao nhất trong kỳ thi Candidacy của khoa và là người duy nhất đạt điểm A trong môn Pattern Recognition kỳ đó. (Và bây giờ khoa tôi đã biết rằng ở châu Á còn có Việt Nam nữa 😀)Lúc này thì tôi đã tự tin hơn rất nhiều. Quan trong hơn, cái đam mê học hỏi và làm việc như hồi cấp ba của tôi dần quay trở lại.Trong vài năm học PhD, tôi nghe quá nhiều về Deep Learning và những thành tựu không tưởng của nó, nhưng tôi vẫn cố tránh vì sợ. Rồi đến cuối 2016, tôi nhận ra rằng mình không thể trốn trành nó được nếu muốn ở lại lâu dài với Machine Learning. Tôi muốn rằng đề tài thứ ba, cũng là đề tài cuối cùng trong PhD của tôi, phải là Deep Learning. Tôi đọc khá nhiều bài báo về Deep Learning cho bài toán mà tôi thích và chợt nhận ra rằng Deep Learning cũng được xây dựng từ những thứ rất cơ bản, và có rất nhiều những thứ cơ bản về Machine Learning mà trước đây tôi tưởng rằng mình rõ, thực ra tôi lại rất mơ hồ.Tôi quyết định hệ thống lại kiến thức về Machine Learning của mình vào sáng sớm ngày 25/12/2016, khi trằn trọc không ngủ được vì đêm hôm trước uống hơi nhiều. Và để cho hấp dẫn hơn, tôi sẽ viết nó dưới dạng một blog, phải đến khi viết ra và chia sẻ với người khác thì mình mới hiểu sâu được nó.Cách học tập tốt nhất là chia sẻ kiến thức. Kiến thức là thứ mà khi cho đi, chúng ta sẽ thu được nhiều hơn.Thời gian đó là kỳ nghỉ đông nên tôi có nhiều thời gian cho việc viết và tạo dựng blog. Trong 4 ngày đầu, tôi xây dựng được bộ khung cho blog và viết liền được 3 bài để ra mắt.Những comments tích cực và số lượng like tăng vọt trong những ngày đầu như một liều thuốc mê kích thích tôi viết cực nhiều trong tháng đầu tiên (7 bài) và thậm chí bỏ bê việc nghiên cứu chính. Dần dần tôi bố trí thời gian hợp lý hơn và ra 1 bài/tuần như bây giờ. (Trước đó tôi lập trình Python rất ít, chủ yếu là Matlab, nhưng rồi vừa làm vừa học, giờ cũng khá hơn rồi.)Qua những thảo luận và những post trên page facebook, tôi tự mình học được ra rất nhiều điều. Có những niềm vui mới, có những mối quan hệ mới, có những cơ hội mới mở ra, hầu hết nằm ngoài kỳ vọng của tôi.Khi tìm được đam mê và niềm vui, tôi thường làm việc rất năng suất. Tôi không dám hứa trước, những sẽ cố gắng hết mình để hết năm nay có thể viết được những gì cần thiết cho các bạn bước chân vào Machine Learning.Niềm vui của các bạn cũng là niềm hạnh phúc của tôi. (Hơi chém tí, nhưng cũng phần nào đúng đấy)Cảm ơn các bạn đã đồng hành cùng tôi ôn tập lại kiến thức về Machine Learning.Nguồn: machinelearningbasicvn",
          "relevence": "no"
        },
        {
          "url": "https://vn.answers.yahoo.com/question/index?qid=20090704110054AAmwpKh",
          "title": "Thuật toán đơn giản và hiệu quả để detech ký tự từ ảnh? | Yahoo Hỏi & Đáp",
          "content": "Thêm bình luậnThêm bình luậnTán gẫu hoặc huênh hoang, nội dung người lớn, thư rác, xúc phạm thành viên khác,hiển thị thêmGây hại cho trẻ vị thành niên, bạo lực hoặc đe dọa, quấy rối hoặc xâm phạm quyền riêng tư, mạo danh hoặc xuyên tạc, gian lận hoặc lừa đảo. hiển thị thêmNếu bạn cho rằng quyền sở hữu trí tuệ của bạn đã bị vi phạm và muốn khiếu nại, vui lòng xem  Chính sách Bản quyền/Quyền sở hữu Trí tuệ  của chúng tôiTán gẫu hoặc huênh hoang, nội dung người lớn, thư rác, xúc phạm thành viên khác,hiển thị thêmGây hại cho trẻ vị thành niên, bạo lực hoặc đe dọa, quấy rối hoặc xâm phạm quyền riêng tư, mạo danh hoặc xuyên tạc, gian lận hoặc lừa đảo. hiển thị thêmNếu bạn cho rằng quyền sở hữu trí tuệ của bạn đã bị vi phạm và muốn khiếu nại, vui lòng xem  Chính sách Bản quyền/Quyền sở hữu Trí tuệ  của chúng tôiTán gẫu hoặc huênh hoang, nội dung người lớn, thư rác, xúc phạm thành viên khác,hiển thị thêmGây hại cho trẻ vị thành niên, bạo lực hoặc đe dọa, quấy rối hoặc xâm phạm quyền riêng tư, mạo danh hoặc xuyên tạc, gian lận hoặc lừa đảo. hiển thị thêmNếu bạn cho rằng quyền sở hữu trí tuệ của bạn đã bị vi phạm và muốn khiếu nại, vui lòng xem  Chính sách Bản quyền/Quyền sở hữu Trí tuệ  của chúng tôi",
          "relevence": "no"
        },
        {
          "url": "http://diendan.congdongcviet.com/threads/t199628::svm-trong-opencv-cho-ios-co-ho-tro-khong.cpp",
          "title": "SVM trong OpenCV cho iOS có hỗ trợ không?",
          "content": "Chào các bạn,\r\nMình hiện tại đang định thực hiện 1 project về phân loại văn bản và sử dụng thuật toán SVM để làm. Nếu như thực hiên bằng c# thì ko có vấn đề gì. Nhưng khi mình làm trên IOS thì không có thư viện hỗ trợ. Theo mình được biết thì trong opencv có SVM. Nhưng không biết Opencv cho IOS có hỗ trợ không. Pro nào biết về mãng này xin chỉ giáo với. Thank.Chắc nhờ bụt xuống giúp thui.help help me.\r\n\t\t\t\t\r\n\t\t\t\t\tĐã được chỉnh sửa lần cuối bởi hoathuongphuoc : 20-11-2013 lúc 11:08 PM.\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t",
          "relevence": "no"
        }
      ]
    }
  ]
}